---
title: "Técnicas Multivariadas: Introducción"
author: "M. en C. Arturo Bell Enríquez García"
output: 
  distill::distill_article:
    toc: true
    toc_float: true
    code_download: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(R.options = list(width = 80))
```

**[VIDEO](https://youtu.be/0y_3iX8oH5c)**

## Matrices de correlación y covarianzas:

Para obtenerlas utilizaremos las mismas funciones que para el cálculo individual (y por consiguiente podremos calcular también la $\rho$ de Spearman). Utilicemos como ejemplo la base de datos `mtcars`:

```{r}
head(mtcars)
```

Ahora estimemos ambas matrices:

```{r}
# Matriz de covarianzas
cov.mat <- cov(mtcars)

# Matriz de correlación
cor.mat <- cor(mtcars)

# Las imprimimos en pantalla
cov.mat
cor.mat
```

Veámos la matriz de correlaciones gráficamente:

```{r}
library(corrplot)
corrplot(cor.mat, method = "ellipse", type = "upper")
```

Ahora utilicemos una función que, en un solo paso, computará la matriz de correlación, realizará una prueba de significancia para cada una y además nos presentará la matriz de manera gráfica:

```{r}
# Descarga la función desde esa url y la carga en memoria
source("http://www.sthda.com/upload/rquery_cormat.r")
# Para fines prácticos se extrae un subconjunto de las columnas
mydata <- mtcars[, c(1,3,4,5,6,7)] 
# Se aplica la función
rquery.cormat(mydata)
```

Otra alternativa es utilizar la función `chart.Correlation(data, histogram)` de la librería `PerformanceAnalytics`, en la cual se muestran todos resultados en una misma gráfica:

```{r}
library(PerformanceAnalytics)
chart.Correlation(mydata, histogram = T, pch = 19)
```

Un comentario final al respecto de estas matrices es que, además de ser la base de las técnicas multivariadas, nos permiten evaluar la asociación entre nuestras variables sin comprometer un modelo predictivo, a la vez que nos permitirán hacer un filtrado de nuestras variables para evitar autocorrelaciones o incluir variables poco informativas, aunque de esto hablaremos más adelante.

## Normalidad multivariada

Para analizar si nuestros datos multivariados se ajustan a una distribución normal multivariada utilizaremos la librería `MVN`, cuya función `mvn(data, mvnTest, multivariatePlot)` nos permite realizar una serie de pruebas tanto multi como univariadas:

### Prueba de Mardia (1970)
Consiste en analizar si los momentos de la distribución Mv de los datos difieren de los esperados de una distribución Normal Mv ([Mardia 1970](http://bit.ly/mardia_test)), las hipótesis nulas son:

1. $H_0: S_{obs} = S_{NMv_{µ, \Sigma}}$
2. $H_0: K_{obs} = K_{NMv_{µ, \Sigma}}$

Donde $\mu$ representa el vector de medias de cada variable y $\Sigma$ la matriz de covarianzas.

Al realizar esta prueba vemos que la distribución Mv observada tiene una curtosis adecuada, aunque se encuentra fuertemente sesgada. Si analizamos los datos univariados vemos que en apariencia 2/6 variables presentan normalidad univariada.

```{r}
library(MVN)
mvntest <- mvn(mydata, mvnTest = "mardia")
mardia.test <- mvntest$multivariateNormality
unitest <- mvntest$univariateNormality

mardia.test
unitest
```

### Prueba de Henze-Zirkler

Si observamos con atención los valores de p, veremos que hay algunos que se encuentran cercanos al umbral de 0.05 por lo que un análisis a partir de los cuantiles de la distribución puede ser una alternativa más informativa. Para ello podemos utilizar la prueba de Henze-Zirkler. Esta prueba considera una hipótesis compuesta, en la cual *la distribución de X es una distribución normal no degenerada*, cuyos resultados son consistentes contra cualquier distribución alternativa no normal (por ello compuesta). La representación está dada en términos de $L^2$ (Distancia de Mahalanobis, más adelante hablaremos sobre medidas de distancia). Al ser una prueba de bondad de ajuste (observado vs. esperado), el estadístico de prueba sigue una distribución $\chi^2$ ([Henze y Zirkler 2007](http://bit.ly/Henze_Zirkler)).

Aplicandola con `mvn()` vemos que también sugiere una falta de normalidad, lo cual podemos comprobar al ver el gráfico Cuantil-Cuantil
```{r}
hz.test <- mvn(mydata, mvnTest = "hz",
               multivariatePlot = "qq")$multivariateNormality
hz.test
```

Ese gráfico, aunque informativo, puede trabajarse para hacerse más agradable a la vista utilizando ggplot2 utilizando la siguiente función personalizada creada a partir del código utilizado para la gráfica anterior:

```{r}
# Funciones personalizadas:

# Extraida de la función mvn(multivariatePlot = "qq") para un gráfico QQ
# para normalidad multivariada utilizando la distancia de mahalanobis

ji2.plot <- function(df){
  # Datos
  n <- dim(mydata)[1] # número de datos
  p <- dim(mydata)[2] # número de grupos
  # Centramos los datos (a-µ(a)) sin escalarlos (sin dividir por su \sigma)
  dif <- scale(mydata, scale = F)
  # Cálculo de la distancia de mahalanobis^2
  d <- diag(dif %*% solve(cov(mydata), tol = 1e-25) %*% t(dif)) 
  r <- rank(d) # Asignación de rangos a las distancias
  # Obtención de los cuantiles teóricos según la distribución ji^2
  ji2 <-  qchisq((r - 0.5)/n, p)
  # Reunimos los objetos en un data.frame para graficar con ggplot2
  ji2.plot.data <- data.frame(d, ji2)
  
  # Graficado
  library(ggplot2)
  ji2.qq <- ggplot(data = ji2.plot.data, aes(x = d, y = ji2)) + 
            geom_point(colour = "deepskyblue4", alpha = 0.5, size = 4) + 
            geom_abline(slope = 1,
                        colour = rgb(118,78,144,
                                     maxColorValue = 255),
                        size = 1) +
            labs(title = 
                   bquote("Gráfico QQ de" ~~ {chi^2} ~~ ", g.l = "~ .(p)),
                 subtitle = bquote("Distancia de Mahalanobis ("~{D^2}~") 
                                   vs. Cuantiles teóricos"),
                 x = element_blank(),
                 y = element_blank(),
                 caption = "Basado en mvn(..., multivariatePlot == \"qq\")"
                 )

  return(ji2.qq)
}

# Tema personalizado
blank_theme <- function(){
  theme(panel.grid.minor = element_blank(),
        panel.grid.major = element_blank(),
        panel.background = element_blank(),
        axis.line = element_blank(),
        aspect.ratio = 1/1.61,
        axis.ticks = element_blank(),
        text = element_text(colour = "gray50"),
        legend.position = "none"
        )
}
```

```{r}
qqjiplot <- ji2.plot(mydata) + blank_theme()
qqjiplot
```

### Prueba de Royston

Esta prueba es una extensión multivariada de la prueba por excelencia para la normalidad univariada: la prueba de Shapiro-Wilk's. Originalmente propuesta en 1983, aunque fue corregida/ampliada por el mismo autor en 1992 ([Royston vs. otras](http://bit.ly/Royston_otros)). Funciona mejor para muestras pequeñas, aunque no se recomienda emplearla con menos de 3 observaciones o con más de 2000.

Su implementación sigue la misma línea que los casos anteriores. Si analizamos el valor de p, veremos que se encuentra en el límite de la significancia a un $\alpha = 0.05$; sin embargo, si consideramos también el gráfico QQ que elaboramos anteriormente, no podemos asumir que esas desviaciones sean despreciables.

```{r}
royston.test <- mvn(mydata, mvnTest = "royston")$multivariateNormality
royston.test
```

## Igualdad de dispersiones multivariadas

Conforme vayamos avanzando en el curso veremos que, entre los supuestos de algunas pruebas, vamos a encontrar el de "igualdad de dispersiones multivariadas"; *i.e.*, igualdad de matrices de covarianza. Una alternativa es utilizar la prueba de [Anderson (2006)](http://bit.ly/Homo_mvdisp), la cual es un análogo multivariado a la prueba de Levene para la homogeneidad de varianzas. La prueba de hipótesis está basada en distancias no euclidianas entre los grupos (*i.e.*, no utiliza el teorema de Pitágoras :( ). Un dato curioso es que este método también se ha utilizado para evaluar la diversidad $\beta$ de una comunidad.

Para su implementación en R utilizaremos la función `betadisper` de la librería `vegan`. Al ser un método basado en distancias, primero habrá que transformar los datos a una matriz de distancias utilizando alguna de las funciones `dist`, `betadiver` o `vegdist`. Necesitamos, además, establecer los grupos a utilizar.

```{r}
library(vegan)
dist.mat <- vegdist(mydata, method = "bray", type = c("median"))
groups <- as.character(mtcars$cyl)

# Por fines prácticos no se muestra,
# ya que calcula una distancia entre cada par de instancias o grupos,
# resultando en matrices sumamente grandes

#dist.mat
```

Ahora utilizaremos la función `betadisper` para comprobar la homogeneidad de dispersiones entre los distintos cilindros. Esta prueba únicamente genera el espacio multivariado para realizar la prueba, realizando un ACP para reducir la dimensionalidad de la base de datos y estimar las distancias a la mediana de cada uno de los grupos establecidos.

```{r}
# Realizar el procedimiento
disp.mv <- betadisper(dist.mat,
                      group = groups, type = "median")
disp.mv
```

Si realizamos la prueba de hipótesis vemos que, al parecer, las dispersiones multiviariadas son similares entre los 3 grupos.

```{r}
# Prueba de hipótesis
anova(disp.mv)
```

Ahora veamos las dispersiones gráficamente:

```{r}
# Análisis gráfico
plot(disp.mv, ellipse = T, hull = F)
```

Al analizar el gráfico vemos que las dispersiones son similares; sin embargo, pareciera que la dispersión del grupo 6 es más pequeña que la de los grupos 4 y 8, en tonces realicemos las comparaciones pareadas univariadas con la prueba Honesta de Diferencias Significativas de Tukey (`TukeyHSD`). Los resultados sugieren que las dispersiones con un $\alpha = 0.05$ son similares, lo cual a su vez pudiera sugerir que hay un equilibrio entre las dispersiones en el eje x con respecto a las dispersiones en el eje y.

```{r}
mod.HSD <- TukeyHSD(disp.mv)
mod.HSD <- data.frame(mod.HSD$group, comp = dimnames(mod.HSD$group)[[1]])
mod.HSD
```

Al igual que en el caso anterior podemos utilizar ggplot para personalizar el gráfico:

```{r}
hsd.plot <- ggplot(data = mod.HSD, 
                   aes(x = comp)) + 
            geom_point(aes(y = diff), 
                       colour = "deepskyblue4", 
                       size = 4, 
                       alpha = 0.7)+ 
            geom_errorbar(aes(ymin = lwr, ymax = upr), 
                          colour = "deepskyblue4") + 
            blank_theme() + 
            labs(title = "Diferencias en dispersión multivariada e IC",
                 subtitle = "Prueba HSD de Tukey",
                 x = "Grupos",
                 y = element_blank()) +
            scale_y_continuous(breaks = NULL) + 
            geom_hline(yintercept = 0,
                       colour = rgb(118,78,144, maxColorValue = 255),
                       linetype = "dashed") +
            annotate("text",
                     x = 0.5, y = 0+0.005, 
                     label = as.character(0),
                     colour = rgb(118,78,144, maxColorValue = 255)
                     ) +
            geom_hline(yintercept = max(mod.HSD$upr),
                       colour = "firebrick", alpha = 0.7,
                       linetype = "dashed") +
            annotate("text",
                     x = 0.5, y = max(mod.HSD$upr)-0.005, 
                     label = as.character(round(max(mod.HSD$upr),2)),
                     colour = "firebrick"
                     ) +
            geom_hline(yintercept = min(mod.HSD$lwr),
                       colour = "firebrick", alpha = 0.7,
                       linetype = "dashed") +
            annotate("text",
                     x = 0.5, y = min(mod.HSD$lwr)-0.005, 
                     label = as.character(round(min(mod.HSD$lwr),2)),
                     colour = "firebrick"
                     ) +
            geom_text(aes(label = paste("p = ", round(p.adj,2)), y = 0),
                       stat = "identity",
                       nudge_y = max(mod.HSD$upr)+0.005, colour = "gray50")
            
hsd.plot
```

## Implicaciones analíticas de la multidimensionalidad

Es importante mencionar que entre más incrementemos la dimensionalidad de nuestro problema más difícil será resumir en un solo resultado las pruebas de nuestros análisis y, en consecuencia, deberemos de considerar distintas técnicas/estrategias que analicen nuestros datos desde distintas perspectivas antes de emitir un juicio o extraer conclusiones. Por esta razón, es sumamente importante que realicemos una selección de variables de manera rigurosa antes de comenzar nuestro análisis, ya que incluir variables innecesariamente únicamente incrementará la varianza de los datos sin aportarnos ninguna información adicional, causando desviaciones de la normalidad, modelos complejos sobre o infra ajustados y pérdidas de poder estadístico. 

En cuanto a la normalidad, hay un par de consideraciones a tener en cuenta. La primera es que si nuestro análisis está basado en la matriz de correlación estaremos cumpliendo el supuesto de normalidad de la técnica que estemos aplicando (de tenerlo), ya que, explícitamente ajustamos los datos a una distribución normal. Por otra parte, es importante mencionar que si nuestros datos en realidad no se ajustan o se encuentran fuertemente desviados de la normalidad, las conclusiones que extraigamos serán únicamente sobre la tendencia más general de nuestros datos (Revisar: desigualdad/teorema de Chebyshev) y, en consecuencia, pueden no ser una representación completa de nuestras muestras. Si esto es importante o no, dependerá de nuesta pregunta de investigación y qué tan fino querramos que sea el análisis.

## Transformaciones o deformaciones

Habiendo tocado el tema de la estandarización, hablemos también del resto de transformaciones. En general, podemos considerar que existen dos tipos de transformaciones:
1. Aquellas que afectan la distribución de los datos (logarítmica)
2. Aquellas que simplemente cambian los límites de la distribución original (MinMax)

¿Cuál utilizar? Dependerá de nuestros objetivos para hacerla, lo cual me lleva al punto de que: NINGUNA transformación debe de ser aplicada sin cuidado. Hay que tener en cuenta que aunque no se cambie la distribución de los datos, el análisis ya no se realiza sobre los datos originales, lo cual puede causar errores de interpretación. Con esto no quiero decir que las transformaciones sean malas, solo que hay que emplearlas con una justificación y asegurarnos de re-transformar los datos antes de hacer inferencias.

Veamos algunas de las transformaciones más comunes y cuáles son sus consecuencias en los datos. Para ello, consideremos estos datos sin transformar. Vemos que están altamente sesgados y en consecuencia bastante alejados de la normalidad:

```{r}
# Función de graficado
kdeplots <- function(data, aes){
  kdeplots <- ggplot(data, aes) + 
            geom_density() + 
            blank_theme() +
            labs(title = "Conteo de peces",
                 subtitle =
                   "Gráfico de densidad con datos y distrintas transformaciones",
                 caption = "Datos: http://bit.ly/comm_transf",
                 x = element_blank(),
                 y = element_blank()) +
            scale_y_continuous(breaks = NULL)
  return(kdeplots)
}

```

```{r}
df <- data.frame(datos = c(38, 1, 13, 2, 13, 20, 150, 9, 28, 6, 4, 43),
                 transf = "originales")

kde.plots <- kdeplots(df, aes(datos, color = transf))
kde.plots
```

### Estandarización

Veamos el efecto de estandarizar los datos; es decir, utilizar la distribución Z. El resultado es la misma distribución, aunque los datos ahora se encuentran escalados en el intervalo [-3,3] indicando a cuantas SD de la media se encuentra cada punto.

```{r}
df2 <- rbind(df, data.frame(datos = (df$datos-mean(df$datos))/sd(df$datos),
                            transf = "Z"))
kde.plots <- kdeplots(df2, aes(datos, color = transf)) + 
             facet_wrap(~transf, scales = "free")
kde.plots
```


### Transformación logarítmica:

Posiblemente la transformación más conocida, utilizada y, en consecuencia, abusada.Se realiza aplicando la ecuación $X_{log} = log_n(X)$. (OJO: si hay ceros será $X_{log} = log_n(X+1)$, ya que el logaritmo de 0 no existe). Veamos qué le pasa a la distribución al aplicarla. La distribución cambió notablemente, ahora se encuentra mucho más cerca de una forma de campana. ¿Cuándo aplicarla? Cuando querramos forzar nuestros datos a una distribución normal para cumplir con los supuestos de alguna prueba paramétrica, linealizar los datos y, equivocadamente, ponerlos en la misma escala que otra variable. Con excepción del último caso, cualquiera de las formas está matemáticamente justificada, solo hay que tener en consideración que los datos no son los originales y que pueden no representar adecuadamente nuestro muestreo. Otro caso en el cual es válido utilizarlo es si queremos ver cuál es cuando tenemos distintos factores y nuestra variable de respuesta es el resultado de su interacción (producto) tal que $Y = a \times b \times c \times d\times ... \times z$ (efecto multiplicativo y no aditivo), ya que el resultado es una distribución con forma log-normal que no es posible capturar con los datos originales. Su re-transformación es $n^{X_{log}}$. Salvo en el último caso, recomiendo contrastar los resultados con una prueba no paramétrica utilizando los datos originales y ver cuáles son las diferencias.

```{r}
df2 <- rbind(df2, data.frame(datos = log(df$datos), transf = "log(x)"))
kde.plots <- kdeplots(df2, aes(datos, color = transf)) +
             facet_wrap(~transf, scales = "free")
kde.plots
```

### Raíz cuadrada

Otra transformación muy empleada, consiste en en obtener la raíz cuadrada de cada uno de los datos ($\sqrt{X}$, OJO: si hay valores negativos no se puede utilizar, hay que pasarlos a valores absolutos o añadir una constante para volverlos positivos). Veamos su efecto en la distribución. En este caso el cambio en la forma no es tan agresivo, y la consecuencia es únicamente que las diferencias entre los valores más altos y los más pequeños se redujo. Su re-transformación es: $\sqrt{x}^2$. Su uso más común es con datos de conteo (abundancias, bacterias en una caja petri, etc.)

```{r}
df2 <- rbind(df2, data.frame(datos = sqrt(df$datos), transf = "sqrt(x)"))
kde.plots <- kdeplots(df2, aes(datos, color = transf)) +
             facet_wrap(~transf, scales = "free")
kde.plots
```

