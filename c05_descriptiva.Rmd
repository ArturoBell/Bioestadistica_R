---
title: "Estadística Descriptiva"
author: "M. en C. Arturo Bell Enríquez García"
output: 
  distill::distill_article:
    toc: true
    toc_float: true
    code_download: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(R.options = list(width = 80))
```

## Librerías

```{r}
library(ggplot2)
```


## Descripciones

En la sesión de visualización hablamos de que hay una gran cantidad de formas en las cuales podemos describir el mundo que nos rodea. Una imágen habla más que mil palabras, y es una forma de resumir nuestros datos; sin embargo, está lejos de ser la única y, de hecho, por lo general buscamos que el gráfico acompañe a un resumen numérico, en forma de un solo valor o índice. A los seres humanos nos gusta la simpleza, después de todo. Bueno, de eso es de lo que vamos a hablar hoy.

Estoy seguro de que los conceptos que revisaremos en esta sesión los has revisado ya con anterioridad, por lo que no te haré el cuento largo, sino que me enfocaré más en las peculiaridades de cada una de las medidas, en qué escenarios son útiles, cuándo no lo son tanto y lo enlazaremos con temas que veremos más adelante. Esto con el objetivo de dedicar más tiempo a algo que merece más atención que recordar por cienmillonésima vez qué es un promedio: las distribuciones de probabilidad y sus usos.

## Medidas de tendencia central

Comencemos hablando con lo que, por lo general, es nuestro mayor interés: hacia dónde tienden nuestros datos. Para ello tenemos algunas **medidas de tendencia central**; es decir, literalmente describimos nuestros datos a partir de dónde se acumulan más.

### Media

La primera de estas medidas es, posiblemente, la más común de todas: **la media** o el promedio. En su forma más simple; es decir, la media **aritmética** la obtenemos sumando todos nuestros datos y dividiéndolos entre su número:

$$
\bar{x} = \frac{\sum_{i = 1}^n x_i}{n}
$$

Esto es conocimiento general, y es algo con lo que todos los estudiantes somos torturados eventualmente. Bueno, más que recordar cómo calcularlo e implementarlo en R (`mean(x)`, donde `x` es un vector de observaciones), pensemos en qué representa. La media es, literalmente, un indicador de hacia qué valor se están acumulando nuestros datos, tal y como si colgáramos cosas en un tendedero (házme un favor y piensa en que el siguiente gráfico está invertido verticalmente):

```{r}
set.seed(0)
x <- data.frame(x = rnorm(100, mean = 0))

plot1 <- ggplot(data = x, aes(x = x)) +
         geom_density(color = "dodgerblue4") +
         theme_bw() +
         geom_vline(xintercept = mean(x$x), color = "dodgerblue4")
plot1
```

En este caso, la mayor parte de nuestros datos están acumulados alrededor de 0. ¿Qué crees que pase si añadimos otros 20 datos, esta vez acumulados en 5? Veámos el cambio:

```{r}
set.seed(0)
x2 <- rbind(x, data.frame(x = rnorm(20, mean = 5)))
plot2 <- plot1 +  geom_density(aes(x = x), data = x2, color = "firebrick")
plot2 + geom_vline(xintercept = mean(x2$x), color = "firebrick")
```

Como era de esperarse, la media se "jaló" a la derecha, y de manera bastante notable. Esos 5 valores extremos tuvieron un peso bastante importante. Literalmente fue como si hubiéramos colgado cinco cosas más en nuestro tendedero, pero alejadas de la ropa que colgamos en un inicio. Aunque la media es una manera muy efectiva de resumir nuestros datos, esto solo es cierto si estos se parecen a una distribución normal (en la sesión de técnicas paramétricas hablaremos duro y tendido con respecto a esto), pero si no, como en nuestro caso con los nuevos valores, es necesario buscar una alternativa.

#### Media ponderada

Una de ellas es una modificación a la media, en la cuál cada valor tiene su propia ponderación o su propia importancia. Esa es la media con la que más padecemos los estudiantes de secundaria hacia arriba, pues el examen tiene una ponderación distinta a las tareas, por ejemplo. En mi caso personal, era un tormento cuando se ponderaba más las tareas que el examen, pero eso es otra historia. ¿Por qué es importante? Porque podemos utilizarla para "regresar" nuestra media a su lugar; de hecho, esta es la base fundamental detrás de las regresiones robustas, en donde el peso de cada observación disminuye según su distancia de 0:

```{r}
# Pesos
w <- max(abs(0-x2$x))/abs(0 - x2$x)
wmean <- weighted.mean(x2$x, w)
plot2 + geom_vline(xintercept = wmean, color = "firebrick")
```

#### Media geométrica

Esta es menos conocida, y representa el promedio de porcentajes, razones o **tasas de crecimiento**. Se expresa como la raiz n-ésima del producto de los n valores: $MG = \sqrt[n]{\Pi_i^nx_i}$. Pensemos en que estimamos la tasa de crecimiento poblacional ($\lambda$) anual de ballenas jorobadas en tres años seguidos, a partir de un modelo de marca-recaptura, y los valores que obtuvimos fueron 1.03, 0.98, 1.4, 0.94:

```{r}
lamb <- c(1.03, 0.98, 1.4, 0.94)
prod(lamb)^(1/length(lamb))
```

Es decir, el crecimiento poblacional promedio fue del 7%.

Otra manera de calcularla es:

```{r}
exp(mean(log(lamb)))
```

¿Te animas a encontrar la igualdad matemática?

### Mediana

Una alternativa más es la mediana. A diferencia de la media, que "busca" hacia donde se están acumulando los datos, la mediana nos indica exactamente que valor se encuentra en el **centro** de nuestra base de datos. Si partimos nuestra base de datos en 100 partes iguales (100%), cada parte representa un **cuantil** (1%). Cada diez cuantiles tenemos un **decil**, cada 25 cuantiles tenemos un **cuartil** (cuartiles 25%, 50% y 75%), y en el cuartil 50 tenemos la **mediana**. Esta es, entonces, mucho menos sensible a valores extremos:

```{r}
plot2 + geom_vline(xintercept = median(x2$x), color = "firebrick")
```

La estimación no es exactamente la misma que la media de los datos originales o de la media ponderada según su distancia a 0; sin embargo, el efecto es notablemente menor que con la media tradicional. Esta resistencia a valores extremos es lo que hace que las técnicas **no paramétricas** estén basadas en la mediana, en vez de la media.


### Moda

La moda corresponde al valor que más se repite en un conjunto de datos. Con datos continuos en el sentido estricto no existe; sin embargo, en muchos casos sí que podemos tener repetidos dependiendo de la escala y la precisión de nuestro instrumento. Otra manera de calcularla es discretizando nuestros datos y encontrar el intervalo más frecuente. Una propiedad interesante de la moda es que su valor corresponde con el valor que tiene la mayor probabilidad dentro de la distribución, por lo que puede ser útil en ciertos casos de Inferencia Bayesiana. Para calcularla podemos utilizar la función `Mode(x)` de la librería `DescTools`:

```{r}
letmode <- DescTools::Mode(c("a", "a", "b", "c", "d"))
letmode
```

A lo largo de este curso no aplicaremos la moda, solo la agregué para que la tengas presente.

## Medidas de dispersión

Al igual que en el caso anterior, repasaremos rápidamente las medidas de dispersión, con el objetivo de explorar la intuición detrás de ellas y su relación con otros conceptos que revisaremos más adelante. Independientemente de cuál utilicemos, todas las medidas de dispersión indican justamente eso, qué tan grande es la variabilidad de una distribución, ya sea de nuestros datos o la distribución muestral del parámetro que estemos estimando.

### Desviación estándar y Varianza

En pocas palabras, la varianza es una medida de la dispersión promedio de los datos; es decir, cuál es **el área** promedio que abarca la dispersión de los datos. En la sección de Multivariado vamos a ver cuál es la relación entre la varianza y la covarianza, a entender a la varianza como un caso especial de la covarianza y ver de dónde sale esa **suma de cuadrados**. Esto último también me lleva a que cada que leas **suma de cuadrados** pienses en una medida de dispersión o en la varianza de los datos.

$$
\sigma^2 = \frac{\sum{(x_i - \mu)^2}}{N}
$$

La desviación estándar, por otra parte, es simplemente la raíz cuadrada de la varianza. Si la varianza representa un área, la desviación estándar representa una **distancia**, la distancia promedio que existe entre cada uno de los datos y la media. Estas dos medidas (la desviación estándar y la varianza) son sumamente útiles y utilizadas en los procesos estadísticos; de hecho, junto con la media, son los principales **parámetros poblacionales** que usualmente queremos estimar a partir de nuestra muestra. Una aclaración es que la ecuación de arriba es para calcular la **varianza poblacional**, mientras que si queremos calcular la **varianza muestral** aplicaremos una corrección con los grados de libertad (que definiremos más adelante)

$$
s^2 = \frac{\sum(x_i - \bar{x})^2}{N-1}
$$

Esta **varianza muestral** se considera un estimador insesgado de la **varianza poblacional**.

#### Estimadores

Este es un buen momento para hablar de los estimadores. ¿Qué es un estimador? Una medida que utilizaremos para estimar un parámetro poblacional. Evidentemente, no puede ser cualquier número ni cualquier medida, debe de tener ciertas características. Particularmente:

1. Insesgado: Es decir, que la media de la **distribución del estimador** sea igual al parámetro. De nuevo, en la sesión de técnicas paramétricas vamos a hablar sobre distribuciones muestrales, el teorema del límite central y su implicación para el supuesto de normalidad que a veces puede ser un dolor de cabeza. Por lo pronto entiende que "la media de la distribución del estimador" hace referencia a que, si hicieramos una cierta cantidad de muestreos y calculamos algún parámetro para cada muestreo, el promedio de esas estimaciones debe ser igual al parámetro poblacional.
2. Consistencia: Es la propiedad en la que un estimador se aproxima al valor del parámetro conforme incrementa el tamaño de muestra, lo cual también tiene que ver con el teorema del límite central que revisaremos más adelante.
3. Eficiencia: La estimación tiene el error estándar más pequeño cuando se compara con otro estimador. Por ejemplo, en una distribución Normal, la media y la mediana son prácticamente iguales; sin embargo, el error estándar de la media es $\frac{\sigma}{\sqrt{n}}$, mientras que el de la mediana es $\approx 1.25$ veces ese valor.
4. Suficiencia: El estimador es, por sí mismo, capaz de transmitir toda la información disponible en la muestra sobre el valor del parámetro.

Por estas 4 razones es que la mayor parte de nuestras inferencias están en relación a la **media poblacional**, estimada a partir de la **media muestral**. La mediana no se considera un buen estimador de la media poblacional si la distribución no es simétrica, pues, como vimos arriba, está sesgada en relación a la media poblacional. Por otra parte, su error estándar es mayor que el error estándar de la media (en términos de sus distribuciones muestrales), ni utiliza todos los datos (solo el cuantil 50).

### Coeficiente de variación

El coeficiente de variación, también conocido como la desviación estándar relativa, es la relación que existe entre la desviación estándar y la media de los datos, tal que:

$$
CV = \frac{\sigma}{\mu}*100
$$
Este es especialmente útil cuando queremos comparar las dispersiones de dos cosas que están en distinta escala, o expresar en un porcentaje qué tan grande es nuestra variabilidad en relación a nuestra estimación. Si hacemos una estimación de tamaño poblacional de 1000 individuos, con un coeficiente de variación del 50% quiere decir que nuestra variabilidad es de la mitad de nuestra estimación.

### Error estándar

Esta medida es especialmente útil en la estimación de los intervalos de confianza de cualquier parámetro, y representa la desviación estándar de su distribución muestral, el cuál podemos estimar como:

$$
\sigma_{\bar{x}} \approx \frac{\sigma_x}{\sqrt{n}}
$$

Normalmente nosotros no estimaremos o interpretaremos esta medida, sino que iremos directamente a los intervalos de confianza para expresar la incertidumbre alrededor de nuestras estimaciones.


## Gráficos

En la sesión de visualización vimos las consideraciones que debemos de tener en cuenta para hacer una visualización efectiva, pero no hablamos de los tipos de gráficos que podemos realizar. En esta sesión, entonces, no entraremos a ver los detalles de la visualización, simplemente hablaremos de los gráficos más comunes, en qué escenarios son útiles y cómo construirlos utilizando `ggplot2`.





