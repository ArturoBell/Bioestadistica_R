---
title: "Agrupamientos, Reducción de la dimensionalidad y Clasificación"
author: "Arturo Bell Enríquez García"
output: 
  html_notebook:
    toc: yes
---

```{r}
# Instalación de paqueterías:
#if (!require(c("ggdendro", "dendextend", "factoextra", "cluster", "NbClust", "FactoMineR", "MASS", "klaR", "vegan", "caTools", "caret"))) {
#  install.packages( c("ggdendro", "dendextend", "factoextra", "cluster", "NbClust", "FactoMineR", "MASS", "vegan", "caTools", "caret"), dependencies = T)
#}
```

## Funciones personalizadas:
```{r}
# Tema personalizado
blank_theme <- function(aspect.ratio = 1/1.61){
  theme(panel.grid.minor = element_blank(),
        panel.grid.major = element_blank(),
        panel.background = element_blank(),
        axis.line = element_blank(),
        aspect.ratio = aspect.ratio,
        axis.ticks = element_blank(),
        text = element_text(colour = "gray50"), # Eliminar
        legend.position = "none"
        )
}
# Función para la estimación de los CP óptimos:

evplot <- function(ev){
    
    # Broken stick model (MacArthur 1957)
      n <- length(ev)
      bsm <- data.frame(j=seq(1:n), p=0)
      bsm$p[1] <- 1/n 
      for (i in 2:n) bsm$p[i] <- bsm$p[i-1] + (1/(n + 1 - i))
      bsm$p <- 100*bsm$p/n
    # Plot eigenvalues and % of variation for each axis
      op <- par(mfrow=c(2,1))
      barplot(ev, main="Eigenvalues", col="bisque", las=2)
        abline(h=mean(ev), col="red")
        legend("topright", "Average eigenvalue", lwd=1, col=2, bty="n")
      barplot(t(cbind(100*ev/sum(ev), bsm$p[n:1])), beside=TRUE, 
              main="% variation", col=c("bisque",2), las=2)
        legend("topright", c("% eigenvalue", "Broken stick model"), 
               pch=15, col=c("bisque",2), bty="n")
      par(op)
  }
```

# Agrupamiento jerárquico (clúster)

Realizar un agrupamiento jerárquico en R es bastante sencillo, únicamente debemos de tener nuestro data.frame cuyos nombres de renglones sean las etiquetas de nuestras instancias a agrupar, calcular las distancias con la función `dist(x, method)` y unir los grupos con la función `hclust(dist, method)`.
```{r}
library(vegan, quietly = T)

df1 <- data.frame(long = c(3, 3.6, 6.5, 7.5, 15, 20), row.names = c("Tt", "Gg", "Gm", "Oo", "Mn", "Bp"))
dist.mat <- dist(df1, method = "euclidean")
hc.av <- hclust(dist.mat, method ="average")
```
Si graficamos este objeto tendremos un dendrograma muy sencillo:
```{r}
library(ggplot2, quietly = T)
library(ggdendro, quietly = T)

ggdendrogram(hc.av, labels = T)
```
Podemos personalizar el dendrograma utilizando la función `set(object, what, value)` de la librería `dendextend`:
```{r}
library(dendextend)
# Primero transformemos nuestro objeto a un dendrograma:
dend <- as.dendrogram(hc.av)

# Cambiemos el color a las ramas:
dend <- set(dend, "branches_k_color",
            value = "deepskyblue4",
            k = 1)

#dend <- set(dend, "branches_k_color",
#            value = c(rgb(0,118, 186, maxColorValue = 255),
#                      rgb(0, 123, 118, maxColorValue = 255),
#                      rgb(255, 147, 0, maxColorValue = 255),
#                      rgb(181, 23, 0, maxColorValue = 255)), 
#            k = 4)
dend <- set(dend, "branches_lwd", 1)

# Cambiemos el color de las etiquetas:
dend <- set(dend, "labels_col",
            value = c(rgb(0,118, 186, maxColorValue = 255),
                      rgb(0, 123, 118, maxColorValue = 255),
                      rgb(255, 147, 0, maxColorValue = 255),
                      rgb(181, 23, 0, maxColorValue = 255)), 
            k = 4)

# Graficador base de R:
plot(dend)
```
Por homogeneidad con el resto del curso, grafiquémoslo utilizando `ggplot2`:
```{r}
ggd1 <- as.ggdend(dend)
ggd1.plot <- ggplot(ggd1, offset_labels = -1, theme = blank_theme()) + 
             ylim(-2.4, max(get_branches_heights(dend))) +
             scale_x_continuous(breaks = NULL) +
             labs(title = "Dendrograma de especies de cetáceos",
                  subtitle = "Agrupamientos por longitud (método: promedio)",
                  x = element_blank(),
                  y = "Distancia euclidiana",
                  caption = "Datos de bit.ly/clust_medium")

ggd1.plot
```
## Ejercicio Mv
Habiendo comprendido cómo funciona el análisis de agrupamientos, apliquémoslo a una base de datos multivariada (`cluster.txt`), la cual consiste en mediciones de variables ambientales en distintos sitios de muestreo y el objetivo es agrupar aquellos que tengan características similares. En el resumen podemos ver que hay diferencias muy importantes en las escalas de las variables, por lo cual será necesario escalar los datos. 
```{r}
clust.df <- read.table("cluster.txt", header = T, row.names = 1) # ¡OJO con row.names!
summary(clust.df)
```
Otro problema, tal vez no tan evidente, es que la distribución de SST se encuentra sumamente sesgada, por lo que habrá que a) aplicar una transformación para normalizar los datos, b) retirarla del análisis. Tomando en cuenta los valores, es bastante probable que las mediciones estén equivocadas, por lo que la retiraremos del análisis.
```{r}
mask <- colnames(clust.df)[colnames(clust.df) != "SST"]
clust.filt <- clust.df[,mask]
colnames(clust.filt)
```

Ahora escalemos todos nuestros datos. Al analizar rápidamente la distribución de cada una de las variables podemos ver que hay algunas con sesgos que pudieran ser importantes; sin embargo, podemos utilizar una medida de distancia que contienda con este tipo de distribuciones para evitar una transformación más agresiva que el escalamiento.
```{r}
clust.scale <- scale(clust.filt)
#clust.scale <- log(clust.filt +1) # Si de todos modos deseas aplicar una transformación para "normalizar" los datos.
summary(clust.scale)
```


Ahora realicemos los agrupamientos. Utilizaremos la distancia Mahalanobis ya que esta no es tan sensible a los valores extremos. El método de agrupamiento será ward.D2, (distancias cuadráticas) para minimizar la varianza intra-grupos:

```{r}
dist.mv1 <- vegdist(clust.scale, method = "mahalanobis")
hc.mv1 <- hclust(dist.mv1, method ="ward.D2")

# Transformemos nuestro objeto a un dendrograma:
dend.mv1 <- as.dendrogram(hc.mv1)

# Cambiemos el color a las ramas:
dend.mv1 <- set(dend.mv1, "branches_k_color",
            value = "deepskyblue4",
            k = 1)

dend.mv1 <- set(dend.mv1, "branches_lwd", 0.7)

ggd1 <- as.ggdend(dend.mv1)
ggd1.plot <- ggplot(ggd1, offset_labels = -1, theme = blank_theme()) + 
             expand_limits(y = -2.4) +
             scale_x_continuous(breaks = NULL) +
             scale_y_continuous(breaks = seq(0,max(get_branches_heights(dend.mv1)),2)) +
             labs(title = "Dendrograma de sitios de muestreo",
                  subtitle = "Método de agrupamiento: Ward.D2",
                  x = element_blank(),
                  y = "Distancia Mahalanobis",
                  caption = "Datos: clust.txt")

ggd1.plot
```
Los agrupamientos con esa combinación de distancia y método de agrupamiento no son claros, veamos qué pasa si consideramos la distancia euclidiana:

```{r}
dist.mv2 <- vegdist(clust.scale, method = "euclidean")
hc.mv2 <- hclust(dist.mv2, method ="ward.D2")

# Transformemos nuestro objeto a un dendrograma:
dend.mv2 <- as.dendrogram(hc.mv2)

# Cambiemos el color a las ramas:
dend.mv2 <- set(dend.mv2, "branches_k_color",
            value = "deepskyblue4",
            k = 1)

dend.mv2 <- set(dend.mv2, "branches_lwd", 0.7)

ggd2 <- as.ggdend(dend.mv2)
ggd2.plot <- ggplot(ggd2, offset_labels = -1, theme = blank_theme()) + 
             expand_limits(y = -2.4) +
             scale_x_continuous(breaks = NULL) +
             scale_y_continuous(breaks = seq(0,max(get_branches_heights(dend.mv2)),2)) +
             labs(title = "Dendrograma de sitios de muestreo",
                  subtitle = "Método de agrupamiento: Ward.D2",
                  x = element_blank(),
                  y = "Distancia Euclidiana",
                  caption = "Datos: clust.txt")

ggd2.plot
```
¿Y la distancia Manhattan? Las agrupaciones resultantes son similares; sin embargo, las medidas de distancia son muy diferentes. Utilicemos el caso anterior para evaluar las agrupaciones.
```{r}
dist.mv3 <- vegdist(clust.scale, method = "manhattan")
hc.mv3 <- hclust(dist.mv3, method ="ward.D2")

# Transformemos nuestro objeto a un dendrograma:
dend.mv3 <- as.dendrogram(hc.mv3)

# Cambiemos el color a las ramas:
dend.mv3 <- set(dend.mv3, "branches_k_color",
            value = "deepskyblue4",
            k = 1)

dend.mv3 <- set(dend.mv3, "branches_lwd", 0.7)

ggd3 <- as.ggdend(dend.mv3)
ggd3.plot <- ggplot(ggd3, offset_labels = -1, theme = blank_theme()) + 
             expand_limits(y = -3.5) +
             scale_x_continuous(breaks = NULL) +
             scale_y_continuous(breaks = seq(0,max(get_branches_heights(dend.mv3)),5)) +
             labs(title = "Dendrograma de sitios de muestreo",
                  subtitle = "Método de agrupamiento: Ward.D2)",
                  x = element_blank(),
                  y = "Distancia Manhattan",
                  caption = "Datos: clust.txt")

ggd3.plot
```
### Evaluación de los agrupamientos:
Para calcular los índices que estiman el número óptimo de agrupaciones utilizaremos la función `NbClust` de la librería con el mismo nombre. 
```{r}
library(NbClust)
res <- NbClust(clust.scale, diss = NULL, distance = "euclidean", method = "ward.D2", index = "all", max.nc = 8)
```
Veamos ahora los grupos y calculemos el coeficiente de correlación cofenético. Una correlación de prácticamente el 70% es más que aceptable, si consideramos que no filtramos la totalidad de los datos. Como dato curioso, al aplicar una transformación logarítmica se mantienen estos grupos pero el coeficiente de correlación cofenético se eleva a 0.8.
```{r}
res$Best.partition
hc.e <- cluster::agnes(dist.mv2, diss = T, method = "ward")
ccc <- cophenetic(hc.e) # Distancias en el dendrograma
ccofen <- cor(dist.mv2, ccc) # Correlación entre distancias reales y graficadas
ccofen
```
Esta forma de ver los agrupamientos no es muy clara, entonces agreguémosla a nuestro gráfico mediante colores de las ramas y etiquetas:

```{r}
# Cambiamos el color de nuestr
dend.mv2 <- set(dend.mv2, "labels_col",
            value = 1:3,
            k = 3)

dend.mv2 <- set(dend.mv2, "branches_k_color",
            value = 1:3,
            k = 3)          
          
dend.mv2 <- set(dend.mv2, "branches_lwd", 0.7)

ggd2 <- as.ggdend(dend.mv2)
ggd2.plot <- ggplot(ggd2, offset_labels = -1, theme = blank_theme()) + 
             expand_limits(y = -2.4) +
             scale_x_continuous(breaks = NULL) +
             scale_y_continuous(breaks = seq(0,max(get_branches_heights(dend.mv2)),2)) +
             labs(title = "Dendrograma de sitios de muestreo",
                  subtitle = "Método de agrupamiento: Ward.D2",
                  x = element_blank(),
                  y = "Distancia Euclidiana",
                  caption = "Datos: clust.txt")

ggd2.plot
```
### Guardado de figuras!!!
```{r}
cairo_pdf("dendrogram.pdf", width = 8, height = 8)
ggd2.plot
dev.off()
```

¿Cómo lo interpretamos? Hay que revisar la base de datos para evitar caer en interpretaciones incorrectas y analizar qué características conforman cada grupo.

# Análisis de Componentes Principales
Por fin llegó la hora de hablar del Análisis de Componentes Principales. Vimos que esta técnica nos permite aproximar nuestros datos multivariados utilizando nuevas variables llamadas Componentes Principales, las cuales son ortogonales (independientes) entre sí. Apliquemos el Análisis de componentes principales a una base de datos con una gran cantidad de variables: 

```{r}
library(factoextra)
library(FactoMineR)
#Base de datos completa
  
x1n <- read.table('Medidas.txt', header = TRUE)
#Base de datos sin nombres de especies

x1 <- x1n[ ,2:length(x1n) ]
summary(x1)
```
Apliquemos el Análisis de Componentes Principales:
```{r}
res.pca <- FactoMineR::PCA(x1, graph = F, ncp = length(x1), scale.unit = T) #Aplicamos el ACP con los datos escalados y centrados
var <- get_pca_var(res.pca) # Extraemos las correlaciones de cada variable con cada componente principal
cors <- (round(abs(var$coord),2))
cors
```

Ahora veamos los resultados de manera gráfica. Primero las variables:
```{r}
fviz_pca_var(res.pca, col.var = "coord",
                       gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
```
¿Qué variables son las más importantes? Otra forma de ver estos resultados es utilizando la librería corrplot:
```{r}
library(corrplot)
corrplot(var$coord, is.corr = T)
```
Podemos extraer las 4 variables más importantes para un componente dado:
```{r}
sort(cors[,1], decreasing = T)[1:4]
sort(cors[,2], decreasing = T)[1:4]
```

Un efecto secundario de reducir la dimensionalidad es que podemos formar agrupaciones, (otra de las aplicaciones del ACP) tal y como en el caso del NMDS. Podemos ver cómo se distribuyeron los individuos según nuestras agrupaciones originales, tal que:

```{r}
fviz_pca_ind(res.pca,
             geom.ind = "point", # Mostrar solo puntos (sin textos)
             col.ind = x1n[,1], # Número de colores (grupos)
             palette = c("#00AFBB", "#E7B800", "#FC4E07"), # Colores a utilizar
             addEllipses = TRUE, # Elipses de concentración
             legend.title = "Especies")
```
Y podemos también hacer un gráfico conjunto, aunque dada la alta dimensionalidad de esta base de datos, los resultados serán poco interpretables.

```{r}
fviz_pca_biplot(res.pca, 
                col.ind = x1n$especie, palette = "jco", 
                addEllipses = TRUE, label = "var",
                col.var = "black", repel = TRUE,
                legend.title = "Especies") 
```

Veamos ahora las varianzas explicadas acumuladas para cada componente:
```{r}
eig.val <- get_eigenvalue(res.pca)
eig.val
```

De todos los resultados anteriores vemos que las primeras dos dimensiones capturan menos del 50% de la varianza total de los datos; por tanto, es conveniente analizar cuántos componentes principales son los óptimos. Una alternativa es realizar el análisis gráfico de la varianza. En este gráfico buscamos un punto de inflexión, el cual indicaría el límite de los PCs representativos.

```{r}
fviz_eig(res.pca, addlabels = T)
```
En este gráfico yo consideraría el primer punto de inflexión en el tercer componente principal, pero veamos otros dos criterios. El primero es el criterio de Kaiser-Guttman, en el cual consideraremos los CP cuyas varianzas explicadas sean superiores al valor medio. El segundo modelo es un modelo de repartición de recursos desarrollado en el campo de la ecología: El modelo Broken-Stick (McArthur 1957). En este modelo, el recurso compartido es la varianza total, entonces consideraremos aquellos CPs que contengan una varianza explicada superior a la esperada según el modelo. 
```{r}
#Kaisser-Guttman y Broken Stick
ev <- as.data.frame(eig.val)$eigenvalue
names(ev) <- paste("PC",seq_along(ev), "")
evplot(ev)
```
Bajo el criterio de Kaiser-Guttman consideraríamos 8 CPs, mientras que en el modelo de Broken-Stick consideraríamos únicamente 4. De cualquier modo, ambos criterios sugieren un número mayor al que yo consideré en el gráfico de la varianza.


# Análisis de Funciones Discriminantes:
Realicemos el análisis sobre los mismos datos para ver si podemos predecir adecuadamente la especie de nuestros individuos a partir de nuestras mediciones. El primer paso es verificar que nuestros predictores puedan separar nuestras clases, utilizando la lambda de Wilks; es decir, realizando un MANOVA. El valor de lambda es muy pequeño, al igual que el valor de p, lo cual sugiere que los predictores son buenos y no descartamos un modelo de funciones discriminantes.
```{r}
mnv <- manova(as.matrix(x1n[,-1]) ~ x1n$especie)
summary.manova(mnv,test="Wilks")
```

Realicemos nuestra división entrenamiento/prueba

```{r}
library(caTools)
library(caret)
set.seed(1111)
sample <- sample.split(x1n$especie, SplitRatio = .75)
train <- subset(x1n, sample == TRUE)
test <- subset(x1n, sample == FALSE)
```

Centremos y estandaricemos los datos:
```{r}
pre_pars <- preProcess(train, method = c("center", "scale"))
train_t <- predict(pre_pars, train)
test_t <- predict(pre_pars, test)
```

Apliquemos el Análisis Discriminante Lineal. Las probabilidades previas representan la proporción de cada grupo en la base de datos, las medias de los grupos las medias de cada grupo para cada variable, los coeficientes aprendidos por el modelo según los datos de entrenamiento y la proporción de la traza es la proporción de la varianza explicada por cada ecuación discriminante.
```{r}
lda_mod <- MASS::lda(especie~., data = train_t)
lda_mod
```

Calculemos la correlación entre las variables originales y las dos funciones discriminantes

```{r}
adj_vals <- as.data.frame(predict(lda_mod, train_t)$x)
adj_vals["especie"] <- train_t["especie"]
corr <- cor(cbind(as.matrix(train_t[,2:32])), 
                   adj_vals[,1:2])
corr
```
```{r}
corrplot::corrplot(corr, is.corr = T)
```
Creemos las gráficas de frecuencias con curva de densidad para la primera función discriminante (gráfico territorial):

```{r}
plot(lda_mod, dimen = 1, type = "both")
```
##Diagnóstico del modelo de funciones discriminantes:
1. Probabilidad de pertenencia de cada individuo a partir de los datos de prueba:
```{r}
p <- predict(lda_mod)$posterior
p.acom <- data.frame(especie = test_t$especie, id = rownames(test_t), p = p)
p.acom
```

2. Prueba de Ji-cuadrada para verificar un acomodo no aleatorio (p<0.005)
```{r}
pred.clase <- predict(lda_mod, test_t)$class
jicuad <- chisq.test(test_t$especie, pred.clase, simulate.p.value = T)
jicuad
```

3. Matriz de confusión:
```{r}
confusionMatrix(pred.clase, as.factor(test_t$especie))
```

4. Gráfico de funciones discriminantes:

```{r}
svd_tot <- sum(lda_mod$svd^2) # svd^2 = SS = varianza
var_exp_tot <- sum(lda_mod$svd[1:2]^2)/svd_tot*100
lda_plot <- ggplot(aes(x = LD1, y = LD2, colour = especie), data = adj_vals) + 
            geom_point(size = 3, alpha = 0.6) +
            geom_hline(yintercept = 0, linetype = "dashed") +
            geom_vline(xintercept = 0, linetype = "dashed") +
            labs(title = "Gráfico de funciones discriminantes",
                 subtitle = paste("Varianza total explicada: ", var_exp_tot, "%"),
                 x = paste("LDA1, Var.: ", round(lda_mod$svd[1]^2/svd_tot*100, 2), "%"),
                 y = paste("LDA2, Var.: ", round(lda_mod$svd[2]^2/svd_tot*100, 2), "%"),
                 caption = "Datos: medidas.txt") +
            blank_theme() + # LEYENDAS
            theme(legend.position = "right",
                  legend.background = element_blank())
lda_plot
```
5. Márgenes de decisión. Utilizamos la base de datos iris porque la alta dimensionalidad de la base de datos `medidas.csv` es demasiada para la librería (465 pares de variables a calcular).
```{r}
klaR::partimat(Species~., data = iris, method = "lda")
```


