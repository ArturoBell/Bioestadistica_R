---
title: "Regresiones Lineales Múltiples"
author: "Arturo Bell Enríquez García"
output:
    html_document:
      df_print: paged
      theme: cosmo
      toc: TRUE
      toc_float: TRUE
---

## Funciones personalizadas

```{r}
# Tema personalizado
blank_theme <- function(aspect.ratio = 1/1.61){
  theme(panel.grid.minor = element_blank(),
        panel.grid.major = element_blank(),
        panel.background = element_blank(),
        axis.line = element_blank(),
        aspect.ratio = aspect.ratio,
        axis.ticks = element_blank(),
        text = element_text(colour = "gray50"), # Eliminar
        legend.position = "none"
        )
}
```

## Regresión Lineal Múltiple

Para esta sección utilizaremos los datos de la base `crime.csv`. El objetivo será predecir la tasa de crimenes/poblacion a partir de otras 88 variables:

La variable que nos interesa predecir es el la columna `ViolentCrimesPerPop`). Apliquemos un modelo de regresión lineal múltiple incluyendo todas las variables, sin dividir los datos en entrenamiento-prueba y sin escalarlos. Tampoco filtraremos los datos para evitar una fuga de información:

```{r}
dr.df <- read.csv("crime.csv")[,-c(1)]
head(dr.df)
```

```{r}
dr.lm <- lm(ViolentCrimesPerPop~., data = dr.df)
summary(dr.lm)
```

```{r}
summary(dr.df$RentQrange)
```


Veamos qué pasa ahora si aplicamos este mismo modelo realizando la división entrenamiento-prueba.

#### Evaluación, el PROBLEMA del exceso de dimensionalidad y el sobreajuste:
Para realizar la división utilizaremos la función `sample.split()` de librería caTools:

```{r}
library(caret)
library(caTools)
set.seed(1111)
sample <- sample.split(dr.df$ViolentCrimesPerPop, SplitRatio = .75)
train <- subset(dr.df, sample == TRUE)
test <- subset(dr.df, sample == FALSE)
```

Ajustemos el modelo de entrenamiento:
```{r}
train.dr.lm <- lm(ViolentCrimesPerPop~., data = train)
summary(train.dr.lm)$r.squared*100
```

Evaluemos el desempeño en los datos de prueba.
```{r}
pred <- predict.lm(train.dr.lm, test[,1:88]) 
R2(pred, test$ViolentCrimesPerPop)*100
```

De estos resultados vemos que el modelo está sobreajustado, ya que el $R^2$ del modelo de prueba menor al de entrenamiento. Tomando esto en consideración, 1) escalemos los datos y b) apliquemos los modelos regularizados.

Para hacer las cosas más organizadas, primero dividamos ambos sets de datos en dos objetos: uno con las variables independientes y otro con la variable dependiente:

```{r}
X_train <- train[,1:88]
y_train <- train$ViolentCrimesPerPop
X_test <- test[,1:88]
y_test <- test$ViolentCrimesPerPop
```

Ahora estandaricemos las variables dependientes:
```{r}
preProc <- preProcess(X_train, method = c("center", "scale"))
X_train_s <- predict(preProc, X_train)
X_test_s <- predict(preProc, X_test)
```

## Regularización L2: Regresión Ridge

Apliquemos la regresión al set de entrenamiento. Debido a que el valor de penalización ($\alpha$ en la clase, $\lambda$ en glmnet) es "arbitrario", podemos utilizar validación cruzada para encontrar el valor que mejor se ajuste a nuestros datos. La validación cruzada consiste en hacer k particiones entrenamiento-prueba de los datos, ajustar un modelo para cada k para cada valor del parámetro que sea de nuestro interés, promediar el valor de error de cada iteración y quedarnos con el valor del parámetro que haya logrado el menor valor de nuestra medida de error. En la librería `glmnet` esto se hace con la función `cv.glmnet`, la cual recibe como argumentos una matriz de variables independientes, un vector con variables dependientes, un valor de alpha que será 0 para regresión Ridge, 1 para Lasso y algún intermedio para red elástica (no la veremos en el curso) y el tipo de medida de error, en este caso MSE.

```{r}
library(glmnet)
cv.ridge <- cv.glmnet(as.matrix(X_train_s), y_train, alpha = 0, type.measure = "mse")
lambdamin <- cv.ridge$lambda.min #Contiene el valor de lambda que otorga el menor valor de error
lambdamin
```

Ajustemos entonces el modelo regularizado con ese valor de lambda. (Df representa el número de coeficientes no negativos, %Dev = R^2). 
```{r}
ridge.lm <- glmnet(as.matrix(X_train_s), y_train, alpha = 0, lambda = lambdamin)
print(ridge.lm)
coef(ridge.lm) # Imprime los coeficientes de la regresión
```
Vemos que el valor de R2 es ligeramente menor al caso anterior; sin embargo, ganamos un poco de poder predictivo en el modelo de prueba:
```{r}
ridge.pred <- predict(ridge.lm, as.matrix(X_test_s))
R2(ridge.pred, y_test)*100
```

Veamos cómo afecta el valor de lambda:

```{r}
for (lambda in c(0, 0.5, 1, 2, 3, 5, 10, 20, 50, round(lambdamin,2))) {
  
  ridge.lm <- glmnet(as.matrix(X_train_s), y_train, alpha = 0, lambda = lambda, standardize = F, standardize.response = F)
  ridge.pred <- predict(ridge.lm, as.matrix(X_test_s))
  
  print(paste("lambda = ", lambda, 
              "; Variables restantes:", round(ridge.lm$df, 2), 
              "; R^2 entrenamiento = ", round(ridge.lm$dev.ratio,2), 
              "; R^2 prueba = ", round(R2(ridge.pred, y_test),2)))
  
}
```

Vemos que sí hay diferencias con respecto al primer caso. Apliquemos la regresión Lasso:

## Regularización L1: Regresión Lasso
```{r}
cv.lasso <- cv.glmnet(as.matrix(X_train_s), y_train, alpha = 1, type.measure = "mse")
lambdamin <- cv.lasso$lambda.min #Contiene el valor de lambda que otorga el menor valor de error
lambdamin
```
Ajustemos entonces el modelo regularizado con ese valor de lambda. (Df representa el número de coeficientes no cero, %Dev = R^2). 
```{r}
lasso.lm <- glmnet(as.matrix(X_train_s), y_train, alpha = 1, lambda = lambdamin)
print(lasso.lm)
coef(lasso.lm) # Imprime los coeficientes de la regresión
```
Vemos que el valor de R2 no es muy diferente al caso anterior, aunque ahora únicamente tenemos 18 coeficientes ≠ 0
```{r}
lasso.pred <- predict(lasso.lm, as.matrix(X_test_s))
R2(lasso.pred, y_test)*100
```
En los datos de prueba tampoco hubo mucha diferencia. Veamos cómo cambia esto al modificar el valor de lambda:

```{r}
for (lambda in c(0, 0.5, 1, 2, 3, 5, 10, 20, 50, round(lambdamin,2))) {
  
  lasso.lm <- glmnet(as.matrix(X_train_s), y_train, alpha = 1, lambda = lambda, standardize = F, standardize.response = F)
  lasso.pred <- predict(lasso.lm, as.matrix(X_test_s))
  
  print(paste("lambda = ", lambda, 
              "; Variables restantes:", round(lasso.lm$df, 2), 
              "; R^2 entrenamiento = ", round(lasso.lm$dev.ratio,2), 
              "; R^2 prueba = ", round(R2(lasso.pred, y_test),2)))
  
}
```

De lo anterior podemos ver que un modelo lineal puede no ser la mejor de las opciones; además, no corroboramos ninguno de los supuestos de la regresión y con una cantidad tan alta de variables es muy posible que muchas estén altamente correlacionadas:
```{r}
library(corrplot)
corrplot(cor(dr.df), method = "ellipse", type = "upper")
```
Podemos también hacer una matriz similar, aunque utilizando gráficos de dispersión:
```{r, fig.height=10, fig.width=10}
library(GGally)
#pairs <- ggpairs(dr.df, progress = F)
#pairs
```

La visualización de los datos se hace en parte junto con la evaluación de la regresión, en el sentido de que solo podemos ver la relación entre los datos observados y predichos por el modelo

```{r}
mlm.data <- data.frame(obs = y_test, s0 = predict(lasso.lm, as.matrix(X_test_s)))
mlm.plot <- ggplot(data = mlm.data, aes(x = obs, y =  s0)) + 
            geom_point() + 
            geom_smooth(method = "lm", colour = rgb(118,78,144, maxColorValue = 255)) +
            labs(title = "Gráfico de dispersión de datos observados y predichos",
                 subtitle = sprintf("R2e: %.2f | R2p: %.2f",round(lasso.lm$dev.ratio,2), round(R2(lasso.pred, y_test),2)),
                 caption = "Datos de prueba. \n El área gris representa el intervalo de confianza de la regresión al 95%",
                 x = "",
                 y = "") +
            theme_bw()
mlm.plot
```

## Ingeniería de variables
El resultado es un modelo que no se encuentra sobreajustado, 
```{r}
library(factoextra)
library(FactoMineR)

X_train_pca <- FactoMineR::PCA(X_train, graph = F, ncp = length(X_train), scale.unit = F)
X_pca_train <- predict(X_train_pca, X_train)$coord
X_pca_test <- predict(X_train_pca, X_test)$coord
```

Evaluemos las correlaciones de los nuevos predictores con nuestra variable de interés y extraigamos aquellas que tengan correlaciones significativas (≠0):
```{r}
#library(Hmisc)
c_mat <- as.data.frame(cor(cbind(X_pca_train, y_train)))$y_train
p_mat <-  as.data.frame(Hmisc::rcorr(cbind(X_pca_train, y_train))$P)$y_train
p_mat <- data.frame(y_train = p_mat[is.na(p_mat) == F], CP = colnames(X_pca_train))
sig_cps <- p_mat$CP[p_mat$y_train <= 0.05]
sig_cps
```
Filtramos nuestros datos:
```{r}
X_filt_train <- X_pca_train[,sig_cps]
X_filt_test <- X_pca_test[,sig_cps]
filt_train <- data.frame(X_filt_train, y_train)
```


Realicemos la regresión múltiple:
```{r}
pca_lm <- train(y_train~.,
                data = filt_train,
                method = "lm")
summary(pca_lm)
```
Los resultados son similares en los datos de entrenamiento, veamos qué pasa con los datos de prueba:
```{r}
pca_tpreds <- predict(pca_lm, as.data.frame(X_filt_test))
round(R2(pca_tpreds, y_test),2)
```

Los resultados también son similares a aquellos de la regresión Lasso. La conclusión que podemos generar es que un modelo lineal como este puede no ser la mejor opción para estos datos. ¿Cómo más podríamos mejorarlo?

## Ejercicio

1. Realiza una regresión Lasso con los CPs. ¿Cuántas variables se mantienen?¿Cambia el desempeño de los modelos?
2. Realiza los diagnósticos de las regresiones Ridge, Lasso y la regresión con CP. ¿Cuál está mejor ajustada? (Performance)
3. Aplica un GLM con las variables restantes en la regresión Lasso. ¿Qué familia utilizas para el error? ¿El ajuste mejora o se mantiene igual?
