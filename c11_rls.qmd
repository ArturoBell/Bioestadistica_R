# Modelo lineal {#sec-rls}

## Librer칤as

```{r}
#| warning: false
#| message: false

library(ggplot2)
library(performance)
library(stats4)
library(Metrics)
```

## Introducci칩n

En much칤simos lugares encontramos, de forma completamente natural, patrones que se repiten una y otra vez, tal y como en la m칰sica. El mundo de la estad칤stica y del **aprendizaje automatizado** se construye de la misma manera, partiendo de peque침os *motivos* que aparecen una y otra vez. En esta sesi칩n vamos a hablar de el, posiblemente, m치s popular de todos: el **modelo lineal**. Hablaremos entonces del caso m치s b치sico y escalaremos paso a paso en la complejidad.

## Regla de tres y el modelo lineal

Antes de empezar a hablar propiamente de la regresi칩n lineal, sus diferencias con la correlaci칩n, en qu칠 consiste, c칩mo aplicarlas y dem치s detalles, demos un paso hacia atr치s y expliquemos el fundamento con manzanitas (literalmente).

Imagina que te digo que gast칠 50 pesos para comprar 10 manzanas y luego te pregunto 쯖u치nto cuesta una? Para responder a la pregunta aplicar치s, aunque no seas consciente, el modelo lineal, pero primero resolvamos el problema con una regla de tres

\begin{align*}
10 游꼝 = \$50 \\
1游꼝 = ?
\end{align*}

Aqu칤 multiplicar칤amos $1游꼝 \times 50\$$ y dividimos entre $10游꼝$, lo cual nos lleva a decir que una manzana me cost칩 5$\$$:

$$
\frac{1游꼝 \times 50 \$}{10游꼝} = 5\frac{\$}{游꼝}
$$

Hasta aqu칤 nada nuevo, as칤 que hagamos ese resultado a un lado por el momento y volvamos al problema del modelo lineal. 쮼n qu칠 consiste un modelo lineal? En utilizar la ecuaci칩n de la recta ($y = a + bx$) para establecer una relaci칩n (lineal, dah) entre dos variables. Con nuestras manzanas podemos representarlo de la siguiente manera, donde $y$ es el dinero ($\$$) gastado para alguna cantidad de manzanas (游꼝):

$$
\$ = a + b*游꼝
$$

쯈u칠 representan $a$ y $b$ Empecemos, por simplicidad did치ctica, definiendo $b$:

\begin{align*}
Si \\
\$ = a + b*游꼝 \\
\Rightarrow \$ - a = b*游꼝 \\
\therefore \frac{\$ - a}{游꼝} = b
\end{align*}

Este peque침o ejercicio algebr치ico nos dice que $b$ es el resultado de **dividir nuestro precio** (menos $a$) entre **el n칰mero de manzanas**. 쯊e suena? 춰Es el **precio por una manzana**! Te preguntar치s: 쯘ntonces **qu칠 es $a$ y por qu칠 no lo consideramos antes? Para darle sentido pensemos en qu칠 har칤a que ambas aproximaciones nos lleven al mismo resultado: que $a$ fuera 0, 쯡o? Pues eso tiene todo el sentido del mundo, pues es el precio de 0 manzanas. Tomando esto en cuenta podemos asignarle nombre a nuestros distintos elementos:

1. $y$ es nuestra variable dependiente (lo que queremos predecir); es decir, el n칰mero de pesos gastados.
2. $x$ es nuestra variable independiente (con lo que vamos a predecir); es decir, el n칰mero de manzanas compradas.
2. $a$ es la **ordenada al origen** o **intercepto**, representada como $\beta_0$ o $\alpha$, indica el precio de 0 manzanas. Matem치ticamente esto lo definimos como el punto donde la recta corta a la ordenada (eje y) o, en palabras m치s sencillas, el punto donde x = 0.
3. $b$ es la **pendiente**, representada como $\beta_1$ o $\beta$, e indica el precio de una manzana. Formalmente es la **tasa de cambio** que existe del eje $x$ al eje $y$; es decir, "cu치ntas unidades nos vamos a mover en el eje y por una unidad en el eje x".

![De la regla de tres al modelo lineal](imgs/ruleofthree.png){#fig-ruleofthree}

쯇or qu칠 se le denomina lineal? Porque si lo graficamos tendremos una l칤nea recta:

```{r}
#| label: fig-linmod
#| fig-cap: "Modelo lineal con el precio por cantidad de manzanas."

manzanas <- seq(1,10)
b <- 5
a <- 0
precio <- data.frame(manzanas, precio = a+b*manzanas)

ggplot(data = precio, aes(x = manzanas, y = precio)) +
  geom_line(color = "dodgerblue4") +
  geom_point(color = "dodgerblue4") +
  scale_x_continuous(breaks = scales::pretty_breaks()) +
  see::theme_lucid() +
  labs(title = "Precio por cantidad de manzanas (n)",
       subtitle = "Modelo: $ = 0 + 5*n",
       x = element_blank(),
       y = element_blank())
```

Aunque esto nos lleva a un peque침o inconveniente o una consideraci칩n. Si utilizamos un modelo lineal estamos asumiendo expl칤citamente (aunque a veces inconscientemente) que el **cambio entre nuestras variables es constante**, determinado por $\beta$. Reflexiona: 쯘n la naturaleza cu치ntos procesos crees que sean realmente lineales? Con esto no quiero decir que debamos de olvidarnos del modelo lineal y que, entonces, esta sesi칩n es una p칠rdida de tiempo, no. Quiero decir que debemos de ser consciente del **supuesto** bajo el cual estamos trabajando, muchas veces para simplificarnos la existencia.

::: {.callout-note}
Recuerdas la definici칩n formal de un modelo? Es una **representaci칩n SIMPLIFICADA de la realidad**; es decir, siempre que involucremos un modelo, sea cual sea, estamos dejando cosas en el tintero. Recuerda: "Todos los modelos est치n equivocados, pero algunos son 칰tiles" [@Box_1976].
:::

## Regresi칩n lineal simple

Antes mencion칠 que la regla de tres es una aplicaci칩n del modelo lineal, lo que no dije es que es, en realidad, un ejercicio de **regresi칩n**. 쯈u칠 es una regresi칩n? Es una parte del **aprendizaje automatizado supervisado**, del cual hablaremos con m치s lujo de detalle en la secci칩n correspondiente, pero podemos definirla como el proceso de **estimar los par치metros de un modelo matem치tico a partir de ciertos datos**. En una regresi칩n **la variable dependiente "siempre" es num칠rica**. 쯈u칠 pasa si tenemos una variable dependiente categ칩rica? Entonces estamos en un escenario de clasificaci칩n, pero no nos adelantemos.

::: {.callout-note}
쯈u칠 es el aprendizaje automatizado? Por el momento entiendelo como la filosof칤a de "ense침ar con ejemplos" llevada a modelos matem치ticos y predicciones. En cualquier modelo de aprendizaje automatizado supervisado tenemos una serie de ejemplos (datos), en los cuales una o m치s variables sirven como predictoras de otra(s); es decir, el objetivo es generar buenas predicciones.
:::

::: {.callout-note}
쯈u칠 es un par치metro en este contexto? Una manera f치cil de entender los par치metros de un modelo es verlos como perillas que regulan c칩mo se transforma lo que est치 a la derecha del s칤mbolo de igualdad para llegar a lo que est치 a la izquierda. Si te das cuenta es b치sicamente lo que sucede con las distribuciones de probabilidad, y con justa raz칩n: las distribuciones de probabilidad son, en s칤 mismas, modelos. No confundas esta definici칩n con la definici칩n de par치metro poblacional.
:::

Ahora bien, hay una gran cantidad de **m칠todos de auste** de una regresi칩n, pero todos se reducen a una cosa: **minimizar una funci칩n de p칠rdida**. 쯈u칠 es una funci칩n de p칠rdida y con qu칠 se come? Es una forma rebuscada de llamarle a la **distancia que existe entre nuestros valores observados y los valores predichos por el modelo** o, en palabras m치s sencillas, qu칠 tan lejos qued칩 la flecha del blanco; es decir, **al ajustar un modelo estamos minimizando su error**. 쮺u치les valores predichos? Aaaah, que bueno que preguntaste. Si vuelves a la @fig-linmod te dar치s cuenta que, utilizando los **par치metros** estimados con nuestra regla de tres ($a$ = 0 y $b$ = 5) calculamos cu치nto gastar칤amos si compraramos desde 1 hasta 10 manzanas. De esos costos solo ten칤amos el dato de que $10 游꼝 = \$50$, todo lo dem치s es una predicci칩n.

::: {.callout-note}
Lo que hicimos fue, de hecho, una **extrapolaci칩n**, pues **predijimos valores fuera del alcance de nuestros datos observados**. Si tuvieramos "huecos" en nuestros datos y quisi칠ramos rellenarlos con el modelo tendr칤amos una **interpolaci칩n**; es decir, **predecir칤amos valores dentro del alcance de nuestros datos observados**.
:::

Pero volvamos a nuestra regresi칩n lineal. El modelo m치s simple es el que veremos en esta sesi칩n: la regresi칩n lineal simple. En esta, tal y como en nuestro ejemplo con las manzanas, describimos la relaci칩n entre dos variables continuas utilizando la ecuaci칩n de la recta. Formalmente este lo expresamos como:

$$
Y = \beta_0 + \beta_1*x + \epsilon
$$

Mencion칠 que existen distintas formas de ajustar sus par치metros, entre las que tenemos (ordenadas de mayor a menor complejidad):

- M칤nimos cuadrados, que veremos a continuaci칩n.
- M치xima verosimilitud, que tambi칠n veremos a continuaci칩n.
- Inferencia Bayesiana, que en realidad incluye como caso especial a la m치xima verosimilitud.
- Descenso estoc치stico de gradiente, en ciertos escenarios de redes neuronales.

Empecemos entonces con el ajuste por m칤nimos cuadrados.

### M칤nimos cuadrados

En este m칠todo de ajuste la funci칩n de p칠rdida es la funci칩n cuadr치tica:

$$
D(y_i - \hat{y_i}) = \sum_{i=1}^n(y_i - \hat{y_i})^2 = \sum_{i=1}^n \epsilon^2
$$

Es decir, **minimizamos la distancia** (diferencia) **cuadr치tica entre los valores observados y los valores predichos**. Anal칤ticamente (c치lculo) el proceso consiste en obtener la derivada parcial de $D(y_i - \hat{y_i})$, igualarla a 0, y encontrar una expresi칩n para $\beta_0$ y $\beta_1$. Te voy a ahorrar toda la [matem치tica correspondiente](https://bit.ly/minimos_cuadrados) y te dar칠, solo como referencia, estas 칰ltimas expresiones. Para $\beta_1$:

$$
  \beta_1 = \frac{\Sigma_{i = 1}^n(x_i - \overline{x})(y_i - \overline{y})}{\Sigma_{i = 1}^n(x_i - \overline{x})^2}
$$

Y para $\beta_0$:

$$
\beta_0 = \overline{y}- \beta_1*\overline{x}
$$

쯇or qu칠 solo como referencia? Porque (afortunadamente para nosotros) `R` ya tiene codificado todo el proceso en la funci칩n `lm(formula, data)` y no tenemos que preocuparnos por nada de eso. 

::: {.callout-note}
Recordar치s que en el @sec-tidyverse habl칠 (y ejemplifiqu칠) sobre c칩mo ajustar una regresi칩n lineal simple, tanto con `R` como con `tidymodels`. En esta sesi칩n `tidymodels` a칰n nos queda "un poco grande", en el sentido de que a칰n no podemos aprovechar todo lo que ofrece, por lo que me voy a limitar a utilizar la forma de `R` base.
:::

Para ejemplificarlo carguemos los datos contenidos en `example_data.csv`:

```{r}
df_reg1 <- read.csv("datos/example_data.csv")
```

Luego grafiqu칠moslos:

```{r}
plot_data_reg1 <- ggplot(data = df_reg1,
                         aes(x = v1, y = v2)) +
                  geom_point(color = "dodgerblue4",
                             alpha = 0.7,
                             size = 2) +
                  labs(title = "Relaci칩n entre v1 y v2") +
                  see::theme_lucid()
plot_data_reg1
```

#### Ajuste y Bondad de ajuste

Ahora ajustemos el modelo de m칤nimos cuadrados (`lm()`) a los datos y veamos los resultados de la regresi칩n (`summary()`):

```{r}
reg1 <- lm(v2~v1, data = df_reg1)
summary(reg1)
```

Describamos la salida elemento por elemento:

- `Call`: es el c칩mo llamamos a la funci칩n. 쯃a raz칩n? En caso de que estemos llamando a la funci칩n `lm` dentro de otra funci칩n, cosa que no hicimos. De cualquier manera, sirve como una forma de verificar que pusimos las cosas en orden.
-`Residuals`: Nos da informaci칩n sobre la distribuci칩n de los residuales (la diferencia entre observado y predicho). Esta es 칰til con fines diagn칩sticos, pero hablaremos m치s a fondo de ellos m치s adelante.
- `Coefficients`: Nos da una tabla con los valores de los par치metros del modelo, donde la pendiente tiene el nombre de la variable predictora, su error est치ndar y una prueba $t$ para cada uno. 쮺ontra qu칠 est치 comparando? Contra un **modelo nulo**; es decir, contra un modelo donde ese par치metro tenga un valor = 0.
- `Residual standard error`: M치s informaci칩n sobre los residuales, aunque en este caso es el error est치ndar. Este es sumamente 칰til para darnos una idea de qu칠 tan preciso es el modelo, pues indica en cu치ntas unidades, en promedio, se desv칤a la predicci칩n de los datos observados. Este valor, dividido entre el promedio de la variable predicha nos da la tasa de error del modelo.
- `Multiple R-squared`: Es el valor del famos칤simo (쯜nfame?) **coeficiente de determinaci칩n** ($R^2$). Si ya has llevado clases de estad칤stica y de regresi칩n lineal es muy posible que lo entiendas como "la varianza de los datos explicada por el modelo". 쯇or qu칠 digo infame? Porque, al igual que el valor de p, es un valor del cu치l se abusa. Nuevamente, los seres humanos somos flojos por naturaleza, por lo que nos gusta resumir las cosas en un solo n칰mero. En este sentido, el $R^2$ es una medida de **bondad de ajuste**; es decir, de qu칠 tan bien ajustado est치 el modelo. Es muy pr치ctico, pues est치 contenido en el intervalo $[0,1]$ y representa un porcentaje; sin embargo, para que podamos confiar en 칠l debemos de haber cubierto con el resto de **supuestos de la RLS**. Personalmtente te sugiero mejor tomar el RSE como medida de ajuste, aunque la interpretaci칩n no sea tan directa.
- `Adjusted R-squared`: Es un ajuste al $R^2$ que lo hace menos optimista, especialmente dise침ado para escenarios de regresi칩n m칰ltiple (de ah칤 el "Multiple" del punto anterior). Por el momento lo vamos a ignorar.
- `F-statistic`: Resultados de un ANOVA que, al igual que el punto anterior vamos a ignorar porque es informativo solo en regresiones m칰ltiples. Es un ANOVA para comparar todo el modelo contra un modelo que tiene solo un intercepto, por lo que aqu칤 solo es redundante con la prueba $t$ para la pendiente.

::: {.callout-note}
쯈u칠 es la bondad de ajuste? Como dice el nombre, qu칠 tan bien ajustado est치 el modelo, en el sentido de qu칠 tan buenas son las predicciones.
:::

Los resultados del modelo los podemos reportar como:

> En el modelo de regresi칩n lineal simple tanto el intercepto ($\beta_0 = -3.72$) como la pendiente ($\beta_1 = 1.17$) son significativamente diferentes de 0 ($\beta_0: t_{\nu = 95} = -5.09; p < 0.0001$; $\beta_1: t_{\nu = 95} = 14.47; p < 0.0001$). El valor de $R^2$ indica que el modelo explica alrededor del 70% de la varianza de los datos, lo cual sugiere un ajuste aceptable, con un error est치ndar de los residuales de 3.095 unidades.

Usualmente acompa침ar칤amos este reporte de un gr치fico, el cual podemos construir con la capa `geom_smooth(method = "lm", se = FALSE)`, tal que:

```{r}
plot_reg1 <- plot_data_reg1 +
             geom_smooth(method = "lm",
                         se = F,
                         colour = rgb(118,78,144,
                                      maxColorValue = 255)) +
             labs(caption = paste("Modelo ajustado: v2 = ", 
                                   round(reg1$coefficients[1],2), 
                                   " + ",
                                   round(reg1$coefficients[2],2),
                                   "*v1 + e"))

plot_reg1
```

Ahora bien, recordar치s que ninguna estimaci칩n es infalible, por lo que tanto el reporte como el gr치fico est치n incompletos. Hablemos entonces de los intervalos de confianza.

##### Intervalos de confianza para los par치metros

En un modelo de regresi칩n lineal tenemos "dos" intervalos de confianza: los intervalos de confianza para la estimaci칩n de los par치metros y el intervalo de confianza para la regresi칩n. Los primeros, como te imaginar치s, representan la incertidumbre alrededor de la estimaci칩n de nuestros par치metros de regresi칩n. Como recordar치s de lo que mencion칠 en el cap칤tulo @sec-descriptiva, estos se construyen a partir de su error est치ndar ($IC_{95\%} = \beta 췀 1.96*EE$) y, a diferencia de lo que vimos en el @sec-ph0, ahora los obtenemos con la funci칩n `confint()`:

```{r}
confint_reg1 <- confint(reg1, level = 0.95)
confint_reg1
```

Con esta informaci칩n podemos realizar un gr치fico donde representemos esta incertidumbre, en donde obtengamos dos l칤mites para la l칤nea de regresi칩n utilizando esos valores, tal que:

```{r}
# Construimos el l칤mite inferior con el l칤mite del 2.5% de los intervalos
df_reg1["inf_int"] <- confint_reg1[1,1] + confint_reg1[2,1]*df_reg1$v1
# Construimos el l칤mite superior con el l칤mite del 97.5% de los intervalos
df_reg1["sup_int"] <- confint_reg1[1,2] + confint_reg1[2,2]*df_reg1$v1
```

A침adi칠ndolos al gr치fico de los datos:

```{r}
plot_reg1 +
  geom_ribbon(data = df_reg1,
              aes(ymin = inf_int,
                  ymax = sup_int),
              fill = "gray60",
              alpha = 0.3) +
  labs(subtitle = "Intervalos de confianza para los par치metros")
```

Pero este no es el gr치fico que usualmente veremos. Si dese치ramos inlcuir esta informaci칩n solo a침adir칤amos los IC al reporte, tal que:

> En el modelo de regresi칩n lineal simple tanto el intercepto ($\beta_0 = -3.72$; $IC_{95\%}: [-5.18, -2.27]$) como la pendiente ($\beta_1 = 1.17$; $IC_{95\%}: [1.02,  1.34]$) son significativamente diferentes de 0 ($\beta_0: t_{\nu = 95} = -5.09; p < 0.0001$; $\beta_1: t_{\nu = 95} = 14.47; p < 0.0001$). El valor de $R^2$ indica que el modelo explica alrededor del 70% de la varianza de los datos, lo cual sugiere un ajuste aceptable, con un error est치ndar de los residuales de 3.095 unidades.

::: {.callout-warning}
En este caso nuestros datos son "adimensionales"; es decir, no tenemos unidades de ninguna variable. Si nuestras variables no son adimensionales debemos de incluir tambi칠n las unidades correspondientes (pesos, manzanas y pesos/manzana, si volvemos a nuestro ejemplo).
:::

##### Intervalo de confianza para la regresi칩n

쯉i el gr치fico anterior no es el que presentamos, cu치l es? Uno que incluya el intervalo de confianza para la regresi칩n *per-se* (tambi칠n llamado para la recta). Recuerdas el RSE? Pues se construye con ese valor. Para incluirlo en el gr치fico solo tenemos que modificar ligeramente `geom_smooth()` y hacer `se = TRUE`:

```{r}
#| label: fig-rls1
#| fig-cap: "Modelo de regresi칩n lineal simple de `v2` y `v1`. La l칤nea morada representa el ajuste lineal, y el 치rea sombreada el intervalo de confianza para la regresi칩n"
plot_reg1 + 
  geom_smooth(method = "lm",
              se = TRUE,
              colour = rgb(118,78,144, maxColorValue = 255))
```
::: {.callout-note}
Estamos obteniendo dos mensajes de `geom_smooth()` porque en `plot_reg1` ya ten칤amos esa capa (con `se = FALSE`). Pudiera llegar a sonar obvio, pero cuando hagas tus gr치ficos NO es necesario que repitas capas.
:::

Nuestro reporte completo quedar칤a entonces como:

En el modelo de regresi칩n lineal simple (@fig-rls1) tanto el intercepto ($\beta_0 = -3.72$; $IC_{95\%}: [-5.18, -2.27]$) como la pendiente ($\beta_1 = 1.17$; $IC_{95\%}: [1.02,  1.34]$) son significativamente diferentes de 0 ($\beta_0: t_{\nu = 95} = -5.09; p < 0.0001$; $\beta_1: t_{\nu = 95} = 14.47; p < 0.0001$). El valor de $R^2$ indica que el modelo explica alrededor del 70% de la varianza de los datos, lo cual sugiere un ajuste aceptable, con un error est치ndar de los residuales de 3.095 unidades.

Y ahora toca abordar el "elefante en el cuarto" y comprobar que nuestra regresi칩n sea confiable.

### Supuestos de la Regresi칩n Lineal

쯈u칠 no ya hab칤amos evaluado la bondad del ajuste? S칤 y no. En realidad hice una peque침a trampa para que veas por qu칠 el $R^2$ no cuenta toda la historia, y por qu칠 no debemos de confiar ciegamente en 칠l. La regresi칩n lineal simple, como buena t칠cnica param칠trica, tiene sus supuestos:

- **Linealidad**: Existe una relaci칩n lineal entre las variables involucradas.
- **Independencia**: El error es independiente; *i.e.*, no hay correlaci칩n entre el error de puntos consecutivos (aplica para series de tiempo).
- **Normalidad**: El error sigue una distribuci칩n normal.
- **Homocedasticidad**: El error tiene una varianza constante para cada valor de $X$.

::: {.callout-note}
Te dar치s cuenta de que todo est치 en t칠rminos del "error", tal y como habl치bamos de distribuciones muestrales de la media en el cap칤tulo @sec-param. Siguiendo la misma l칩gica, nuestras inferencias para comprobar los supuestos las haremos sobre los residuales.
:::

Si no cumplimos con uno, varios, o ninguno, la confiabilidad de nuestro modelo de regresi칩n para fines de interpretaci칩n va disminuyendo. Los primeros dos son bastante l칩gicos. El primero es auto-explicativo: si la relaci칩n no es lineal, el modelo lineal no es suficiente para describirla. El segundo tiene que ver con datos de series de tiempo y algo que se conoce como **autocorrelaci칩n**, pero esto se reduce a que el error del punto $t_i$ no dependa del punto $t_{i-1}$.

El **supuesto de normalidad**, para variar, requiere de un poco m치s de explicaci칩n: **la distribuci칩n que debe ser normal es la del error**. En ning칰n lugar se habla de que $Y$ o $X$ deban de estar normalmente distribuidos, solamente el error. Esto puede sonarte extra침o, pero tiene todo el sentido del mundo: si estamos optimizando el modelo a partir de los residuales, 쯣or qu칠 habr칤a de importarnos la distribuci칩n cruda de las variables?

El **supuesto de homocedasticidad**, por otra parte, es an치logo al supuesto de homogeneidad de varianzas pero, nuevamente, nos interesa qu칠 pasa con el error, no con las variables originales (por favor, no hagas una prueba de Levene con tus variables como grupos). En este caso lo importante es que el error sea parejo, independientemente de si tenemos valores peque침os o grandes del predictor.

쮺칩mo evaluamos estos supuestos? Mayoritariamente con gr치ficos de dispersi칩n, pero vamos uno a uno:

- **Linealidad**: Con un gr치fico de residuales. En este gr치fico tenemos los residuales estandarizados (cada residual menos el promedio de los residuales, dividido entre la desviaci칩n est치ndar) en el eje $y$, y los valores ajustados por el modelo (predicciones) en el eje $x$. Se pueden a침adir dos referencias: una l칤nea de referencia horizontal en $y = 0$ y una curva LOESS (hablaremos un poco de este modelo en el @sec-nolin). Si la relaci칩n entre nuestras variables predicha y predictora es perfectamente lineal, entonces todos los puntos caer치n sobre la l칤nea de referencia y la curva LOESS ser치 completamente horizontal en 0. Entre menos lineal sea la relaci칩n m치s se alejar치n los puntos de la l칤nea y mayores curvaturas tendr치 el modelo LOESS. Adicionalmente, puede servir para identificar valores extremos (puntos que caigan fuera de $[-1.96, 1.96]$ si se trabaja a un 95% de confianza).
- **Independencia**: Con un gr치fico de autocorrelaci칩n. Lo vamos a obviar porque el an치lisis de series de tiempo est치 fuera del alcance de este curso.
- **Normalidad**: Una prueba de normalidad de los residuales y con un gr치fico cuantil-cuantil (QQ plot). En este gr치fico se grafican (valga la redundancia) los residuales en el eje $y$, y cuantiles te칩ricos seg칰n una distribuci칩n normal en el eje $x$. Adem치s se traza una l칤nea de referencia con una pendiente de 1, que representa una distribuci칩n normal perfecta. El objetivo es que la prueba de normalidad sea no significativa y que los residuales caigan lo m치s cercanamente posible a la l칤nea de referencia.
- **Homogeneidad de varianzas**: Una prueba @BreuschPagan_1979 y un gr치fico de escala-locaci칩n (scale-location). La prueba, como te imaginar치s, tiene la hip칩tesis de nulidad de que el error (residuales) est치 homog칠neamente distribuido. En el gr치fico tenemos en el eje $y$ la ra칤z cuadrada del absoluto de los residuales estandarizados ($\sqrt{|re|}$) y en el eje $x$ los valores ajustados. 쯇or qu칠 la ra칤z del absoluto? Ese detalle ya es clavarse demasiado, y prefiero que nos adentremos bien a otro tema un poco m치s adelante, as칤 que conformemonos por saber que queremos que los puntos est칠n distribuidos de manera aleatoria (homog칠nea) en todo el eje $x$. 쯈u칠 quiere decir esto? que no tengamos una mayor dispersi칩n de los puntos (varianza de los residuales) en valores ajustados peque침os (a la izquierda del gr치fico) que en los valores m치s grandes (derecha del gr치fico), lo que se ver칤a como un > imaginario, o viceversa, o alg칰n otro patr칩n.

쯄uchos gr치ficos? Tal vez. Podr칤amos hacerlo a mano, o podemos aprovechar la librer칤a `performance` y obtenerlos todos de una vez con la funci칩n `check_model(object)`, donde `object` es el objeto con los resultados del ajuste. Esta funci칩n nos da todos los gr치ficos en un solo paso:

```{r, fig.height=8, fig.width=6}
#| label: fig-performance
#| fig-cap: "Gr치ficos diagn칩sticos de la regresi칩n."
reg1_diags <- performance::check_model(reg1)
reg1_diags
```

Y se acab칩 el encanto. Resulta que no cumplimos con ninguno de los supuestos, y entonces nuestro $R^2 \approx 0.7$ nos minti칩 vilmente. Cada quien reacciona de forma diferente a cu치ndo alguien le miente, pero lo que es seguro que pase es que desconfiar치, al menos un poco, de todo lo que esa persona le diga en un futuro. En defensa del $R^2$, no es su culpa y, de hecho, estoy siendo demasiado duro con 칠l. Me explico: el $R^2$ es confiable como medida de ajuste s칤 y solo si se cumplen los mismos supuestos de la regresi칩n lineal simple, lo cual no es el caso; *ergo*, no pod칤amos confiar en 칠l desde un principio porque nuestros datos no lo permiten. El problema es que se ha malversado su uso, y muchas personas lo utilizan como si fuera el 칰nico sello de garant칤a, cuando en realidad es el 칰ltimo.

Pero volvamos a nuestros gr치ficos. En la @fig-performance se ven muy peque침os, y hay un par que no hemos explicado:

1. **Posterior predictive check**: Este tipo de gr치ficos es uno de los m치s socorridos en inferencia Bayesiana, para comprobar que la distribuci칩n posterior obtenida por el modelo sea consistente con los datos observados. Aqu칤 no tenemos un modelo Bayesiano, pero s칤 que podemos utilizar la informaci칩n de la distribuci칩n de los par치metros (estimaci칩n y error est치ndar) para simular datos aleatorios. Si el modelo tiene una buena capacidad predictiva, entonces los datos observados $Y$ deben de caer dentro del 치rea comprendida por las l칤neas de los datos predichos $\hat{Y}$. En este caso, la capacidad predictiva del modelo no es del todo mala, solamente tenemos una sobre-estimaci칩n notable en valores cercanos a 10. Esto nos habla de la robustez del modelo lineal, pero no por ello hay que abusar de 칠l.

```{r}
plot(performance::check_predictions(reg1)) +
  see::theme_lucid()
```

2. **Linearity**: Es el gr치fico de residuales. Desafortunadamente no hay una forma de obtenerlo en una sola l칤nea, as칤 que tocar치 construirlo a mano y, de paso, modificar un par de cosas. La primera es graficar los residuales estandarizados (menos su media y divididos entre su desviaci칩n est치ndar) para poder darnos una idea de si tenemos valores extremos o no. La segunda es asignar una escala de colores para facilitarlo. Para poder hacer esto vamos a pasarle a `ggplot()` directamente el objeto de regresi칩n, y en `aes()` vamos a utilizar dos atributos ocultos del objeto `reg1`: `.fitted` con los valores predichos y `.stdresid` con los residuales estandarizados. En el gr치fico resultante podemos ver que hay varios valores con residuales altos (tonos rojizos), y algunos extremos (> 1.96). La curva LOESS no se ve exageradamente desviada de la l칤nea de referencia, lo cual indica que un modelo lineal puede no ser tan mala elecci칩n.

```{r}
# Inicializar el espacio gr치fico
ggplot(data = reg1, # Objeto de regresi칩n
       # Datos ajustados y residuales estandarizados
       aes(x = .fitted, y = .stdresid,
           colour = .stdresid)) +
  # Gr치fico de dispersi칩n
  geom_point(size = 3,
             alpha = 0.7,
             show.legend = F) +
  # Referencia en 0
  geom_hline(yintercept = 0,
             colour = "black",
             linetype = "dashed") +
  # Referencia loess
  geom_smooth(method = "loess",
              colour = "#3aaf85") +
  # Gradiente de colores
  # "#cd201f": color para los extremos (rojo)
  # "#1b6ca8": color para el punto intermedio (0)
  # Escala de -2 a 2 (deber칤a ser 1.96)
  # oob: 쯤u칠 hacer con datos fuera de los l칤mites?
  # scales::squish : marcarlos como si estuvieran en el l칤mite
  scale_color_gradient2(low = "#cd201f",
                        midpoint = 0, 
                        mid = "#1b6ca8", 
                        high = "#cd201f",
                        breaks = c(-2, 0, 2),
                        limits = c(-2, 2),
                        oob = scales::squish) +
  labs(title = "Linearity",
       subtitle = "Reference line should be flat and horizontal",
       y = "Standardized residuals",
       x = "Fitted values") +
  see::theme_lucid()
```

3. **Homogeneity of variance**: Homocedasticidad/Heterocedasticidad. Con un poco de imaginaci칩n puedes trazar un >, indicando que hay una mayor varianza en los valores peque침os que en los valores grandes. La prueba @BreuschPagan_1979, por otra parte, sugiere que la varianza es homog칠nea. Este es un t칤pico caso donde conviene errar en el lado de la precauci칩n y profundizar en el an치lisis antes de sacar una conclusi칩n. 쯈u칠 pasa con nuestro > si quitamos el punto extremo con un residual estandarizado > 4? Se vuelve mucho menos marcado, 쯡o? Posiblemente sea eso lo que est치 viendo la prueba y que no est칠 siendo enga침ada por ese punto. Dada la estructura de los datos (muchos acumulados en valores peque침os y pocos en valores grandes), yo decidir칤a que no se cumple el supuesto de homogeneidad de varianzas.

```{r}
reg1_homoced <- performance::check_heteroscedasticity(reg1)
plot(reg1_homoced) +
  see::theme_lucid()
reg1_homoced
```

4. **Influential Observations**: Este es otro gr치fico diagn칩stico que no hab칤a mencionado porque no est치 directamente relacionado con los supuestos. Este gr치fico se conoce como **gr치fico de apalancamiento**, y se침ala observaciones que pudieran estar "enga침ando" o afectando de manera importante la estimaci칩n de la recta. En otras palabras, que "jalen" la recta hacia ellos, por estar extremadamente lejos de la tendencia central de $y$ para ese punto $x$. En menos palabras: nos permite identificar "outliers". Hay una gran cantidad de m칠todos, y la funci칩n `performance::check_outliers()` califica cada valor con una nota compuesta por el promedio de los resultados binarios ("outlier" o no, 1 o 0) de cada m칠todo. Representa la probabilidad de que cada observaci칩n sea clasificada como "outlier" por al menos un m칠todo. Se considera un "outlier" si su calificaci칩n es superior o igual a 0.5 (l칤neas verdes punteadas); es decir, vamos a buscar puntos que est칠n fuera del "cono" formado por los contornos. En este caso ninguno est치 fuera del contorno, pero el punto 1 se ve sospechoso.

```{r}
reg1_outliers <- performance::check_outliers(reg1)
plot(reg1_outliers)
```

5. **Normality**: Por 칰ltimo, el gr치fico de normalidad.  No es de sorprender que tengamos "desviaciones de la normalidad" bastante marcadas en algunos puntos, especialmente cerca de la cola derecha de la distribuci칩n. La prueba de normalidad de los residuales tambi칠n rechaza que se cumpla el supuesto.

```{r}
reg1_norm <- performance::check_normality(reg1)
plot(reg1_norm, type = "qq")
reg1_norm
```

Independientemente de mi "bullying" al $R^2$, ahora ya sabes todo lo que implica hacer una regresi칩n lineal simple, y que es mucho m치s que simplemente picar botones en alguna suite estad칤stica o utilizar la funci칩n `lm` en `R` o alguna otra funci칩n en otro lenguaje de programaci칩n.

### Predicci칩n vs. Interpretaci칩n

Ahora bien, mencion칠 en varias ocasiones cosas relacionadas con la "predicci칩n" y la "interpretaci칩n". Pues resulta que, como vimos arriba, para fines predictivos no importan demasiado los supuestos, y antes de que agarres un trinche y una antorcha escucha lo que tengo que decir. Antes te dije que la regresi칩n forma parte del aprendizaje automatizado supervisado y, como tal, su principal (por no decir 칰nico) objetivo es la predicci칩n. Un modelo de regresi칩n exitoso es un modelo que pueda predecir adecuadamente, punto. 쯏 la interpretaci칩n? Esa es otra cara de la moneda. De hecho, est치n inversamente correlacionadas, en el sentido de que entre m치s poderosa es una t칠cnica, menos interpretable es. De todos estos detalles vamos a hablar m치s adelante en el @sec-clasif, pero por el momento quiero que te quedes con lo siguiente:

::: {.callout-important}
La validaci칩n de supuestos es solo necesaria si nos interesa explicar los par치metros del modelo, no si solo nos interesan sus predicciones.
:::

Sin ir demasiado lejos, el modelo que construimos arriba no cumple con el supuesto de normalidad  (los dem치s est치n en la cuerda floja) y a칰n as칤 las predicciones posteriores (en el *posterior predictive check*) se ven bastante aceptables. Desafortunadamente para nostros, usualmente nos interesa la interpretaci칩n, as칤 que hay que hacer la tarea completa.

쯏 si solo me interesan las predicciones? Bueno, igual hay que verificar algunas cosas, pero eso lo veremos en el @sec-clasif.

### M치xima verosimilitud

Bueno, ya sabemos c칩mo aplicar, interpretar, y validar los supuestos de una regresi칩n lineal en `R` utilizando el m칠todo de m칤nimos cuadrados, pero antes te mencion칠 que tambi칠n hab칤a otros m칠todos entre los que se encuentra el **ajuste por m치xima verosimilitud**. Entonces es necesario explicar qu칠 es la verosimilitud para luego ver c칩mo maximizarla, 쯡o crees?

Recordemos por un momento lo que revisamos en el @sec-probabilidad sobre la probabilidad. Dijimos que, cuando tenemos resultados mutuamente excluyentes y exhaustivos (todos los resultados), la suma de todas sus probabilidades es exactamente 1, tal que:

$$
\sum_i^n p_i == 1
$$

Hasta aqu칤 todo bien, pero 쯗칩nde entra la **verosimilitud**? Si buscas en el diccionario de la Real Academia Espa침ola te vas a encontrar con una de sus siempre 칰tiles definiciones: "Cualidad de veros칤mil", por lo que hay que definir veros칤mil: "**que tiene apariencia de verdadero**". Eso ya tiene m치s sentido. En un escenario de investigaci칩n nosotros podemos plantear m칰ltiples hip칩tesis, que no son necesariamente excluyentes entre s칤, entonces no podemos simplemente utilizar la probabilidad. Entendamos la verosimilitud con un ejemplo:

Imagina que alguien a quien conoces te dice que uno de sus amigos tiene poderes ps칤quicos. T칰, como persona de ciencia, decides ponerlo a prueba. Acuerdan una reuni칩n y le pones un "desaf칤o" simple: vas a lanzar diez volados, y el debe de adivinar el resultado. Al final, 칠l adivina correctamente 7/10 volados. Sin dejarte llevar por tu escepticismo planteas algunas hip칩tesis: i) simple coincidencia, el tama침o de muestra no es lo suficientemente grande; ii) la moneda no es del todo "justa", sino que tiende a caer m치s hacia cierto lado; iii) esta persona tiene una visi칩n cin칠tica sobre-humana y puede ver qu칠 es lo que est치 arriba antes de que atrapes la moneda, iv) esta persona realmente tiene algo de clarividente. 쮺u치l crees que sea m치s veros칤mil? Espero que me digas que la primera hip칩tesis, especialmente despu칠s de lo que vimos en el @sec-probabilidad. Si por el contrario hubieran sido 1000 lanzamientos y hubiera adivinado 700, la historia ser칤a otra, pero con 7/10 puede ser un capricho del mundo. Eso que hicimos fue, justamente, un ajuste por **m치xima verosimilitud**: **seleccionar la hip칩tesis m치s veros칤mil de entre un conjunto dado**, solo que vamos a cambiar hip칩tesis por valores de par치metros.

쮽ormalmente? Un ajuste por m치xima verosimilitud consiste en **estimar par치metros** de un modelo, dado un conjunto de observaciones, en donde se encuentra **valores que maximicen la verosimilitud de las observaciones dados los valores de los par치metros**. Puesto de otra manera, buscamos el conjunto de valores que maximicen la probabilidad de que nos hayamos encontrado nuestros datos, seg칰n el modelo que escogimos. Esto se parece mucho a lo que vimos en el @sec-ph0 sobre el nivel de significancia, solo que nuestro modelo "deja de ser una distribuci칩n de probabilidades" para ser un modelo de regresi칩n. 쯇or qu칠 las comillas? Porque nuestro error no puede quedar "suelto", pero en m치xima verosimilitud podemos trabajar con **cualquier distribuci칩n de probabilidad que se ajuste a nuestro problema**; de hecho, esto es lo que da lugar a los **modelos lineales generalizados**, pero eso lo veremos en el @sec-glm.

쮺칩mo lo llevamos a la pr치ctica? Primero quiero que te des la oportunidad de ver una relaci칩n interesante que puede ahorrarte mucho trabajo, o que puede abrirte la puerta a otro tipo de an치lisis.

#### M칤nimos cuadrados, m치xima verosimilitud e inferencia Bayesiana

Esta parte es completamente te칩rica y asumo que el ver procedimientos algebraicos no te supone un problema. De no ser as칤, puedes saltar al final para obtener la idea clave. Si decides que te interesa, vamos all치.

Recordemos que un problema de **regresi칩n** consiste en **estimar los par치metros de un modelo matem치tico dado un conjunto de observaciones**, y que tenemos una gran diversidad de formas de hacerlo. Comencemos hablando del m칠todo de ajuste m치s com칰n para una RLS: **m칤nimos cuadrados**. Como mencion칠 antes, con este m칠todo **minimizamos** la **funci칩n de p칠rdida cuadr치tica**; es decir:

\begin{align*}
\epsilon = y - \hat{y}\\
L = \epsilon^2
\end{align*}

Cuando utilizamos este m칠todo **asumimos** algunas cosas, entre ellas que nuestros **residuales** (no nuestra variable) se encuentran **normalmente distribuidos**. Aunque este m칠todo funciona, se prefiere utilizar m칠todos probabil칤sticos [@Gerrodette_2011], tal como la aproximaci칩n por **m치xima verosimilitud**. Antes utilizamos un ejemplo pr치ctico para definirla, pero ahora aproxim칠mosla desde el **teorema de Bayes**:

$$
p(\theta|x) = \frac{p(x|\theta)p(\theta)}{p(x)}
$$

Que podemos simplificar como:

$$
Posterior = \frac{verosimilitud \times previa}{evidencia}
$$

El teorema de Bayes es lo que da lugar al **parad칤gma de la inferencia Bayesiana**, el cual no vemos en este curso; sin embargo explicar el teorema es bastante sencillo: **쯤u칠 tan probable es una hip칩tesis ($\theta$), dada cierta evidencia (datos, $x$)? (probabilidad posterior, $p(\theta|x)$)**. Para responderlo vamos a obtener la **relaci칩n que hay entre qu칠 tan probable es la evidencia, dada la hip칩tesis ($p(x|\theta)$, nuestra verosimilitud), qu칠 tan probable creamos nosotros que es nuestra hip칩tesis (probabilidad previa, $p(\theta)$) y la probabilidad de la evidencia en s칤 misma ($p(x)$)**. Obtener la probabilidad de la evidencia es un tema en s칤 mismo (en realidad solo la aproximamos), as칤 que lo vamos a obviarla de la ecuaci칩n. Recordar치s que en la inferencia estad칤stica frecuentista partimos del hecho de que **no sabemos nada** sobre nuestro problema, y podemos entonces, al menos de manera te칩rica, establecer eso en el teorema de Bayes, lo cual nos lleva a **cancelar nuestros t칠rminos de previa y evidencia** y terminar con la siguiente equivalencia:

$$
Posterior \equiv verosimilitud
$$

Este **caso especial** de la **inferencia Bayesiana** tiene un nombre: **Estimaci칩n M치xima A posteriori** (Maximum A posteriori Estimate, MAP) y es **equivalente a la estimaci칩n puntual de un ajuste por m치xima verosimilitud**. 쯇or qu칠? Porque en esa aproximaci칩n tratamos de encontrar valores de nuestros par치metros que maximicen la verosimilitud de las observaciones, dados los par치metros. De manera matem치tica definimos la equivalencia como:

$$
p(x|\theta) \equiv L(\theta|x) \implies p(x_1, x_2, ..., x_n|\theta)
$$

Otro de nuestros supuestos en este paradigma es que las muestras son independientes entre s칤, por lo que podemos expandir nuestra probabilidad conjunta con $P(A,B) = P(A)P(B)$:

$$
L(\theta|x_1, x_2, ..., x_n) \equiv p(x_1|\theta)p(x_2|\theta),...,p(x_n|\theta)= \prod p(x_i|\theta)
$$

Y este t칠rmino es lo que queremos maximizar, por lo cual lo podemos escribir tal que:

$$
\begin{matrix} max \\ \theta \end{matrix}
\left\{ \prod p(x_i|\theta) \right\}
$$

El problema es que ni a nosotros ni a las computadoras nos gusta hacer multiplicaciones, por lo que podemos aplicar un logaritmo para convertir el productorio en una sumatoria. Recuerda: el logaritmo de un producto es igual a la suma del logaritmo de cada uno de sus componentes, por lo tanto:

$$
\begin{matrix} max \\ \theta \end{matrix}
\left\{ log \left( \prod p(x_i|\theta) \right) \right\} \implies
\begin{matrix} max \\ \theta \end{matrix}
\left\{ \sum_i^n log(p(x_i|\theta)) \right\}
$$

Esta 칰ltima parte era un poco innecesaria, nada m치s que un breviario cultural para que conocieras por qu칠 utilizamos logaritmos de verosimilitud, cosa que haremos m치s adelante, pero ahora vayamos al meollo del asunto:

:::{.callout-important}
Uno de los supuestos del ajuste por m칤nimos cuadrados es un error normalmente distribuido. En m치xima verosimilitud podemos ajustar nuestro error a cualquier distribuci칩n de probabilidad. Si utilizamos a la distribuci칩n normal como la distribuci칩n del error, entonces **m칤nimos cuadrados, m치xima verosimilitud (distribuci칩n normal) e inferencia Bayesiana (verosimilitud normal y previas muy planas)** dan **estimaciones equivalentes**.
:::

#### Ajuste por m치xima verosimilitud

En este punto puedes estar en uno de estos escenarios: a) lograste seguir toda la explicaci칩n y se te hizo l칩gica (si fue as칤, 춰felicidades! Eres un tan 침o침o o 침o침a como yo); b) seguiste la explicaci칩n y se te hizo l칩gica, pero no terminaste de entender el teorema de Bayes (igualmente, 춰felicidades! Vas para 침o침o/침o침a que chutas); c) lo le칤ste pero te perdiste solo con las ecuaciones (tambi칠n 춰felicidades!, tienes la intenci칩n de convertirte en 침o침o/침o침a); o d) saltaste directamente a lo importante (춰felicidades a ti tambi칠n! Tienes una vida 游). Si est치s en los casos c y d, y puede que b seguramente no est칠s del todo convencido de que m칤nimos cuadrados y m치xima verosimlitud con un error normal sean equivalentes. Si est치s en el caso a, te gustar칤a una demostraci칩n. 쯏 si no te interesa? Igual la vamos a hacer.

A diferencia de la implementaci칩n de una regresi칩n por m칤nimos cuadrados, ajustar el modelo mediante m치xima verosimilitud no es tan intuitivo. El primer paso es establecer manualmente nuestra funci칩n de verosimilitud, ajustando una distribuci칩n normal a los residuales:

```{r}
data <- df_reg1[c("v1", "v2")]
LL <- function(b0, b1, mu, sigma){
  # Encontrar los residuales. Modelo a ajustar
  R = data$v2 - data$v1 * b1 - b0
  
  # Calcular la verosimilitud. Residuales con distribuci칩n normal.
  
  R = suppressWarnings(dnorm(R, mu, sigma))
  
  # Sumar el logaritmo de las verosimilitudes para
  # todos los puntos de datos.
  -sum(log(R))
}
```

Ahora ajustemos el modelo que acabamos de crear, utilizando la funci칩n `stats4::mle(fun, start = list())` (*maximum likelihood estimation), donde `fun` es la funci칩n de verosimilitud a ajustar y `start` son los valores iniciales de los par치metros. En este paso lo que estamos haciendo es estimar los dos par치metros (media y desviaci칩n est치ndar) que mejor describen los datos:

```{r}
mle_fit <- mle(LL, start = list(b0 = 1, b1 = 1, sigma = 1), 
               fixed = list(mu = 0), 
               nobs = length(data$v2))

summary(mle_fit)
```

::: {.callout-note}
Hay que definir valores iniciales para los par치metros porque, a diferencia de por m칤nimos cuadrados, el proceso de minimizaci칩n del negativo de la suma del logaritmo de la verosimilitud es **iterativo** mediante un algoritmo de b칰squeda. El m치s com칰n es el algoritmo de b칰squeda de Newton-Raphson (o Newton-Fourier). 쮸 qu칠 me refiero con iterativo? A que la computadora variar치 los par치metros hasta llegar a la soluci칩n "optima":

![Ajuste iterativo de par치metros](imgs/iterative.gif){#fig-iterative}
:::

Esta salida fue un poco m치s simple que la salida de la funci칩n `lm()` pero, 쯙ue de diferente la estimaci칩n? Si vemos los coeficientes de nuestro ajuste por m칤nimos cuadrados veremos que la estimaci칩n puntual es la misma, y los errores est치ndares son pr치cticamente iguales:

```{r}
summary(reg1)$coefficients
```

Adem치s, el error est치ndar de la estimaci칩n por m칤nimos cuadrados es pr치cticamente igual al `sigma` de la estimaci칩n por m치xima verosimilitud:

```{r}
summary(reg1)$sigma
```

Moraleja: no utilices estimaci칩n por m치xima verosimilitud si vas a utilizar una distribuci칩n normal. Lo 칰nico que ganas es hacer pasos adicionales. 쮺칩mo utilizar otras distribuciones? Eso lo veremos a detalle en el @sec-glm.

## Correlaci칩n y covarianza

S칠 que la sesi칩n hasta este momento ha sido larga y tediosa, pero el tema de regresi칩n merece entrar a la teor칤a para no obtener conclusiones equivocadas. Dejemos de lado esa parte y cerremos hablando de dos conceptos relacionados: la correlaci칩n y la covarianza.

La **covarianza** nos indica **c칩mo var칤a una variable en relaci칩n a otra**. La **correlaci칩n** describe **la relaci칩n entre dos variables**. Ya me imagino la expresi칩n de confusi칩n que tienes mientras te preguntas 쯘ntonces son lo mismo? Pues no, la diferencia es que la **correlaci칩n** es un **칤ndice**; es decir, est치 contenida en el intervalo $[-1, 1]$, por lo que esta mide no solo la direcci칩n, sino la "fuerza" o el grado de linealidad de la relaci칩n, donde 0 es una relaci칩n lineal nula. Matem치ticamente la diferencia es que la correlaci칩n entre dos variables es su covarianza dividida entre el producto de sus desviaciones est치ndar:

\begin{align*}
cov(X,Y) = \frac{\Sigma_{i=1}^n(X_i-\bar{X})(Y_i-\bar{Y})}{n-1} \\
cor(X,Y) = \frac{cov(X,Y)}{\sigma_x * \sigma_y}
\end{align*}

Es decir, son conceptos que est치n muy relacionados entre s칤. En ambos el **signo** del valor indica la **direcci칩n de la relaci칩n**, mientras que la **magnitud** indica la **fuerza** de la relaci칩n. El problema con la covarianza es que no tiene l칤mites, entonces no puedes saber si una covarianza de 500 es particularmente grande salvo que tengas otra covarianza con la cual comparar, mientras que una correlaci칩n de 0.7 es una correlaci칩n moderadamente fuerte.

Muy seguramente por tu cabeza haya pasado la pregunta: "si ambas nos dicen c칩mo es la relaci칩n entre dos variables, 쯖u치l es la diferencia con la regresi칩n?". Pues que la **regresi칩n es un modelo predictivo**, mientras que la **correlaci칩n/covarianza es un estad칤stico descriptivo**. Aqu칤 no estamos comprometiendo que haya una tasa de cambio de $X$ hacia $Y$, ni estamos interesados en qu칠 valor de $X$ le corresponde a $y = 0$. Aqu칤 **no nos interesa predecir, sino describir**. Si no quieres comprometerte con todo lo que implica un modelo predictivo (a칰n nos falt칩 ver el tema del sobre-ajuste), solo calcula el **coeficiente de correlaci칩n** correspondiente.

쯇or qu칠 correspondiente? Porque tenemos m치s de una forma de calcular la correlaci칩n, y cada una tiene sus bemoles. La ecuaci칩n que vimos arriba es para el **coeficiente de correlaci칩n de Pearson**, el cual elevado al cuadrado nos da el coeficiente de determinaci칩n que vimos antes. Como tal, es un coeficiente **param칠trico** y tiene algunos supuestos:

1. **Variables en escala de intervalo o raz칩n**
2. **Relaci칩n lineal** entre ambas variables. S칤, asume que el cambio de una variable a otra es constante.
3. **Normalidad** Ambas variables deben de tener una distribuci칩n aproximadamente normal, por lo que aqu칤 s칤 nos interesa la distribuci칩n de nuestras variables.
4. Cada observaci칩n debe de tener el par de datos $(v1, v2)$.

Estos supuestos, a estas alturas, son autoexplicativos. 쯈u칠 pasa si no cumplimos con alguno de los primeros tres? Podemos utilizar la alternativa **no param칠trica**: el coeficiente de correlaci칩n $\rho$ de Spearman. En este los supuestos son:

1. **Variables al menos en escala ordinal**
2. **Relaci칩n mon칩tona entre ambas variables**; es decir, que la relaci칩n vaya en un solo sentido (conforme aumenta una aumenta la otra, o conforme aumenta la otra disminuye), independientemente de que la tasa de cambio de una a otra no sea constante.
3. Cada observaci칩n debe de tener el par de datos $(v1, v2)$

Bastante m치s relajado, 쯡o? Pero esto no quiere decir que solo debas de utilizar este coeficiente, utiliza el que m치s se ajuste a tus datos particulares.

::: {.callout-note}
En el @sec-nopar vamos a hablar de las t칠cnicas no param칠tricas y sus ventajas y desventajas con respecto a las t칠cnicas param칠tricas.
:::

Ahora bien, 쯖칩mo obtenemos estos coeficientes en `R`? Muy sencillo, con las funciones `cor(X, Y)` y `cov(X, Y)`:

::: {.callout-note}
Estos coeficientes son sim칠tricos, por lo que asignar variables $x$ y $y$ como dependientes e independientes es un error. 쯅otaste que arriba todo lo puse en t칠rminos de $v1$ y $v2$?
:::

Primero, generemos un par de variables donde la segunda sea una funci칩n lineal de la primera:

```{r}
df1 <- data.frame(v1 = -20:20)
df1["v2"] <- (10+2*df1$v1)
df1
```

Ahora obtengamos la covarianza:

```{r}
covar <- cov(df1$v1, df1$v2)
covar
```

Y el 칤ndice de correlaci칩n de `Pearson`:

```{r}
corre <- cor(df1$v1, df1$v2)
corre
```

Aqu칤 queda tambi칠n demostrado por qu칠 la covarianza es tan dif칤cil de interpretar por s칤 sola, pues en este caso con una relaci칩n lineal perfecta fue de 287, pero si cambias los valores de `v1` de alguna manera vas a obtener otro valor, mientras que la correlaci칩n seguir치 siendo 1. Gr치ficamente:

```{r}
ggplot(data = df1, aes(x = v1, y = v2)) +
  geom_point(colour = "dodgerblue4", alpha = 0.8) +
  labs(title = "Relaci칩n entre v2 y v1") +
  annotate("text", x = 15, y = 0,
           label = paste("R = ", round(corre, 2))) +
  annotate("text", x = 15, y = -5,
           label = paste("Cov. = ", round(covar, 2))) +
  see::theme_lucid()
```

쯈u칠 pasa cuando nos alejamos de esta relaci칩n lineal?

```{r}
df1["v3"] <- (-df1$v1^2)
corre <- cor(df1$v1, df1$v3)
covar <- cor(df1$v1, df1$v3)

ggplot(data = df1, aes(x = v1, y = v3)) +
  geom_point(colour = "dodgerblue4", alpha = 0.8) +
  labs(title = "Relaci칩n entre v2 y v1") +
  annotate("text", x = 15, y = 0,
           label = paste("R = ", round(corre, 2))) +
  annotate("text", x = 15, y = -50,
           label = paste("Cov. = ", round(covar, 2))) +
  see::theme_lucid()
```

Otro ejemplo:

```{r}
df1["v4"] <- sin(df1$v1)
corre <- cor(df1$v1, df1$v4)
covar <- cor(df1$v1, df1$v4)

ggplot(data = df1, aes(x = v1, y = v4)) +
  geom_point(colour = "dodgerblue4", alpha = 0.8) +
  labs(title = "Relaci칩n entre v2 y v1") +
  annotate("text", x = 15, y = 3,
           label = paste("R = ", round(corre, 2))) +
  annotate("text", x = 15, y = 2,
           label = paste("Cov. = ", round(covar, 2))) +
  expand_limits(y = c(5, -3)) +
  see::theme_lucid()
```

Aunque todas estas relaciones pueden ser predichas, un coeficiente de correlaci칩n lineal no es capaz de capturarlas y, por definici칩n, tampoco un modelo de regresi칩n lineal. 쯈u칠 hacer en estos casos? Veremos algunas alternativas en el @sec-nolin. Ahora ejemplifiquemos el coeficiente de correlaci칩n de Spearman. Primero, y para dejar m치s claro el concepto, veamos la diferencia entre una relaci칩n mon칩tona y una no mon칩tona:

```{r}
df2 <- data.frame(v1 = df1$v1[df1$v1 > 0], 
                  v2 = df1$v1[df1$v1 > 0]^2,
                  mono = "mon칩tona")

df2 <- rbind(df2, data.frame(v1= df1$v1, 
                             v2 = df1$v3, 
                             mono = "no mon칩tona"))

ggplot(data = df2, aes(x = v1, y = v2)) +
  geom_point(colour = "dodgerblue4", alpha = 0.8) +
  facet_wrap(~mono, nrow = 2, scales = "free_y") +
  see::theme_lucid() +
  theme(aspect.ratio = 1/1.61)
```
Y ahora calculemos los coeficientes de correlaci칩n de Pearson y Spearman para la relaci칩n mon칩tona. La diferencia fue de 0.3, lo cual no es muy grande, pero conforme nos empezamos a alejar de escenarios ideales el coeficiente de Pearson empieza a perder sensibilidad y confiabilidad.

```{r}
paste("Pearson = ",
      round((cor(df2$v1[df2$mono == "mon칩tona"],
                 df2$v2[df2$mono == "mon칩tona"], 
                 method = "pearson")), 2)
      )
paste("Spearman = ",
      round((cor(df2$v1[df2$mono == "mon칩tona"],
                 df2$v2[df2$mono == "mon칩tona"], 
                 method = "spearman")), 2)
      )
```

Ya para cerrar, 쯥on estos los 칰nicos coeficientes de correlaci칩n? Para nada, tenemos tambi칠n: $\tau$ de Kendall, $\phi k$ (@Baaketal_2019), V de Cramer, [Predictive Power Score](https://bit.ly/PPS_medium) y el coeficiente de M치xima Informaci칩n (@Reshefetal_2011). Te invito a que leas m치s sobre ellos, sus ventajas y sus desventajas. Predictive Power Score no es direccional, por ejemplo (@fig-pps).

![Predictive Power Score en una relaci칩n parab칩lica](imgs/pps.png){#fig-pps}

Por fin llegamos al final de esta extensa (pero espero no aburrida) sesi칩n. Espero que la hayas encontrado 칰til, y que te motive a hacer toda la chamba detr치s de un modelo de predicci칩n como lo es la regresi칩n lineal simple.

## Ejercicio

Aunque me encantar칤a que para esta sesi칩n no hubiera ejercicio, es importante que practiques lo que discutimos aqu칤. El ejercicio que vas a realizar es realizar una regresi칩n entre las variables `LT` (longitud total) y `AM` (altura m치xima) de los datos de peces haem칰lidos de los datos `Haem.csv`. Responde:

1. La regresi칩n asume un cambio direccional entre las variables $X$ y $Y$; es decir, el cambio en una modifica a la otra. 쮺u치l es la variable dependiente? `LT` o `AM`? 쯇or qu칠?
2. Calcula la correlaci칩n entre ambas variables. 쯈u칠 coeficiente utilizas y por qu칠?
3. Realiza la regresi칩n lineal simple. 쯈u칠 m칠todo de ajuste utilizas y por qu칠?
4. Reporta los resultados de la regresi칩n (incluyendo el gr치fico correspondiente).
5. Realiza la comprobaci칩n de los supuestos. 쮼s confiable el modelo para fines de predicci칩n? 쯏 para interpretaci칩n?










