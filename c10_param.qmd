# Técnicas paramétricas

## Librerías
```{r}
#| message: false
library(ggplot2)
library(corrplot)
library(gridExtra)
library(rstatix)
library(dplyr)
library(ggpubr)
```

## Introducción

Por fin dejaremos atrás los fundamentos de la estadística. Tal vez 10 sesiones con fundamentos se te hayan hecho largas, pero todos los conceptos que hemos visto tienen una razón de ser. No podía hablar de pruebas de hipótesis sin que antes supiéramos qué es la probabilidad y qué representa. No podíamos empezar a aplicar procedimientos estadísticos sin antes revisar las bases de `R`. No podíamos hacer visualizaciones efectivas sin hablar de las heurísticas correspondientes. En fin, creo que ya me entiendes. A partir de la siguiente sesión vamos a revisar solo la teoría correspondiente a cada prueba/técnica, asumiendo que ya cuentas con las bases que establecimos en las sesiones anteriores.

## ¿Por qué paramétricas?

Lo primero que tenemos que abordar es ¿por qué se les conoce como pruebas paramétricas? Porque las inferencias que vamos a realizar son sobre los **parámetros poblacionales**, usualmente la media ($\mu$) y la varianza ($\sigma$). Ejemplos de pruebas paramétricas tenemos muchos, entre los más famosos tenemos la $t$ de Student que revisamos en la sesión anterior, los análisis de la varianza (ANOVAs) que revisaremos en esta sesión, la correlación de Pearson y la regresión lineal que veremos en la siguiente, entre otros.

::: {.callout-note}
Nota para breviario cultural: todas las pruebas que mencioné con anterioridad son aplicaciones del modelo lineal general. ¿Eso qué significa? Que se asume que los grupos son separables linealmente, o que la relación entre dos o más variables es lineal.
:::

Independientemente de la prueba o de la técnica, todas las técnicas paramétricas tienen al menos tres supuestos; *i.e*, requieren de al menos tres cosas:

1. Que la **distribución muestreal del valor de interés sea normal**.
2. Que las **varianzas sean homogéneas**.
3. Que las muestras sean independientes.

La tercera sale sola si planteamos bien nuestro diseño experimental; es decir, que cada dato sea "independiente" de los demás. En otras palabras, que no sean mediciones de los mismos individuos, por ejemplo, o que las respuestas de cada individuo subsecuente no dependan del que estamos midiendo ahorita. Los primeros dos requieren de un poco más de explicación, así que vayamos paso a paso y hablemos de cada uno, empezando por la mayor tortura: el supuesto de normalidad.

## Supuesto de Normalidad

Existe por ahí un artículo titulado "Normalidad estadística y biología: una relación tortuosa". Si bien es cierto que el cómo se desarrollaron los análisis, y por lo tanto sus conclusiones, no tiene ni pies ni cabeza, ni ningún tipo de fundamento, el título me parece bastante llamativo porque desde que nos empezamos a aproximar a las pruebas de hipótesis se nos habla del supuesto de normalidad o de que "nuestros datos deben de ser normales". Bueno, hablemos a fondo de este supuesto, de qué es, qué no es, y por qué es importante.

### Versión corta

Si no quieres entrar en demasiados detalles puedes quedarte con lo que voy a decir aquí y seguir con tu camino. Si te interesa conocer un poco más puedes leer después la versión larga.

En pocas palabras, el supuesto de normalidad se lee "La distribución **muestral** de la **media** es **normal**". Si nosotros realizamos muestreos con tamaños de muestra $n$ y para cada uno estimamos la media, conforme $n \rightarrow \infty$ la distribución de esa media se volverá normal. Esa es la **distribución muestreal de la media**. Esta es una relación no controversial pues, aunque con $n \geq 30$ prácticamente lo garantizamos tenemos un problema: debemos de generalizar para cualquier $n$ y simplificar, lo que nos lleva a que "las poblaciones muestreadas sigan una distribución normal". Esto puede no ser práctico probarlo, pero sí que podemos utilizar nuestra muestra y ver si esta proviene de una distribución normal; *ergo*, aplicamos una prueba de normalidad para ver si nuestros datos se ajustan o no a una distribución normal.

### Versión larga

Posiblemente la versión corta haya sido demasiado corta, y fue a propósito, pues me interesa que le des al menos una hojeada a la versión larga.

Volvamos al asunto de que estamos tratando con la **distribución muestral de la media**. Esto es un problemón, porque una muestra **NO** contiene información sobre ella. A final de cuentas, una distribución muestral es el producto de distintas muestras, entonces debemos de llenar esos "vacíos" de información utilizando el centro, la dispersión y otros estadísticos descriptivos, y **asumimos** una distribución normal. ¿Por qué una distribución normal? Podría ser extremadamente breve y decirte que porque muchísimos atributos se distribuyen naturalmente de manera estadísticamente normal, pero en realidad es debido a otro concepto: el Teorema del Límite Central.

#### Teorema del Límite Central

Este teorema establece que: "Dadas muestras aleatorias e independientes con N observaciones cada una, la distribución de sus medias se aproxima a una distribución normal conforme N incrementa, INDEPENDIENTEMENTE de la distribución poblacional"; es decir, mientras N sea grande, $\bar{x} \sim Normal$. Para probar esto podemos hacer un ejercicio en el cual simulemos una población con distribución Gamma, cuya zona de mayor densidad se encuentra desplazada a la izquierda:

```{r}
set.seed(0)
datos <- data.frame(x = 1:1000, y = rgamma(1000, 1))
gamma <- ggplot(data = datos, aes(y)) + 
         geom_density(fill = rgb(118,78,144,
                                 maxColorValue = 255),
                      alpha = 0.5, colour = "white") +
         theme_bw() +
         labs(title = "Distribución Gamma",
              x = element_blank(),
              y = element_blank()) +
         theme(text = element_text(colour = "gray40"))

#cairo_pdf("gamma.pdf", family = "Montserrat",
#          height = 5, width = 5*1.6, pointsize = 20)
gamma
#dev.off()
```

Con nuestra población definida, podemos seleccionar algunos tamaños de muestra, realizar 1000 muestreos aleatorios, obtener la media de cada muestreo y graficar su distribución. Primero para N = 3:

```{r}
N = 3
medias <- data.frame(x = 1:1000,
                     y = replicate(1000,
                                   mean(sample(datos$y, N))))


dist_n3 <- ggplot(data = medias, aes(y)) +
           geom_density(fill = "dodgerblue4",
                        alpha = 0.5, colour = "white") +
           theme_bw() +
           labs(title = sprintf("Distribución muestreal con N = %d", N),
                x = element_blank(),
                y = element_blank()) +
           theme(text = element_text(colour = "gray40"))

#cairo_pdf("n_3.pdf", family = "Montserrat",
#          height = 5, width = 5*1.6, pointsize = 20)
dist_n3
#dev.off()
```

Ahora para N = 10. La distribución se aproxima más a una distribución normal:

```{r}
N = 10
medias <- data.frame(x = 1:1000,
                     y = replicate(1000,
                                   mean(sample(datos$y, N))))

dist_n10 <- ggplot(data = medias, aes(y)) +
            geom_density(fill = "dodgerblue4",
                         alpha = 0.5, colour = "white") +
            theme_bw() +
            labs(title = sprintf("Distribución muestreal con N = %d",
                                 N),
                 x = element_blank(),
                 y = element_blank()) +
            theme(text = element_text(colour = "gray40"))

#cairo_pdf("n_10.pdf", family = "Montserrat",
#          height = 5, width = 5*1.6, pointsize = 20)
dist_n10
#dev.off()
```

Con N = 30 la distribución es más cercana a una normal que a la gamma, por lo que usualmente se acepta que: con N≥30 la distribución muestreal de la media DEBERÁ ser normal:

```{r}
N = 30
medias <- data.frame(x = 1:1000,
                     y = replicate(1000, mean(sample(datos$y, N))))


dist_n30 <- ggplot(data = medias, aes(y)) +
            geom_density(fill = "dodgerblue4",
                         alpha = 0.5, colour = "white") +
            theme_bw() +
            labs(title = sprintf("Distribución muestreal con N = %d",
                                 N),
                 x = element_blank(),
                 y = element_blank()) +
            theme(text = element_text(colour = "gray40"))

#cairo_pdf("n_30.pdf", family = "Montserrat",
#           height = 5, width = 5*1.6, pointsize = 20)
dist_n30
#dev.off()
```

Para comprobar, hagámos el ejercicio con una distribución uniforme; es decir, en la cual todos los valores tienen la misma probabilidad de ser obtenidos (desviaciones debido al generador de números "aleatorios"):

```{r}
N = 30
datos <- data.frame(x = 1:1000, y = runif(1000))
unif <- ggplot(data = datos, aes(y)) + 
        geom_density(fill = rgb(118,78,144,
                                maxColorValue = 255),
                     alpha = 0.5, colour = "white") +
        theme_bw() +
        labs(title = "Distribución \"uniforme\"",
             x = element_blank(),
             y = element_blank()) +
        theme(text = element_text(colour = "gray40"))

#cairo_pdf("unif.pdf", family = "Montserrat",
#           height = 5, width = 5*1.6, pointsize = 20)
unif
#dev.off()
```

```{r}
medias <- data.frame(x = 1:1000,
                     y = replicate(1000,
                                   mean(sample(datos$y, N))))

dist_n30 <- ggplot(data = medias, aes(y)) +
            geom_density(fill = "dodgerblue4",
                         alpha = 0.5,
                         colour = "white") +
            theme_bw() +
            labs(title = sprintf("Distribución muestreal con N = %d",
                                 N),
                 x = element_blank(),
                 y = element_blank()) +
            theme(text = element_text(colour = "gray40"))

#cairo_pdf("n_30u.pdf", family = "Montserrat",
#          height = 5, width = 5*1.6, pointsize = 20)
dist_n30
#dev.off()
```

Un aspecto importante a considerar es la "Primera Propiedad Conocida" de la distribución normal: *dadas muestras aleatorias e independientes con N observaciones cada una (tomadas de una distribución normal), la distribución de medias muestreales es normal e insesgada (i.e., centrada en la media poblacional), independientemente del tamaño de N*. Por lo tanto, aún con un tamaño de muestra de 1 debería dar una distribución parecida a la normal. Comprobemos:

```{r}
N = 1
datos <- data.frame(x = 1:1000, y = rnorm(1000))
norm <- ggplot(data = datos, aes(y)) + 
        geom_density(fill = rgb(118,78,144,
                                maxColorValue = 255),
                     alpha = 0.5,
                     colour = "white") +
        theme_bw() +
        labs(title = "Distribución Normal",
             x = element_blank(),
             y = element_blank()) +
        theme(text = element_text(colour = "gray40"))

#cairo_pdf("norm.pdf", family = "Montserrat",
#          height = 5, width = 5*1.6, pointsize = 20)
norm
#dev.off()
```

```{r}
medias <- data.frame(x = 1:1000,
                     y = replicate(1000,
                                   mean(sample(datos$y, N))))

dist_n1 <- ggplot(data = medias, aes(y)) +
           geom_density(fill = "dodgerblue4",
                        alpha = 0.5, colour = "white") +
           theme_bw() +
           labs(title = sprintf("Distribución muestreal con N = %d",
                                N),
                x = element_blank(),
                y = element_blank()) +
           theme(text = element_text(colour = "gray40"))

#cairo_pdf("n_1.pdf", family = "Montserrat",
#          height = 5, width = 5*1.6, pointsize = 20)
dist_n1
#dev.off()
```

La implicación de esta propiedad es que entre menos "normal" (en términos de su distribución estadística) sea nuestra población de estudio, necesitaremos un mayor tamaño de muestra para que nuestra distribución muestral de la media sea normal. El problema surge cuando nos debemos de enfrentar a tamaños de muestra pequeños (n < 30). 

Aunque siempre podemos asumir (literalmente) que nuestra población se encuentra normalmente distribuida y "capitalizar en la robustez del modelo estadístico subyacente", abusando del TLC, o reconocer que tamaños de muestra más pequeños nos pueden acercar lo suficiente (n > 30 es para casos extremos) podemos hacerlo mejor. La tercera opción es la evaluación formal, la cual consiste en hacer una prueba de bondad de ajuste para conocer si nuestros datos se desvían o no de una distribución normal teórica. Antes de entrar a esos métodos, analicemos la prueba de bondad de ajuste más conocida: la prueba $\chi^2$ de independencia.

## Pruebas de bondad de ajuste

### $\chi^2$ de independencia

Esta prueba nos permite probar si la distribución de nuestros datos (frecuencias de variables nominales) son iguales a una distribución teórica. El ejemplo más sencillo lo tenemos al evaluar si la distribución de sexos en una población es diferente de 1:1. En este caso, la distribución de nuestros datos es binomial (dos categorías, verdadero/falso, éxito/fracaso, macho/hembra, etc.). En nuestro muestreo contamos 142 machos y 190 hembras. Coloquemos esos datos en un objeto y realicemos la prueba:

```{r}
sexos <- c(machos = 142, hembras = 190)
sex_chi <- chisq.test(sexos)
sex_chi
```

Veamos la distribución teórica gráficamente y veamos la ubicación del estadístico de prueba:

```{r}
chi_data <- data.frame(x = rchisq(1000, 1))

chisq_plot <- ggplot(data = chi_data, aes(x)) +
              geom_density(fill = rgb(118,78,144,
                                      maxColorValue = 255),
                           alpha = 0.5, colour = "white") +
              geom_vline(xintercept = sex_chi$statistic,
                         color = "firebrick") +
              annotate(geom = "text",
                       x = sex_chi$statistic+1.1, y = 1,
                       label = sprintf("X^2 = %.2f",
                                       round(sex_chi$statistic, 2))) +
              theme_bw() +
              labs(title = sprintf("Distribución X^2 teórica (g.l = %d)",
                                   sex_chi$parameter),
                   x = element_blank(),
                   y = element_blank()) +
             theme(text = element_text(colour = "gray40"))
#cairo_pdf("chi_plot.pdf", family = "Montserrat",
#           height = 5, width = 5*1.6, pointsize = 20)
chisq_plot
#dev.off()
```

Partiendo del valor de p podemos concluir que la proporción fue diferente de nuestro modelo teórico 1:1, pero ¿qué pasa si nos interesara comprobar si es diferente a otra proporción, por ejemplo 40% machos y 60% hembras? En ese caso únicamente debemos de proporcionar un vector `p` en el cual establezcamos la probabilidad correspondiente a cada grupo:

```{r}
chisq.test(sexos, p = c(0.4, 0.6))
```

Aquí nuestros datos no ridiculizan a nuestra hipótesis de nulidad, por lo que no podemos rechazarla. Un ejemplo más complejo es el de la presentación, en donde tratamos de probar si el proceso de vacunación hizo alguna diferencia en el estado de salud de los empleados o, en otras palabras, ¿la incidencia de pneumonía fue la misma, INDEPENDIENTEMENTE de si los empleados se vacunaron o no? Al igual que en el caso anterior, coloquemos los datos en un objeto:

```{r}
vacunas <- data.frame(no_vacuna = c(23, 8, 61),
                      vacuna = c(5, 10, 77),
                      row.names = c("neumococo", "otra_neumonia",
                                    "sin_neumonia"))
vacunas
```

Ahora apliquemos la prueba:

```{r}
vacs <- chisq.test(vacunas)
vacs
```

Como era de esperarse al ver las frecuencias, la incidencia de pneumonía aparentemente no fue la misma entre los empleados vacunados y los que no se vacunaron. En este caso, podemos extraer aún más información, tal y como la dependencia entre las variables. Para ello accederemos al atributo residuals de la salida de chisq.test, el cual representa los residuales de Pearson para cada celda:

```{r}
vacs$residuals
```

Valores positivos muestran una asociación positiva entre las variables correspondientes; es decir, la incidencia de neumonía por neumococo aumentó (signo positivo) en aquellos empleados que no fueron vacunados y viceversa, valores negativos muestran una asociación negativa; es decir, la incidencia disminuyó en aquellos que sí fueron vacunados. Si nuestro interés fuera saber qué tanto contribuyó cada celda al valor de $\chi^2$ podemos elevar cada residual al cuadrado y dividirlo entre el valor de $\chi^2$ observado, tal que:

```{r}
contrib <- 100*((vacs$residuals^2)/vacs$statistic)
contrib
```

Evidentemente, los residuales más grandes tuvieron la mayor contribución que, en este caso, estuvo dada por la incidencia de neumonía por neumococo en ambos grupos. Podemos ver estos resultados de manera gráfica utilizando la librería corrplot:

```{r}
corrplot::corrplot(contrib, is.corr = F)
```

Ahora que tenemos una idea sobre cómo funcionan las pruebas de bondad de ajuste, podemos regresar a hablar sobre las pruebas de normalidad.

### Supuesto de Normalidad

Como imaginarás, las pruebas de normalidad son pruebas de bondad de ajuste en donde la distribución teórica es una distribución normal, aunque el modo en el cual se evalúan las desviaciones de la normalidad (*i.e.*, las diferencias) es diferente para cada prueba. Para aplicarlas, utilizaremos la base de datos de muestras independientes del archivo `datos_t`, particularmente la columna DC:

```{r}
dc <- openxlsx::read.xlsx("datos/datos_t.xlsx", sheet = 1)
dc
```

Podemos hacer una primera valoración utilizando un gráfico de densidad con un gráfico de densidad normal teórico superpuesto:

```{r}
set.seed(0)
norm_plot <- ggplot(data = dc, aes(DC)) +
             geom_density(fill = rgb(118,78,144,
                                     maxColorValue = 255),
                          colour = "white", alpha = 0.5) +
             stat_function(fun = dnorm, n = 100,
                           args = list(mean = mean(dc$DC),
                                       sd = sd(dc$DC))) +
             # Límites expandidos para visualizar el
             # kde normal "completo".
             # El kde observado se encuentra extendido más allá
             # de los límites de los datos:
             xlim(c(40, 65)) + 
             theme_bw() +
             labs(title = "Gráfico de densidad de DC vs. normal teórica",
                  x = element_blank(),
                  y = element_blank()) +
             theme(text = element_text(colour = "gray40"))

#cairo_pdf("norm_plot.pdf", height = 5, width = 5*1.6, pointsize = 20)
norm_plot
#dev.off()
```

¿Qué opinas? Apliquemos ahora las pruebas de normalidad:

#### Prueba de Shapiro-Wilk
La prueba más conocida para evaluar la normalidad de un conjunto de datos es la prueba de Shapiro-Wilk. Su estadístico de prueba (W) se calcula de una manera poco amigable, pero conceptualmente implica ordenar los valores de la muestra y evaluar las desviaciones (diferencias) con respecto a la media, la varianza y su covarianza (este concepto se retoma más adelante) esperadas. En pocas palabras, la covarianza indica cuánto cambia una variable (la media) con respecto a otra (la varianza). 

¿Qué tiene que ver la covarianza con el Supuesto de Normalidad? Tiene que ver con la Segunda Propiedad Conocida de la Distribución Normal, la cual establece que *Dadas observaciones aleatorias e independientes (de una distribución normal), la media muestral y la varianza muestral son independientes*. En otras palabras, cuando tomas una muestra y la usas para estimar tanto la media como la varianza de la población, qué tanto puedes equivocarte sobre la media es independiente de qué tanto puedes equivocarte sobre la varianza. Esta es una característica **única** de la distribución normal y es una de las razones por la que la prueba de S-W es de las más (por no decir la más) utilizada y recomendada, especialmente para muestras pequeñas. En algunos estudios de simulación como [este](http://www.ukm.my/jsm/pdf_files/SM-PDF-40-6-2011/15%20NorAishah.pdf) ha demostrado ser más sensible a las desviaciones de la normalidad que la prueba de Kolmogorov-Smirnov, aunque antes de explicarla apliquemos la prueba de S-W:

```{r}
shapiro.test(dc$DC)
```

El valor de p no nos permite rechazar nuestra hipótesis de nulidad a un $\alpha = 0.05$, por lo que podemos concluir que los datos se ajustan a una distribución normal. Vuelve al gráfico de densidad normal, ¿qué opinas?

Como añadido, visualicemos la segunda propiedad conocida de la distribución normal:

```{r}
means <- NA
sds <- NA

for (i in 1:1000) {
  norm_data <- rnorm(10)
  means[i] <- mean(norm_data)
  sds[i] <- sd(norm_data)
}

mean_sd <- data.frame(mean = means, sd = sds)

prop_2 <- ggplot(data = mean_sd, aes(x = mean, y = sd)) +
          geom_point(color = "dodgerblue4", size = 2, alpha = 0.5) +
          theme_bw() +
          labs(title = 
                 "Segunda Propiedad Conocida de la Distribución Normal",
               subtitle = "1000 muestreos de una población normal",
               x = "Media",
               y = "Desviación Estándar") +
          theme(text = element_text(colour = "gray40"))
#cairo_pdf("prop_2.pdf", family = "Montserrat",
#          height = 5, width = 5*1.6, pointsize = 20)
prop_2
#dev.off()
```

Con una distribución Gamma:

```{r}
means <- NA
sds <- NA

for (i in 1:1000) {
  gamma_data <- rgamma(10, shape = 1)
  means[i] <- mean(gamma_data)
  sds[i] <- sd(gamma_data)
}

mean_sd <- data.frame(mean = means, sd = sds)

prop_g <- ggplot(data = mean_sd, aes(x = mean, y = sd)) +
          geom_point(color = "dodgerblue4", size = 2, alpha = 0.5) +
          theme_bw() +
          labs(title =
                 "Segunda Propiedad Conocida de la Distribución Normal",
               subtitle = "1000 muestreos de una población gamma",
               x = "Media",
               y = "Desviación Estándar") +
          theme(text = element_text(colour = "gray40"))

#cairo_pdf("prop_g.pdf", family = "Montserrat",
#           height = 5, width = 5*1.6, pointsize = 20)
prop_g
#dev.off()
```

Ejericio: Realiza el mismo gráfico para la columna DC y para la columna CH.

#### Prueba Kolmogorov-Smirnov

A diferencia de la prueba S-W, la prueba K-S compara las función de densidad acumulada empírica (observada) vs. una función de densidad acumulada teórica (no necesariamente normal), lo cual causa que sea sensible a desviaciones en el centro de la distribución pero no en las colas; sin embargo, es importante mencionar, que la prueba K-S es convergente; es decir, que conforme $N \rightarrow \infty$ la prueba converge a la "respuesta verdadera" en términos de probabilidad. Esta razón hace que esta prueba no se recomiende para tamaños de muestra pequeños. Para aplicarla:

```{r}
ks.test(dc$DC, "pnorm")
```

A diferencia del caso anterior, esta prueba si tuvo evidencia suficiente para ridiculizar nuestra hipótesis nula, por lo que podemos concluir que nuestros datos no se ajustan a una distribución normal. Vuelve nuevamente al gráfico KDE. ¿Qué opinas?

Veamos las densidades acumuladas:

```{r}
# Generamos una cdf normal teórica:
cdf <- data.frame(norm = rnorm(1000,
                               mean = mean(dc$DC),
                               sd = sd(dc$DC)))

# Graficamos una vs. la otra:
cdfplot <- ggplot(data = dc, aes(DC)) +
           stat_ecdf(geom = "step",
                     colour = rgb(118, 78, 144,
                                  maxColorValue = 255),
                     alpha = 1) +
           stat_ecdf(data = cdf, aes(norm),
                     geom = "line", colour = "black") +
           theme_bw() +
           labs(title = "Densidades acum. empírica y teórica para DC",
                x = element_blank(),
                y = element_blank())

#cairo_pdf("cdf.pdf", family = "Montserrat",
#          height = 5, width = 5*1.6, pointsize = 20)
cdfplot
#dev.off()
```

Conjuntando con el gráfico kde original podemos ver por qué la prueba K-S arrojó un resultado significativo, ya que hubo desviaciones importantes en la zona central. Interpretar correctamente un gráfico CDF NO es sencillo y requiere de experiencia, por lo que únicamente lo incluí para acompañar a la prueba que se basa en la densidad acumulada.

Habiendo explicado dos de las pruebas de normalidad más comunes, pasemos a los análisis paramétricos. El primero de ellos lo revisamos durante la clase de pruebas de hipótesis: la prueba t de Student, por lo que pasaremos directamente al Análisis de la Varianza.

## Análisis de la Varianza

En términos simples, podemos pensar en el ANOVA como una extensión de la prueba t-Student a más de dos grupos a comparar. Durante la clase de Comparaciones Multivariadas abordamos el riesgo que conlleva realizar múltiples pruebas de hipótesis (comparaciones) en nuestros datos; es decir, el problema de realizar dos o más comparaciones entre grupos como si se tratara de pruebas independientes. Por el momento, solo ten en mente que se incrementa la posibilidad de obtener un falso positivo únicamente por azar, por lo que hay que utilizar una técnica adecuada y es ahí donde entra el ANOVA o, mejor dicho, los ANOVAs. Como te imaginarás, estas pruebas nos permiten comparar medias entre más de dos grupos, aunque aquí la comparación se realiza de manera global y la hipótesis alternativa se expresa como "Al menos una de las medias es diferente". Esto quiere decir que el ANOVA no nos dirá entre qué par(es) de grupos se encontraron las diferencias, sino que habrá que acompañarlo de una prueba post-hoc. Esta prueba es la prueba de diferencias honestas (HSD) de Tukey, la cual se encuentra basada en la distribución de los rangos estudentizados y fue diseñada para no incrementar la probabilidad de falsos positivos al realizar múltiples comparaciones. En esta sesión revisaremos tres modaliades de ANOVA: de una vía, de dos vías y factorial, de menor a mayor complejidad, aunque estos no son los únicos. Entre los demás diseños de ANOVA se encuentran el ANOVA de medidas repetidas (estudios de crecimiento en laboratorio con medidas intermedias entre el inicio y el final, por ejemplo) o el ANOVA anidado, en el cual el diseño es similar a una muñeca rusa.

Antes de aplicar y explicar los modelos de ANOVA, es necesario desarrollar una intuición sobre el procedimiento. El nombre "Análisis de Varianza" viene de que, literalmente, se utilizan las varianzas para comparar las medias. Aunque el proceso matemático implica calcular promedios de promedios, varias sumas de cuadrados y cuadrados medios del error, podemos resumirlo para fines prácticos en que la comparación se realiza mediante una razón/cociente, tal que:

$$F = \frac{\sigma^2_{entre}}{\sigma^2_{dentro}}$$

Sé que esto puede sonar muy poco intuitivo, pero si nos detenemos un poco a analizar la ecuación podemos darle mucho sentido. La varianza dentro de los grupos podemos considerarla como la varianza "promedio" de cada grupo (razón por la que es importante que estas sean homogéneas entre todos nuestros grupos), mientras que la varianza entre los grupos representa la "separación" (dispersión) entre los grupos (sin considerar el error). Partiendo de esto, es evidente que si la varianza entre grupos es muy grande en relación a la varianza dentro de los grupos podemos inferir que existe un efecto del factor de agrupamiento pues "no hay" (ojo a las comillas y los supuestos) otra forma de que las distribuciones de los grupos se desplacen.

Gráficamente la varianza dentro de los grupos se representaría de la siguiente manera:

```{r}
anov_sim <- data.frame(grupo = as.factor(c(rep("A", 1000),
                                           rep("B", 1000))),
                       y = c(rnorm(1000, mean = 10, sd = 1),
                             rnorm(1000, mean = 20, sd = 1)))

dentro_plot <- ggplot(data = anov_sim,
                      aes(y, fill = grupo, alpha = 0.5)) +
               geom_density(trim = T, show.legend = F,
                            colour = "white") +
               theme_minimal() +
               labs(title = "Varianza dentro de los grupos",
                    x = element_blank(),
                    y = element_blank()) +
               scale_y_continuous(labels = NULL) +
               xlim(c(5, 25))
dentro_plot
```

Mientras que la varianza entre los grupos podemos, para fines de interpretación, visualizarla como la varianza dada por ambos grupos. En realidad esto representaría la varianza total y la varianza entre los grupos es el resultado de eliminar la varianza dada por el error, pero sigamos con el ejemplo:

```{r}
anov_sim$tot <- rnorm(200, mean = 15, sd = sd(anov_sim$y))
entre_plot <- ggplot(data = anov_sim, aes(tot)) + 
              geom_density(fill = "dodgerblue4",
                           alpha = 0.5, colour = "white") +
              theme_minimal() +
              labs(title = "Varianza entre los grupos",
                   x = element_blank(),
                   y = element_blank()) +
              scale_y_continuous(labels = NULL) +
              xlim(c(5, 25))
entre_plot
```

Visualizándolas como si de un cociente se tratara es posible darse cuenta cómo la varianza "entre" los grupos es mucho mayor que la varianza dentro de los grupos, lo cual daría un valor de la razón de varianzas muy alto, sugiriendo un efecto del factor de agrupamiento.

```{r}
#cairo_pdf("anova_plot.pdf", family = "Montserrat",
#           height = 5, width = 5*1.6, pointsize = 20)
gridExtra::grid.arrange(entre_plot, dentro_plot)
#dev.off()
```

Veamos qué pasa cuando las medias son más cercanas entre sí:

```{r}
anov_sim2 <- data.frame(grupo = as.factor(c(rep("A", 1000),
                                            rep("B", 1000))),
                        y = c(rnorm(1000, mean = 10, sd = 1),
                              rnorm(1000, mean = 11, sd = 1)))
anov_sim2$tot <- rnorm(2000, mean(10.5), sd(anov_sim2$y))
dentro_plot2 <- ggplot(data = anov_sim2,
                       aes(y, fill = grupo, alpha = 0.5)) +
               geom_density(trim = T, show.legend = F,
                            colour = "white") +
               theme_minimal() +
               labs(title = "Varianza dentro de los grupos",
                    x = element_blank(),
                    y = element_blank()) +
               scale_y_continuous(labels = NULL) +
               xlim(c(5, 15))
entre_plot2 <- ggplot(data = anov_sim2, aes(tot)) + 
              geom_density(fill = "dodgerblue4",
                           alpha = 0.5, colour = "white") +
              theme_minimal() +
              labs(title = "Varianza entre los grupos",
                   x = element_blank(),
                   y = element_blank()) +
              scale_y_continuous(labels = NULL) +
              xlim(c(5, 15))
#cairo_pdf("anova_plot2.pdf", family = "Montserrat",
#           height = 5, width = 5*1.6, pointsize = 20)
gridExtra::grid.arrange(entre_plot2, dentro_plot2)
#dev.off()
```

### Supuesto de homogeneidad de Varianzas

Como podrás imaginar, el que las varianzas de los grupos no sean homogéneas generará un sesgo al momento de calcular el cociente y, en consecuencia, el nivel de significancia de la prueba. Esto es lo que da origen al Supuesto de Homogeneidad de Varianzas. Existe una gran diversidad de pruebas, cada una con sus consideraciones, fortalezas y desventajas, pero analizaremos únicamente las (posiblemente) más conocidas.

#### Prueba de Bartlett

La prueba de Bartlett se considera como la prueba Uniformemente Más Poderosa; es decir, la que es menos propensa a cometer un falso negativo para cualquier valor de $\alpha$. Este poder, sin embargo, tiene sus bemoles o su bemol, mejor dicho. Esta prueba se apoya TOTALMENTE en que la variable de interés en cada factor se encuentra normalmente distribuída (¡Hola de nuevo, Supuesto de Normalidad!). De violarse este supuesto el valor de $\alpha_v$ (verdadero) para la prueba puede ser mayor o menor al definido por nosotros ($\alpha_n$, nominal). De manera particular, si la distribución de la variable analizada presenta una curtosis negativa el $\alpha_v$ será menor al nominal, mientras que con una curtosis positiva será el caso contrario. Esto lleva a que hagamos una prueba más o menos estricta de lo que habíamos planeado originalmente y que nuestros resultados no sean confiables. De cualquier manera, veamos cómo aplicarla:

```{r}
bartlett.test(y~grupo, data = anov_sim)
```

En este caso, no podemos ridiculizar nuestra hipótesis de nulidad, por lo que podemos concluir que las varianzas entre nuestros grupos son homogéneas (y deben serlo, pues así las especificamos).

#### Prueba de Levene

Es la alternativa recomendada por muchos a la prueba de Bartlett. Aunque no es tan poderosa, sí es robusta a las violaciones al supuesto de normalidad, de modo que el $\alpha$ verdadero es muy similar al nominal para una gran cantidad de distribuciones, aunque es insensible a distribuciones simétricas con colas altas como la t de Student o doble exponencial (también conocida como distribución de Laplace). Aplicarla también es sumamente sencillo:

```{r}
car::leveneTest(y~grupo, data = anov_sim)
```

Como era de esperarse, el resultado es consistente con la prueba de Bartlett para este caso.

### ANOVA de una sola vía

Habiendo revisado los conceptos básicos detrás del ANOVA, podemos pasar a aplicar algunos modelos. El más sencillo es el ANOVA de una sola vía, el cual es el caso más sencillo; es decir, comparamos una sola variable numérica entre los niveles de un solo factor (pesos finales para tres alimentos distintos, por ejemplo). Para ejemplificarlo utilizaremos la base `datos1` que se trabajó para la tarea de Intervalos de confianza, con una columna extra: id, el cual es un identificador para cada individuo. Esta columna fue añadida únicamente para ejemplificar un caso de ANOVA posterior. En este ejemplo, compararemos los pesos totales entre los tres periodos (OJO: este es un diseño para un ANOVA factorial, únicamente lo utilizaremos como ejemplo).

El primer paso es, evidentemente, cargar la base de datos:

```{r}
df <- read.table("datos/Datos1 2.csv", header = F, skip = 1, sep = ",")
colnames(df) <- c("Dieta", "Periodo", "Rep", "LT", "PT", "id")
df$Periodo <- factor(df$Periodo, levels = c("I", "M", "F"))
head(df)
```

#### Comprobación de supuestos

El segundo paso es la comprobación de supuestos. Primero el de Normalidad. Tenemos varios grupos, por lo que las **pruebas de normalidad son para cada grupo**.

```{r}
#Normalidad
## Data.frame a llenar
norm <- data.frame(grupo = NA, W = NA, p = NA)

## Niveles a probar:
lvls <- levels(df$Periodo)

for (i in seq_along(lvls)) {
  temp <- shapiro.test(df$PT[df$Periodo == lvls[i]])
  norm[i,] <- c(lvls[i], temp$statistic, temp$p.value)
}
norm
```

La prueba de S-W sugiere que no hay desviaciones significativas de la normalidad. Corroboremos con un gráfico de violín. Al parecer, los resultados son coherentes con la distribución de los datos.

```{r}
ggplot(data = df, aes(x = Periodo, y = PT, fill = Periodo)) +
  geom_violin(alpha = 0.5, show.legend = F) +
  labs(title = "Distribución de PT en los tres momentos de medición",
       x = element_blank(),
       y = element_blank()) +
  theme_bw()
```

Ahora el supuesto de igualdad de varianzas, utilizando la prueba de Levene. Podemos concluir que las varianzas no son homogéneas, por lo que la recomendación sería recurrir a una prueba no paramétrica; sin embargo, sigamos con el ejercicio y escalando la complejidad del análisis antes de saltar apresuradamente a conclusiones.

```{r}
car::leveneTest(PT~Periodo, data = df)
```

#### Aplicación del ANOVA

El siguiente paso es aplicar el ANOVA. El valor de p es bastante bajo, lo cual ridiculiza nuestra hipótesis de nulidad y concluimos que al menos un par de medias son significativamente diferentes entre sí (F(2, 155) = 574.3; p < 0.0001).

```{r}
una_via <- aov(PT~Periodo, data = df)
summary(una_via)
```

Te has de preguntar, ¿las medias de qué grupos son diferentes? Eso no lo podemos saber con un ANOVA, sino que hay que aplicar una **prueba post-hoc**: una prueba que evalúe las diferencias entre cada par de grupos.

#### Prueba post-hoc

El último paso es aplicar la prueba post-hoc. Esta prueba se construye a partir de la distribución de rangos estudentizados, y fue diseñada para evitar el conflicto entre el $\alpha$ y el número de comparaciones, por lo que la interpretación del valor de p es directa (sin correcciones).

::: {.callout-important}
¿Por qué no aplicar una prueba $t$ para cada par de grupos? Porque en ese caso inflaríamos nuestro $\alpha$. Hablaremos más de esta relación y algunas correcciones al valor de p en la sesión de [hipótesis multivariadas](). Por el momento quédate con que cada prueba para comparar múltiples grupos (tipo ANOVA) tiene su prueba *post-hoc*.
:::

En este caso, el valor de p fue muy pequeño para las tres comparaciones, por lo que rechazamos nuestra hipótesis de nulidad en los tres casos. El resto de la tabla es también informativo, pues nos indica la magnitud de las diferencias y sus intervalos de confianza (tal y como en la prueba t de Student):

```{r}
TukeyHSD(una_via)
```

Con esos 4 pasos terminamos nuestro ANOVA de una vía. Pasemos entonces al ANOVA de dos vías.

### ANOVA de dos vías

Si una vía es a un factor, dos vías es a dos factores. En este análisis compararemos el efecto de ambos factores simultáneamente, pero de manera independiente; es decir, aunque se hará la comparación para ambos, no se considerará la interacción entre ellos. 

::: {.callout-note}
¿Qué es la interacción? Eso lo veremos más adelante, pero se resume a que el efecto de un factor dependa del efecto de otro.
:::

Nuestro segundo factor será la Dieta. Los pasos son exactamente los mismos que en el anterior:

#### Comprobación de Supuestos

Dado que ya comprobamos los supuestos para el factor Periodo, solo habrá que hacerlo para el factor Dieta:

```{r}
#Normalidad
df$Dieta <- factor(df$Dieta, levels = c("A", "B", "C"))
## Data.frame a llenar
norm <- data.frame(grupo = NA, W = NA, p = NA)

## Niveles a probar:
lvls <- levels(df$Dieta)

for (i in seq_along(lvls)) {
  temp <- shapiro.test(df$PT[df$Dieta == lvls[i]])
  norm[i,] <- c(lvls[i], temp$statistic, temp$p.value)
}
norm
```

Debido a que el factor dieta incluye el efecto del periodo de medición y detectamos diferencias entre ellos, es esperable que no se cumpla el supuesto de normalidad. En este caso, el diseño sería mejor analizado utilizando un ANOVA factorial que uno de dos vías pero, al igual que en el caso anterior, seguiremos únicamente para fines ilustrativos.

Para la homogeneidad de varianzas la interpretación es la misma, aunque la consecuencia es la contraria. No violamos el supuesto de homogeneidad de varianzas debido a que tampoco se violó entre los periodos. Esto da un poco más de respaldo a seguir con el análisis, pues es más robusto a la violación del supuesto de normalidad que al de homogeneidad de varianzas.

```{r}
car::leveneTest(PT~Dieta, data = df)
```

#### Aplicación del ANOVA.

El ANOVA de dos vías es un caso especial del ANOVA factorial, en el cuál únicamente hay dos factores y NO se considera su interacción, por lo que el modo de declararlo es una fórmula en la cuál los factores se consideran de manera aditiva: `Respuesta~Factor1+Factor2`. La forma tradicional de reportar los resultados de este ANOVA sería: hubo un efecto significativo de las dietas (F(2, 153) = 11.45; p < 0.0001) y de los periodos (F(2, 153) = 560.42; p < 0.0001).

```{r}
dos_vias <- aov(PT~Dieta+Periodo, data = df)
summary(dos_vias)
```

#### Prueba post-hoc. 

En este caso tuvimos valores de p muy pequeños para ambos factores, realicemos la prueba HSD de Tukey. Al ver la salida puedes interpretar que esta es una lista, y que podríamos acceder a los resultados de cualquier factor utilizando el operador `$` (`TukeyHSD(aov_obj)$factor`). Aquí, las diferencias se encontraron entre la dieta C y las otras dos, pero no entre A y B. 

```{r}
TukeyHSD(dos_vias)
```

Considerando el diseño factorial de la base de datos, ¿cómo interpretarías estos resultados? ¿podemos confiar en ellos? La respuesta que yo esperaría es que no, pues si el experimento fue bien diseñado, al inicio todos los animales debían tener aproximadamente las mismas características y vimos tanto gráficamente como en ambos ANOVAs que hubo un crecimiento. Veamos qué pasa con las distribuciones utilizando un gráfico de interacción.

```{r}
ggplot(data = df, aes(x = Dieta, y = PT, fill = Periodo)) +
  geom_violin(alpha = 0.5, show.legend = T) +
  labs(title = "Distribución de PT en los tres momentos de medición",
       x = element_blank(),
       y = element_blank()) +
  theme_bw()
```

Es evidente que en los tres tratamientos hubo un crecimiento, el cual además parece haber sido bastante similar. Este es un ejemplo del error de tipo III que mencionaba en la clase de pruebas de hipótesis: utilizar la matemática correcta para responder la pregunta equivocada. Veamos qué pasa si realizamos un ANOVA factorial.

### ANOVA factorial

Como te podrás imaginar a partir de lo mencionado sobre el ANOVA de dos vías, este ANOVA es la versión más generalizada en la cual podemos utilzar más de dos factores y analizar su interacción. Sigamos con la base anterior, en este caso considerando también el factor réplica:

```{r}
df$Rep <- factor(df$Rep, levels = c("A", "B"))
```

#### Comprobación de supuestos

No hay sorpresas en ninguno de los dos casos, las interpretaciones de los resultados son las mismas que en el caso anterior; es decir, este NO es el modo correcto de comprobar la normalidad. Cuando hablemos del ANOVA de medidas repetidas veremos un ejemplo de cómo hacerlo de manera correcta (normalidad de un factor dados los niveles del otro factor).

```{r}
## Data.frame a llenar
norm <- data.frame(grupo = NA, W = NA, p = NA)

## Niveles a probar:
lvls <- levels(df$Rep)

for (i in seq_along(lvls)) {
  temp <- shapiro.test(df$PT[df$Rep == lvls[i]])
  norm[i,] <- c(lvls[i], temp$statistic, temp$p.value)
}
norm
```

```{r}
car::leveneTest(PT~Rep, data = df)
```

#### Aplicación del ANOVA

La única diferencia con el caso anterior es que esta vez utilizaremos el operador `*` para añadir los nuevos términos, en vez de hacerlo de forma aditiva. Haciendo esto la tabla del ANOVA cambia, en donde primero aparece el efecto de cada factor analizado de manera independiete (como si hubieramos hecho un ANOVA de "tres vías") y después los términos de interacción. La interacción entre dos factores representa un efecto combinado de los factores involucrados en la variable analizada; es decir, cuando hay interacción entre dos factores el efecto de uno "depende" del el nivel del otro. 

```{r}
fact <- aov(PT~Dieta*Periodo*Rep, data = df)
summary(fact)
```

#### Prueba post-hoc

En este caso el único término de interacción con resultados significativos es la interacción entre Periodo y Réplica (`Periodo:Rep`), lo cual indica que el comportamiento de los periodos fue diferente entre réplicas. Realicemos las pruebas post-hoc correspondientes. Aunque encontramos un efecto significativo de las réplicas, este factor únicamente tiene dos niveles, por lo que realizar la prueba post-hoc es ocioso y, por tanto, la realizaremos únicamente para Periodo:Rep. Nota que debido a la presencia del operador `:` en el nombre del término es necesario utilizar comillas para poder acceder a ese atributo:

```{r}
TukeyHSD(fact)$"Periodo:Rep"
```

Vemos que prácticamente todos los contrastes fueron significativos, con excepción del periodo inicial (p = 0.9). Esto sugeriría que el comportamiento de las réplicas no fue homogéneo a través del tiempo. Si regresamos brevemente a la tabla del ANOVA veremos que hubo 22 observaciones faltantes, las cuales corresponden a la mortalidad durante el experimento y podrían también explicar estos cambios. Debido a la impraciticidad/imposibilidad de marcar o identificar cada guppy no es posible aplicar un anova de medidas repetidas con estos datos; sin embargo, podemos ejemplificarlo con otros datos.

### ANOVA de medidas repetidas

El ANOVA de medidas repetidas es otro de los modelos de ANOVA, el cual podemos considerar como una extensión de la prueba t para muestras dependientes; es decir, en la cual los mismos individuos fueron medidos en más de dos ocasiones, denominado ANOVA de medidas repetidas de una vía. Si tenemos no solo los distintos tiempos de medición sino también factores adicionales entonces tendremos ANOVAs de medidas repetidas de dos vías (tiempo y un factor adicional) o de tres vías (tiempo y dos factores adicionales). Al igual que en el ANOVA "normal" comencemos desde abajo con el de una vía.

#### ANOVA de medidas repetidas de una vía

Carguemos los datos de ejemplo (selfesteem de la librería datarium), los cuales son una medida de autoestima medida en tres ocasiones distintas:

```{r}
selfesteem <- datarium::selfesteem
head(selfesteem)
```

La base se encuentra en formato corto, por lo que habrá que pasarla a formato largo:

```{r}
estima <- reshape2::melt(selfesteem, # Datos a modificar
                         # Identificador para cada individuo
                         id.vars = "id",
                         # Variables en columnas
                         measure.vars = c("t1", "t2", "t3"),
                         # Nombre de la nueva variable de agrupamiento
                         variable.name = "tiempo",
                         # Nombre de la nueva variable medida
                         value.name = "estima")
head(estima)
```

##### Comprobación de supuestos

Apliquemos entonces la prueba de Shapiro-Wilk a los datos agrupados, esta vez simplificando con `tidy`:

```{r}
# Toma el data.frame, agrúpalo por tiempo y para cada nivel
# aplica la función shapiro_test a la columna estima:
estima |>  group_by(tiempo) |>  shapiro_test(estima)
```

::: {.callout-warning}
Nota que la función es `shapiro_test`, la cual está contenida en `rstatix`, y no `shapiro.test` de `R` base. Si tratas de pasar `shapiro.test` te encontrarás con un error pues no está pensada para ser utilizada con `pipe`s.
:::

Ahora la homocedasticidad o, mejor dicho, el supuesto de esfericidad. Este supuesto es una "extensión" del supuesto de homogeneidad de varianzas. Definimos esfericidad como la condición en la que las varianzas de las **diferencias** entre todas las combinaciones de los niveles de interés son iguales. La violación de este supuesto conlleva un incremento en la probabilidad de un falso positivo; es decir, vuelve a la prueba demasiado "liberal" o "crédula". Aunque este supuesto es sumamente importante, no necesitamos probarlo directamente, pues la función con la que implementaremos el análisis hace la prueba correspondiente (prueba de Mauchly para esfericidad) y, además, aplica una corrección (corrección de Greenhouse-Geisser) a los grados de libertad de aquellos factores que violen el supuesto.

##### Aplicación del ANOVA

Esta vez no utilizaremos la notación de fórmula ni tan siquiera la función aov, sino que recurriremos a la función `anova_test()` de la librería `rstatix` para hacer más intuitiva la declaración, donde `data` es el data.frame con los datos (dah!), `dv` es la variable dependiente; es decir, nuestra variable a comparar, `wid` es un identificador único para cada individuo y `within` el factor dentro del cual queremos hacer las comparaciones:

```{r}
anova_rep1 <- rstatix::anova_test(data = estima,
                                  dv = estima, wid = id,
                                  within = tiempo)
get_anova_table(anova_rep1)
```

Lo primero que llama la atención es que la tabla de ANOVA reporta resultados para una prueba de tipo III. OJO! esto no tiene nada que ver con el error tipo III que mencioné en la clase de pruebas de hipótesis (ese error no es formal). El tipo de prueba hace referencia al tipo de ANOVA que se está realizando o, mejor dicho, al modo en el que se calculan las sumas de cuadrados. Si te interesa leer más al respecto, visita [este](https://www.ibm.com/docs/en/spss-statistics/29.0.0?topic=model-sum-squares) o [este](http://www.statsoft.com/textbook/) enlace. 

Otra cosa que debe llamar tu atención es el término `ges`. Este es el factor de corrección de Greenhouse-Geisser a los grados de libertad. El modo de reportar estos resultados sería algo como *"las medidas de autoestima a través del tiempo fueron significativamente diferentes ($F_{2,18}$ = 55.5, p < 0.0001; $\eta^2$ generalizado = 0.82)"*. El término $\eta^2$ generalizado lo puedes encontrar también como $\hat{\epsilon}$.

##### Prueba post-hoc

Debido a que las medidas son repetidas no podemos aplicar la prueba de diferencias honestas de Tukey, pero sí podemos aplicar pruebas t de Student pareadas y corregir el valor de p con una corrección de Bonferroni. En la sección de multivariado se abordará esta corrección, pero entiéndela en este momento como el modo de evitar que incrementemos la probabilidad de un falso positivo y, en consecuencia, deberemos de interpretar los valores de la columna p.adj. Viendo la tabla, es posible concluir que hubo diferencias entre las medidas de autoestima en los tres periodos.

```{r}
pwt <- pairwise_t_test(data = estima,
                       estima~tiempo, paired = T,
                       p.adjust.method = "bonferroni")
pwt
```

#### ANOVA de medidas repetidas de dos vías
Al igual que en el ANOVA "normal", hablamos de dos vías cuando tenemos dos factores, en este caso son el tiempo y alguno adicional. Para ejemplificarlo utilizaremos la base de datos `selfesteem2` de `datarium`.

```{r}
selfesteem2 <- datarium::selfesteem2
estima2 <- reshape2::melt(selfesteem2, # Datos a modificar
                         # Identificadores para cada individuo
                         id.vars = c("id", "treatment"),
                         # Variables en columnas
                         measure.vars = c("t1", "t2", "t3"),
                         # Nombre de la nueva variable de agrupamiento
                         variable.name = "tiempo",
                         # Nombre de la nueva variable medida
                         value.name = "estima")
head(estima2)
```

##### Comprobación de supuestos
Al igual que en el ANOVA de medidas repetidas, únicamente comprobaremos el Supuesto de Normalidad, solo que aquí lo haremos considerando la "anidación" de los factores; es decir, que las medidas repetidas fueron para cada tratamiento:

```{r}
estima2 |> group_by(treatment, tiempo) |> shapiro_test(estima)
```

Aparentemente hubo desviaciones de la normalidad en el t1 para el grupo control. Podemos corroborarlo con un gráfico QQ. Aparentemente es culpa de de algunos puntos ligeramente fuera del resto de la tendencia ubicados en el centro. Recordemos que el ANOVA es robusto a ciertas violaciones de la normalidad, y en este caso no parecen ser especialmente serias. Sigamos con el análisis.

```{r, fig.height= 7, fig.width=7/1.6}
ggpubr::ggqqplot(estima2, "estima", ggtheme = theme_bw()) +
  facet_grid(tiempo~treatment, labeller = "label_both")
```

##### Aplicación del ANOVA
Utilizaremos la misma estructura que en el caso anterior; la unica diferencia es que al argumento `within` le pasaremos un vector con dos factores:

```{r}
anova_rep2 <- anova_test(data = estima2,
                         dv = estima, wid = id,
                         within = c(treatment, tiempo))
get_anova_table(anova_rep2)
```

De la tabla podemos concluir que todos los contrastes fueron significativos, lo cual indica que hubo diferencias entre los tratamientos (F(1, 11), = 15.5; p = 0.02), entre los tiempos (F(1.31, 14.37), = 27.4; p < 0.0001) y tambien un efecto combinado (F(2,22) = 30.4; p < 0.0001). Debido a que los efectos principales ("solos") no son suficientes para describir los datos, el proceso post-hoc es un poco más complicado que en el caso anterior.

##### Pruebas post-hoc

Debido a la significancia del término de interacción es necesario descomponerlo en:

a) Efecto principal simple; es decir, un modelo de una vía de la primera variable para cada nivel de la segunda. Debido a que hacerlo a mano es un poco tedioso, encadenemos el proceso:

```{r}
anova_rep2_post1 <- estima2 |> 
                    # Agrupa la base por cada nivel de tiempo
                    group_by(tiempo) |>
                    # ANOVA de medidas repetidas para cada nivel
                    anova_test(dv = estima, wid = id, 
                               within = treatment) |>
                    # Extrae los resultados
                    get_anova_table() |> 
                    # Ajusta los valores de p
                    adjust_pvalue(method = "bonferroni")

anova_rep2_post1
```

b) Aplicar una prueba t de Student para datos dependientes en los términos significativos. Debido a que tratamiento tiene solo dos niveles, realizar este proceso es redundante; de hecho, los valores de p serán iguales a los mostrados atrás; sin embargo, hagámoslo con fines demostrativos:

```{r}
pwt_2 <- estima2 |>
         group_by(tiempo) |>
         pairwise_t_test(estima~treatment, paired = T,
                         p.adjust.method = "bonferroni")
pwt_2
```

Esto es todo para la clase de hoy. Es una clase bastante extensa y aún con ello se quedaron fuera algunas variantes de ANOVA; sin embargo, creo que estos cubren los casos más generales. ¡Nos vemos en la siguiente!