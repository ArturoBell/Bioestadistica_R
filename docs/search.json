[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bioestadística Aplicada con R y RStudio",
    "section": "",
    "text": "Prefacio\n¡Hola! Te doy la bienvenida al sub-curso de Técnicas Multivariadas, parte del curso de Bioestadística Aplicada con R y RStudio, ofertado por Dr. Plancton.\nEste es el libro de acompañamiento del curso. Aquí encontrarás tanto la teoría como el código que se aborda en el curso, dispuestos en una manera que facilita su lectura, y cuyo objetivo es simplemente proveer una versión lista para ser revisada en cualquier momento, sin necesidad de iniciar RStudio. Cada sección del libro tiene enlaces a los videos correspondientes, en caso de que prefieras ver y escuchar la explicación.\nTen en cuenta que puede existir un desfase entre el material del libro y los videos. La razón es que el material se actualizará para mejorar el contenido, la entrega o la explicación siempre que sea posible, lo cual es fácil de hacer en el libro y las libretas con el código, pero no en los videos; sin embargo, en el momento en el que el desfase sea lo suficientemente grande, también se actualizarán los videos de las secciones correspondientes."
  },
  {
    "objectID": "index.html#objetivos-de-aprendizaje",
    "href": "index.html#objetivos-de-aprendizaje",
    "title": "Bioestadística Aplicada con R y RStudio",
    "section": "Objetivos de aprendizaje",
    "text": "Objetivos de aprendizaje\nEl objetivo de este curso es que seas capaz no solo de implementar distintas técnicas de análisis de datos utilizando R, sino que también puedas ser crítico con tus resultados y que minimices, en la medida de lo posible, el sesgo algorítmico. En este curso aprenderás los fundamentos detrás de las pruebas vistas, en donde abordaremos la teoría desde un punto de vista práctico, buscando que puedas formarte una intuición propia. También trataremos de desmitificar el valor de p y, sobre todo, cómo no interpretarlo. En las partes más abstractas te adentrarás en el aprendizaje automatizado, y verás que hay vida más allá del \\(R^2\\) (regresiones) y la exactitud (clasificaciones). Adicionalmente, y no por ello menos importante, te adentrarás en la visualización de datos y aprenderás a realizar reportes que faciliten compartir y leer tu trabajo.\nUno de los errores más comunes al enseñar estadística con R es tratar de enseñar ambas cosas al mismo tiempo. En este curso, R es solo un medio y no un fin; es decir, no es un curso de programación en R tanto como es un curso de ciencia de datos aplicada; sin embargo, hay una amplia introducción al lenguaje al inicio, y espero que el explicar línea por línea el código te permita familiarizarte con el lenguaje. Dicho esto, el curso fue diseñado para que personas con nulo o muy poco conocimiento de programación en general puedan seguirlo, y siempre podrás contactarnos en caso de que no hayamos explicado algo adecuadamente."
  },
  {
    "objectID": "index.html#discord-y-acceso-al-material",
    "href": "index.html#discord-y-acceso-al-material",
    "title": "Bioestadística Aplicada con R y RStudio",
    "section": "Discord y acceso al material",
    "text": "Discord y acceso al material\nSi bien es cierto que puedes utilizar y acceder a todo el material del curso desde esta página, te recomiendo encarecidamente unirte al servidor de Discord utilizando tu enlace único (enviado a tu correo al registrarte), pues ahí podrás interactuar no solamente con tus compañeros y compañeras, sino también con tu profesor. Un último comentario, NO es necesario que instales el cliente de Discord en tu computadora o dispositivo móvil, aunque si lo haces podrás recibir las notificaciones sobre modificaciones que se hagan al material."
  },
  {
    "objectID": "index.html#estructura-del-libro",
    "href": "index.html#estructura-del-libro",
    "title": "Bioestadística Aplicada con R y RStudio",
    "section": "Estructura del libro",
    "text": "Estructura del libro\nEste libro es, como ya mencioné, el material utilizado en el curso y, por lo tanto, está dividido en distintas secciones, cada una formada por distintos capítulos (sesiones). Esta división trata de seguir un órden lógico, desde los conceptos y temas más fundamentales hasta los procedimientos más abstractos y, por lo tanto, todos los temas están conectados: cada capítulo asume que ya no tienes ningún problema con los anteriores.\nEn la esquina superior izquierda hay una barra de búsqueda, por si deseas realizar alguna consulta rápida. Debajo tienes todas las secciones y capítulos, en la porción central el contenido y a la derecha la tabla de contenidos del tema que estás leyendo. Referente al contenido, al inicio de cada capítulo tienes el video al tema correspondiete (ojo al desfase), mientras que en el texto encontrarás algunas anotaciones con información relevante al tema:\n\n\n\n\n\n\nNota\n\n\n\nAquí habrá información adicional al tema que se está tratando.\n\n\n\n\n\n\n\n\nTip\n\n\n\nAquí habrá consejos relacionados con la implementación de una técnica, o sobre su interpretación, o sobre la intuición detrás del tema que se esté abordando.\n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nAquí habrá puntos/conceptos/procedimientos con los cuales hay que tener mucho cuidado y la razón.\n\n\n\n\n\n\n\n\nImportante\n\n\n\nAquí habrá información que es sumamente importante que tengas presente.\n\n\nTambién encontrarás resaltados especiales para diferenciar claramente si estoy haciendo referencia a código en texto, funciones, librerías o software o a algunos conceptos clave. Todo el texto que veas con formato de hipervínculo te llevará a la referencia correspondiente: una figura, un capítulo, un enlace externo o una referencia bibliográfica."
  },
  {
    "objectID": "index.html#compartir-o-no-compartir-he-ahí-el-dilema",
    "href": "index.html#compartir-o-no-compartir-he-ahí-el-dilema",
    "title": "Bioestadística Aplicada con R y RStudio",
    "section": "¿Compartir o no compartir? He ahí el dilema",
    "text": "¿Compartir o no compartir? He ahí el dilema\nNo hay nada que te impida compartir este libro y con ello todo el curso. De hecho, nada me daría más gusto que saber que el curso te fue lo suficientemente agradable y útil como para dirigirlo a alguien más, pero te pido que lo hagas lo menos posible. Tu acceso al curso es vitalicio, pero ni el contenido ni el material son estáticos, por lo que dependemos de las nuevas ventas para poder seguir mejorándolo y mantenerlo en línea. Si quieres compartirlo puedes utilizar tu código de referido, el cuál da un descuento adicional a cualquier otro descuento o promoción.\nDicho esto, te doy nuevamente la bienvenida y espero que el curso cubra con tus expectativas (y un poco más)."
  },
  {
    "objectID": "s0_preparacion.html#r",
    "href": "s0_preparacion.html#r",
    "title": "Preparación",
    "section": "R",
    "text": "R\nEvidentemente, lo primero que deberás instalar es R. Puedes encontrar el instalador de la versión más reciente para tu sistema operativo en CRAN (The Comprehensive R Archive Network).\nSimplemente da click en el enlace al servidor más cercano a tu ubicación, por ejemplo https://cran.itam.mx/.\nDespués simplemente descarga la versión que corresponde a tu sistema operativo.\nIndependientemente del sistema operativo que utilices, la instalación consiste en ejecutar el instalador y seguir los pasos indicados en su ventana, dando click en aceptar y continuar según sea necesario.\n\n\n\n\n\n\nImportante\n\n\n\nEl material del curso fue desarrollado con la versión 4.2.1 “Funny-Looking Kid” de R. No puedo garantizar que el código funcione sin problemas con versiones previas."
  },
  {
    "objectID": "s0_preparacion.html#rstudio",
    "href": "s0_preparacion.html#rstudio",
    "title": "Preparación",
    "section": "RStudio",
    "text": "RStudio\nUna vez instalado R podemos instalar RStudio. Más adelante veremos cuál es la diferencia entre ambos, pero por el momento piensa en RStudio como una ventana para R. Primero, dirígete a https://posit.co/download/rstudio-desktop/.\n\n\n\n\n\n\nNota\n\n\n\n¿Por qué el enlace para descargar RStudio es de posit? En octubre del 2022 la empresa se volvió posit. Aquí puedes leer más al respecto, pero el resumen es que el nombre de la empresa cambió, pero el nombre de su ambiente integrado de desarrollo (IDE, RStudio) se mantendrá igual.\n\n\nEn la sección All Installers ubica tu sistema operativo y descarga el instalador correspondiente\nUna vez descargado, ejecuta el instalador y sigue los pasos que aparecen en pantalla."
  },
  {
    "objectID": "s0_preparacion.html#quarto-y-tinytex",
    "href": "s0_preparacion.html#quarto-y-tinytex",
    "title": "Preparación",
    "section": "Quarto y tinytex",
    "text": "Quarto y tinytex\nCon estas dos instalaciones sería más que suficiente para empezar a trabajar; sin embargo, podemos ir más allá y utilizar RStudio para crear reportes, artículos científicos, libros (incluyendo tesis), páginas web, presentaciones, y más. Esto solía (y puede) hacerse con librerías como rmarkdown, distilled o bookdown; sin embargo, tenemos la siguiente generación: Quarto. En el curso veremos una introducción para hacer reportes y, de hecho, este material fue escrito en documentos Quarto. Al igual que en los casos anteriores, dirígete a la página de descargas y descarga e instala la versión correspondiente a tu sistema operativo.\n\n\n\n\n\n\nImportante\n\n\n\nNO verás un ejecutable (acceso directo) después de que se haya realizado la instalación. Para utilizar Quarto necesitaremos de a) la línea de comandos o b) una interfaz, que en nuestro caso es RStudio. Debajo de los enlaces de descarga hay más información sobre cómo utilizarlo, incluyendo detalles sobre su uso en VS Code, Jupyter y en un editor de textos (línea de comandos). Puedes explorarlos, aunque aquí verás los detalles correspondientes para RStudio.\n\n\nAdicional a Quarto, y si deseas exportar tus documentos a archivos PDF, es necesario instalar una distribución de LaTeX. Si ya cuentas con alguna, puedes saltarte este paso, de lo contrario puedes o realizar la instalación completa o simplemente instalar TinyTeX desde aquí. Si optas por instalar TinyTeX y no tienes nada de experiencia con R, te recomiendo seguir los pasos de su página una vez que hayas pasado por el ?sec-bases-r. No te preocupes, hay un recordatorio al final de la sesión."
  },
  {
    "objectID": "s0_preparacion.html#instalaciones-opcionales",
    "href": "s0_preparacion.html#instalaciones-opcionales",
    "title": "Preparación",
    "section": "Instalaciones “opcionales”",
    "text": "Instalaciones “opcionales”\nAdicionalmente, te recomiendo encarecidamente (por no decir te solicito) que instales, dependiendo de tu sistema operativo, algunas herramientas. Tienes la descripción de cada una, así como desde donde realizar su instalación:\n\nWindows: Rtools. Esta es una serie de herramientas que nunca vas a ver cuando se utilicen, pero que son un dolor de cabeza si no cuentas con ellas y tienes la pésima fortuna de necesitar compilar un paquete desde su código fuente, o la dicha de querer publicar un paquete. A final de cuentas permiten justamente eso, administrar esas ejecuciones. Solía ser parte de la instalación de R en Windows, pero ya no es así, por lo que recomiendo la instales siguiendo las instrucciones oficiales (selecciona la versión más nueva de Rtools disponible).\nmacOS: XQuartz. R es un lenguaje con un enfoque muy fuerte hacia la generación de gráficos, lo cual permite hacer visualizaciones de muy alta calidad. Desafortunadamente, algunas cosas se apoyan del sistema de ventanas X.Org X Window System y que, de no estar disponible, pueden dar algunos dolores de cabeza. Apple solía incluir una implementación en la aplicación X11 en las versiones de OS X 10.5 a 10.7, pero ya no en sistemas más modernos. Es ahí donde entra el proyecto XQuartz. El instalador está disponible en la página del proyecto.\nmacOS: Xcode Command Line Tools. Si bien es cierto que macOS juega un papel importante en algunos círculos de desarrollo de software, no incluye algunas herramientas necesarias para el mismo objetivo que Rtools en Windows: compilar desde fuente. En este caso, hay dos formas de instalar lo que necesitamos, ambas provistas por Apple: a) instalar el entorno de desarrollo Xcode (40 GB) o, mi recomendación si no desarrollas aplicaciones para SOs de Apple, b) instalar las herramientas de línea de comando. Xcode lo puedes instalar directamente desde tu App Store, mientras que las herramientas de línea de comando desde el Terminal con el comando xcode-select --install. Puedes abrir el terminal utilizando Spotlight (CMD + espacio), o ejecutando su aplicación, ubicada en Aplicaciones -> Utilidades.\n\nUna vez instalado todo esto estás más que listo para avanzar con el programa del curso. Recuerda que si tuviste algún problema siempre puedes contactarme en el servidor de Discord del curso."
  },
  {
    "objectID": "s04_mv.html#objetivo-de-aprendizaje",
    "href": "s04_mv.html#objetivo-de-aprendizaje",
    "title": "Técnicas Multivariadas",
    "section": "Objetivo de aprendizaje",
    "text": "Objetivo de aprendizaje\nEn esta sección del curso llegarás al punto de máxima abstracción y te adentrarás en diversas técnicas que te permitirán obtener conclusiones a partir de datos multivariados. Comenzarás analizando las relaciones entre tus variables mediante matrices de varianzas/covarianzas, formarás agrupaciones, compararás las mediciones multivariadas entre grupos, realizarás clasificaciones y, por último, realizarás regresiones múltiples y verás cómo controlar la complejidad de tus modelos."
  },
  {
    "objectID": "c14_intromv.html#librerías",
    "href": "c14_intromv.html#librerías",
    "title": "1  Técnicas multivariadas: Introducción",
    "section": "1.1 Librerías",
    "text": "1.1 Librerías\n\nlibrary(ggplot2)\nlibrary(corrplot)\nlibrary(PerformanceAnalytics)\nlibrary(MVN)\nlibrary(vegan)"
  },
  {
    "objectID": "c14_intromv.html#introducción",
    "href": "c14_intromv.html#introducción",
    "title": "1  Técnicas multivariadas: Introducción",
    "section": "1.2 Introducción",
    "text": "1.2 Introducción\nEn esta sección final del curso vamos a abordar problemas que se tornan sumamente complejos, pues estaremos lidiando con más de dos variables a la vez. Esto usualmente deriva de que una variable sola no es suficientemene informativa, o de que el fenómeno analizado es el resultado de la “interacción” de distintas variables. De cualquier manera, esto nos lleva a tener múltiples mediciones de distintas instancias.\n\n\n\n\n\n\nNota\n\n\n\nEn esta sección nos vamos a referir a una instancia como un conjunto de mediciones de distintas variables de una misma unidad observacional. Pueden ser distintas morfometrías de un mismo cráneo, o mediciones satelitales de distintas variables ambientales en una coordenada dada (Figura 1.1).\n\n\n\nFigura 1.1: Ejemplos de problemas multivariados.\n\n\n\n\nAl momento de añadir nuevas variables vamos a modificar también el cómo trabajamos con nuestros datos. Mientras que con análisis uni o bivariados podíamos tener bases de datos en formato corto (una columna para cada grupo) o largo (una columna con la variable de respuesta y otra con la de agrupamiento), aquí vamos a tener matrices de datos, en donde cada renglón es una instancia (individuo, unidad muestral), y cada columna representa una variable o un atributo diferente:\n\n\n\nFigura 1.2: De columnas a matrices.\n\n\n\n1.2.1 Álgebra lineal\nEsto me lleva a un (tal vez) tedioso recordatorio o una (tal vez) tediosa explicación sobre el álgebra lineal. ¿La razón? El tratar con cada columna por separado no solo es extremadamente ineficiente, sino que también nos lleva a incrementar la probabilidad de cometer errores de tipo I si nos ponemos a hacer pruebas uni o bivariadas para cada par de columnas (hablaremos más de esto en el Capítulo 3), por lo que es mejor trabajar con todas las variables al mismo tiempo.\n\nVamos a ver muchas matrices, algunas muy extensas por incluir las operaciones aritméticas correspondientes. NO te preocupes, el objetivo es solo que añadas a tu breviario cultural de dónde salen las matrices de covarianza y correlación.\n\nSean dos matricesn \\(A\\) y \\(B\\), cada una con \\(m = 3\\) renglones y \\(n = 3\\) columnas:\n\\[\\begin{align*}\nA = \\left[\n  \\begin{matrix}\n  a_{1,1} & a_{1,2} & a_{1,3}\\\\\n  a_{2,1} & a_{2,2} & a_{2,3}\\\\\n  a_{3,1} & a_{3,2} & a_{3,3}\\\\\n  \n  \\end{matrix}\n  \\right] \\\\\nB = \\left[\n  \\begin{matrix}\n  b_{1,1} & b_{1,2} & b_{1,3}\\\\\n  b_{2,1} & b_{2,2} & b_{2,3}\\\\\n  b_{3,1} & b_{3,2} & b_{3,3}\\\\\n  \n  \\end{matrix}\n  \\right]\n\\end{align*}\\]\n\n\n\n\n\n\nImportante\n\n\n\nEn una matriz cada columna y cada renglón es un vector.\n\n\nTal y como vimos en el ?sec-bases-r, una de las operaciones más comunes al tratar con matrices es la transposición (denotada como \\(^T\\)); es decir, que las columnas se vuelvan renglones, y que los renglones se vuelvan columnas, tal que \\(A_{(m,n)} \\Rightarrow A^T_{(n,m)}\\):\n\\[\\begin{align*}\nA^T =\n\\left[\n\n\\begin{matrix}\na_{1,1} & a_{2,1} & a_{3,1}\\\\\na_{1,2} & a_{2,2} & a_{3,2}\\\\\na_{1,3} & a_{2,3} & a_{3,3}\\\\\n\n\\end{matrix}\n\\right]\\\\\nB^T =\n\\left[\n\n\\begin{matrix}\nb_{1,1} & b_{2,1} & b_{3,1}\\\\\nb_{1,2} & b_{2,2} & b_{3,2}\\\\\nb_{1,3} & b_{2,3} & b_{3,3}\\\\\n\n\\end{matrix}\n\\right]\n\\end{align*}\\]\nTambién podemos querer hacer operaciones básicas. La suma y resta de matrices está dada por cada elemento, tal que \\(S_{m,n} = (A_{m,n} \\pm B_{m,n})\\):\n\\[\\begin{align*}\nS = \\left[\n\\begin{matrix}\n(a_{1,1} \\pm b_{1,1}) &\n(a_{1,2} \\pm b_{1,2}) &\n(a_{1,3} \\pm b_{1,3})\n\\\\\n(a_{2,1} \\pm b_{2,1}) &\n(a_{2,2} \\pm b_{2,2}) &\n(a_{2,3} \\pm b_{2,3})\n\\\\\n(a_{3,1} \\pm b_{3,1}) &\n(a_{3,2} \\pm b_{3,2}) &\n(a_{3,3} \\pm b_{3,3})\n\\\\\n\n\n\\end{matrix}\n\\right]\n\\end{align*}\\]\n\n\n\n\n\n\nImportante\n\n\n\nPara hacer una suma o resta de dos matrices es necesario que \\((m_A,n_A) = (m_B,n_B)\\)\n\n\nLa multiplicación, desafortunadamente, no es tan simple. Aquí tenemos dos tipos de productos: el producto escalar (interno; \\(E_{m,n} = (A_m \\cdot B_n)\\)) y el producto externo. Solo vamos a explicar el primero porque es el que nos interesa para los fines del curso. En este la operación está dada como \\(E_{m,n} = \\sum a_{m} \\times b_n\\); es decir, el valor de cada celda de la matriz es el resultado de sumar los productos de los valores de la columna \\(m\\) por los valores del renglón \\(n\\):\n\\[\\begin{align*}\nE = \\left[\n\\begin{matrix}\n(\n(a_{1,1} \\times b_{1,1}) +  \n(a_{1,2} \\times b_{2,1}) +\n(a_{1,3} \\times b_{3,1})\n)&\n(\n(a_{1,1} \\times b_{1,2}) +  \n(a_{1,2} \\times b_{2,2}) +\n(a_{1,3} \\times b_{3,2})\n)&\n(\n(a_{1,1} \\times b_{1,3}) +  \n(a_{1,2} \\times b_{2,3}) +\n(a_{1,3} \\times b_{3,3})\n)\\\\\n(\n(a_{2,1} \\times b_{1,1}) +  \n(a_{2,2} \\times b_{2,1}) +\n(a_{2,3} \\times b_{3,1})\n)&\n(\n(a_{2,1} \\times b_{1,2}) +  \n(a_{2,2} \\times b_{2,2}) +\n(a_{2,3} \\times b_{3,2})\n)&\n(\n(a_{2,1} \\times b_{1,3}) +  \n(a_{2,2} \\times b_{2,3}) +\n(a_{2,3} \\times b_{3,3})\n)\\\\\n(\n(a_{3,1} \\times b_{1,1}) +  \n(a_{3,2} \\times b_{2,1}) +\n(a_{3,3} \\times b_{3,1})\n)&\n(\n(a_{3,1} \\times b_{1,2}) +  \n(a_{3,2} \\times b_{2,2}) +\n(a_{3,3} \\times b_{3,2})\n)&\n(\n(a_{3,1} \\times b_{1,3}) +  \n(a_{3,2} \\times b_{2,3}) +\n(a_{3,3} \\times b_{3,3})\n)\\\\\n\\end{matrix}\n\\right]\n\\end{align*}\\]\n\n\n\n\n\n\nImportante\n\n\n\nDebido a que estamos multiplicando los renglones (\\(m\\)) de una matriz por las columnas (\\(n\\)) de la otra, es necesario que \\(m_A = n_B\\).\n\n\n¿Para qué puse todos esos chorizos de operaciones matriciales? Créeme que no fue para presumir que puedo escribir matrices en LaTeX/Markdown, sino porque son la base de (posiblemente) las estructuras más importantes para las técnicas multivariadas: las matrices de covarianzas y de correlación."
  },
  {
    "objectID": "c14_intromv.html#de-sigma-a-sigma-la-matriz-de-covarianzas",
    "href": "c14_intromv.html#de-sigma-a-sigma-la-matriz-de-covarianzas",
    "title": "1  Técnicas multivariadas: Introducción",
    "section": "1.3 De \\(\\sigma\\) a \\(\\Sigma\\): La matriz de covarianzas",
    "text": "1.3 De \\(\\sigma\\) a \\(\\Sigma\\): La matriz de covarianzas\nEmpecemos por la covarianza pues, como recordarás del ?sec-rls, la correlación es solo un cociente de la covarianza y, de hecho, comenzemos recordando qué es matemáticamente la covarianza entre dos variables: la esperanza matemática (\\(E\\); el promedio, vamos) del producto de la diferencia de cada valor de cada variable menos la media de la variable:\n\\[\nCov(x,y) = E[(x-\\mu_x)(y-\\mu_y)]\n\\]\nSi la covarianza de \\(x\\) y \\(y\\) es el producto de centrar (retirarle la media) a cada variable, podemos ver a la varianza como la covarianza de una variable con respecto a sí misma:\n\\[\nCov(y,y) = Var(y) = \\sigma^2(y) \\Rightarrow E[(y-\\mu_y)(y-\\mu_{y})] = E[(y-\\mu_y)^2]\n\\] Estos son las covarianzas y varianzas poblacionales, pero recordarás que nosotros trabajamos con las varianzas (y por extensión covarianzas) muestrales, lo que se vería como:\n\\[\\begin{align*}\ns^2 = cov(y,y) = \\frac{\\sum_i^n(y_i-\\bar{y})^2}{n-1} \\\\\ncov(x,y) = \\frac{\\sum_i^n(x_i-\\bar{x})(y_i-\\bar{y})}{n-1}\n\\end{align*}\\]\nAl ser esto así podemos entonces calcular, al mismo tiempo, todas las covarianzas y varianzas. ¿Cómo? Siguiendo tres pasos:\n\nCentrar la matriz (\\(a_{m,n}-\\bar{a}_n\\))\nMultiplicar su transpuesta por la matriz original (\\(C^T_A \\cdot C_A\\)):\n\n\n\n\n\n\n\nImportante\n\n\n\nRecuerda que para realizar el producto interno de dos matrices necesitamos que \\(m_a = n_b\\), por lo que el orden de la multiplicación debe de ser, forzosamente, \\(C^T_A \\cdot C_A\\); es decir, la transpuesta por la original\n\n\n\\[\\begin{align*}\nC^T_A =\n\\left[\n\n\\begin{matrix}\n(a_{1,1} - \\bar{a}_1) &\n(a_{2,1} - \\bar{a}_1)\n\\\\\n(a_{1,2} - \\bar{a}_2) &\n(a_{2,2} - \\bar{a}_2)\n\\\\\n\n\\end{matrix}\n\\right]\n\n\\cdot\n\nC_A =\n\n\\left[\n\\begin{matrix}\n(a_{1,1} - \\bar{a}_1) &\n(a_{1,2} - \\bar{a}_2)\n\\\\\n(a_{2,1} - \\bar{a}_1) &\n(a_{2,2} - \\bar{a}_2)\n\\\\\n\n\\end{matrix}\n\\right]\n\n=\n\\end{align*}\\]\n\\[\\begin{align*}\n=\n\\left[\n\n\\begin{matrix}\n\n[((a_{1,1} - \\bar{a}_1)\n\\times\n(a_{1,1} - \\bar{a}_1))\n+\n((a_{2,1} - \\bar{a}_1)\n\\times\n(a_{2,1} - \\bar{a}_1))]\n\n&\n\n[((a_{1,1} - \\bar{a}_1)\n\\times\n(a_{1,2} - \\bar{a}_2))\n+\n((a_{2,1} - \\bar{a}_1)\n\\times\n(a_{2,2} - \\bar{a}_2))]\n\n\\\\\n[((a_{1,2} - \\bar{a}_2)\n\\times\n(a_{1,1} - \\bar{a}_1))\n+\n((a_{2,2} - \\bar{a}_2)\n\\times\n(a_{2,1} - \\bar{a}_1))]  \n\n&\n\n[((a_{1,2} - \\bar{a}_2)\n\\times\n(a_{1,2} - \\bar{a}_2))\n+\n((a_{2,2} - \\bar{a}_2)\n\\times\n(a_{2,2} - \\bar{a}_2))]\n\\\\\n\n\\end{matrix}\n\\right]\n\\end{align*}\\]\n\n\n\n\n\n\nImportante\n\n\n\nSi pones atención a las celdas que están en la diagonal te darás cuenta de que los productos que conforman la suma son iguales, por lo que podemos simplificarlos como como productos cuadráticos.\n\n\n\nMultiplicar por \\(\\frac{1}{m-1}\\), para completar nuestra varianza muestral; es decir, vamos a dividir cada elemento de nuestra matriz sobre el número de observaciones (renglones) que tenemos menos uno (grados de libertad):\n\n\\[\\begin{align*}\n=\n\\left[\n\n\\begin{matrix}\n\n[(a_{1,1}-\\bar{a}_1)^2+(a_{2,1}-\\bar{a}_1)^2]\n\n&\n\n[((a_{1,1} - \\bar{a}_1)\n\\times\n(a_{1,2} - \\bar{a}_2))\n+\n((a_{2,1} - \\bar{a}_1)\n\\times\n(a_{2,2} - \\bar{a}_2))]\n\n\\\\\n[((a_{1,2} - \\bar{a}_2)\n\\times\n(a_{1,1} - \\bar{a}_1))\n+\n((a_{2,2} - \\bar{a}_2)\n\\times\n(a_{2,1} - \\bar{a}_1))]  \n\n&\n\n[(a_{1,2}-\\bar{a}_2)^2+(a_{2,2}-\\bar{a}_2)^2]\n\\\\\n\n\\end{matrix}\n\\right]\n\n\\cdot \\frac{1}{2-1}\n\\end{align*}\\]\nSi simplificamos todo esto, tenemos nuestra matriz de covarianzas:\n\\[\\begin{align*}\n\\Sigma = \\left[\n\\begin{matrix}\ns^2(x) & cov(x,y) \\\\\ncov(x,y) & s^2(y)\n\\end{matrix}\n\\right]\n\\end{align*}\\]"
  },
  {
    "objectID": "c14_intromv.html#matriz-de-correlación",
    "href": "c14_intromv.html#matriz-de-correlación",
    "title": "1  Técnicas multivariadas: Introducción",
    "section": "1.4 Matriz de correlación",
    "text": "1.4 Matriz de correlación\nAhora recordemos que la correlación entre dos variables es el resultado de dividir su covarianza entre el producto de sus desviaciones estándar (que ez la raíz cuadrada de la varianza), tal que:\n\\[\ncor(x,y) = \\frac{cov(x,y)}{\\sigma_x \\sigma_y} = \\frac{\\sum X_cY_c}{\\sqrt{\\sum X^2_c \\sum Y^2_c}}\n\\]\nPartiendo de esta definición, calcular la matriz de correlación también se reduce a tres pasos:\n\nCentrar y estandarizar la matriz; es decir, a cada valor le vamos a restar la media de su columna y lo vamos a dividir entre la desviación estándar de su columna \\(\\left(\\frac{a_{m,n}-\\bar{a}_n}{\\sigma_n} \\right)\\).\nMultiplicar la matriz transpuesta por la matriz original (\\(E_A^T \\cdot E_A\\)).\nMultiplicar por \\(\\frac{1}{m}\\)\n\n\\[\\begin{align*}\n\nE^T_A =\n\\left[\n\\begin{matrix}\n\\frac{a_{1,1} - \\bar{a}_1}{\\sigma_1} &\n\\frac{a_{2,1} - \\bar{a}_1}{\\sigma_1}\n\\\\\n\\frac{a_{1,2} - \\bar{a}_2}{\\sigma_2} &\n\\frac{a_{2,2} - \\bar{a}_1}{\\sigma_2}\n\\\\\n\n\\end{matrix}\n\\right]\n\n\\cdot\n\nE_A =\n\n\\left[\n\\begin{matrix}\n\\frac{a_{1,1} - \\bar{a}_1}{\\sigma_1} &\n\\frac{a_{1,2} - \\bar{a}_2}{\\sigma_2}\n\\\\\n\\frac{a_{2,1} - \\bar{a}_1}{\\sigma_1} &\n\\frac{a_{2,2} - \\bar{a}_2}{\\sigma_2}\n\\\\\n\n\\end{matrix}\n\\right]\n\n\\cdot \\frac{1}{2}\n\n= \\\\\n\nr =\n\\left[\n\\begin{matrix}\n1 & cor(x,y) \\\\\ncor(x,y) & 1\n\\end{matrix}\n\\right]\n\\end{align*}\\]\nY con esto tenemos nuestra matriz de correlaciones.\n\n\n\n\n\n\nImportante\n\n\n\nSi te das cuenta, el orden de los pasos permite que aprovechemos las características del producto matricial interno para poder estimar todas las covarianzas o correlaciones “juntas”. Si bien es cierto que a mano puede sonar a que no hay mucha diferencia, esto simplifica mucho las cosas cuando empezamos a escalar en los procedimientos."
  },
  {
    "objectID": "c14_intromv.html#matrices-de-covarianza-y-corrlación-en-r",
    "href": "c14_intromv.html#matrices-de-covarianza-y-corrlación-en-r",
    "title": "1  Técnicas multivariadas: Introducción",
    "section": "1.5 Matrices de covarianza y corrlación en R",
    "text": "1.5 Matrices de covarianza y corrlación en R\nAfortunadamente para nosotros, para obtener estas matrices en R utilizaremos las mismas funciones que para el cálculo individual (y por consiguiente podremos calcular también la \\(\\rho\\) de Spearman). Utilicemos como ejemplo la base de datos mtcars:\n\nhead(mtcars)\n\n\n\n  \n\n\n\nAhora estimemos ambas matrices:\n\n# Matriz de covarianzas\ncov_mat <- cov(mtcars)\n# Matriz de correlación\ncor_mat <- cor(mtcars)\n# Las imprimimos en pantalla\ncov_mat\n\n             mpg         cyl        disp          hp         drat          wt\nmpg    36.324103  -9.1723790  -633.09721 -320.732056   2.19506351  -5.1166847\ncyl    -9.172379   3.1895161   199.66028  101.931452  -0.66836694   1.3673710\ndisp -633.097208 199.6602823 15360.79983 6721.158669 -47.06401915 107.6842040\nhp   -320.732056 101.9314516  6721.15867 4700.866935 -16.45110887  44.1926613\ndrat    2.195064  -0.6683669   -47.06402  -16.451109   0.28588135  -0.3727207\nwt     -5.116685   1.3673710   107.68420   44.192661  -0.37272073   0.9573790\nqsec    4.509149  -1.8868548   -96.05168  -86.770081   0.08714073  -0.3054816\nvs      2.017137  -0.7298387   -44.37762  -24.987903   0.11864919  -0.2736613\nam      1.803931  -0.4657258   -36.56401   -8.320565   0.19015121  -0.3381048\ngear    2.135685  -0.6491935   -50.80262   -6.358871   0.27598790  -0.4210806\ncarb   -5.363105   1.5201613    79.06875   83.036290  -0.07840726   0.6757903\n             qsec           vs           am        gear        carb\nmpg    4.50914919   2.01713710   1.80393145   2.1356855 -5.36310484\ncyl   -1.88685484  -0.72983871  -0.46572581  -0.6491935  1.52016129\ndisp -96.05168145 -44.37762097 -36.56401210 -50.8026210 79.06875000\nhp   -86.77008065 -24.98790323  -8.32056452  -6.3588710 83.03629032\ndrat   0.08714073   0.11864919   0.19015121   0.2759879 -0.07840726\nwt    -0.30548161  -0.27366129  -0.33810484  -0.4210806  0.67579032\nqsec   3.19316613   0.67056452  -0.20495968  -0.2804032 -1.89411290\nvs     0.67056452   0.25403226   0.04233871   0.0766129 -0.46370968\nam    -0.20495968   0.04233871   0.24899194   0.2923387  0.04637097\ngear  -0.28040323   0.07661290   0.29233871   0.5443548  0.32661290\ncarb  -1.89411290  -0.46370968   0.04637097   0.3266129  2.60887097\n\ncor_mat\n\n            mpg        cyl       disp         hp        drat         wt\nmpg   1.0000000 -0.8521620 -0.8475514 -0.7761684  0.68117191 -0.8676594\ncyl  -0.8521620  1.0000000  0.9020329  0.8324475 -0.69993811  0.7824958\ndisp -0.8475514  0.9020329  1.0000000  0.7909486 -0.71021393  0.8879799\nhp   -0.7761684  0.8324475  0.7909486  1.0000000 -0.44875912  0.6587479\ndrat  0.6811719 -0.6999381 -0.7102139 -0.4487591  1.00000000 -0.7124406\nwt   -0.8676594  0.7824958  0.8879799  0.6587479 -0.71244065  1.0000000\nqsec  0.4186840 -0.5912421 -0.4336979 -0.7082234  0.09120476 -0.1747159\nvs    0.6640389 -0.8108118 -0.7104159 -0.7230967  0.44027846 -0.5549157\nam    0.5998324 -0.5226070 -0.5912270 -0.2432043  0.71271113 -0.6924953\ngear  0.4802848 -0.4926866 -0.5555692 -0.1257043  0.69961013 -0.5832870\ncarb -0.5509251  0.5269883  0.3949769  0.7498125 -0.09078980  0.4276059\n            qsec         vs          am       gear        carb\nmpg   0.41868403  0.6640389  0.59983243  0.4802848 -0.55092507\ncyl  -0.59124207 -0.8108118 -0.52260705 -0.4926866  0.52698829\ndisp -0.43369788 -0.7104159 -0.59122704 -0.5555692  0.39497686\nhp   -0.70822339 -0.7230967 -0.24320426 -0.1257043  0.74981247\ndrat  0.09120476  0.4402785  0.71271113  0.6996101 -0.09078980\nwt   -0.17471588 -0.5549157 -0.69249526 -0.5832870  0.42760594\nqsec  1.00000000  0.7445354 -0.22986086 -0.2126822 -0.65624923\nvs    0.74453544  1.0000000  0.16834512  0.2060233 -0.56960714\nam   -0.22986086  0.1683451  1.00000000  0.7940588  0.05753435\ngear -0.21268223  0.2060233  0.79405876  1.0000000  0.27407284\ncarb -0.65624923 -0.5696071  0.05753435  0.2740728  1.00000000\n\n\nVeámos la matriz de correlaciones gráficamente:\n\ncorrplot::corrplot(cor_mat, method = \"ellipse\", type = \"upper\")\n\n\n\n\nAhora utilicemos una función que, en un solo paso, computará la matriz de correlación, realizará una prueba de significancia para cada una y además nos presentará la matriz de manera gráfica:\n\n# Descarga la función desde esa url y la carga en memoria\nsource(\"http://www.sthda.com/upload/rquery_cormat.r\")\n# Para fines prácticos se extrae un subconjunto de las columnas\nmydata <- mtcars[, c(1,3,4,5,6,7)] \n# Se aplica la función\nrquery.cormat(mydata)\n\n\n\n\n$r\n        hp  disp    wt  qsec  mpg drat\nhp       1                            \ndisp  0.79     1                      \nwt    0.66  0.89     1                \nqsec -0.71 -0.43 -0.17     1          \nmpg  -0.78 -0.85 -0.87  0.42    1     \ndrat -0.45 -0.71 -0.71 0.091 0.68    1\n\n$p\n          hp    disp      wt  qsec     mpg drat\nhp         0                                   \ndisp 7.1e-08       0                           \nwt   4.1e-05 1.2e-11       0                   \nqsec 5.8e-06   0.013    0.34     0             \nmpg  1.8e-07 9.4e-10 1.3e-10 0.017       0     \ndrat    0.01 5.3e-06 4.8e-06  0.62 1.8e-05    0\n\n$sym\n     hp disp wt qsec mpg drat\nhp   1                       \ndisp ,  1                    \nwt   ,  +    1               \nqsec ,  .       1            \nmpg  ,  +    +  .    1       \ndrat .  ,    ,       ,   1   \nattr(,\"legend\")\n[1] 0 ' ' 0.3 '.' 0.6 ',' 0.8 '+' 0.9 '*' 0.95 'B' 1\n\n\nOtra alternativa es utilizar la función chart.Correlation(data, histogram) de la librería PerformanceAnalytics, en la cual se muestran todos resultados en una misma gráfica:\n\nPerformanceAnalytics::chart.Correlation(mydata, histogram = T, pch = 19)\n\n\n\n\nUn comentario final al respecto de estas matrices es que, además de ser la base de las técnicas multivariadas, nos permiten evaluar la asociación entre nuestras variables sin comprometer un modelo predictivo, a la vez que nos permitirán hacer un filtrado de nuestras variables para evitar autocorrelaciones o incluir variables poco informativas, aunque de esto hablaremos más adelante."
  },
  {
    "objectID": "c14_intromv.html#normalidad-multivariada",
    "href": "c14_intromv.html#normalidad-multivariada",
    "title": "1  Técnicas multivariadas: Introducción",
    "section": "1.6 Normalidad multivariada",
    "text": "1.6 Normalidad multivariada\nPara analizar si nuestros datos multivariados se ajustan a una distribución normal multivariada utilizaremos la librería MVN, cuya función mvn(data, mvnTest, multivariatePlot) nos permite realizar una serie de pruebas tanto multi como univariadas:\n\n1.6.1 Prueba de Mardia (1970)\nConsiste en analizar si los momentos de la distribución Mv de los datos difieren de los esperados de una distribución Normal Mv (Mardia 1970), las hipótesis nulas son:\n\n\\(H_0: S_{obs} = S_{NMv_{µ, \\Sigma}}\\)\n\\(H_0: K_{obs} = K_{NMv_{µ, \\Sigma}}\\)\n\nDonde \\(\\mu\\) representa el vector de medias de cada variable y \\(\\Sigma\\) la matriz de covarianzas.\nAl realizar esta prueba vemos que la distribución Mv observada tiene una curtosis adecuada, aunque se encuentra fuertemente sesgada. Si analizamos los datos univariados vemos que en apariencia 2/6 variables presentan normalidad univariada.\n\nmvntest <- MVN::mvn(mydata, mvnTest = \"mardia\")\nmardia_test <- mvntest$multivariateNormality\nunitest <- mvntest$univariateNormality\nmardia_test\n\n\n\n  \n\n\nunitest\n\n\n\n  \n\n\n\n\n\n1.6.2 Prueba de Henze-Zirkler\nSi observamos con atención los valores de p, veremos que hay algunos que se encuentran cercanos al umbral de 0.05 por lo que un análisis a partir de los cuantiles de la distribución puede ser una alternativa más informativa. Para ello podemos utilizar la prueba de Henze-Zirkler. Esta prueba considera una hipótesis compuesta, en la cual la distribución de X es una distribución normal no degenerada, cuyos resultados son consistentes contra cualquier distribución alternativa no normal (por ello compuesta). La representación está dada en términos de \\(L^2\\) (Distancia de Mahalanobis, más adelante hablaremos sobre medidas de distancia). Al ser una prueba de bondad de ajuste (observado vs. esperado), el estadístico de prueba sigue una distribución \\(\\chi^2\\) (Henze y Zirkler 2007).\nAplicandola con mvn() vemos que también sugiere una falta de normalidad, lo cual podemos comprobar al ver el gráfico Cuantil-Cuantil\n\nhz_test <- MVN::mvn(mydata, mvnTest = \"hz\",\n                    multivariatePlot = \"qq\")$multivariateNormality\n\n\n\nhz_test\n\n\n\n  \n\n\n\nEse gráfico, aunque informativo, puede trabajarse para hacerse más agradable a la vista utilizando ggplot2 utilizando la siguiente función personalizada creada a partir del código utilizado para la gráfica anterior:\n\n# Funciones personalizadas:\n# Extraida de la función mvn(multivariatePlot = \"qq\") para un gráfico QQ\n# para normalidad multivariada utilizando la distancia de mahalanobis\nji2_plot <- function(df){\n  # Datos\n  n <- dim(mydata)[1] # número de datos\n  p <- dim(mydata)[2] # número de grupos\n  # Centramos los datos (a-µ(a)) sin escalarlos (sin dividir por su \\sigma)\n  dif <- scale(mydata, scale = F)\n  # Cálculo de la distancia de mahalanobis^2\n  d <- diag(dif %*% solve(cov(mydata), tol = 1e-25) %*% t(dif)) \n  r <- rank(d) # Asignación de rangos a las distancias\n  # Obtención de los cuantiles teóricos según la distribución ji^2\n  ji2 <-  qchisq((r - 0.5)/n, p)\n  # Reunimos los objetos en un data.frame para graficar con ggplot2\n  ji2_plot_data <- data.frame(d, ji2)\n  \n  # Graficado\n  library(ggplot2)\n  ji2_qq <- ggplot(data = ji2_plot_data, aes(x = d, y = ji2)) + \n            geom_point(colour = \"deepskyblue4\", alpha = 0.5, size = 4) + \n            geom_abline(slope = 1,\n                        colour = rgb(118,78,144,\n                                     maxColorValue = 255),\n                        size = 1) +\n            labs(title = \n                   bquote(\"Gráfico QQ de\" ~~ {chi^2} ~~ \", g.l = \"~ .(p)),\n                 subtitle = bquote(\"Distancia de Mahalanobis (\"~{D^2}~\") \n                                   vs. Cuantiles teóricos\"),\n                 x = element_blank(),\n                 y = element_blank(),\n                 caption = \"Basado en mvn(..., multivariatePlot == \\\"qq\\\")\"\n                 )\n  return(ji2_qq)\n}\n\n\nqqjiplot <- ji2_plot(mydata) + theme_bw()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\nqqjiplot\n\n\n\n\n\n\n1.6.3 Prueba de Royston\nEsta prueba es una extensión multivariada de la prueba por excelencia para la normalidad univariada: la prueba de Shapiro-Wilk’s. Originalmente propuesta en 1983, aunque fue corregida/ampliada por el mismo autor en 1992 (Royston vs. otras). Funciona mejor para muestras pequeñas, aunque no se recomienda emplearla con menos de 3 observaciones o con más de 2000.\nSu implementación sigue la misma línea que los casos anteriores. Si analizamos el valor de p, veremos que se encuentra en el límite de la significancia a un \\(\\alpha = 0.05\\); sin embargo, si consideramos también el gráfico QQ que elaboramos anteriormente, no podemos asumir que esas desviaciones sean despreciables.\n\nroyston_test <- mvn(mydata, mvnTest = \"royston\")$multivariateNormality\nroyston_test"
  },
  {
    "objectID": "c14_intromv.html#igualdad-de-dispersiones-multivariadas",
    "href": "c14_intromv.html#igualdad-de-dispersiones-multivariadas",
    "title": "1  Técnicas multivariadas: Introducción",
    "section": "1.7 Igualdad de dispersiones multivariadas",
    "text": "1.7 Igualdad de dispersiones multivariadas\nConforme vayamos avanzando en el curso veremos que, entre los supuestos de algunas pruebas, vamos a encontrar el de “igualdad de dispersiones multivariadas”; i.e., igualdad de matrices de covarianza. Una alternativa es utilizar la prueba de Anderson (2005), la cual es un análogo multivariado a la prueba de Levene para la homogeneidad de varianzas. La prueba de hipótesis está basada en distancias no euclidianas entre los grupos (i.e., no utiliza el teorema de Pitágoras :( ). Un dato curioso es que este método también se ha utilizado para evaluar la diversidad \\(\\beta\\) de una comunidad.\nPara su implementación en R utilizaremos la función betadisper de la librería vegan. Al ser un método basado en distancias, primero habrá que transformar los datos a una matriz de distancias utilizando alguna de las funciones dist, betadiver o vegdist. Necesitamos, además, establecer los grupos a utilizar.\n\nlibrary(vegan)\ndist_mat <- vegan::vegdist(mydata, method = \"bray\", type = c(\"median\"))\ngroups <- as.character(mtcars$cyl)\n# Por fines prácticos no se muestra,\n# ya que calcula una distancia entre cada par de instancias o grupos,\n# resultando en matrices sumamente grandes\n#dist.mat\n\nAhora utilizaremos la función betadisper para comprobar la homogeneidad de dispersiones entre los distintos cilindros. Esta prueba únicamente genera el espacio multivariado para realizar la prueba, realizando un ACP para reducir la dimensionalidad de la base de datos y estimar las distancias a la mediana de cada uno de los grupos establecidos.\n\n# Realizar el procedimiento\ndisp_mv <- vegan::betadisper(dist_mat,\n                             group = groups, type = \"median\")\ndisp_mv\n\n\n    Homogeneity of multivariate dispersions\n\nCall: vegan::betadisper(d = dist_mat, group = groups, type = \"median\")\n\nNo. of Positive Eigenvalues: 18\nNo. of Negative Eigenvalues: 13\n\nAverage distance to median:\n      4       6       8 \n0.09587 0.06171 0.08251 \n\nEigenvalues for PCoA axes:\n(Showing 8 of 31 eigenvalues)\n   PCoA1    PCoA2    PCoA3    PCoA4    PCoA5    PCoA6    PCoA7    PCoA8 \n1.392690 0.091453 0.040689 0.037914 0.025951 0.009016 0.007642 0.004007 \n\n\nSi realizamos la prueba de hipótesis vemos que, al parecer, las dispersiones multiviariadas son similares entre los 3 grupos.\n\n# Prueba de hipótesis\nanova(disp_mv)\n\n\n\n  \n\n\n\nAhora veamos las dispersiones gráficamente:\n\n# Análisis gráfico\nplot(disp_mv, ellipse = T, hull = F)\n\n\n\n\nAl analizar el gráfico vemos que las dispersiones son similares; sin embargo, pareciera que la dispersión del grupo 6 es más pequeña que la de los grupos 4 y 8, en tonces realicemos las comparaciones pareadas univariadas con la prueba Honesta de Diferencias Significativas de Tukey (TukeyHSD). Los resultados sugieren que las dispersiones con un \\(\\alpha = 0.05\\) son similares, lo cual a su vez pudiera sugerir que hay un equilibrio entre las dispersiones en el eje x con respecto a las dispersiones en el eje y.\n\nmod_HSD <- TukeyHSD(disp_mv)\nmod_HSD <- data.frame(mod_HSD$group, comp = dimnames(mod_HSD$group)[[1]])\nmod_HSD\n\n\n\n  \n\n\n\nAl igual que en el caso anterior podemos utilizar ggplot para personalizar el gráfico:\n\nhsd_plot <- ggplot(data = mod_HSD, \n                   aes(x = comp)) + \n            geom_point(aes(y = diff), \n                       colour = \"deepskyblue4\", \n                       size = 4, \n                       alpha = 0.7)+ \n            geom_errorbar(aes(ymin = lwr, ymax = upr), \n                          colour = \"deepskyblue4\") + \n            theme_bw() + \n            labs(title = \"Diferencias en dispersión multivariada e IC\",\n                 subtitle = \"Prueba HSD de Tukey\",\n                 x = \"Grupos\",\n                 y = element_blank()) +\n            scale_y_continuous(breaks = NULL) + \n            geom_hline(yintercept = 0,\n                       colour = rgb(118,78,144, maxColorValue = 255),\n                       linetype = \"dashed\") +\n            annotate(\"text\",\n                     x = 0.5, y = 0+0.005, \n                     label = as.character(0),\n                     colour = rgb(118,78,144, maxColorValue = 255)\n                     ) +\n            geom_hline(yintercept = max(mod_HSD$upr),\n                       colour = \"firebrick\", alpha = 0.7,\n                       linetype = \"dashed\") +\n            annotate(\"text\",\n                     x = 0.5, y = max(mod_HSD$upr)-0.005, \n                     label = as.character(round(max(mod_HSD$upr),2)),\n                     colour = \"firebrick\"\n                     ) +\n            geom_hline(yintercept = min(mod_HSD$lwr),\n                       colour = \"firebrick\", alpha = 0.7,\n                       linetype = \"dashed\") +\n            annotate(\"text\",\n                     x = 0.5, y = min(mod_HSD$lwr)-0.005, \n                     label = as.character(round(min(mod_HSD$lwr),2)),\n                     colour = \"firebrick\"\n                     ) +\n            geom_text(aes(label = paste(\"p = \", round(p.adj,2)), y = 0),\n                       stat = \"identity\",\n                       nudge_y = max(mod_HSD$upr)+0.005, colour = \"gray50\")\n            \nhsd_plot"
  },
  {
    "objectID": "c14_intromv.html#implicaciones-analíticas-de-la-multidimensionalidad",
    "href": "c14_intromv.html#implicaciones-analíticas-de-la-multidimensionalidad",
    "title": "1  Técnicas multivariadas: Introducción",
    "section": "1.8 Implicaciones analíticas de la multidimensionalidad",
    "text": "1.8 Implicaciones analíticas de la multidimensionalidad\nEs importante mencionar que entre más incrementemos la dimensionalidad de nuestro problema, más difícil será resumir en un solo resultado las pruebas de nuestros análisis y, en consecuencia, deberemos de considerar distintas técnicas/estrategias que analicen nuestros datos desde distintas perspectivas antes de emitir un juicio o extraer conclusiones. Por esta razón, es sumamente importante que realicemos una selección de variables de manera rigurosa antes de comenzar nuestro análisis, ya que incluir variables innecesariamente únicamente incrementará la varianza de los datos sin aportarnos ninguna información adicional, causando desviaciones de la normalidad, modelos complejos sobre o infra ajustados y pérdidas de poder estadístico.\nEn cuanto a la normalidad, es importante mencionar que si nuestros datos en realidad no se ajustan, o se encuentran fuertemente desviados de la normalidad, las conclusiones que extraigamos serán únicamente sobre la tendencia más general de nuestros datos (Revisar: desigualdad/teorema de Chebyshev) y, en consecuencia, pueden no ser una representación completa de nuestras muestras. Si esto es importante o no, dependerá de nuesta pregunta de investigación y qué tan fino querramos que sea el análisis."
  },
  {
    "objectID": "c14_intromv.html#transformaciones-o-deformaciones",
    "href": "c14_intromv.html#transformaciones-o-deformaciones",
    "title": "1  Técnicas multivariadas: Introducción",
    "section": "1.9 Transformaciones o deformaciones",
    "text": "1.9 Transformaciones o deformaciones\nHabiendo tocado el tema de la estandarización, hablemos también del resto de transformaciones. En general, podemos considerar que existen dos tipos de transformaciones:\n\nAquellas que afectan la distribución de los datos (e.g., logarítmica)\nAquellas que simplemente cambian los límites de la distribución original (e.g., MinMax)\n\n¿Cuál utilizar? Dependerá de nuestros objetivos para hacerla, lo cual me lleva al punto de que: NINGUNA transformación debe de ser aplicada sin cuidado. Hay que tener en cuenta que, aunque no se cambie la distribución de los datos, el análisis ya no se realiza sobre los datos originales, lo cual puede causar errores de interpretación. Con esto no quiero decir que las transformaciones sean malas, solo que hay que emplearlas con una justificación y asegurarnos de re-transformar los datos antes de hacer inferencias.\nVeamos algunas de las transformaciones más comunes y cuáles son sus consecuencias en los datos. Para ello, consideremos estos datos sin transformar, los cuales están altamente sesgados y, en consecuencia, bastante alejados de la normalidad:\n\n# Función de graficado\nkdeplots <- function(data, aes){\n  kdeplots <- ggplot(data, aes) + \n            geom_density(show.legend = F) + \n            theme_bw() +\n            labs(title = \"Conteo de peces\",\n                 subtitle =\n                   \"Gráfico de densidad con datos y distrintas transformaciones\",\n                 caption = \"Datos: http://bit.ly/comm_transf\",\n                 x = element_blank(),\n                 y = element_blank()) +\n            scale_y_continuous(breaks = NULL)\n  return(kdeplots)\n}\n\ndf <- data.frame(datos = c(38, 1, 13, 2, 13, 20, 150, 9, 28, 6, 4, 43),\n                 transf = \"originales\")\nkde_plots <- kdeplots(df, aes(datos, color = transf))\nkde_plots\n\n\n\n\n\n1.9.1 Estandarización\nVeamos el efecto de estandarizar los datos; es decir, utilizar la distribución Z. La estandarización consiste en centrar la variable (restarle su media) y dividirla entre su desviación estándar. El resultado es la misma distribución, aunque los datos ahora se encuentran escalados e indican a cuantas SD de la media se encuentra cada punto.\n\ndf2 <- rbind(df, data.frame(datos = (df$datos-mean(df$datos))/sd(df$datos),\n                            transf = \"Z\"))\nkde_plots <- kdeplots(df2, aes(datos, color = transf)) + \n             facet_wrap(~transf, scales = \"free\")\nkde_plots\n\n\n\n\n\n\n1.9.2 Transformación logarítmica\nPosiblemente la transformación más conocida, utilizada y, en consecuencia, abusada. Se realiza aplicando la ecuación \\(X_{log} = log_n(X)\\). (OJO: si hay ceros será \\(X_{log} = log_n(X+1)\\), ya que el logaritmo de 0 no existe). Esta transformación cambia notablemente la distribución de los datos, acercándolos a una forma de campana. ¿Cuándo aplicarla? Cuando querramos forzar nuestros datos a una distribución normal para cumplir con los supuestos de alguna prueba paramétrica, linealizar los datos y, equivocadamente, ponerlos en la misma escala que otra variable. Con excepción del último caso, cualquiera de las formas está matemáticamente justificada, solo hay que tener en consideración que los datos no son los originales y que pueden no representar adecuadamente nuestro muestreo. Otro caso en el cual es válido utilizarlo es si queremos ver cuál es cuando tenemos distintos factores y nuestra variable de respuesta es el resultado de su interacción (producto) tal que \\(Y = a \\times b \\times c \\times d\\times ... \\times z\\) (efecto multiplicativo y no aditivo), ya que el resultado es una distribución con forma log-normal que no es posible capturar con los datos originales. Su re-transformación es \\(n^{X_{log}}\\). Salvo en el último caso, recomiendo contrastar los resultados con una prueba no paramétrica utilizando los datos originales y ver cuáles son las diferencias.\n\ndf2 <- rbind(df2, data.frame(datos = log(df$datos), transf = \"log(x)\"))\nkde_plots <- kdeplots(df2, aes(datos, color = transf)) +\n             facet_wrap(~transf, scales = \"free\")\nkde_plots\n\n\n\n\n\n\n1.9.3 Raíz cuadrada\nOtra transformación muy empleada, consiste en en obtener la raíz cuadrada de cada uno de los datos (\\(\\sqrt{X}\\), OJO: si hay valores negativos no se puede utilizar, hay que pasarlos a valores absolutos o añadir una constante para volverlos positivos). Veamos su efecto en la distribución. En este caso el cambio en la forma no es tan agresivo, y la consecuencia es únicamente que las diferencias entre los valores más altos y los más pequeños se redujo. Su re-transformación es: \\(\\sqrt{x}^2\\). Su uso más común es con datos de conteo (abundancias, bacterias en una caja petri, etc.)\n\ndf2 <- rbind(df2, data.frame(datos = sqrt(df$datos), transf = \"sqrt(x)\"))\nkde_plots <- kdeplots(df2, aes(datos, color = transf)) +\n             facet_wrap(~transf, scales = \"free\")\nkde_plots"
  },
  {
    "objectID": "c14_intromv.html#aprendizaje-automatizado",
    "href": "c14_intromv.html#aprendizaje-automatizado",
    "title": "1  Técnicas multivariadas: Introducción",
    "section": "1.10 Aprendizaje Automatizado",
    "text": "1.10 Aprendizaje Automatizado\nAhora hablemos sobre la diferencia entre estadística multivariada y aprendizaje automatizado. Hasta este momento la mayor parte de nuestros problemas han consistido en tener unos datos, encasillarlos en un conjunto de reglas utilizando programación o pruebas de significancia estadística para obtener una respuesta (e.g., “las medias de los grupos fueron diferentes; Figura 1.3); sin embargo, vimos en los ?sec-rls y ?sec-nolin que podemos decirle a la computadora que nos diga bajo qué reglas están jugando nuestros datos. Eso justamente es el aprendizaje automatizado Figura 1.4, Figura 1.5. Este Machine Learning es una parte de la estadística, en la cuál la computadora”aprende” a partir de ejemplos que nosotros le proporcionamos. En los temas de regresíon teníamos pares de datos \\(x\\) y \\(y\\), y le dijimos a la computadora que encontrara un conjunto de parámetros que permitieran transormar los valores de \\(x\\) a los valores de \\(y\\) con el menor error posible.\n\n\n\nFigura 1.3: Paradigma tradicional para resolver problemas.\n\n\nDentro del aprendizaje automatizado tenemos dos categorías generales:\n\nAprendizaje supervisado: Aquí se engloban las técnicas que, como nuestras regresiones, mapean los valores de nuestros predictores hacia los valores de la variable (o las variables) a predecir. Esta categoría se sub-divide en dos clases adicionales, que están definidas por las características de la variable dependiente:\n\nRegresión: En esta la variable dependiente es una variable numérica (puede ser continua u ordinal). Tanto los modelos lineales como no lineales forman parte de esta clase de técnicas.\nClasificación: Si la regresión trabaja con variables numéricas (ordinales o continuas), la clasificación trabaja con variables nominales (algunas pueden trabajar con variables ordinales); es decir, lo que vamos a predecir son etiquetas de clases o grupos (de ahí “clasificación”). Tenemos una gran cantidad de algoritmos de clasificación, desde los más simples que suponen diferencias lineales entre las clases hasta algunos altamente complejos con grandes cantidades de parámetros como las redes neuronales convolucionales.\n\n\n\n\n\nFigura 1.4: Aprendizaje supervisado.\n\n\n\nAprendizaje no supervisado: Aquí se engloban las técnicas o algoritmos que permiten extraer patrones ocultos en los datos. En el aprendizaje automatizado no tenemos una variable dependiente/de respuesta, solamente un conjunto de observaciones multivariadas de las cuales queremos extraer información que nos sea útil. Aquí también tenemos diversas clases, pero podemos resumirlas en dos:\n\nReducción de la dimensionalidad: Aquí el objetivo es poder “resumir” nuestros datos multivariados en solo una o dos valores, conservando la mayor cantidad de la información original. Aquí encontramos técnicas como los Ánálisis de Componentes Principales (PCA), Análisis de Coordenadas Principales (PCoA), Escalamiento Multidimensional No Métrico (NMDS). Si ya has tenido un acercamiento previo a estas técnicas puede que esto no te cuadre del todo, pues el NMDS (por ejemplo) es una técnica de ordenación. Y no estarías mal, pero en el proceso estamos reduciendo la dimensional original de los datos para poder realizar la ordenación. Nuestra entrada en estas técnicas es un conjunto de observaciones multivariadas, y la salida otro conjunto de observaciones compuesto por un menor número de variables\nAgrupamientos: El nombre lo dice todo, su objetivo es el formar grupos. ¿Cómo? Igual a como lo haríamos nosostros: juntando las instancias que más se parecen entre sí en un solo grupo, y mandando a las que no se parecen a otro grupo. Nuestra entrada es un conjunto de observaciones multivariadas, y la salida la agrupación a la que pertenece cada una.\n\n\n\n\n\nFigura 1.5: Aprendizaje no supervisado.\n\n\n\n\n\n\n\n\nNota\n\n\n\n¿Por qué “supervisado” y “no supervisado”? Porque en el primero tenemos una función de pérdida (error) que podemos supervisar y optimizar para mejorar nuestras predicciones, mientras que en el segundo no.\n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nAunque en la clasificación y en los agrupamientos estemos lidiando con grupos o clases, no son lo mismo. La clasificación es aprendizaje supervisado, la agrupación es aprendizaje no supervisado.\n\n\nAdicional a estas dos tenemos otras dos menos conocidas: el aprendizaje por refuerzo y el aprendizaje semi-supervisado, pero esos quedan fuera del alcance de este curso.\nCon esto terminamos esta sesión. Sé que estuvo fuertemente cargada de teoría y matemática, pero es necesario que tengas al menos una noción de dónde surgen las cosas. No es necesario que te las aprendas de memoria, o que sepas calcular las matrices a mano, solo que entiendas la lógica detrás de los procedimientos para que no abuses de ellos. Como recompensa por llegar hasta aquí, no hay ejercicio para esta sesión ;).\n\n\n\n\nAnderson MJ. 2005. Distance-Based Tests for Homogeneity of Multivariate Dispersions. Biometrics 62:245-253. DOI: 10.1111/j.1541-0420.2005.00440.x."
  },
  {
    "objectID": "c15_nosup.html#librerías",
    "href": "c15_nosup.html#librerías",
    "title": "2  Aprendizaje No Supervisado",
    "section": "2.1 Librerías",
    "text": "2.1 Librerías\n\nlibrary(ggplot2)\nlibrary(vegan)\nlibrary(ggdendro)\nlibrary(dendextend)\nlibrary(FactoMineR)\nlibrary(factoextra)\nlibrary(ggrepel)"
  },
  {
    "objectID": "c15_nosup.html#introducción",
    "href": "c15_nosup.html#introducción",
    "title": "2  Aprendizaje No Supervisado",
    "section": "2.2 Introducción",
    "text": "2.2 Introducción\nEn el Capítulo 1 hablamos de que en el aprendizaje no supervisado vamos a extraer patrones o grupos contenidos en los datos, lo cual podemos hacer mediante la reducción de la dimensionalidad o formando agrupaciones. Bueno, hablemos a fondo de ambas y veamos algunas técnicas."
  },
  {
    "objectID": "c15_nosup.html#agrupamientos",
    "href": "c15_nosup.html#agrupamientos",
    "title": "2  Aprendizaje No Supervisado",
    "section": "2.3 Agrupamientos",
    "text": "2.3 Agrupamientos\nEmpecemos por lo “sencillo”: formar agrupaciones. ¿Por qué las comillas? Para responder a esa pregunta pongamos un ejemplo. Observa la siguiente imagen y dime: ¿cuántos grupos formarías? ¿qué “bicho” entra en cuál grupo?\n\n\n\nFigura 2.1: Artrópodos a agrupar\n\n\nUna forma de hacerlo es poner a las mariposas/polillas en un grupo, los caracoles en otro, las arañas en otro y las abejas en otro:\n\n\n\nFigura 2.2: Artrópodos agrupados\n\n\nFue bastante sencillo, ¿no? A los seres humanos no nos resulta especialmente complicado categorizar, pero para la computadora hay un total de 115,975 combinaciones de estas imágenes. ¿Cómo saber cuáles son las agrupaciones relevantes? Hay una gran variedad de técnicas: Agrupamientos jerárquicos, máquinas de soporte vectorial, k-medias (k-medianas o k-medioides), DBSCAN, T-SNE, entre otras, cada una con sus peculiaridades, ventajas y desventajas, dadas por la lógica con la que trabajan y, en consecuencia, con diversos niveles de “éxito” según los datos con los que estemos trabajando:\n\n\n\nFigura 2.3: Técnicas de agrupaciones en distintos juegos de datos sintéticos. Nota: los datos no tienen grupos “reales”, solo algunas formas interesantes (Tomado de la documentación de Scikit-Learn).\n\n\n\n2.3.1 Agrupamiento jerárquico\nDe toda esta diversidad de técnicas hay una que se ha destacado en el área de la biología: el agrupamiento jerárquico, también llamado análisis clúster. Esta técnica tiene una lógica muy sencilla e intuitiva (lo cual es parte de la razón por la que sea tan popular): Calcula una distancia entre todos los pares de observaciones, forma un grupo con el par con la menor distancia y repite hasta que termines con todos los datos. Veamos cómo es esto paso a paso con unos datos de ejemplo:\n\nLongitudes totales de algunas especies de cetáceos.\n\n\nEspecie\nLongitud total (m)\n\n\n\n\nTursiops truncatus (Tt)\n3\n\n\nGrampus griseus (Gg)\n3.6\n\n\nGlobicephala macrorhyncus (Gm)\n6.5\n\n\nOrcinus orca (Oo)\n7.5\n\n\nMegaptera novaeangliae (Mn)\n15\n\n\nBalaenoptera physalus (Bp)\n20\n\n\n\nEntonces los pasos con los que agruparíamos estas especies, según su longitud total, son:\n\nCalcular una matriz de distancias/disimilaridades. Literalmente ver qué tanto se parecen nuestras especies entre sí. ¿Cómo hacemos esto? Utilizando alguna medida de distancia o disimilaridad, pero dejemos el tema de la selección de la medida para después y utilicemos la distancia Euclidiana:\n\n\n\n\n\n\n\nNota\n\n\n\n¿Qué es la distancia Euclidiana? Es una medida basada en el famosísimo teorema de Pitágoras ((¿creías que no servía para nada?):\n\\[D_{a,b} = \\sqrt\\sum_{i = 1}^n (q_i - p_i)^2\\]\n\n\n\nFigura 2.4: Teorema de Pitágoras\n\n\nEsta medida de distancia representa también la geometría que utilizamos en el día a día, por lo que la interpretación en casos univariados es directa, representando las unidades de diferencia que hay entre una observación y otra.\n\n\n\n\n\nFigura 2.5: Matriz de distancias. Se calcula la distancia entre cada par de observaciones y se disponen todas en forma matricial. Está solo la diagonal inferior; es decir, solo los datos de la diagonal hacia abajo, pues la diagonal superior es una imagen especular de lo que está abajo.\n\n\n\nIdentificar el par con la menor distancia y formar un grupo con ellos. Esto corresponde al método de unión (agrupamiento o amalgamiento) simple o sencillo, en el que encontramos la menor distancia en la matriz, que corresponde a Tt y Gg (\\(D = 0.6\\)), por lo que formamos un primer grupo [Tt, Gg], el cual estará representado por el promedio de Tt y Gg: 3.3.\nVolver a calcular la matriz de distancias/disimilaridades, sustituyendo las observaciones agrupadas por su grupo; es decir, ahora ya no vamos a incluir Tt y Gg en el cálculo de la matriz, sino que incluiremos [Tt, Gg] = 3.3:\n\n\n\n\nFigura 2.6: Matriz de distancias después de formar la primera agrupación entre [Tt, Gg]\n\n\n\nRepetir 2 y 3 hasta haber agrupado todas las observaciones.\n\n\n\n\nFigura 2.7: Agrupamiento jerárquico utilizando las distancias.\n\n\nAl final tenemos nuestro agrupamiento jerárquico [[[Tt, Gg], [Gm, Oo]], [Mn, Bp]]. ¿Por qué jerárquico? Porque tenemos grupos dentro de otros grupos. El grupo [Tt, Gg], por ejemplo, forma parte de un grupo más grande, donde se une con el grupo [Gm, Oo] y este grupo de grupos, a su vez, es parte de otro grupo más grande, donde se une con [Mn, Bp]. Ahora bien, esta notación es compacta, pero puede ser muy incómoda de leer si tenemos una gran cantidad de datos, por lo que en su lugar utilizamos una estructura conocida como dendrograma.\n\n\n2.3.2 El dendrograma\nUn dendrograma es una visualización de datos en la que se muestra al mismo tiempo las agrupaciones y las distancias a las que se unieron simulando el crecimiento de las ramas de un árbol por lo que podemos ver no solo quiénes forman qué grupos, sino que tan parecidos son los miembros de cada uno y qué tan diferentes son con los demás grupos. El dendrograma resultante de nuestro ejemplo es el siguiente:\n\n\n\nFigura 2.8: Dendrograma y cetáceos.\n\n\nComo ves en la figura Figura 2.8, nuestro dendrograma está compuesto por algunas estructuras:\n\nRaíz: Es la distancia a la que se unen todos nuestros grupos. En nuestro ejemplo falta porque no tenemos un “grupo hermano”; es decir, un grupo que sea cercano a nuestras especies de cetáceos, pero que no sea un cetáceo. Podríamos incluir un hipopótamo, por ejemplo. Si tenemos ese grupo hermano podemos graficar la raíz, y entonces nuestro dendrograma estará enraizado, de lo contrario tenemos un dendrograma no enraizado. El árbol comienza a “crecer” desde este punto (un solo grupo) hasta tener separados todos los puntos en el conjunto de datos.\nRamas: Cada una de las líneas verticales del árbol. Indican el “flujo” de las similaridades, en el sentido de que seguirán extendiéndose hasta la distancia en la que se forme una nueva agrupación.\nNodo: Es el punto donde se unen dos ramas; es decir, el punto en el que se forma una nueva agrupación.\nHojas: Son las “ramas terminales”, que se puede entender como un grupo de ramas que representan cada una de las instancias que conforman la base de datos. Las instancias más parecidas son las contiguas, y el parecido disminuye conforme nos alejamos de cada punto.\nClúster: Es un conjunto de hojas. ¿Cómo lo definimos? Esa pregunta merece que le dediquemos tiempo, pero primero hablemos de otros dos conceptos que dictan la forma final de nuestro dendrograma.\n\n\n\n2.3.3 Hablemos de distancias\nComo te darás cuenta, es un proceso iterativo y que puede llevar bastante tiempo, pero también es bastante sencillo. Ahora bien, ¿qué medidas de distancia/disimilaridad hay y cuál utilizar? Resulta que la medida de distancia/disimilaridad que seleccionemos determina la geometría del espacio en el que estamos haciendo el análisis. ¿La qué cosa de quien? Es más fácil entenderlo si te digo que un triángulo trazado en un papel tiene una sumatoria de sus ángulos diferente a un triángulo trazado en una esfera. Un ejercicio que puedes realizar para ver las consecuencias de moverse en diferentes espacios geométricos es, justamente, trazar un triángulo sobre una naranja, después retirarle la cáscara, ponerla sobre una superficie plana, y ver qué pasa con el triángulo que trazaste originalmente. Otra forma es una representación visual. En la Figura 2.9 tienes representadas tres distancias: la distancia Euclidiana (amarillo), Manhattan (rojo) y Chebyshev (azul). En todos los casos la distancia desde cualquier punto del perímetro hacia el centro es de 4 unidades, pero la forma del perímetro resultante es diferente porque el espacio geométrico es diferente:\n\n\n\nFigura 2.9: Formas de los espacios geométricos definidos por las distancias Euclidiana, Manhattan y Chebyshev. Las tres son casos especiales de la distancia Minkowski, con diferentes valores de \\(r\\)\n\n\n\n\n\n\n\n\nNota\n\n\n\n¿Es lo mismo una medida de distancia que una de disimilaridad? Sí y no. Desde el punto de vista práctico, sí, ambas nos permiten darnos una idea de quién se junta con quien; sin embargo, formalmente no son lo mismo. Una distancia cumple con la desigualdad triangular, mientras que una disimilaridad no. ¿Qué es la desigualdad triangular? Que la distancia más corta entre dos puntos es una linea recta. En una disimilaridad esto no es el caso, pues el espacio geométrico no es Euclidiano. Pinta dos puntos sobre la superficie de una naranja y únelos con una línea. No es recta, ¿o sí?\n\n\nEsto, obviamente tiene consecuencias, y hace que algunas distancias o disimilaridades funcionen mejor en ciertos casos y peor en otros. Shirkhorshidi, Aghabozorgi & Wah (2015) compilaron la siguiente lista, con algunas distancias y detalles muy útiles sobre su aplicación:\n\n\n\nFigura 2.10: Medidas de distancia y sus peculiaridades.\n\n\n¿Cuál utilizar? Como ves en la Figura 2.10, depende de la naturaleza de tus datos pero, por lo general, la distancia Euclidiana es un buen punto de partida.\n\n\n2.3.4 Métodos de unión\nLa otra cosa que debemos de decidir es el criterio bajo el cuál vamos a unir nuestros grupos. En el ejercicio con nuestros cetáceos utilizamos el método simple/sencillo, pero no es el único. En todos los métodos se busca minimizar algo, la diferencia es el qué se minimiza. Tomemos el siguiente problema, donde queremos unir el punto solitario con algo de los otros clústers:\n\n\n\nFigura 2.11: ¿Con quién voy?\n\n\n\nSimple: También llamado sencillo o método del “vecino más cercano”, en el que se minimiza la separación mínima entre clústers; es decir, vamos a calcular la matriz de distancias y formar una agrupación con los grupos que tengan los puntos con la menor distancia:\n\n\n\n\nFigura 2.12: ¿Con mi vecino más cercano?\n\n\n\nCompleto: También lo puedes encontrar como el método del “vecino más lejano”, porque minimiza la separación máxima entre clústers; es decir, se calcula la matriz de distancias, se seleccionan los puntos de cada grupo entre los que se encuentra la distancia, y de esos se agrupan los que tengan la menor. ¿Rebuscado? Sin duda, pero gráficamente es más sencillo:\n\n\n\n\nFigura 2.13: ¿Con mi vecino más lejano?\n\n\n\nCentroide: Si en el método simple se minimiza la separación mínima entre clústers, y en el método completo la separación máxima, debe de haber un punto intermedio. El método del centroide minimiza la distancia entre los puntos centrales de los clústers:\n\n\n\n\nFigura 2.14: ¿Con mi vecino central?\n\n\n\nPromedio: En este se minimiza la distancia promedio entre clústers; es decir, calcula todas las distancias entre los puntos, obtiene el promedio de la distancia por cada par de grupos, y forma uno nuevo con los grupos/puntos que resultaron en la menor distancia promedio.\nWard: Este es un poco más abstracto, pues minimiza la suma de cuadrados del error (ESS) o, en otras palabras, la varianza de los grupos resultantes. Es decir que primero agrupa cada par de grupos disponible, calcula la varianza resultante y se queda con la agrupación que tenga la menor.\n\n¿Cómo decidir cuál utilizar? Este es un análisis no supervisado, por lo que no podemos utilizar algún criterio matemático para definir si nuestra selección de distancia/método de unión es la correcta. Lo que sí podemos hacer es ver si tienen sentido las agrupaciones resultantes, lo cual me lleva a la siguiente parte.\n\n\n2.3.5 ¿Cuántos grupos?\nAhora sí, ¿con qué magia obscura determinamos cuántos grupos tenemos y quién pertenece a cuál? La forma es muy sencilla desde el punto de vista práctico: seleccionar un nivel de corte en nuestro dendrograma; es decir, una distancia en la cual el árbol deja de exenderse, y el número de grupos está dado por el número de ramas inmediatamente debajo:\n\n\n\nFigura 2.15: Cortando el árbol. Las líneas horizontales representan algunos posibles niveles de corte, el número a su derecha el número de grupos resultantes. Los colores de las etiquetas de las hojas representan el resultado de formar 4 agrupaciones.\n\n\nNuevamente, este es un análisis no supervisado, por lo que no tenemos etiquetas de grupos para evaluar el agrupamiento; sin embargo, podemos utilizar distintas medidas (30, de hecho) para estimar el nivel de corte. ¿Cuántas? 30, por ejemplo, y luego formar un ensamble; es decir, tomar el voto mayoritario; es decir, quedarnos con el número de agrupaciones que tenga el mayor número de “votantes”. El último “sello de garantía” de un análisis clúster es el coeficiente de correlación cofenética, el cuál es una medida de lo bien o mal representadas que están las distancias reales en el dendrograma.\n\n\n2.3.6 Implementación\nRealizar un agrupamiento jerárquico en R es bastante sencillo, únicamente debemos de tener nuestro data.frame cuyos nombres de renglones sean las etiquetas de nuestras instancias a agrupar, calcular las distancias con la función dist(x, method) y unir los grupos con la función hclust(dist, method).\n\ndf1 <- data.frame(long = c(3, 3.6, 6.5, 7.5, 15, 20),\n                  row.names = c(\"Tt\", \"Gg\", \"Gm\", \"Oo\", \"Mn\", \"Bp\"))\ndist_mat <- dist(df1, method = \"euclidean\")\nhc_av <- hclust(dist_mat, method = \"average\")\n\nEste objeto podemos graficarlo con ggplot, utilizando la librería ggdendro, lo cual nos da un dendrograma bastante sencillo:\n\nggdendrogram(hc_av, labels = T) +\n  theme_bw() +\n  labs(x = element_blank(),\n       y = \"Distancia Euclidiana\")\n\n\n\n\nSi bien es cierto que este dendrograma es “suficiente”, podemos personalizarlo utilizando la función dendextend::set(object, what, value):\n\n# Transformamos los agrupamientos a un dendrograma\ndend <- as.dendrogram(hc_av)\n\n# Cambiamos el color a las ramas\ndend <- set(dend, \"branches_k_color\",\n            value = \"deepskyblue4\",\n            k = 1)\n\n# Cambiamos el ancho de las ramas\ndend <- set(dend, \"branches_lwd\", 1)\n\n# Cambiamos el color de las etiquetas asumiendo 4 grupos\ndend <- set(dend, \"labels_col\",\n            value = c(rgb(0,118, 186, maxColorValue = 255),\n                      rgb(0, 123, 118, maxColorValue = 255),\n                      rgb(255, 147, 0, maxColorValue = 255),\n                      rgb(181, 23, 0, maxColorValue = 255)), \n            k = 4)\n\nCon estas líneas añadimos las características visuales que queremos que tenga el dendrograma, pero falta graficarlo utilizando ggplot:\n\ngg_dend <- as.ggdend(dend)\ngg_dendro <- ggplot(gg_dend,\n                    offset_labels = -1,\n                    theme = theme_bw()) +\n             ylim(-2.4, max(get_branches_heights(dend))) +\n             scale_x_continuous(breaks = NULL) +\n             labs(title = \"Dendrograma de especies de cetáceos\",\n                  subtitle = \"Agrupamientos por longitud (método: promedio)\",\n                  x = element_blank(),\n                  y = \"Distancia euclidiana\",\n                  caption = \"Datos de bit.ly/clust_medium\")\n\ngg_dendro\n\n\n\n\n\n\n\n\n\n\nNota\n\n\n\nSi te das cuenta, solo indicamos en set(dend, labels_col) que tenemos cuatro grupos y pasamos un vector con cuatro colores, los cuales se asignaron en nuestro dendrograma. ¿Cómo supo dendextend quién iba en qué grupo? Recuerda que lo que buscamos es un nivel de corte en el dendrograma, por lo que solo hay una forma de agrupar los datos en la que tenemos cuatro grupos resultantes.\n\n\nAhora bien, esta es una técnica bastante sencilla de implementar (la mayor parte del “engorro” la hicimos al graficar), pero eso no quiere decir que debamos utilizarla a diestra y siniestra sin tener cuidado de qué datos le damos a R. Es necesario que nuestras variables estén aproximadamente en la misma escala (unidades, decenas, milésimas, etc.), pues de lo contrario las variables con las magnitudes más grandes van a llevar un mayor peso durante la agrupación. Por otro lado, también hay que tener cuidado con variables que estén muy sesgadas, pues esa distribución va a modificar el cálculo de las distancias y, por lo tanto, los agrupamientos finales.\n\n\n2.3.7 Ejercicio multivariado\nYa sabemos cómo funciona el análisis de ahrupamientos y también cómo implementarlo en R. ¿Qué falta? Aplicarlo con un conjunto de datos con más observaciones y variables. Para esto utilizaremos los datos cluster.txt, en los cuales tenemos mediciones de variables ambientales en distitntos sitios de muestreo, y nuestro objetivo es agrupar aquellos que tengan caracterísiticas similares. Si vemos el resumen de los datos podemos ver que hay diferencias sumamente importantes en las escalas de las variables, por lo que será necesario escalar los datos:\n\nclust_df <- read.table(\"datos/cluster.txt\",\n                       header = T, row.names = 1) # ¡OJO con row.names!\nsummary(clust_df)\n\n      Temp            NH4             NO3              OD             Prof    \n Min.   :16.90   Min.   :0.540   Min.   :0.400   Min.   :0.810   Min.   : 27  \n 1st Qu.:18.20   1st Qu.:0.900   1st Qu.:1.830   1st Qu.:2.650   1st Qu.: 76  \n Median :19.60   Median :0.980   Median :2.500   Median :3.800   Median :122  \n Mean   :20.31   Mean   :1.047   Mean   :2.394   Mean   :4.103   Mean   :109  \n 3rd Qu.:21.40   3rd Qu.:1.250   3rd Qu.:3.120   3rd Qu.:5.840   3rd Qu.:148  \n Max.   :25.80   Max.   :1.570   Max.   :4.210   Max.   :8.010   Max.   :199  \n     Trans            Caud             SST              STD       \n Min.   : 4.00   Min.   : 0.520   Min.   :   4.0   Min.   : 86.9  \n 1st Qu.: 6.00   1st Qu.: 1.960   1st Qu.:  50.0   1st Qu.:101.5  \n Median :10.00   Median : 5.580   Median : 134.0   Median :104.3  \n Mean   :17.62   Mean   : 5.877   Mean   : 254.3   Mean   :108.7  \n 3rd Qu.:18.00   3rd Qu.: 8.100   3rd Qu.: 298.7   3rd Qu.:113.7  \n Max.   :56.00   Max.   :15.440   Max.   :1163.3   Max.   :137.7  \n      PO4             DBO5            DQO       \n Min.   :0.000   Min.   :20.21   Min.   : 5.00  \n 1st Qu.:0.250   1st Qu.:21.29   1st Qu.:16.00  \n Median :0.400   Median :23.27   Median :32.00  \n Mean   :0.391   Mean   :27.45   Mean   :32.52  \n 3rd Qu.:0.600   3rd Qu.:32.62   3rd Qu.:50.00  \n Max.   :0.870   Max.   :59.82   Max.   :62.00  \n\n\nAdemás de las diferencias en órdenes de magnitud, tenemos otro problema no tan evidente: la distribución de SST se encuentra bastante sesgada, por lo que habrá que a) aplicar una transformación para “normalizar” los datos o b) retirarla del análisis. No sabemos a qué corresponde SST, por lo que no sabemos si las mediciones están bien o mal, por lo que es mejor errar por precavidos y retirarla del análisis.\n\nmask <- colnames(clust_df)[colnames(clust_df) != \"SST\"]\nclust_filt <- clust_df[, mask]\ncolnames(clust_filt)\n\n [1] \"Temp\"  \"NH4\"   \"NO3\"   \"OD\"    \"Prof\"  \"Trans\" \"Caud\"  \"STD\"   \"PO4\"  \n[10] \"DBO5\"  \"DQO\"  \n\n\nAhora sí, podemos escalar todos nuestros datos. Si observamos el resumen es evidente que hay algunas otras variables que también están sesgadas; sin embargo, podemos utilizar una medida de distancia que contienda con este tipo de distribuciones para evitar una transformación más agresiva:\n\nclust_scale <- scale(clust_filt)\nsummary(clust_scale)\n\n      Temp             NH4               NO3                OD         \n Min.   :-1.315   Min.   :-1.8357   Min.   :-1.9515   Min.   :-1.4896  \n 1st Qu.:-0.814   1st Qu.:-0.5326   1st Qu.:-0.5518   1st Qu.:-0.6573  \n Median :-0.275   Median :-0.2430   Median : 0.1039   Median :-0.1370  \n Mean   : 0.000   Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  \n 3rd Qu.: 0.418   3rd Qu.: 0.7343   3rd Qu.: 0.7108   3rd Qu.: 0.7859  \n Max.   : 2.112   Max.   : 1.8926   Max.   : 1.7776   Max.   : 1.7675  \n      Prof             Trans              Caud               STD         \n Min.   :-1.5885   Min.   :-0.8294   Min.   :-1.19044   Min.   :-1.4307  \n 1st Qu.:-0.6393   1st Qu.:-0.7076   1st Qu.:-0.87045   1st Qu.:-0.4734  \n Median : 0.2518   Median :-0.4640   Median :-0.06603   Median :-0.2898  \n Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.00000   Mean   : 0.0000  \n 3rd Qu.: 0.7555   3rd Qu.: 0.0232   3rd Qu.: 0.49395   3rd Qu.: 0.3266  \n Max.   : 1.7435   Max.   : 2.3373   Max.   : 2.12502   Max.   : 1.9003  \n      PO4                DBO5              DQO          \n Min.   :-1.45097   Min.   :-0.7880   Min.   :-1.43661  \n 1st Qu.:-0.52313   1st Qu.:-0.6704   1st Qu.:-0.86246  \n Median : 0.03358   Median :-0.4548   Median :-0.02734  \n Mean   : 0.00000   Mean   : 0.0000   Mean   : 0.00000  \n 3rd Qu.: 0.77585   3rd Qu.: 0.5631   3rd Qu.: 0.91217  \n Max.   : 1.77792   Max.   : 3.5244   Max.   : 1.53851  \n\n\n\n\n\n\n\n\nNota\n\n\n\nRecuerda que si trabajamos con datos estandarizados estamos escalando nuestros datos a valores de una distribución Z, los cuáles representan a cuántas desviaciones estándar estamos de la media.\n\n\nAhora realicemso los agrupamientos utilizando la distancia Mahalanobis (que no es tan sensible a valores extremos) y el método de unión Ward.D2 (unión Ward con distancias cuadráticas) para minimizar la varianza intra-grupos:\n\ndist_mv1 <- vegdist(clust_scale, method = \"mahalanobis\")\nhc_mv1 <- hclust(dist_mv1, method = \"ward.D2\")\n\n# Transformamos a dendrograma\ndend_mv1 <- as.dendrogram(hc_mv1)\n\n# Cambiamos el color de las ramas\ndend_mv1 <- set(dend_mv1, \"branches_k_color\",\n            value = \"deepskyblue4\",\n            k = 1)\n\n# Cambiamos el ancho de las ramas\ndend.mv1 <- set(dend_mv1, \"branches_lwd\", 0.7)\n\n# Graficado\nggd1 <- as.ggdend(dend_mv1)\nggd1_plot <- ggplot(ggd1, offset_labels = -1, theme = theme_bw()) + \n             expand_limits(y = -2.4) +\n             scale_x_continuous(breaks = NULL) +\n             scale_y_continuous(breaks =\n                                  seq(0,max(get_branches_heights(dend_mv1)),\n                                      2)) +\n             labs(title = \"Dendrograma de sitios de muestreo\",\n                  subtitle = \"Método de agrupamiento: Ward.D2\",\n                  x = element_blank(),\n                  y = \"Distancia Mahalanobis\",\n                  caption = \"Datos: clust.txt\")\n\nggd1_plot\n\n\n\n\n¿Puedes decirme cuántos grupos hay? Veamos qué pasa si utilizamos la distancia Euclidiana:\n\ndist_mv2 <- vegdist(clust_scale, method = \"euclidean\")\nhc_mv2 <- hclust(dist_mv2, method =\"ward.D2\")\n\n# Transformemos nuestro objeto a un dendrograma:\ndend_mv2 <- as.dendrogram(hc_mv2)\n\n# Cambiemos el color a las ramas:\ndend_mv2 <- set(dend_mv2, \"branches_k_color\",\n            value = \"deepskyblue4\",\n            k = 1)\n\ndend_mv2 <- set(dend_mv2, \"branches_lwd\", 0.7)\n\nggd2 <- as.ggdend(dend_mv2)\nggd2_plot <- ggplot(ggd2, offset_labels = -1, theme = theme_bw()) + \n             expand_limits(y = -2.4) +\n             scale_x_continuous(breaks = NULL) +\n             scale_y_continuous(breaks = \n                                  seq(0,\n                                      max(get_branches_heights(dend_mv2)),\n                                      2)) +\n             labs(title = \"Dendrograma de sitios de muestreo\",\n                  subtitle = \"Método de agrupamiento: Ward.D2\",\n                  x = element_blank(),\n                  y = \"Distancia Euclidiana\",\n                  caption = \"Datos: clust.txt\")\n\nggd2_plot\n\n\n\n\n¿Y con la distancia Manhattan? Las agrupaciones resultantes son similares; sin embargo, las medidas de distancia son muy diferentes. Entonces, arbitrariamete, nos quedaremos con la distancia Euclidiana.\n\ndist_mv3 <- vegdist(clust_scale, method = \"manhattan\")\nhc_mv3 <- hclust(dist_mv3, method =\"ward.D2\")\n\n# Transformemos nuestro objeto a un dendrograma:\ndend_mv3 <- as.dendrogram(hc_mv3)\n\n# Cambiemos el color a las ramas:\ndend_mv3 <- set(dend_mv3, \"branches_k_color\",\n            value = \"deepskyblue4\",\n            k = 1)\n\ndend_mv3 <- set(dend_mv3, \"branches_lwd\", 0.7)\n\nggd3 <- as.ggdend(dend_mv3)\nggd3_plot <- ggplot(ggd3, offset_labels = -1, theme = theme_bw()) + \n             expand_limits(y = -3.5) +\n             scale_x_continuous(breaks = NULL) +\n             scale_y_continuous(breaks =\n                                  seq(0,max(get_branches_heights(dend_mv3)),\n                                             5)) +\n             labs(title = \"Dendrograma de sitios de muestreo\",\n                  subtitle = \"Método de agrupamiento: Ward.D2)\",\n                  x = element_blank(),\n                  y = \"Distancia Manhattan\",\n                  caption = \"Datos: clust.txt\")\n\nggd3_plot\n\n\n\n\n\n\n2.3.8 ¿Cuántos grupos?\nPara calcular los 30 índices de los que podemos echar mano para estimar el número “óptimo” de agrupaciones utilizaremos la función NbClust de la librería con el mismo nombre. La salida de esta función nos dice el número de índices que proponen cada número de agrupamientos (4 propusieron 12, 11 propusieron 3, etc.) y una conclusión: “De acuerdo con la regla de la mayoría, el mejor número de clústers es 3”. Además de estos índices numéricos tenemos dos índices gráficos: Hubert y D. En ambos el número óptimo está dado por una “rodilla” (un punto de inflexión).\n\nn_gps <- NbClust::NbClust(data = clust_scale,\n                          diss = dist_mv2,\n                          distance = NULL,\n                          method = \"ward.D2\",\n                          index = \"all\", max.nc = 8)\n\n\n\n\n*** : The Hubert index is a graphical method of determining the number of clusters.\n                In the plot of Hubert index, we seek a significant knee that corresponds to a \n                significant increase of the value of the measure i.e the significant peak in Hubert\n                index second differences plot. \n \n\n\n\n\n\n*** : The D index is a graphical method of determining the number of clusters. \n                In the plot of D index, we seek a significant knee (the significant peak in Dindex\n                second differences plot) that corresponds to a significant increase of the value of\n                the measure. \n \n******************************************************************* \n* Among all indices:                                                \n* 4 proposed 2 as the best number of clusters \n* 11 proposed 3 as the best number of clusters \n* 1 proposed 5 as the best number of clusters \n* 2 proposed 7 as the best number of clusters \n* 5 proposed 8 as the best number of clusters \n\n                   ***** Conclusion *****                            \n \n* According to the majority rule, the best number of clusters is  3 \n \n \n******************************************************************* \n\n\n\n\n\n\n\n\nNota\n\n\n\nAlgunas peculiaridades a la cuáles poner atención: la función a) recibe los datos a agrupar (después de cualquier procesado); b) puede recibir o la matriz de distancias (diss) o calcularla internamente (distance = \"method\"); c) podemos decir qué índice queremos utilizar con el argumento index (todos con index = \"all\"); y d) hay que definir un número máximo de clústers a evaluar. Este número debe de ser lo suficientemente grande para permitir al algoritmo probar distintas cantidades de agrupaciones y lo suficientemente pequeño para seguir siendo interpretable. En este cso tenemos 21 sitios, por lo que tener 10 o más grupos pudiera ya no tener caso.\n\n\nYa tenemos nuesro número de grupos (\\(k = 3\\)), pero ¿podemos confiar en nuestro dendrograma? Calculemos el coeficiente de correlación cofenético:\n\nccc <- cophenetic(hc_mv2)\n# Correlación entre distancias reales y graficadas\nccofen <- cor(dist_mv2, ccc, method = \"spearman\")\nccofen\n\n[1] 0.6868661\n\n\nFue de prácticamente el 70%, que para nuestros objetivos es suficiente. Ahora veamos nuestros grupos en el dendrograma:\n\n# Cambiamos el color de nuestr\ndend_mv2 <- set(dend_mv2, \"labels_col\",\n                value = 1:3,\n                k = 3)\n\ndend_mv2 <- set(dend_mv2, \"branches_k_color\",\n            value = 1:3,\n            k = 3)          \n          \ndend_mv2 <- set(dend_mv2, \"branches_lwd\", 0.7)\n\nggd2 <- as.ggdend(dend_mv2)\nggd2_plot <- ggplot(ggd2, offset_labels = -1, theme = theme_bw()) + \n             expand_limits(y = -2.4) +\n             scale_x_continuous(breaks = NULL) +\n             scale_y_continuous(breaks = seq(0,max(get_branches_heights(dend_mv2)),2)) +\n             labs(title = \"Dendrograma de sitios de muestreo\",\n                  subtitle = \"Método de agrupamiento: Ward.D2\",\n                  x = element_blank(),\n                  y = \"Distancia Euclidiana\",\n                  caption = \"Datos: clust.txt\")\n\nggd2_plot\n\n\n\n\n¿Cómo podemos interpretar este dendrograma? Hay un montón de maneras, pero todas se reducen a ver cómo se relacionan estos grupos con las variables originales; es decir, no vamos a simplemente asumir que tenemos una zona alta, intermedia y baja del río solo porque en un grupo tengamos los primerios sitios, en otro los que le siguen, y en un último los faltantes. Esos son cuentos chinos. Una forma es revisar los centroides (promedios) de cada variable por cada grupo:\n\naggregate(clust_filt, by = list(n_gps$Best.partition), FUN = mean)\n\n\n\n  \n\n\n\nEsto evidencía gradientes en cada variable, algunos positivos (Temp, NH4, por ejemplo) y algunos negativos (e.g., NO3, OD). ¿Qué quiere decir esto? Bueno, esa interpretación ya depende del conocimiento que tengamos del área (tanto del conocimiento como del área geográfica).\n\n\n\n\n\n\nNota\n\n\n\n¿Es esta la única manera de estimar el número de grupos? No. Puedes seleccionar solo una de estas medidas (incluso puedes hacerlo dentro de NbClust con el argumento index), pero también puedes utilizar la librería pvclust. Su funcionamiento es demasiado rebuscado para describirlo en una nota, pero puedes ver la documentación correspondiente."
  },
  {
    "objectID": "c15_nosup.html#reducción-de-la-dimensionalidad",
    "href": "c15_nosup.html#reducción-de-la-dimensionalidad",
    "title": "2  Aprendizaje No Supervisado",
    "section": "2.4 Reducción de la dimensionalidad",
    "text": "2.4 Reducción de la dimensionalidad\nHablemos ahora de la otra parte del aprendizaje no supervisado: la reducción de la dimensionalidad. Una manera “sencilla” de entenderla es lo que pasa cuando nosotros nos ponemos al sol (somos tridimensionales) y vemos nuestra sombra (un plano): perdemos los detalles más finos, pero nuestra estructura general se mantiene. Con nuestros datos multidimensionales haremos algo similar (Figura 2.16): proyectar datos multidimensionales a un espacio con menos dimensiones (usualmente una o dos).\n\n\n\nFigura 2.16: Proyectando a menos dimensiones. En tres dimensiones solo vemos una nube de puntos de colores, en dos dimensiones vemos que están “agrupados”, mientras que en una sola tenemos un gradiente continuo.\n\n\nInfortunadamente para nosotros, el proceso de reducir dimensiones no es tan sencillo como ir poniendo las coordenadas de dimensiones enteras en 0, pero antes de entrar a ver el cómo, veamos el por qué, utilizando una de las aplicaciones más famosas: el índice multivariado del ENSO.\n\n2.4.1 Índice Multivariado del ENSO\nSi eres parte de alguna carrera de ciencias marinas es muy posible que hayas escuchado del fenómeno del Niño, si no, aprenderás algo interesante. Su nombre completo es “El Niño Oscilación del Sur” (El Niño Southern Oscilation, ENSO; Figura 2.17), y hace referencia a un fenómeno de variabilidad ambiental interanual que tiene una fase cálida (“El Niño”) y una fase fría (“La Niña”).\nDurante la fase fría (condiciones “normales”) los vientos alisios soplan con fuerza del este al oeste en el ecuador, arrastrando consigo la capa superficial del océano y acumulando agua en las costas del Pacífico central occidental. Como sabrás, el ecuador recibe la mayor cantidad de calor solar, por lo que el agua arrastrada es agua caliente, haciendo que la termoclina sea más profunda en el occidente que en el oriente del Pacífico. Esto permite que haya una surgencia costera (emergimiento de agua profunda, rica en nutrientes, a la superficie) muy importante que sostiene pesquerías importantísimas en la corriente de Humboldt. Además, la radiación solar también hace que haya evaporación en el ecuador, por lo que el aire que circula vaya recogiendo humedad. Conforme este aire caliente va recogiendo humedad se va enfriando, perdiendo densidad, y comienza a ascender, lo cual se ve acelerado cuando choca con el continente en el Pacífico occidental. ¿Qué pasa cuando enfriamos aire húmedo rápidamente? Se forma condensación, lo cual se traduce en lluvias abundantes en esa zona. El aire entonces pierde toda su humedad, se enfría y la circulación en la atmósfera alta se invierte, por lo que hacia América viaja aire frío y seco (denso), lo cual ocasiona que descienda en América, reiniciando el ciclo. A este proceso se le conoce como celda de circulación de Walker, y como resultado tenemos una zona de baja presión en el Indo-Pacífico (sube aire) y una zona de alta presión (baja aire) cerca de América.\nDurante la fase cálida (“El Niño”) los vientos alisios se debilitan (en algunas zonas del Pacífico central el viento sopla en dirección contraria), por lo que el agua caliente que se había acumulado en el occidente se regresa a América, hundiendo la termoclina, lo que causa que la surgencia costera “recicle” el agua caliente superficial, en vez de subir agua fría, y que llueva en América (se “invierte” la celda de Walker).\n\n\n\nFigura 2.17: Fases cálida (“El Niño”) y fría (“La Niña”) del ENSO.\n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nSimplifiqué la explicación del ENSO lo más que pude sin dejar demasiadas cosas del proceso en el tintero, pero el costo es que haya algunas imprecisiones en la explicación. Si te interesa este tema, te recomiendo que leas más al respecto. Entre tus lecturas te encontrarás el por qué de “El Niño”.\n\n\nComo te podrás imaginar, es un proceso sumamente complejo (de hecho comunica el Atlántico con el Pacífico, pero eso es otra historia) en el que se ven modificadas una gran cantidad de variables ambientales y cuyas consecuencias económicas son muy importantes (se modifica la pesca y la agricultura en prácticamente tres continentes). Podemos monitorear cada variable de manera individual, pero ¿no sería mejor poder resumirlas todas en un solo número? Pues eso fue lo que hicieron Wolter & Timlin (1998), aplicando un Análisis de Componentes principales a seis variables (presión atmosférica a nivel del mar (P), componentes zonal (U) y meridional (V) del viento superficial, temperatura superficial del mar (S), temperatura superficial del aire (A), y la fracción total de nubes en el cielo (C)) en el Pacífico tropical, lo cual da lugar al Índice Multivariado del ENSO (Figura 2.18).\n\n\n\nFigura 2.18: Índice Multivariado del ENSO (MEI) y las variables que lo construyen.\n\n\n\n\n\n\n\n\nNota\n\n\n\nEl índice que se utiliza ahora es la versión 2 del MEI, el cual considera la radiación de onda larga (OLR) en vez de la la temperatura superficial del aire y la fracción total de nubes en el cielo.\n\n\n\n\n2.4.2 Análisis de componentes principales\nEspero que este ejemplo te haya motivado, pues el MEI es uno de los instrumentos más importantes en el monitoreo de las condiciones ambientales del Pacífico, así es que vamos a ver cómo funciona la técnica que lo origina: el análisis de componentes principales (PCA). Este análisis aproxima los datos utilizando una menor cantidad de variables (con \\(\\mu = 0\\) y \\(\\sigma = 1\\)), las cuales son ortogonales (independientes) entre sí. ¿Cómo hace esto? Respuesta corta: buscando la mejor proyección de los datos originales (Figura 2.19).\n\n\n\nFigura 2.19: Proyección de datos bivariados a una sola variable.\n\n\n\n\n\n\n\n\nNota\n\n\n\nVamos a ver un poco de los detalles matemáticos detrás del PCA. No te preocupes, solo es para expandir la noción detrás del PCA más allá de solo “buscar la mejor proyección de los datos originales”. Al igual que en el ?sec-rls, puedes quedarte para toda la explicación o saltar al corolario al final.\n\n\nEsa respuesta corta fue, tal vez, muy corta, así que entremos en más detalles. El PCA encuentra una de dos líneas: a) una línea que minimice el error promedio; b) una línea que maximice la distancia al origen (suma de cuadrados, \\(SS\\)). Curiosamente, es más fácil encontrar la segunda línea utilizando un viejo conocido: el teorema de Pitágoras. Observa la Figura 2.20. Si hacemos la longitud de \\(c\\) lo suficientemente grande, el valor de \\(b\\) se hace más pequeño, ya que \\(a\\) no se mueve.\n\n\n\n\n\n\nImportante\n\n\n\nSi lo que estamos encontrando es una línea que maximice la distancia al origen, y esta línea representa una suma de cuadrados, esa línea está capturando una porción de la varianza de los datos.\n\n\n\n\n\nFigura 2.20: Teorema de Pitágoras, de nuevo. Dado que ya tenemos la longitud de \\(a\\), dada por el dato en sí mismo, podemos encontrar una línea teórica que minimice \\(b\\) (el error), o que maximice \\(c\\) (la distancia al origen). Por facilidad, esta línea \\(c\\) es la proyección de los datos.\n\n\nEsta nueva línea es nuestra proyección. En el espacio multivariado cada componente principal (CP) es una combinación lineal de nuestras variables originales, donde la pendiente de cada variable representa qué tanto contribuye esa variable a la varianza capturada por el CP. Después de haber encontrado esa primera línea vamos a girarla hasta que tenga una pendiente de 0º, el resultado es un nuevo eje (variable) llamado componente principal (CP).\n\n\n\nFigura 2.21: Variables ortogonales\n\n\nLo interesante es que si escalamos nuestra línea \\(c^2\\) a que tenga una longitud de 1 tenemos un eigenvector, en donde las pendientes se convierten en proporciones que dan las cargas factoriales de cada variable a cada CP. Por otra parte, si \\(c^2\\) es una suma de cuadrados es también el eigenvalor de cada CP, y la \\(\\sqrt{SS}\\) es su valor singular. Esto del valor singular es solo breviario cultural, lo que sí es importante es que tenemos una suma de cuadrados, tal y como tenemos en la definición de la varianza (Capítulo 1), por lo que si la dividimos entre los grados de libertad de la muestra podemos obtener la varianza capturada por ese CP. ¿Por qué por ese CP? ¿Recuerdas que mencioné en la definición de PCA que aproximamos los datos utilizando una menor cantidad de variables? Bueno, esta línea que encontramos es solo una de estas variables (llamadas CPs). El siguiente paso es trazar un nuevo eje (variable, CP) a exactamente 90º de la primera (Figura 2.21), y después repetir el proceso hasta haber capturado toda la varianza. Ya que hicimos esto podemos estimar la porción de la varianza explicada por el CP dividiendo su varianza explicada entre la varianza total:\n\\[\n\\sigma^2_{PC_i} = \\frac{SS}{n-1} \\therefore \\sigma^2_{T} = \\sum_{i = 1}^k\\sigma^2_{PC_i} \\therefore \\sigma^2_{exp_i} = \\frac{\\sigma^2_{PC_i}}{\\sigma^2_T}\n\\]\n\n\n\n\n\n\nNota\n\n\n\n¿Qué son los eigenvalores y eigenvectores? Las eigencosas son de las cosas más abstractas en el álgebra lineal, pero podemos resumirlas como que los eigenvalores son una medida de dispersión, mientras que los eigenvectores son una medida de dirección.\n\n\n\n\n\n\n\n\nImportante\n\n\n\nEn resumen: el PCA es una técnica de reducción de dimensionalidad en la que los datos originales se aproximan mediante combinaciones lineales (componentes principales), las cuales son independientes (ortogonales) entre sí. Este procedimiento permite resumir los datos originales en una o dos variables que preservan la mayor cantidad de la varianza original posible.\n\n\nDespués de esta vuelta (para algunos innecesaria), apliquemos el PCA.\n\n2.4.2.1 Implementación\nUtilicemos una base de datos con una cantidad interesante de variables (31): Medidas.txt:\n\nbiom <- read.table(\"datos/Medidas.txt\", header = TRUE)\nhead(biom)\n\n\n\n  \n\n\n\nLa primera columna tiene etiquetas de especies, las cuales no utilizaremos en este momento, por lo que podemos descartarla:\n\npca_data <- biom[, 2:length(biom)]\nsummary(pca_data)\n\n     furcal            base             radio              alto        \n Min.   :0.2527   Min.   :0.07088   Min.   :0.04408   Min.   :0.05319  \n 1st Qu.:0.2716   1st Qu.:0.09108   1st Qu.:0.06748   1st Qu.:0.06709  \n Median :0.2753   Median :0.09679   Median :0.07090   Median :0.07140  \n Mean   :0.2757   Mean   :0.09645   Mean   :0.07048   Mean   :0.07116  \n 3rd Qu.:0.2801   3rd Qu.:0.10142   3rd Qu.:0.07430   3rd Qu.:0.07534  \n Max.   :0.3202   Max.   :0.11394   Max.   :0.08311   Max.   :0.09723  \n     gordo             angos              ojo             X1erarc       \n Min.   :0.02679   Min.   :0.02996   Min.   :0.02142   Min.   :0.03485  \n 1st Qu.:0.04139   1st Qu.:0.03465   1st Qu.:0.02451   1st Qu.:0.04332  \n Median :0.04402   Median :0.03577   Median :0.02673   Median :0.05129  \n Mean   :0.04405   Mean   :0.03616   Mean   :0.02740   Mean   :0.05158  \n 3rd Qu.:0.04680   3rd Qu.:0.03745   3rd Qu.:0.02875   3rd Qu.:0.05899  \n Max.   :0.05859   Max.   :0.04727   Max.   :0.08191   Max.   :0.07738  \n     dist.              largo              cabez             hueso        \n Min.   :0.007179   Min.   :0.004202   Min.   :0.07311   Min.   :0.02857  \n 1st Qu.:0.030485   1st Qu.:0.022402   1st Qu.:0.08476   1st Qu.:0.03236  \n Median :0.035368   Median :0.029488   Median :0.08951   Median :0.03415  \n Mean   :0.031366   Mean   :0.026533   Mean   :0.08860   Mean   :0.03488  \n 3rd Qu.:0.038824   3rd Qu.:0.034574   3rd Qu.:0.09284   3rd Qu.:0.03582  \n Max.   :0.048008   Max.   :0.051843   Max.   :0.09815   Max.   :0.10411  \n     mandib           hueso.1            arriba            cabeza       \n Min.   :0.03141   Min.   :0.02348   Min.   :0.01855   Min.   :0.07118  \n 1st Qu.:0.03598   1st Qu.:0.03917   1st Qu.:0.02546   1st Qu.:0.08309  \n Median :0.03980   Median :0.04413   Median :0.02952   Median :0.08796  \n Mean   :0.03923   Mean   :0.04498   Mean   :0.02946   Mean   :0.08890  \n 3rd Qu.:0.04247   3rd Qu.:0.04995   3rd Qu.:0.03391   3rd Qu.:0.09483  \n Max.   :0.04758   Max.   :0.08783   Max.   :0.03828   Max.   :0.10760  \n      boca             abiert           cachete           largo.1        \n Min.   :0.02828   Min.   :0.01810   Min.   :0.02938   Min.   :0.008724  \n 1st Qu.:0.03476   1st Qu.:0.02348   1st Qu.:0.03911   1st Qu.:0.012881  \n Median :0.03830   Median :0.02531   Median :0.04308   Median :0.014240  \n Mean   :0.03954   Mean   :0.02547   Mean   :0.04237   Mean   :0.014366  \n 3rd Qu.:0.04368   3rd Qu.:0.02741   3rd Qu.:0.04508   3rd Qu.:0.015936  \n Max.   :0.05552   Max.   :0.03248   Max.   :0.06553   Max.   :0.026475  \n     ancho            orificio           densi            hocic        \n Min.   :0.00274   Min.   :0.01968   Min.   :0.6990   Min.   :0.01546  \n 1st Qu.:0.02663   1st Qu.:0.03741   1st Qu.:0.7782   1st Qu.:0.02491  \n Median :0.03031   Median :0.04381   Median :0.8451   Median :0.02929  \n Mean   :0.03253   Mean   :0.04429   Mean   :0.8401   Mean   :0.02937  \n 3rd Qu.:0.03447   3rd Qu.:0.05085   3rd Qu.:0.9031   3rd Qu.:0.03294  \n Max.   :0.22628   Max.   :0.07390   Max.   :1.1139   Max.   :0.04922  \n    interbr            ojoatra            proye              base.1       \n Min.   :0.007687   Min.   :0.02685   Min.   :0.007313   Min.   :0.01546  \n 1st Qu.:0.014457   1st Qu.:0.03796   1st Qu.:0.013341   1st Qu.:0.02052  \n Median :0.015776   Median :0.04050   Median :0.014982   Median :0.02238  \n Mean   :0.016304   Mean   :0.04067   Mean   :0.016217   Mean   :0.02300  \n 3rd Qu.:0.017609   3rd Qu.:0.04256   3rd Qu.:0.016613   3rd Qu.:0.02474  \n Max.   :0.028804   Max.   :0.08124   Max.   :0.127873   Max.   :0.04476  \n     altura             area             Intesti      \n Min.   :0.02514   Min.   :0.001431   Min.   :0.1295  \n 1st Qu.:0.03815   1st Qu.:0.006723   1st Qu.:0.1774  \n Median :0.04422   Median :0.009716   Median :0.1960  \n Mean   :0.04474   Mean   :0.010638   Mean   :0.1980  \n 3rd Qu.:0.04928   3rd Qu.:0.014274   3rd Qu.:0.2150  \n Max.   :0.08398   Max.   :0.027268   Max.   :0.3246  \n\n\nApliquemos el Análisis de Componentes Principales:\n\n#Aplicamos el PCA con los datos escalados y centrados\nbiom_pca <- FactoMineR::PCA(pca_data,\n                            graph = F,\n                            ncp = length(pca_data),\n                            scale.unit = T) \n\nRecordarás que el PCA está basado en el teorema de Pitágoras, por lo que es necesario que todas las variables estén en la misma escala. Podemos utilizar la función scale() como hicimos para el análisis clúster, o podemos indicarle a R que lo haga automáticamente si utilizamos la función FactoMineR::PCA() con el argumento scale.unit. Podemos ver los resultados de manera numérica:\n\nsummary(biom_pca)\n\n\nCall:\nFactoMineR::PCA(X = pca_data, scale.unit = T, ncp = length(pca_data),  \n     graph = F) \n\n\nEigenvalues\n                       Dim.1   Dim.2   Dim.3   Dim.4   Dim.5   Dim.6   Dim.7\nVariance               6.146   4.632   2.809   2.271   1.685   1.550   1.260\n% of var.             19.826  14.943   9.061   7.325   5.436   5.001   4.065\nCumulative % of var.  19.826  34.769  43.829  51.154  56.590  61.591  65.655\n                       Dim.8   Dim.9  Dim.10  Dim.11  Dim.12  Dim.13  Dim.14\nVariance               1.108   1.059   0.966   0.845   0.756   0.727   0.628\n% of var.              3.574   3.416   3.117   2.725   2.440   2.344   2.025\nCumulative % of var.  69.230  72.646  75.763  78.487  80.927  83.271  85.295\n                      Dim.15  Dim.16  Dim.17  Dim.18  Dim.19  Dim.20  Dim.21\nVariance               0.530   0.512   0.450   0.398   0.377   0.355   0.308\n% of var.              1.708   1.653   1.452   1.285   1.215   1.144   0.992\nCumulative % of var.  87.003  88.656  90.108  91.393  92.608  93.752  94.745\n                      Dim.22  Dim.23  Dim.24  Dim.25  Dim.26  Dim.27  Dim.28\nVariance               0.285   0.266   0.208   0.193   0.168   0.143   0.133\n% of var.              0.920   0.858   0.671   0.622   0.543   0.462   0.428\nCumulative % of var.  95.665  96.523  97.194  97.815  98.358  98.820  99.248\n                      Dim.29  Dim.30  Dim.31\nVariance               0.107   0.076   0.050\n% of var.              0.345   0.244   0.162\nCumulative % of var.  99.593  99.838 100.000\n\nIndividuals (the 10 first)\n             Dist    Dim.1    ctr   cos2    Dim.2    ctr   cos2    Dim.3    ctr\n1        |  6.196 | -1.768  0.489  0.081 |  2.830  1.663  0.209 |  2.494  2.129\n2        | 10.682 | -2.869  1.288  0.072 |  2.272  1.072  0.045 |  2.522  2.177\n3        |  4.520 | -2.199  0.757  0.237 |  1.517  0.478  0.113 |  0.853  0.249\n4        |  4.079 | -2.569  1.033  0.397 |  1.812  0.681  0.197 |  0.994  0.338\n5        |  4.642 | -2.728  1.165  0.345 |  2.026  0.852  0.190 |  1.343  0.618\n6        |  4.142 | -2.810  1.236  0.460 | -0.572  0.068  0.019 |  0.359  0.044\n7        |  3.740 | -1.366  0.292  0.133 |  1.066  0.236  0.081 |  0.765  0.200\n8        |  5.488 | -3.158  1.560  0.331 |  2.032  0.857  0.137 |  2.699  2.494\n9        |  4.753 | -2.983  1.392  0.394 |  1.637  0.556  0.119 |  1.324  0.600\n10       |  6.548 | -2.115  0.700  0.104 |  3.068  1.954  0.220 |  1.928  1.272\n           cos2  \n1         0.162 |\n2         0.056 |\n3         0.036 |\n4         0.059 |\n5         0.084 |\n6         0.008 |\n7         0.042 |\n8         0.242 |\n9         0.078 |\n10        0.087 |\n\nVariables (the 10 first)\n            Dim.1    ctr   cos2    Dim.2    ctr   cos2    Dim.3    ctr   cos2  \nfurcal   | -0.174  0.494  0.030 |  0.113  0.275  0.013 | -0.221  1.732  0.049 |\nbase     | -0.082  0.109  0.007 |  0.085  0.155  0.007 | -0.317  3.582  0.101 |\nradio    | -0.294  1.409  0.087 | -0.045  0.044  0.002 | -0.246  2.160  0.061 |\nalto     | -0.129  0.272  0.017 |  0.323  2.251  0.104 |  0.442  6.968  0.196 |\ngordo    |  0.243  0.959  0.059 |  0.256  1.414  0.065 |  0.496  8.744  0.246 |\nangos    | -0.193  0.608  0.037 |  0.422  3.838  0.178 |  0.162  0.930  0.026 |\nojo      | -0.159  0.411  0.025 | -0.209  0.942  0.044 |  0.060  0.129  0.004 |\nX1erarc  | -0.764  9.497  0.584 |  0.282  1.715  0.079 | -0.005  0.001  0.000 |\ndist.    | -0.436  3.098  0.190 |  0.285  1.758  0.081 |  0.155  0.854  0.024 |\nlargo    | -0.650  6.870  0.422 |  0.357  2.755  0.128 |  0.220  1.720  0.048 |\n\n\nLa salida es bastante extensa, pero lo que realmente nos interesa es lo que aparece en Eigenvalues: La varianza de cada componente, el porcentaje de la varianza total que representa, y cuánta varianza se ha acumulado hasta ese componente, pero siempre es mejor ver los resultados de manera gráfica con algunas funciones de la librería factoextra:\n\nfactoextra::fviz_pca_var(biom_pca, col.var = \"coord\",\n                         gradient.cols = c(\"#00AFBB\",\n                                           \"#E7B800\",\n                                           \"#FC4E07\"))\n\n\n\n\nEn este gráfico tenemos representada la importancia de variables (carga factorial) para cada componente principal. Entre más larga (y naranja) sea la flecha, mayor es su importancia. Entre más horizontal esté, más contribuye al componente principal representado en el eje \\(x\\), y entre más vertical esté, más contribuye al componente principal puesto en el eje \\(y\\), y la dirección de la relación con el CP está dada por el signo. En nuestro ejemplo, las variables abiert, boca, arriba, mandib y cabez están positivamente relacionadas con el CP1, pero negativamente relacionadas con el CP2; es decir, entre más grande sea la longitud de la mandíbula, por ejemplo, más grande será la coordenada en el CP1, pero también se hará más pequeña en el CP2. Otra forma de ver estos resultados es tratarlos como si fueran una matriz de correlaciones:\n\nbiom_pca_vars <- factoextra::get_pca_var(biom_pca)\ncorrplot::corrplot(biom_pca_vars$coord)\n\n\n\n\n¿Por qué es esto importante? El PCA reduce las dimensiones tratando de preservar la mayor cantidad de varianza (covarianza) posible en cada PC. Una consecuencia es que los puntos más parecidos se acercarán más entre sí, mientras que los más alejados se alejarán más. A lo que esto nos lleva es a una forma de ordenación, entonces podemos graficar este nuevo espacio reducido y ver cómo quedan distribuidos nuestros datos:\n\nfviz_pca_ind(biom_pca,\n             geom.ind = \"point\", # Mostrar solo puntos (sin textos)\n             col.ind = biom[,1], # Variable de agrupamiento\n             palette = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), # Colores a utilizar\n             addEllipses = TRUE, # Elipses de concentración\n             legend.title = \"Especies\")\n\n\n\n\nAquí podemos ver que las especies jordani y labarcae se encuentran bien diferenciadas en el espacio multivariado, mientras que consocium tiene mediciones intermedias.\n\n\n\n\n\n\nAdvertencia\n\n\n\nAlgunas personas consideran el PCA como una técnica de ordenación; sin embargo, el objetivo no es ese, sino solo una consecuencia de reducir las dimensiones, tal y como vimos en la Figura 2.16.\n\n\n¿Cómo están dadas las diferencias? Puedes volver al gráfico de variables del PCA y ver cómo apuntan las flechas. Por ejemplo, labarcae tiene medidas más grandes en abiert, boca, arriba, mandib y cabez que las otras dos. Una forma más directa sería graficar ambas cosas a la vez:\n\nfviz_pca_biplot(biom_pca, \n                col.ind = biom$especie, palette = \"jco\", \n                addEllipses = TRUE, label = \"var\",\n                col.var = \"black\", repel = TRUE,\n                legend.title = \"Especies\") \n\n\n\n\n\n\n\n\n\n\nNota\n\n\n\nEn el Capítulo 4 vamos a ver cómo aquí estamos dejando de lado la información más valiosa de toda la base de datos: las etiquetas de los grupos. ¿La razón? Aquí las estamos utilizando solo como una ayuda visual para la interpretación, más que como datos que informen nuestros análisis.\n\n\nAhora bien, ya sabemos cómo interpretar nuestros componentes principales. La pregunta es, ¿capturan la suficiente información de la varianza?\n\n\n2.4.2.2 Evaluación de los componentes principales\nRegresemos a las varianzas explicadas por cada componente:\n\neig_val <- get_eigenvalue(biom_pca)\neig_val\n\n       eigenvalue variance.percent cumulative.variance.percent\nDim.1  6.14608922       19.8260942                    19.82609\nDim.2  4.63221636       14.9426334                    34.76873\nDim.3  2.80882601        9.0607291                    43.82946\nDim.4  2.27068129        7.3247783                    51.15424\nDim.5  1.68514008        5.4359358                    56.59017\nDim.6  1.55020741        5.0006691                    61.59084\nDim.7  1.26002881        4.0646091                    65.65545\nDim.8  1.10797871        3.5741249                    69.22957\nDim.9  1.05894728        3.4159590                    72.64553\nDim.10 0.96627425        3.1170137                    75.76255\nDim.11 0.84470657        2.7248599                    78.48741\nDim.12 0.75624648        2.4395048                    80.92691\nDim.13 0.72656100        2.3437452                    83.27066\nDim.14 0.62761130        2.0245526                    85.29521\nDim.15 0.52955055        1.7082276                    87.00344\nDim.16 0.51240639        1.6529238                    88.65636\nDim.17 0.45010263        1.4519440                    90.10830\nDim.18 0.39821456        1.2845631                    91.39287\nDim.19 0.37664998        1.2149999                    92.60787\nDim.20 0.35475436        1.1443689                    93.75224\nDim.21 0.30760753        0.9922823                    94.74452\nDim.22 0.28534181        0.9204574                    95.66498\nDim.23 0.26608062        0.8583246                    96.52330\nDim.24 0.20787739        0.6705722                    97.19387\nDim.25 0.19269314        0.6215908                    97.81546\nDim.26 0.16829393        0.5428837                    98.35835\nDim.27 0.14322652        0.4620210                    98.82037\nDim.28 0.13259938        0.4277399                    99.24811\nDim.29 0.10706999        0.3453871                    99.59350\nDim.30 0.07572896        0.2442870                    99.83778\nDim.31 0.05028750        0.1622177                   100.00000\n\n\nLas primeras dos dimensiones (los primeros dos CPs) capturan menos del 50% de la varianza total; por lo tanto, es prudente preguntarse cuántos CPs debemos de considerar. Una alternativa es el análisis gráfico de la varianza con un scree-plot. En este gráfico buscamos un punto de inflexión, el cuál indicaría el límite de los PCs representativos:\n\nfviz_eig(biom_pca, addlabels = T)\n\n\n\n\n¿Dónde considerarías que está el primer punto de inflexión? Yo en el tercer o cuarto componente, pero veamos otros dos criterios. El primero es el criterio de Kaiser-Guttman, en el cual consideraremos aquellos CPs cuya varianza explicada sea superior al valor promedio. El segundo criterio es un modelo de repartición de recursos desarrollado en la ecología de comunidades: el modelo Broken-Stick [McArthur_1957], donde el recurso compartido es la varianza total, entonces consideraremos aquellos CPs que tengan una varianza explicada superior a la esperada según el modelo. Para graficar ambos criterios a la vez utilizaremos una función personalizada (evplot):\n\n# Función para la estimación de los CP óptimos:\nevplot <- function(ev){\n    # Author:\n  \n    # Broken stick model (MacArthur 1957)\n      n <- length(ev)\n      bsm <- data.frame(j=seq(1:n), p=0)\n      bsm$p[1] <- 1/n \n      for (i in 2:n) bsm$p[i] <- bsm$p[i-1] + (1/(n + 1 - i))\n      bsm$p <- 100*bsm$p/n\n      \n    # Plot eigenvalues and % of variation for each axis\n      op <- par(mfrow=c(2,1), mar = c(1,1,1,1))\n      barplot(ev, main=\"Eigenvalues\", col=\"bisque\", las=2)\n        abline(h=mean(ev), col=\"red\")\n        legend(\"topright\", \"Average eigenvalue\", lwd=1, col=2, bty=\"n\")\n      barplot(t(cbind(100*ev/sum(ev), bsm$p[n:1])), beside=TRUE, \n              main=\"% variation\", col=c(\"bisque\",2), las=2)\n        legend(\"topright\", c(\"% eigenvalue\", \"Broken stick model\"), \n               pch=15, col=c(\"bisque\",2), bty=\"n\")\n      par(op)\n}\n\n# Extraemos los eigenvalores\nev <- as.data.frame(eig_val)$eigenvalue\n# Asignamos nombres a los valores\nnames(ev) <- paste(\"CP\",seq_along(ev), \"\")\n# Graficamos con la función personalizada\nevplot(ev)\n\n\n\n\nBajo el criterio de Kaiser-Guttman consideraríamos 8 CPs, mientras que en el modelo de Broken-Stick consideraríamos únicamente 4. De cualquier modo, ambos criterios sugieren un número mayor al que yo consideré en el gráfico de la varianza. ¿Qué pasa entonces con nuestra interpretación original con respecto a la separación de las especies? Ya no es tan robusta, pero no significa que no sea válida. El MEI (v1), por ejemplo, es el primer componente principal y representa “solo” el 31% de la varianza total, pero aún así es útil para conocer en qué fase del fenómeno ENSO (y su intensidad) se encuentra un mes/año dado.\nAhora bien, si la ordenación es solo una consecuencia de la reducción de la dimensionalidad, ¿tenemos alguna técnica donde el objetivo sea la ordenación?"
  },
  {
    "objectID": "c15_nosup.html#ordenación",
    "href": "c15_nosup.html#ordenación",
    "title": "2  Aprendizaje No Supervisado",
    "section": "2.5 Ordenación",
    "text": "2.5 Ordenación\nEn una ordenación el objetivo es que las observaciones (instancias) que más se parezcan entre sí se mantengan lo más cercanas posible. Un ejemplo de esto, análogo al PCA, es el Escalamiento multidimensional no métrico (NMDS).\n\n2.5.1 Escalamiento Multidimensional No Métrico (NMDS)\nA diferencia del PCA en el cual es importante que las variables estén centradas y estandarizadas (o que las variables sean del mismo tipo; e.g., biometrías en distintas especies de peces), el Escalamiento Multidimensional No Métrico (NMDS) es una técnica más flexible al estar basada en órdenes de rangos (distancias) para la ordenación. Como resultado, puede aceptar una variedad mayor de tipos de datos. Otra diferencia es que en el ACP el objetivo primordial no es la ordenación, sino la reducción de la dimensionalidad, mientras que en NMDS sí se busca una ordenación.\nEl NMDS ordena los objetos según sus similitudes, y en el proceso representa el espacio en menos dimensiones. Esta técnica minimiza la deformación de las distancias (representada por el “estrés”). Todo eso suena muy bien, ¿hay algún bemol? Sí. Es un proceso iterativo; es decir, requiere de múltiples vueltas para llegar a una solución estable. Veamos entonces cómo funciona utilizando los siguientes datos tridimensionales:\n\n\n\nFigura 2.22: ¿Cómo nos ordenamos?\n\n\n\nCalcular la matriz de distancias y asignarles un rango, donde a la menor distancia le corresponde 1.\n\n\n\n\nFigura 2.23: Matriz de distancias y sus rangos.\n\n\n\nProponer un acomodo “aleatorio” en el nuevo espacio con menos dimensiones. Digo “aleatorio” porque en realidad se siguen algunas heurísticas para comenzar desde un ordenamiento no tan alejado.\n\n\n\n\nFigura 2.24: Primer acomodo.\n\n\n\nVolver a calcular la matriz de distancias y sus rangos y evaluar las diferencias. En este caso, la distancia entre A y B es demasiado grande, por lo que los acercamos en un nuevo acomodo:\n\n\n\n\nFigura 2.25: Segundo acomodo.\n\n\n\nRepetir la operación hasta que la diferencia entre la matriz de distancias y rangos originales con la representación sea mínima; es decir, minimizar el estrés. Como regla general, valores de estrés de 0-0.1 son considerados buenos; de 0.1-0.2, aceptables; de 0.2-0.3, altos; y con más de 0.3 se considera que la representación mostrada es “aleatoria”\n\nPara analizar este caso utilicemos los mismos datos del caso sitios (clúster):\n\n# Matriz de distancias\ndist_nmds <- dist_mv2\nmds <- metaMDS(dist_nmds, distance = \"euclidean\", k = 2, trace = T)\n\nRun 0 stress 0.1033851 \nRun 1 stress 0.1033851 \n... Procrustes: rmse 6.525117e-05  max resid 0.0002061651 \n... Similar to previous best\nRun 2 stress 0.1033851 \n... Procrustes: rmse 5.502531e-06  max resid 1.157986e-05 \n... Similar to previous best\nRun 3 stress 0.1033851 \n... Procrustes: rmse 0.0001252853  max resid 0.0003980628 \n... Similar to previous best\nRun 4 stress 0.1033851 \n... Procrustes: rmse 2.745622e-05  max resid 8.616622e-05 \n... Similar to previous best\nRun 5 stress 0.1188198 \nRun 6 stress 0.1642917 \nRun 7 stress 0.2027265 \nRun 8 stress 0.2139949 \nRun 9 stress 0.1188198 \nRun 10 stress 0.1925188 \nRun 11 stress 0.1188198 \nRun 12 stress 0.1642917 \nRun 13 stress 0.1188198 \nRun 14 stress 0.3751682 \nRun 15 stress 0.1033851 \n... Procrustes: rmse 3.963516e-05  max resid 0.0001197702 \n... Similar to previous best\nRun 16 stress 0.1188198 \nRun 17 stress 0.1033851 \n... Procrustes: rmse 7.537842e-05  max resid 0.0002391825 \n... Similar to previous best\nRun 18 stress 0.1033851 \n... Procrustes: rmse 0.0001100975  max resid 0.0003495652 \n... Similar to previous best\nRun 19 stress 0.1033851 \n... Procrustes: rmse 0.0001003504  max resid 0.0003197301 \n... Similar to previous best\nRun 20 stress 0.1188198 \n*** Best solution repeated 8 times\n\nmds_dims <- data.frame(NMDS1 = mds$points[,1], NMDS2 = mds$points[,2])\n\nmds_plot_data <- cbind(mds_dims, clust_scale)\n\nDado que pasamos metaMDS(..., trace = T), R nos mostró los resultados de cada iteración. La iteración (Run) 0 es el acomodo inicial, y tuvo un estrés de 0.10. A partir de ahí comienza a mover los puntos y evaluar si es menor o peor utilizando una prueba de Procrstes, que compara si dos geometrías son diferentes o no. Al final, realizó 20 iteraciones, de las cuales 7 fueron la mejor solución. Este estrés de 0.10 es aceptable, por lo que podemos seguir con la interpretación, que es similar a lo que hicimos con el PCA:\n\n# Extraemos los coef. de determinación de cada variable ~ NMDS (flechas)\nfit <- envfit(mds, clust_scale)\narrow <- data.frame(fit$vectors$arrows,\n                    R = fit$vectors$r,\n                    P = fit$vectors$pvals)\narrow[\"Variable\"] <- rownames(arrow)\n\n# Extraemos aquellas que tengan una corr. significativamente diferente de 0\narrow_p <- subset(arrow, P <= 0.05)\n\n#Ordenamos de manera descendente según su valor de R2\narrow_p <- arrow_p[order(arrow_p$R, decreasing = T), ]\nhead(arrow_p)\n\n\n\n  \n\n\n\n¿Cuál fue la variable más importante? Ahora veamos el NMDS resultante:\n\nmds_plot_data[\"group\"] <- LETTERS[n_gps$Best.partition]\nmds_plot <- ggplot(mds_plot_data, aes(NMDS1, NMDS2)) +\n            geom_point(aes(color = group),\n                       alpha = 0.7) +\n            geom_label_repel(aes(label = rownames(clust_scale))) +\n            stat_ellipse(aes(fill = group), \n                         type = \"t\", size = 1, geom = \"polygon\", \n                         alpha = 0.2, show.legend = F) +\n            labs(title = \"Escalamiento Multidimensional no métrico (NMDS)\",\n                 subtitle = paste('Estrés =',round(mds$stress,3)),\n                 caption = \"Datos: cluster.txt\") +\n            theme_bw() +\n            scale_color_manual(name = \"Grupo\",\n                               values = c(\"firebrick\", \"forestgreen\", \"black\")) +\n            scale_fill_manual(name = \"Grupo\",\n                              values = c(\"firebrick\", \"forestgreen\", \"black\")) #+\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n#             geom_segment(data = arrow_p,\n#                          aes(x=0, y=0,\n#                              xend = NMDS1, yend = NMDS2, lty = Variable),\n# \n# #Flechas escaladas según su R^2\n#                          arrow = arrow(length = unit(.25, \"cm\")*arrow_p$R)\n#                         )\n\nmds_plot\n\n\n\n\nSi lo comparas con el dendrograma te darás cuenta de que, efectivamente, los puntos de cada grupo son más cercanos entre sí que con los demás grupos:\n\nggd2_plot\n\n\n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nEl objetivo del NMDS NO es la formación de grupos. Al igual que en los gráficos del PCA, las etiquetas de los grupos son solo con fines de visualización.\n\n\nCon esto llegamos al final de esta sesión. Espero que la encuentres de utilidad, o cuando menos interesante."
  },
  {
    "objectID": "c15_nosup.html#ejercicio",
    "href": "c15_nosup.html#ejercicio",
    "title": "2  Aprendizaje No Supervisado",
    "section": "2.6 Ejercicio",
    "text": "2.6 Ejercicio\nAplica un ACP a esta misma base de datos.\n\n¿Los datos deben de centrarse y estandarizarse?\nRealiza el ACP. ¿Cuántos CPs considerarías?\n¿Cuál es la varianza explicada entre los dos primeros componentes principales?\n¿Cuáles son las variables más imporantes para esos dos componentes?\n¿Hay diferencias en la ordenación con respecto a los otros dos métodos?\nRealizar las tres técnicas con las biometrías, y comparar sus resultados. (Opcional)\n\n\n\n\n\nShirkhorshidi AS, Aghabozorgi S, Wah TY. 2015. A comparison study on similarity and dissimilarity measusres in clustering continuous data. PLoS ONE 10:e0144059. DOI: 10.1371/journal.pone.0144059.\n\n\nWolter K, Timlin MS. 1998. Measuring the strength of ENSO events: How does 1997/98 rank. Weather 53:315-324."
  },
  {
    "objectID": "c16_mvcomps.html#librerías",
    "href": "c16_mvcomps.html#librerías",
    "title": "3  Hipótesis Multivariadas",
    "section": "3.1 Librerías",
    "text": "3.1 Librerías\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\nlibrary(dplyr)\n# library(Hotelling)\n# library(GridExtra)\nlibrary(GGally)\nlibrary(vegan)\nlibrary(pairwiseAdonis)"
  },
  {
    "objectID": "c16_mvcomps.html#introducción",
    "href": "c16_mvcomps.html#introducción",
    "title": "3  Hipótesis Multivariadas",
    "section": "3.2 Introducción",
    "text": "3.2 Introducción\nVamos a dejar por un momento el tema del aprendizaje automatizado para hablar sobre pruebas de significancia multivariadas, pero es necesario que antes retomemos un tema que dejé pendiente en la ?sec-param: el problema de las múltiples hipótesis."
  },
  {
    "objectID": "c16_mvcomps.html#controlando-el-error",
    "href": "c16_mvcomps.html#controlando-el-error",
    "title": "3  Hipótesis Multivariadas",
    "section": "3.3 Controlando el error",
    "text": "3.3 Controlando el error\nHasta antes de esta sección de multivariado habíamos abordado problemas en donde respondíamos preguntas individuales a nuestros datos, principalmente en términos de pruebas de significancia; sin embargo, en el ?sec-param vimos que había que utilizar un ANOVA seguido de una prueba post-hoc al comparar las medias una variable numérica entre más de dos dos grupos para evitar inflar nuestro valor de \\(\\alpha\\), pero no entramos en más detalles.\n¿Qué pasa cuando contrastamos simultáneamente más de una hipótesis? Conforme incrementamos la cantidad de preguntas incrementamos la probabilidad de obtener respuestas incorrectas. ¿Por qué? Porque tenemos un error por cada tipo de pregunta. Esto hace que sea necesario corregir de alguna manera para que la tasa de error a nivel de la familia (Family-wise Error Rate) se mantenga constante. es decir que nuestro valor de \\(\\alpha\\) represente la probabilidad de que tengamos al menos un error de tipo I.\nEsta probabilidad de encontrar al menos un falso positivo al realizar múltiples comparaciones independientes incrementa a una tasa de \\(1-(1-\\alpha)^m\\), donde \\(m\\) es el número de comparaciones que estamos realizando (King & Eckersley, 2019):\n\nerror <- data.frame(m = 1:20)\nerror[\"P_alpha\"] <- 1 - (1-0.05)^error$m\n\nerror_rate <- ggplot(data = error, aes(m, P_alpha)) + \n              geom_line(color = rgb(118,78,144, maxColorValue = 255)) +\n              scale_y_continuous(breaks = NULL) +\n              scale_x_continuous(breaks = c(1, 2, 5, 10, 20)) +\n              expand_limits(y = c(0,1)) +\n              theme_bw() +\n              geom_hline(yintercept = 0.05,\n                         colour = \"deepskyblue4\", linetype = \"dashed\") +\n              geom_hline(yintercept = 0.5,\n                         colour = \"firebrick\", linetype = \"dashed\",\n                         alpha = 0.5) +\n              geom_hline(yintercept = 1,\n                         colour = \"firebrick\", linetype = \"dashed\",\n                         alpha = 0.5) +\n              annotate(\"text\", x = 0, y = 0.08,\n                       label = \"0.05\", colour = \"deepskyblue4\", \n                       alpha = 0.5) + \n              annotate(\"text\", x = 0, y = 0.53,\n                       label = \"0.5\", colour = \"firebrick\",\n                       alpha = 0.5) +\n              annotate(\"text\", x = 0, y = 0.97,\n                       label = \"1\", colour = \"firebrick\",\n                       alpha = 0.5) +\n              labs(title = \"P(error) al incrementar el número de pruebas (m)\",\n                   subtitle = bquote({1 - (1- alpha)^m}),\n                   x = element_blank(),\n                   y = element_blank(),\n                   caption = \"King & Eckersley (2019)\"\n                   )\n\nerror_rate\n\n\n\n\nNotarás que con \\(m = 10\\) ya nos acercamos peligrosamente a tener un 50% de probabilidad de encontrarnos al menos un falso positivo. Creo que estarás conmigo que eso es una apuesta bastante arriesgada cuando originalmente pensabas que la probabilidad era del 5%.\nImagina que estamos contratados en una farmacéutica como científicos de datos, y que nuestra jefa nos da la tarea de hacer los ensayos clínicos para un nuevo medicamento diseñado para incrementar la fuerza de voluntad de los estudiantes para aprender estadística. Nosotros vamos y llegamos a la conclusión de que el medicamento no tuvo un efecto sobre ese atributo en particular. Evidentemente ni a la jefa ni a sus superiores les agrada la noticia, por lo que deciden explotarnos laboralmente y hacer un ensayo clínico en el que probemos otras 1000 cosas (reducir la ansiedad, mejorar la memoria, etc.), y nos piden que utilicemos el mismo valor de \\(\\alpha\\) de antes: 0.05. Como empleados responsables que quieren evitar un despido nosotros hacemos el colosal ensayo, y encontramos la siguiente distribución de efectos:\n\nset.seed(45)\nn <- 1000\nrobs <- data.frame(z = rnorm(n))\nrobs[\"color\"] <- ifelse(robs$z < -1.96 | robs$z > 1.96, \n                        \"firebrick\", \"deepskyblue4\")\n\nrobs_plot <- ggplot(data = robs, aes(z)) + \n             geom_density(color = \"deepskyblue4\") + \n             theme_bw() + \n             labs(title = \n                    \"Observaciones aleatorias a un alpha de 0.05\",\n                  subtitle = \"Bandas indican límites del LS   \",\n                  caption = paste(n, \" datos simulados\"),\n                  x = \"Z\",\n                  y = element_blank()\n                  ) +\n             geom_vline(xintercept = -1.96, \n                        colour = rgb(118,78,144, maxColorValue = 255), \n                        linetype = \"dashed\"\n                        ) +\n             geom_vline(xintercept = 1.96, \n                        colour = rgb(118,78,144, maxColorValue = 255), \n                        linetype = \"dashed\"\n                        ) +\n             geom_point(aes(x = z, y = 0), \n                        color = robs$color, alpha = 0.3) +\n             scale_y_continuous(breaks = NULL) +\n             scale_x_continuous(breaks = c(-3, -1.96, -1, 0, 1, 1.96, 3), \n                                labels = \n                                  as.character(c(-3, -1.96, -1, 0, 1, 1.96, 3))\n                                ) +\n             annotate(\"text\", x = 3, y = 0.5, \n                      label = paste(\"# sign = \", \n                                    length(robs$color[robs$color ==\n                                                        \"firebrick\"]),\n                                    \"/\", n),\n                      colour = \"firebrick\"\n                      )\n\nrobs_plot\n\n\n\n\n¡Tuvimos un efecto significativo en 58! Hacemos el reporte correspondiente a nuestra jefa e inmediatamente la empresa se dispone a vender el producto. ¿Te hace algún tipo de ruido ese valor de 58/1000? Es sospechosamente cercano a nuestra probabilidad de \\(\\alpha\\) de 0.05, ¿no? Eso quiere decir que cuando menos una parte de esos efectos significativos son productos del azar. ¿La consecuencia de haber hecho así el análisis? La empresa fue demandada por publicidad engañosa porque el medicamento no cumplió con lo prometido.\n¿Cómo podíamos haber evitado la avalancha de demandas? Hay una alternativa sumamente sencilla: la corrección de Bonferroni. Lo único que teníamos que hacer era dividir nuestro valor de \\(\\alpha\\) nominal (con el que estamos trabajando) entre el número de contrastes a realizar. El resultado es que la tasa de error se vuelve prácticamente constante:\n\nerror[\"P_bonf\"] <- 1 - (1-(0.05/error$m))^error$m\n\nerror_corr <- ggplot(data = error, aes(m, P_bonf)) + \n              geom_line(color = rgb(118,78,144, maxColorValue = 255)) +\n              scale_y_continuous(breaks = NULL) +\n              scale_x_continuous(breaks = c(1, 2, 5, 10, 20)) +\n              expand_limits(y = c(0.04, 0.06)) +\n              theme_bw() +\n              geom_hline(yintercept = 0.05,\n                         colour = \"deepskyblue4\", linetype = \"dashed\") +\n              geom_hline(yintercept = 0.04,\n                         colour = \"lightslategray\", linetype = \"dashed\",\n                         alpha = 0.5) +\n              geom_hline(yintercept = 0.06,\n                         colour = \"lightslategray\", linetype = \"dashed\",\n                         alpha = 0.5) +\n              annotate(\"text\", x = 0, y = 0.0505,\n                       label = \"0.05\", colour = \"deepskyblue4\",\n                       alpha = 0.5) + \n              annotate(\"text\", x = 0, y = 0.0405,\n                       label = \"0.04\", colour = \"lightslategray\",\n                       alpha = 0.5) +\n              annotate(\"text\", x = 0, y = 0.0595,\n                       label = \"0.06\",colour = \"lightslategray\",\n                       alpha = 0.5) +\n              labs(title = \"P(error) al corregir según el número de pruebas (m)\",\n                   subtitle = bquote({1 - (1- (alpha/m))^m}),\n                   x = element_blank(),\n                   y = element_blank(),\n                   caption = \"King & Eckersley (2019)\"\n                   )\n\nerror_corr\n\n\n\n\nApliquemos entonces esta simple corrección a nuestro ensayo clínico y veamos cómo se distribuyen nuestros efectos:\n\nn <- 1000\na_corr <- (0.05/n)/2\nsig_lev <- abs(qnorm(a_corr))\nrobs <- data.frame(z = rnorm(n))\nrobs[\"color\"] <- ifelse(robs$z < -sig_lev | robs$z > sig_lev,\n                        \"firebrick\", \"deepskyblue4\")\n\nrcor_plot <- ggplot(data = robs, aes(z)) + \n             geom_density(color = \"deepskyblue4\") + \n             theme_bw() + \n             labs(title = \"Observaciones aleatorias a un alpha de 0.05\",\n                  subtitle = \"Bandas indican límites del LS corregido\",\n                  caption = paste(n, \" datos simulados\"),\n                  x = \"Z\",\n                  y = element_blank()\n                  ) +\n             geom_vline(xintercept = -sig_lev, \n                        colour = rgb(118,78,144, maxColorValue = 255), \n                        linetype = \"dashed\"\n                        ) +\n             geom_vline(xintercept = sig_lev, \n                        colour = rgb(118,78,144, maxColorValue = 255), \n                        linetype = \"dashed\"\n                        ) +\n             geom_point(aes(x = z, y = 0), \n                        color = robs$color, alpha = 0.3) +\n             scale_y_continuous(breaks = NULL) +\n             scale_x_continuous(breaks = c(-3, -1.96, -1, 0, 1, 1.96, 3), \n                                labels =\n                                  as.character(c(-3, -1.96, -1, 0, 1, 1.96, 3))\n                                ) +\n             annotate(\"text\", x = 2.5, y = 0.5, \n                      label = paste(\"# sign = \", \n                                    length(robs$color[robs$color ==\n                                                        \"firebrick\"]),\n                                    \"/\", n),\n                      colour = \"firebrick\"\n                      )\n\nrcor_plot\n\n\n\n\nAhora no tuvimos ningún efecto significativo. Básicamente le dimos un placebo a todos nuestros sujetos experimentales. Vamos con nuestra jefa, le damos el reporte correspondiente, se pone furiosa porque la empresa perdió dinero y le va a tocar una llamada de atención, pero preferible eso a tener pérdidas millonarias derivadas de la fabricación en masa de un producto que no sirve, y la subsecuente lluvia de demandas.\n¿Eso quiere decir que siempre vamos a utilizar una corrección de Bonferroni? Como mucho en esta vida, depende. ¿De qué? De la respuesta que demos a la misma pregunta que deberíamos de hacernos al seleccionar un valor de \\(\\alpha\\): ¿Qué prefiero sea más probable, un falso positivo o un falso negativo? A final de cuentas, si disminuímos la probabilidad de tener falsos positivos, por lógica incrementamos la probabilidad de tener falsos negativos. ¿La razón? Nos estamos haciendo más conservadores y, de hecho, esa es una de las principales críticas (Perneger, 1998) a la corrección de Bonferroni, que es muy conservadora e incrementa \\(\\beta\\). Hay otra crítica un poco más filosófica que tiene que ver con la pregunta de si cambia la interpretación de UNA prueba porque se hicieron otras 999, pero no entraremos en esos detalles.\nAhora sí, podemos seguir con las pruebas multivariadas."
  },
  {
    "objectID": "c16_mvcomps.html#t2-de-hotelling",
    "href": "c16_mvcomps.html#t2-de-hotelling",
    "title": "3  Hipótesis Multivariadas",
    "section": "3.4 \\(T^2\\) de Hotelling",
    "text": "3.4 \\(T^2\\) de Hotelling\nHasta el momento hemos visto que los análisis multivariados son abstractos. Podemos, sin ningún problema visualizar una distribución normal bivariada, solo hay que graficar una variable contra la otra (Figura 3.1), ¿pero con veinte variables? Nuestra imaginación de seres tridimensionales no llega hasta allá.\n\n\n\nFigura 3.1: Distribuciones normales bivariadas\n\n\nLa solución es hacer una prueba multivariada (para mantener nuestro error controlado), y acompañarla de pruebas univariadas. La primera de estas prueba es la prueba \\(T^2\\) de Hotelling, que es una generalización multivariada de la prueba \\(t\\) de Student. Como tal, es una prueba paramétrica, por lo que existen los supuestos de normalidad multivariada y de igualdad de matrices de covarianza (en el Capítulo 1 ya revisamos esos temas, así que obviaré el procedimiento). Las ecuaciones son una adaptación de la prueba \\(t\\) hacia el espacio multivariado, pasando de trabajar con una o dos medias a trabajar con uno o dos vectores de medias:\n\nPara una muestra o muestras pareadas:\n\n\\[\\begin{align*}\nT^2 = n(\\bar{x}- \\mu)^TC^{-1}(\\bar{x}-\\mu) \\\\\nC = \\frac{1}{n-1}\\sum_{i=1}^n{(x_i-\\bar{x})(x_i-\\bar{x})^T} \\\\\n\\nu = p, n-p\n\\end{align*}\\]\n\nPara dos muestras independientes\n\n\\[\\begin{align*}\nT^2 = n(\\bar{x_1}- \\bar{x_2})^TC^{-1}(\\bar{x_1}- \\bar{x_2}) \\\\\nC =\n\\frac{(n_1 - 1)C_1 + (n_2-1)C_2}\n{n_1 + n_2 - 2} \\\\\n\n\\nu = p, n_1 + n_2 - p - 1\n\\end{align*}\\]\nEl estadístico de prueba sigue una distribución \\(F\\) de Fisher, como en el ANOVA:\n\\[\nF = \\frac{n - p}{p(n-1)}T^2\n\\]\n\n\n\n\n\n\nNota\n\n\n\nEl objetivo de mostrar las ecuaciones no es que te las aprendas, solo que veas la lógica detrás de las “generalizaciones” que vamos a ver más adelante.\n\n\n\n3.4.1 Implementación en R\nPara todas las pruebas que revisaremos el día de hoy vamos a utilizar los datos de palmerpenguins que utilizamos para los gráficos del ?sec-descriptiva, por lo que vamos a filtrar la base penguins para quedarnos solo con las variables numéricas sobre peso o longitud. Tiene mucho que no vemos algo de tidyverse, así que pongámoslo en práctica:\n\nnum_data <- penguins |>\n            select(contains(c(\"mm\", \"g\", \"species\"))) |> \n            na.omit()\nhead(num_data)\n\n\n\n  \n\n\n\nLa prueba la aplicaremos con la función Hoteling::hotelling.test(x, y), donde \\(x\\) y \\(y\\) son las matrices de datos de cada grupo.\n\nx <- num_data |> filter(species == \"Adelie\") |> select(where(is.numeric))\ny <- num_data |> filter(species == \"Gentoo\") |> select(where(is.numeric))\n\nhot_t2 <- Hotelling::hotelling.test(x = x, y = y)\nhot_t2\n\nTest stat:  4586.7 \nNumerator df:  4 \nDenominator df:  269 \nP-value:  0 \n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nLa función hotelling.test puede recibir una fórmula, pero hay un error y tanto el estadístico de prueba como el valor de p resultan en NaN (“Not a Number).\n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nEl valor de p no es exactamente 0, solo es extremadamente pequeño (del orden \\(\\times10^{-16}\\) o aún más pequeño) y la función lo muestra como 0.\n\n\n\n\n\n\n\n\nTip\n\n\n\n¿Cómo diferenciar entre dónde utilizar filter y dónde select? filter extrae renglones, mientras que select extrae columnas.\n\n\nSi la distribución del estadístico de prueba es la distribución \\(F\\), entonces los resultados se interpretan como los resultados de un ANOVA: Hay diferencias en al menos una de las medias (\\(F_{4,269} = 4586.7; p < 0.0001\\)).\n\n3.4.1.1 Comparaciones univariadas\nRealizaremos 4 comparaciones univariadas. ¿tiene sentido aplicar una corrección de Bonferroni?\n\n# Formamos un objeto para la aplicación de la prueba t\nt2_data <- num_data |>\n           filter(species == \"Adelie\" | species == \"Gentoo\")\n\n# Prueba t para lcada variable\nt.test(bill_length_mm~species, data = t2_data)\n\n\n    Welch Two Sample t-test\n\ndata:  bill_length_mm by species\nt = -24.725, df = 242.58, p-value < 2.2e-16\nalternative hypothesis: true difference in means between group Adelie and group Gentoo is not equal to 0\n95 percent confidence interval:\n -9.407672 -8.019303\nsample estimates:\nmean in group Adelie mean in group Gentoo \n            38.79139             47.50488 \n\nt.test(bill_depth_mm~species, data = t2_data)\n\n\n    Welch Two Sample t-test\n\ndata:  bill_depth_mm by species\nt = 25.337, df = 271.98, p-value < 2.2e-16\nalternative hypothesis: true difference in means between group Adelie and group Gentoo is not equal to 0\n95 percent confidence interval:\n 3.102837 3.625651\nsample estimates:\nmean in group Adelie mean in group Gentoo \n            18.34636             14.98211 \n\nt.test(flipper_length_mm~species, data = t2_data)\n\n\n    Welch Two Sample t-test\n\ndata:  flipper_length_mm by species\nt = -34.445, df = 261.75, p-value < 2.2e-16\nalternative hypothesis: true difference in means between group Adelie and group Gentoo is not equal to 0\n95 percent confidence interval:\n -28.79018 -25.67652\nsample estimates:\nmean in group Adelie mean in group Gentoo \n            189.9536             217.1870 \n\nt.test(body_mass_g~species, data = t2_data)\n\n\n    Welch Two Sample t-test\n\ndata:  body_mass_g by species\nt = -23.386, df = 249.64, p-value < 2.2e-16\nalternative hypothesis: true difference in means between group Adelie and group Gentoo is not equal to 0\n95 percent confidence interval:\n -1491.183 -1259.525\nsample estimates:\nmean in group Adelie mean in group Gentoo \n            3700.662             5076.016 \n\n\nEn todos los casos rechazamos la hipótesis de nulidad (\\(p < 0.0001\\)), lo cual no es de sorprender: los pingüinos Adelie son más pequeños que los Gentoo (aunque sorprende el resultado de la profundidad del pico). Aquí normalmente haríamos un gráfico de acompañamiento, pero mejor escalemos al caso para más de dos grupos."
  },
  {
    "objectID": "c16_mvcomps.html#análisis-multivariado-de-la-varianza",
    "href": "c16_mvcomps.html#análisis-multivariado-de-la-varianza",
    "title": "3  Hipótesis Multivariadas",
    "section": "3.5 Análisis Multivariado de la Varianza",
    "text": "3.5 Análisis Multivariado de la Varianza\nEs, como te imaginarás, una extensión multivariada del ANOVA factorial. Aquí ya no entraré en los detalles de la prueba, pues vimos los fundamentos del ANOVA en el ?sec-param, y cómo comprobar los supuestos básicos en Capítulo 1. Solo es importante conocer que se puede realizar con diversos estadísticos de prueba, entre los que destacan\n\nLa traza de Pillai: Contenida en el intervalo \\([0,1]\\), donde valores cercanos a 1 sugieren diferencias en al menos uno de los grupos en el espacio multivariado. Este es el estadístico más robusto a la violación de los supuestos del MANOVA (normalidad multivariada y homogeneidad de las matrices de covarianza).\nLa \\(\\lambda\\) de Wilk. También contenida en el intervalo \\([0,1]\\), aunque aquí son los valores más cercanos a 0 los que sugieren las diferencias. Si se cumplen todos los supuestos, esta es la prueba más relacionada con el criterio de la razón de verosimilitud (más sobre esto en Capítulo 6).\n\nY que el procedimiento \\(post-hoc\\) tiene dos pasos:\n\nUn ANOVA por cada variable dependiente.\nUna prueba HSD de Tukey en los ANOVAs significativos.\n\n\n3.5.1 Implementación\nLa implementación de la prueba en R tiene un paso muy engorroso, especialmente en casos con muchísimas variables: hay que seleccionar manualmente las columnas con las variables dependientes y unirlas con la función cbind. Podemos aplicar la prueba con la función car::Manova, la cual recibe un modelo ajustado con la función lm:\n\nmanova_model <- lm(cbind(bill_length_mm,\n                         bill_depth_mm,\n                         flipper_length_mm,\n                         body_mass_g)~species,\n                   data = penguins)\ncar::Manova(manova_model, test.statistic = \"Wilks\")\n\n\nType II MANOVA Tests: Wilks test statistic\n        Df test stat approx F num Df den Df    Pr(>F)    \nspecies  2  0.018785   528.87      8    672 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nO también con la función manova de R base:\n\nres_manova <- manova(cbind(bill_length_mm,\n                           bill_depth_mm,\n                           flipper_length_mm,\n                           body_mass_g)~species,\n                     data = penguins)\nsummary(res_manova)\n\n           Df Pillai approx F num Df den Df    Pr(>F)    \nspecies     2 1.6366   379.49      8    674 < 2.2e-16 ***\nResiduals 339                                            \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nPosteriormente podemos utilizar este objeto para realizar un ANOVA por variable. Nuevamente, ¿es necesario corregir los valores de p?\n\nsummary.aov(res_manova)\n\n Response bill_length_mm :\n             Df Sum Sq Mean Sq F value    Pr(>F)    \nspecies       2 7194.3  3597.2   410.6 < 2.2e-16 ***\nResiduals   339 2969.9     8.8                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response bill_depth_mm :\n             Df Sum Sq Mean Sq F value    Pr(>F)    \nspecies       2 903.97  451.98  359.79 < 2.2e-16 ***\nResiduals   339 425.87    1.26                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response flipper_length_mm :\n             Df Sum Sq Mean Sq F value    Pr(>F)    \nspecies       2  52473 26236.6   594.8 < 2.2e-16 ***\nResiduals   339  14953    44.1                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response body_mass_g :\n             Df    Sum Sq  Mean Sq F value    Pr(>F)    \nspecies       2 146864214 73432107  343.63 < 2.2e-16 ***\nResiduals   339  72443483   213698                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n2 observations deleted due to missingness\n\n\nY por último aplicamos una prueba post-hoc HSD de Tukey para cada variable (solo con la primera por simplicidad):\n\nTukeyHSD(aov(bill_length_mm~species, data = num_data))\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = bill_length_mm ~ species, data = num_data)\n\n$species\n                      diff       lwr        upr     p adj\nChinstrap-Adelie 10.042433  9.024859 11.0600064 0.0000000\nGentoo-Adelie     8.713487  7.867194  9.5597807 0.0000000\nGentoo-Chinstrap -1.328945 -2.381868 -0.2760231 0.0088993\n\n\nAhora sí, podemos construir un gráfico de acompañamiento. Podemos construir un gráfico de interacción para cada variable. Hacerlo a mano puede resultar extremadamente tedioso, así que hagámoslos con un ciclo for. Primero, creemos objetos con la información básica que vamos a necesitar: los nombres de las variables, una lista vacía para llenar, y un espacio gráfico con los elementos que se reutilizan en todos los casos para evitar repeticiones.\n\n# Nombres de las variables\nvar_names <- colnames(num_data)[colnames(num_data)!=\"species\"]\n\n# Lista vacía\nplot_list <- list()\n\n# Inicializamos el espacio gráfico base\nbase_plot <- ggplot(data = num_data,\n                    aes(x = species,\n                        colour = species)) +\n             theme_bw() \n\nLuego aplicamos el ciclo for. Como en este caso tenemos los nombres de columnas en un vector de cadenas de caracter necesitamos de dos pasos: convertir la cadena de caracteres a un símboo con la función sym() y luego quitarle las comillas con el operador !! dentro de aes:\n\n# Para cada variable:\n\nfor (variable in var_names){\n  plot_var <- sym(variable)\n  plot_list[[variable]] <- base_plot +\n    geom_violin(aes(y = !!plot_var),\n                show.legend = F) +\n    geom_boxplot(aes(y = !!plot_var),\n                 width = 0.1,\n                 show.legend = F) +\n    labs(x = element_blank(),\n         y = element_blank(),\n         title = variable)\n}\n\nLuego podemos graficarlas todas juntas con la función GridExtra::grid.arrange(), solo que hay que “forzarla” utilizando la función do.call(FUN, args) para evitar un error:\n\ndo.call(gridExtra::grid.arrange, args = plot_list)\n\n\n\n\nOtra forma sería con facet_wrap, tratando las variables como si fueran variables de agrupamiento (¿te animas a programarlo?), pero otra manera, tal vez más sencilla de representar toda esta información en un solo panel es un gráfico de coordenadas paralelas:\n\nparcoord_plot <- ggparcoord(data = num_data,\n                            columns = 1:4,\n                            groupColumn = 5,\n                            showPoints = TRUE,\n                            scale = \"std\",\n                            order = \"anyClass\",\n                            alphaLines = 0.5) +\n                 theme_bw() +\n                 labs(title = \"Gráfico de coordenadas paralelas\",\n                      subtitle = \"Valores estandarizados\",\n                      y = element_blank(),\n                      x = element_blank(),\n                      caption = \"Datos: Iris\") +\n                 scale_color_discrete(name = \"Especie\")\nparcoord_plot"
  },
  {
    "objectID": "c16_mvcomps.html#análisis-permutacional-multivariado-de-la-varianza",
    "href": "c16_mvcomps.html#análisis-permutacional-multivariado-de-la-varianza",
    "title": "3  Hipótesis Multivariadas",
    "section": "3.6 Análisis Permutacional Multivariado de la Varianza",
    "text": "3.6 Análisis Permutacional Multivariado de la Varianza\nVamos a cerrar este capítulo con la alternativa no paramétrica al MANOVA: el Análisis Permutacional Multivariado de la Varianza (MANOVA). En este análisis se hace una partición geométrica del espacio multivariado; es decir, es una prueba basada en distancias. Además, es una prueba basada en un análisis permutacional, donde “revuelve” las etiquetas de la matriz de distancias.\n\n\n\n\n\n\nNota\n\n\n\nTe recomiendo visitar este sitio web para que te familiarices con los análisis permutacionales de una manera visual.\n\n\nLa prueba trabaja en tres pasos:\n\nCalcula un valor de \\(F = \\frac{SS_A}{SS_R}\\cdot \\left[\\frac{N-g}{g-1} \\right]\\) para el acomodo original de los datos.\nPermuta las etiquetas de la matriz de distancias \\(k\\) veces, calculando un valor de \\(F\\) para cada permutación. Esto permite “crear” una distribución empírica con todas las \\(F\\).\nCalcula el valor de p comparando la F observada vs. la distribución “creada”.\n\nDebido a la naturaleza permutacional del análisis es muy importante que tengamos factores balanceados; i.e., que todos los factores tengan aproximadamente la misma cantidad de observaciones. La función que utilizaremos (adonis2) compensa esta parte, pero tampoco hay que abusar, y esto me lleva a la implementación.\n\n3.6.1 Implementación\nLo primero, evidentemente, es calcular la matriz de distancias pero, al igual que con el análisis clúster, es importante que todas nuestras variables estén en la misma escala:\n\n# Escalado de los datos\nscaled_data <- as.data.frame(scale(num_data[,1:4]))\n\n# Objeto con los niveles de agrupamiento, anidados o no\nspecies <- num_data$species\n\n# Cálculo de la matriz de distancias\ndist_mat <- dist(scaled_data,\n                 method = \"euclidean\")\n\nLuego aplicamos el PERMANOVA\n\nperm_res <- adonis2(dist_mat~species,\n                    data = scaled_data,\n                    permutations = 999)\nperm_res\n\n\n\n  \n\n\n\nRechazamos la hipótesis de nulidad, y aprendimos que hay diferencias entre las medias multivariadas de al menos un par de especies. El gráfico de acompañamiento para esta prueba es el resultado de un análisis de coordenadas principales (un PCA, pero con cualquier medida de distancia, no solo Euclidiana), o un NMDS. Dado que el análisis lo hicimos con una matriz de distancias euclidianas podemos también hacer un gráfico de PCA:\n\ndist_pca <- FactoMineR::PCA(scaled_data,\n                            graph = F,\n                            ncp = length(num_data),\n                            scale.unit = F)\nfactoextra::fviz_pca_ind(dist_pca,\n                         geom.ind = \"point\",\n                         col.ind = species,\n                         addEllipses = T,\n                         palette = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n                         legend.title = \"Especies\")\n\n\n\n\nComo era de esperarse hubo diferencias muy marcadas, principalmente en los pingüinos Gentoo pero ¿en qué variables? No existe una prueba post-hoc como tal para el PERMANOVA, pero podemos hacer un PERMANOVA para cada par de especies (por último, ¿es necesario corregir los valores de p de estos PERMANOVAs?) con la función pairwiseAdonis::pairwise.adonis2:\n\npw_adonis <- pairwise.adonis2(x = dist_mat~species,\n                              data = cbind(scaled_data, species))\npw_adonis\n\n$parent_call\n[1] \"dist_mat ~ species , strata = Null , permutations 999\"\n\n$Adelie_vs_Gentoo\n          Df SumOfSqs      R2      F Pr(>F)    \nspecies    1   823.01 0.72176 705.58  0.001 ***\nResidual 272   317.27 0.27824                  \nTotal    273  1140.28 1.00000                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$Adelie_vs_Chinstrap\n          Df SumOfSqs      R2      F Pr(>F)    \nspecies    1   166.95 0.39664 142.65  0.001 ***\nResidual 217   253.95 0.60336                  \nTotal    218   420.90 1.00000                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$Gentoo_vs_Chinstrap\n          Df SumOfSqs    R2      F Pr(>F)    \nspecies    1   359.23 0.617 304.48  0.001 ***\nResidual 189   222.99 0.383                  \nTotal    190   582.21 1.000                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nattr(,\"class\")\n[1] \"pwadstrata\" \"list\"      \n\n\nY luego iríamos por cada par de especies viendo entre qué variables están las diferencias. ¿Engorroso? Lo que le sigue, pero hay que hacerlo si te interesa saber en qué variables están las diferencias, caso contrario puedes saltarte esta útlima parte y solo reportar los resultados de los PERMANOVAs para cada par de especies.\nEsto es todo para esta sesión. Espero que haya sido de tu agrado y que te sea útil en algún momento. En la siguiente sesión volveremos al aprendizaje automatizado, esta vez para hablar de algo que podemos aplicar en vez de los MANOVAs: la clasificación.\n\n\n\n\nKing AP, Eckersley RJ. 2019. Chapter 8 - Inferential Statistics V: Multiple and Multivariate Hypothesis Testing. In: King AP, Eckersley RJ eds. Statistics for Biomedical Engineers and Scientists. Academic Press, 173-199. DOI: https://doi.org/10.1016/B978-0-08-102939-8.00017-7.\n\n\nPerneger TV. 1998. What’s wrong with Bonferroni adjustments. BMJ 316:1236-1238. DOI: 10.1136/bmj.316.7139.1236."
  },
  {
    "objectID": "c17_clasif.html#librerías",
    "href": "c17_clasif.html#librerías",
    "title": "4  Aprendizaje supervisado: Clasificación",
    "section": "4.1 Librerías",
    "text": "4.1 Librerías\n\nlibrary(palmerpenguins)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(vip)"
  },
  {
    "objectID": "c17_clasif.html#introducción",
    "href": "c17_clasif.html#introducción",
    "title": "4  Aprendizaje supervisado: Clasificación",
    "section": "4.2 Introducción",
    "text": "4.2 Introducción\nHasta el momento hemos visto dos formas de involucrar respuestas categóricas: crearlas con alguna técnica de agrupamientos, o utilizarlas para hacer comparaciones con pruebas de significancia. En este capítulo vamos a hablar de (en mi opinión) una mejor alternativa (dependiendo de tu objetivo): la clasificación. La clasificación es una técnica de aprendizaje supervisado en la que el objetivo es predecir clases (etiquetas) a partir de los valores de los predictores. Al ser aprendizaje supervisado podemos evaluar su desempeño; es decir, qué tan bien estamos prediciendo nuestras etiquetas. Si tenemos un buen desempeño, podemos utilizar alguna medida de importancia de variables para explicar el modelo. Si no, toca volear mesas, jalarnos los cabellos, y regresar a pensar qué es lo que está fallando: ¿el modelo seleccionado no es el óptimo? ¿Debíamos de utilizar algún preprocesamiento? ¿el modelo no está optimizado adecuadamente? Al terminar este capítulo deberías de ser capaz de responder adecuadamente a todas estas preguntas. En la Figura 4.1 tienes una pequeña comparación entre los agrupamientos, la comparación y la clasificación.\n\n\n\nFigura 4.1: Comparación entre agrupamientos, comparación y clasificación"
  },
  {
    "objectID": "c17_clasif.html#clasificación-y-clasificadores",
    "href": "c17_clasif.html#clasificación-y-clasificadores",
    "title": "4  Aprendizaje supervisado: Clasificación",
    "section": "4.3 Clasificación y clasificadores",
    "text": "4.3 Clasificación y clasificadores\nYa sabemos en qué consiste la clasificación, ahora veamos el fundamento detrás de todos los modelos de clasificación (clasificadores). Hoy en día uno de los temas que está muy de moda es la visión por computadora. Aunque se puede hacer un modelo no supervisado para la identificación de objetos, en muchos casos tenemos algo como lo siguiente: un conjunto de imágenes de un objeto (elefantes) y otro conjunto de imágenes de otro (jirafas). Le damos este par de etiquetas e imágenes a la computadora para entrenar un clasificador; es decir, para ajustar los parámetros del modelo. Posteriormente verificamos que el modelo funcione bien pasándole una imagen sin etiquetar y que sea el clasificador quien le asigne una etiqueta. Si el modelo aprendió lo que tenía que aprender debería de ser capaz de decirnos que una imagen de una jirafa es una jirafa, y no un elefante.\n\n\n\nFigura 4.2: Clasificación de imágenes de elefantes y jirafas.\n\n\n\n\n\n\n\n\nImportante\n\n\n\nEste proceso de entrenamiento y prueba debemos de aplicarlo con todos los modelos de aprendizaje supervisado, independientemente de si son regresiones o clasificaciones.\n\n\nYa tenemos una idea del qué hace la clasificación, pero nos falta el cómo. Para regresión hemos hablado de distintos modelos (lineales, ?sec-rls o no lineales, ?sec-nolin), y en clasificación es lo mismo, tenemos una gran cantidad de métodos, cada uno con sus supuestos, fortalezas, y debilidades. En los renglones de la Figura 4.3 tenemos tres conjuntos de datos bivariados con dos clases (roja y azul) cada uno, y en las columnas los márgenes de decisión de los clasificadores; es decir, el límite entre ambas clases que aprendió cada clasificador. Esta es la gran diferencia entre los clasificadores: la complejidad de los límites que son capaces de aprender. En la tercera columna tenemos una máquina de soporte vectorial con un kernel lineal (linear SVM); es decir, que solamente puede aprender un límite lineal entre las clases. La consecuencia es que tiene un desempeño muy pobre en el segundo conjunto de datos, pero uno bastante aceptable el tercero. A su derecha tenemos también una máquina de soporte vectorial, solo que esta vez con un kernel RBF (RBF SVM) que puede aprender un límite más complejo. Como resultado el desempeño es mejor en todos los casos que en su contraparte lineal. Y así podríamos ir analizando caso por caso, viendo quién funciona mejor en qué tipo de casos, pero verás una tendencia clara: los clasificadores con límites de decisión más flexibles son los que tienen un mejor desempeño, independientemente del problema.\n\n\n\nFigura 4.3: Comparación de clasificadores\n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nEso son excelentes noticias, ¿no? Simplemente seleccionemos el algoritmo más flexible de todos y quitémonos de problemas. NO, para nada. Usualmente, conforme incrementa la flexibilidad del modelo, incrementa su complejidad."
  },
  {
    "objectID": "c17_clasif.html#consideraciones-al-construir-un-modelo-de-aprendizaje-automatizado",
    "href": "c17_clasif.html#consideraciones-al-construir-un-modelo-de-aprendizaje-automatizado",
    "title": "4  Aprendizaje supervisado: Clasificación",
    "section": "4.4 Consideraciones al construir un modelo de aprendizaje automatizado",
    "text": "4.4 Consideraciones al construir un modelo de aprendizaje automatizado\nEsto último me lleva a hablar sobre con qué cosas debemos de tener cuidado al generar un modelo de aprendizaje automatizado, pues va mucho más allá de solo utilizar lm() y reportar un \\(R^2\\).\n\n4.4.1 Ajuste (sub y sobre)\nEn los ?sec-rls y ?sec-nolin vimos cómo entrenar y “evaluar” un modelo de regresión, pero no pusimos demasiada atención a la parte de la bondad de ajuste. ¿Por qué? Porque evaluar un modelo PREDICTIVO a partir de los datos que lo entrenaron es un SINSENTIDO. Es el equivalente a que yo te diga que al final del curso va a haber un examen, que te proporcione una guía de estudio, que tú como estudiante diligente la resuelvas y la estudies, y que el examen tenga exactamente las mismas preguntas. ¿Qué representaría tu calificación? ¿Lo que aprendiste sobre el curso o qué tan bien memorizaste la guía de estudio? Evidentemente lo segundo. En el caso de los modelos de aprendizaje supervisado pasa lo mismo: “aprenden” la información contenida en los datos, pero en el peor de los casos solo “memorizan” los datos que se les dieron (Figura 4.4).\n\n\n\nFigura 4.4: Probar un modelo de ML con datos de entrenamiento es ponerse una medalla a uno mismo por su buen desempeño: no es una evaluación objetiva.\n\n\n¿Cómo contendemos con esto? Hacemos una división de nuestros datos: la mayor parte de los datos (usualmente alrededor del 75%) se utiliza para entrenamiento, mientras que los datos restantes se utilizan para probar el modelo final:\n\n\n\nFigura 4.5: División entrenamiento/prueba.\n\n\nEsto lo que nos permite es evaluar objetivamente el desempeño de nuestro modelo, pues nunca ha visto los datos de prueba. Volviendo a la analogía del examen final y la guía de estudio, sería como si yo esta vez te diera un examen que abarque los mismos temas que en la guía, pero con problemas que te hagan pensar o abordarlos desde otra perspectiva para que realmente demuestres tu dominio sobre el temario, y no solo que resolviste la guía.\nEsta “memorización” tiene un nombre: sobreajuste, que se define formalmente como una poca capacidad de generalización del modelo; es decir, que vamos a tener medidas de bondad de ajuste (o medidas de evaluación) mucho mejores en los datos de entrenamiento que en los datos de prueba. En todo escenario de aprendizaje supervisado buscamos un modelo que sea capaz de extraer la mayor cantidad de información posible de los datos, sin llegar a memorizar cada dato (sobreajuste), pero tampoco pasarnos de simples (subajuste; Figura 4.6)\n\n\n\nFigura 4.6: Tres modelos ajustados a un conjunto de datos: a) un modelo cuadrático que captura adecuadamente la relación entre las variables, no predice exactamente ningún punto, pero tampoco hay casos que estén extremadamente lejos (ajuste óptimo); b) un modelo lineal que no es suficiente para capturar adecuadamente la relación entre las variables (subajustado); y c) un modelo polinomial de orden n-1 que memorizó a la perfección los datos de entrenamiento, pero al que si se le da un dato fuera de ellos no podrá predecir adecuadamente (sobreajustado).\n\n\n\n\n\n\n\n\nNota\n\n\n\n¿A qué medidas de bondad de ajuste me refiero? A aquellas que tienen que ver con la capacidad predictiva del modelo. En regresiones serían medidas como el \\(R^2\\) o el \\(MSE\\). Más adelante explicaremos a detalle cómo evaluar el desempeño con un clasificador, pero es con una matriz de confusión, la curva de Características del Operador Receptor (ROC) y el área bajo esta (AUC-ROC).\n\n\nAdemás de la parte práctica/filosófica tenemos otro problema que resolver: la compensación/balance de sesgo-varianza. El sesgo lo entendemos como la diferencia entre la predicción promedio de nuestro modelo y el valor real que queremos predecir (nuestro error), de manera que modelos con un alto sesgo no le prestan demasiada atención a los datos (error grande) y en consecuencia están sobre-simplificados. La varianza en este contexto la entendemos como la variabilidad del modelo de predicción para un punto dado, de modo que un modelo con una alta varianza le presta demasiada atención a los datos de entrenamiento y no generaliza bien a datos que nunca ha visto. ¿Por qué compensación? Porque, como ya había mencionado superficialmente antes, entre más complejo sea el modelo, mayor es la probabilidad del sobreajuste. ¿Qué tan severo puede ser? Tan severo como en la siguiente figura:\n\n\n\nFigura 4.7: Relación entre el desempeño de un modelo en realción a su complejidad, evaluado en datos de entrenamiento y en datos de prueba. A mayor complejidad incrementa el desempeño en los datos de entrenamiento, pero no es así con datos que no ha visto.\n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nCuando hablo de la complejidad del modelo no me refiero solo a seleccionar un modelo que sea más complejo que otro, sino también a la cantidad de variables. No porque podamos meter cientos o miles de variables a un modelo quiere decir que debemos hacerlo, hay que hacer una rigurosa selección de variables para reducir la diferencia entre los datos de entrenamiento y los de prueba.\n\n\n\n4.4.1.1 Validación cruzada y optimización\nCon esto llegamos a la validación cruzada y la optimización de hiper-parámetros. La validación cruzada consiste en partir los datos de entrenamiento en \\(k\\) conjuntos de entrenamiento/prueba, evaluar el desempeño para cada uno, y promediarlos (Figura 4.8)\n\n\n\nFigura 4.8: Validación cruzada y cómo repartir los datos para entrenar, optimizar y validar un modelo de aprendizaje supervisado.\n\n\nLo que conseguimos con esto es una evaluación más robusta del desempeño de nuestro modelo, pues ya estamos viendo qué pasa con distintas combinaciones de datos vistos/no vistos. Pudiera llegar a suceder que tenemos mucha suerte y que la partición original se preste para obtener buenas métricas, o tener muy mala suerte y que se preste para tener malas métricas. Y para acabar de “redondear” lo escépticos que somos con nuestro modelo, después de haber probado con distintos \\(k\\) sub-conjuntos (k-folds), lo volvemos a probar con los datos de prueba originales (hay quien los denomina en este contexto datos de validación).\n\n\n\n\n\n\nNota\n\n\n\nAsí como no muestreamos solo un individuo para hacer inferencias, tampoco consideramos una sola partición entrenamiento prueba. ¿Cuántas sub-divisiones debemos de hacer? Pueden ser tan pocas como 5, o tantas como n-1, pero un “estándar” (más bien una convención por comodidad) es que la validación cruzada sea de orden 10 (10 k-folds).\n\n\n\n\n\n\n\n\nImportante\n\n\n\nEl tamaño de la partición entrenamiento-prueba original y el orden de la validación cruzada dependen del número de datos que tengas. Si tu conjunto de prueba queda con un tamaño muy reducido (pensemos menos de diez observaciones), mejor solo quédate con la validación cruzada en todos tus datos.\n\n\n¿Y qué pinta la “optimización de hiperparámetros” aquí? Para empezar, ¿qué es la optimización de hiperparámetros? Pues bien, resulta que muchos modelos de aprendizaje supervisado tienen no solo los parámetros que queremos ajustar (por ejemplo las pendientes en la regresión lineal), sino que también tienen otros parámetros que controlan su complejidad. Esto puede ser el valor de \\(\\alpha\\) o \\(\\lambda\\) en una regresión regularizada como las que veremos en el Capítulo 5, o el número máximo de predictores que considera un árbol de decisión. Es importante optimizar esos hiperparámetros, para no terminar con un modelo innecesariamente complejo que no va a tener una buena capacidad predictiva, y la mejor manera de hacerlo es utilizando validación cruzada.\n\n\n\n4.4.2 Fuga de información\nHay otra cosa con la que debemos de tener muchísimo cuidado al construir cualquier modelo de aprendizaje automatizado: la fuga de información. Volviendo a nuestra analogía del examen sería el equivalente a que alguien te soplara las respuestas del examen, o a que sacaras un acordeón. ¿Cómo pasa esto? No prestando atención a la información que le damos al modelo. El conjunto de prueba debe de aislarse completamente del de entrenamiento. ¿Necesitas pre-procesar los datos de una manera (escalarlos, por ejemplo)? Escala los datos de entrenamiento y utiliza esos parámetros para escalar los datos de prueba. Si vas a centrar y estandarizar, separa tus datos, calcula las medias y desviaciones de cada variable con los datos de entrenamiento y con esas medias y desviaciones centra y estandariza ambos conjuntos de datos; es decir, no escales la base completa, de lo contrario le estás dando al modelo información sobre datos que no debería de ver en ningún momento antes de la evaluación. Tampoco las escales por separado, pues eso te introducirá un sesgo aleatorio derivado de la división.\nOtro caso de fuga de información, aún menos evidente que el anterior, es tener casos en los que tengamos algo tipo “la variable \\(A\\) lógicamente implica la respuesta \\(Y\\)”. ¿Cómo es esto? Imagina que una tienda departamental que vende únicamente joyas, películas y electrónica (raro, lo sé) nos contrata para saber cuánto va a gastar un cliente en joyería (el segmento con mayores utilidades) a partir de lo que gasta en las otras dos cosas, por lo que nos da los registros cliente a cliente en la siguiente tabla:\n\n\n\nFigura 4.9: Registro de ventas de una tienda departamental imaginaria. ¿Qué cliente no da información útil para predecir el gasto en joyería a partir de lo gastado en películas y electrónica?\n\n\n¿Qué cliente salta a la vista? Espero que me digas que el 11125, pues la única razón por la que el cliente está en el registro es porque compró algo de joyería. Nada más. Esto hace que nuestro modelo aprenda algo, pero ese algo es una peculiaridad y no algo relevante sobre lo modelado.\n\n\n\n\n\n\nNota\n\n\n\nEl tema de la fuga de información es sumamente extenso, pero una excelente lecura al respecto es Kaufman et al. (2012).\n\n\nAhora sí, hablemos de cómo evaluar un clasificador."
  },
  {
    "objectID": "c17_clasif.html#evaluación-de-un-clasificador",
    "href": "c17_clasif.html#evaluación-de-un-clasificador",
    "title": "4  Aprendizaje supervisado: Clasificación",
    "section": "4.5 Evaluación de un clasificador",
    "text": "4.5 Evaluación de un clasificador\nHace un par de subtemas mencioné someramente las formas con las que evaluamos la calidad de las predicciones de un clasificador. Bien, ahora entremos a los detalles correspondientes.\n\n4.5.1 Matriz de confusión\nLa matriz de confusión no es otra cosa mas que una tabla de contingencia que relaciona las clases observadas y las predichas por el clasificador. En un caso de clasificación binaria (dos clases objetivo) tendríamos una estructura como la siguiente:\n\nMatriz de confusión para un clasificador binario. V: verdadero; F, falso; P: positivo; N: negativo; es decir, en la diagonal tenemos las clasificaciones correctas (verdaderos positivo y negativo), y encima y debajo los errores (falsos positivo y negativo).\n\n\n\nA\nB\n\n\n\n\n\\(\\hat{A}\\)\nVP\nFP\n\n\n\\(\\hat{B}\\)\nFN\nVN\n\n\n\nSi lo piensas con atención, esto es todo lo que necesitamos: en qué acertó y en qué se equivocó, aunque esto se vuelve muy impráctico cuando tenemos más de dos clases, y no nos permite evaluar rápidamente nuestro clasificador. Es aquí donde entran las medidas derivadas de esta matríz. Expliquémoslas todas con la siguiente matriz de confusión:\n\nEjemplo de matriz de confusión.\n\n\n\nA\nB\n\n\n\n\n\\(\\hat{A}\\)\n4\n1\n\n\n\\(\\hat{B}\\)\n2\n5\n\n\n\n\nExactitud (Accuracy): Representa el porcentaje de clasificaciones correctas, independientemente de si fueron verdaderos positivos o verdaderos negativos. Es decir, la suma de los elementos en la diagonal dividida entre el número total de observaciones. En nuestro ejemplo el clasificador tuvo una exactitud del 75%.\n\n\\[Exact = \\frac{VP+VN}{N} = \\frac{4+5}{12} = \\frac{3}{4} =0.75\\]\n\nPrecisión (Precision): Representa el porcentaje de clasificaciones positivas correctas; es decir, de todas las clasificaciones que fueron positivas, qué proporción era en realidad positiva. Nuestro clasificador de ejemplo tuvo una precisión del 80%:\n\n\\[Prec = \\frac{VP}{VP+FP} = \\frac{4}{4+1} = \\frac{4}{5} = 0.8 \\]\n\nTasa de verdaderos positivos (True Positive Rate, TPR). También conocida como sensibilidad. Esta indica qué porcentaje de los verdaderos positivos se clasificó correctamente; es decir, dividimos los verdaderos positivos entre la suma de estos y los falsos negativos, pues debieron de haber sido verdaderos positivos. Nuestro clasificador de ejemplo tuvo una sensibilidad del 66%\n\n\\[TPR = \\frac{VP}{VP+FN} = \\frac{4}{6} \\approx 0.66\\]\n\nTasa de verdaderos negativos (True Negative Rate, TNR). También conocida como especificidad. Es la proporción de los verdaderos negativos que se clasificó correctamente, siguiendo la misma lógica de la TPR. Nuestro clasificador de ejemplo tuvo una especificidad del 80%.\n\n\\[TNR = \\frac{VN}{VN+FP} = \\frac{5}{6} \\approx 0.8\\]\n\nF1. Esta es una medida que relaciona la precisión y la tasa de verdaderos positivos para dar una medida de la robustez del modelo. En nuestro modelo de ejemplo fue del 77%:\n\n\\[F1 = \\frac{2*Prec*TPR}{Prec + TPR} = 2*\\frac{0.75*0.8}{0.75+0.8} \\approx 0.77\\]\n\nTasa de No Información (No-Information Rate, NIR). Es el porcentaje que representa la clase mayoritaria del total, y representa la exactitud que tendría un modelo “bobo”; es decir, un modelo que predijera siempre esa clase mayoritaria. Nuestras clases están balanceadas, por lo que la NIR es del 50%:\n\n\\[NIR = max(n_i) = \\frac{6}{12} = 0.5\\]\nAdemás de estas R nos da otras dos que también son útiles:\n\n\\(p(Exact > NIR)\\): el resultado de una prueba binomial para ver si la exactitud del modelo es superior a la de un modelo bobo.\nKappa de Cohen: es un índice de la confiabilidad interna del clasificador. En pocas palabras, es el porcentaje de clasificaciones correctas que no son atribuíbles al azar. Como buen índice, está contenido en el intervalo \\([-1,1]\\), pero el valor máximo alcanzable en nuestro problema depende de qué tan balanceadas (o no) estén nuestras clases. Entre más balanceadas estén, más fácil será alcanzar valores altos, y viceversa.\n\nSi bien todas estas medidas representan distintos aspectos de la clasificación, hay un par de formas de evaluación más que, para la mayor parte de los problemas, dan una mejor representación de la capacidad predictiva del modelo.\n\n\n4.5.2 Curva de Características del Operador Receptor (ROC)\nSi has escuchado algo sobre los modelos de distribución de especies (MAXENT o similares), seguramente también hayas escuchado sobre la curva ROC. Esto es porque esos modelos son, en realidad, clasificadores que predicen si una especie puede estar (positivo) o no (falso) en un lugar determinado, pero eso es salirnos del tema.\nLa curva ROC muestra la relación entre la TPR y la FPR (que es el complemento de la TPR: 1-TPR) a distintos niveles, lo cual da una evaluación más confiable de la capacidad predictiva del modelo (Meyer-Baese & Schmid, 2014) donde el mejor modelo llevará la “curva” hacia la esquina superior izquierda (una tasa de verdaderos positivos perfecta en todos los casos), y un modelo bobo estará cercano a una línea central de referencia con pendiente de 1, lo que representa una clasificación al azar:\n\n\n\nFigura 4.10: Curva ROC y sus partes. Entre mejor sea el modelo más se acercará a la esquina superior izquierda. Un modelo que solo haga predicciones al azar (bobo) estará en la línea amarilla. Si la curva se va debajo de esa línea solo quiere decir que el modelo está prediciendo la clase positiva como negativa.\n\n\n\n\n4.5.3 Área bajo la curva ROC (ROC-AUC)\nLa curva ROC es sumamente útil para ver qué tan bueno o malo es el modelo, pero siempre es más fácil lidiar con un solo número. Es ahí donde entra el área bajo la curva (AUC-ROC). Esta área bajo la curva está en el intervalo [0,1], donde 1 es una clasificación perfecta, 0.5 una clasificación aleatoria (no hubo clasificación), y 0 es que el modelo reciprocó (invirtió) la clase a predecir.\nEl AUC tiene dos bondades:\n\nEs invariable con respecto a la escala; es decir, mide qué tan bien se clasifican las predicciones, más que dar sus valores absolutos.\nEs invariable con respecto al umbral de clasificación. Cuando entrenamos un modelo de clasificación podemos seleccionar un umbral para que el modelo diga que algo es la clase positiva. Entre más bajo sea este umbral, más positivos vamos a tener. El AUC mide la calidad de las predicciones del modelo, independientemente de qué umbral se haya seleccionado.\n\nEstas dos bondades, sin embargo, tienen dos bemoles:\n\nLa invarianza de escala puede ser inconveniente. A veces necesitamos resultados de probabilidad bien calibrados (¿qué probabilidad hay de que una observación sea clasificada como A y no como B?), y el AUC no es eso.\nLa invarianza al umbral de clasificación puede ponernos el pie. En casos en los que tenemos grandes disparidades en el costo de falsos negativos con relación a falsos positivos es imperante minimizar uno de esos errores de clasificación, y el AUC no es la medida para eso.\n\nNo hay un criterio claro para decir “qué tanto es tantito” y definir si un modelo es “bueno” o “malo”, pero Hosmer, Lemeshow & Sturdivant (2013) dan los siguientes intervalos:\n\n0.5: no hay clasificación\n(0.5, 0.7]: clasificación pobre\n(0.7, 0.8]: clasificación aceptable\n(0.8, 0.9]: excelente clasificación\n(0.9, 1]: clasificación excepcional\n\nAhora sí, después de toda esta teoría podemos aprender a implementar, evaluar e interpretar clasificadores."
  },
  {
    "objectID": "c17_clasif.html#árboles-de-decisión-y-ensambles",
    "href": "c17_clasif.html#árboles-de-decisión-y-ensambles",
    "title": "4  Aprendizaje supervisado: Clasificación",
    "section": "4.6 Árboles de decisión y ensambles",
    "text": "4.6 Árboles de decisión y ensambles\nEn la primera versión del curso hablé sobre clasificadores “tradicionales” que se han empleado en el área de la biología (regresión logística y análisis de funciones discriminantes lineales), pero creo que vale la pena hablar de algunas alternativas con márgenes de decisión más flexibles y que son casi igual de interpretables.\n\n\n\n\n\n\nNota\n\n\n\nLa regresión logística la vamos a retomar en el Capítulo 6 porque es un modelo lineal con un error binomial.\n\n\n¿Por qué “casi” igual de interpretables? Porque usualmente entre más flexible es un modelo, más complejo es su funcionamiento, menos simple es explicar cómo hace las predicciones y, por lo tanto, sacar conclusiones sobre cómo influyen las variables. Pero empecemos por lo sencillo.\n\n4.6.1 Árboles de decisión\nLos árboles de decisión son un modelo que “clasifica según el principio de partición recursiva, donde el espacio de atributos se parte recursivamente en regiones que contienen observaciones con valores de respuesta similares” (Strobl, Malley & Tutz, 2009). A veces me pregunto porque los matemáticos hacen definiciones tan agresivas, pero tiene mucho sentido. No explicaré qué es la recursividad porque sería desviarnos del tema, pero podemos simplificar la definición de un árbol de decisión como un modelo que clasifica a partir de decisiones binarias sobre nuestros datos, como si fuera un diagrama de flujo. Imagina que queremos decidir si vamos a surfear o no (asume que sabemos), entonces seguiríamos un diagrama de flujo como el siguiente:\n\n\n\nFigura 4.11: ¿Surfear o no? He ahí el dilema\n\n\n\n\n\n\n\n\nNota\n\n\n\nLas partes del árbol de decisión son básicamente las mismas que las del dendrograma (Capítulo 2). La diferencia es que no tenemos una raíz, cada nodo (punto de decisión) involucra una sola variable, y al final no tenemos hojas, sino nodos terminales.\n\n\nAplicado a nuestros datos la lógica es la misma: encontrar límites en cada variable que permitan llevar la decisión hacia una clase u otra. El algoritmo (Breiman, 2001) tiene un objetivo: que después de cada decisión incremente la “pureza” de los nodos que le siguen; es decir, que con cada decisión se abarque la mayor cantidad de datos posible hasta lograr una clasificación perfecta. Esto se logra minimizando la impureza Gini. Este índice tiene bastante teoría detrás, pero podemos simplificarlo como la tasa de error que obtendríamos si retiramos una variable, de modo que un buen predictor minimizará ese error y dará como resultado nodos más puros [ShogaRangswamy_2018]. En pocas palabras: vamos a seleccionar la variable con la que nos equivoquemos menos al hacer las decisiones.\nComo te habrás dado cuenta, es un algoritmo con una lógica que nos es fácil de entender, pues es muy similar a cómo tomamos decisiones (si A, entonces B) y, de hecho, esa es una de sus ventajas:\n\nLos árboles de decisión son fáciles de interpretar: su lógica binaria y la representación visual hace que interpretarlos sea sencillo. Además, la estructura jerárquica facilita saber qué atributos son los más importantes e incluye la interacción (por el momento entendámosla como la correlación) entre ellos.\nRequieren de poca o nula preparación de los datos: pueden lidiar con muchos tipos de variables, sean continuas o discretas, e incluso pueden lidiar con datos faltantes.\nSon flexibles: Aunque los estamos viendo en el tema de clasificación, podemos utilizarlos también para problemas de regresión. De hecho, si utilizas el algoritmo de Breiman (2001) que describimos antes, también se les conoce como Classification and Regression Trees (CART).\n\nComo todo lo demás en esta vida, si hay ventajas también hay desventajas:\n\nPropensos al sobreajuste. ¿Recuerdas que buscamos perfeccionar la clasificación lo más posible? Si dejamos que el árbol crezca indefinidamente va a, literalmente, memorizar los datos que le dimos para entrenarlo.\nSon estimadores con alta varianza, de modo que pequeñas variaciones dentro de los datos pueden producir árboles muy diferentes.\nAlto costo computacional, derivado de la aproximación “codiciosa” (greedy) tomada durante su construcción (entiendelo como evaluar todo al mismo tiempo). Entre más datos y clases con límites complejos tengamos, más tiempo o poder computacional (o ambos) vamos a necesitar para entrenar el modelo.\n\nLa tercer desventaja la podemos “obviar”, pues para fines de investigación no vamos a necesitar entrenar y obtener el modelo prácticamente en tiempo real, pero las primeras dos sí que son importantes, y nos llevan a hablar de los ensambles de árboles, particularmente los bosques aleatorios.\n\n\n4.6.2 Bosques aleatorios\nPara empezar, ¿qué es un ensamble? Es juntar múltiples modelos de aprendizaje automatizado para obtener un modelo con una mejor capacidad predictiva. Si nuestro modelo base son árboles de decisión y juntamos varios, es natural llamar a su ensamble un “bosque”. O bueno, solo a uno de sus ensambles, porque en realidad hay más modelos (AdaBoost, XGBoost, bagged trees) que son ensambles de árboles. ¿Por qué seleccionar a los bosques aleatorios? Realmente por practicidad, pues tanto AdaBoost como XGBoost tendrían razones similares:\n\nComparten las fortalezas de los árboles de decisión, con excepción de la interpretación (ya hablaremos de eso en el ejemplo).\nNulifican la principal desventaja de los árboles de decisión: al ser varios árboles se minimiza la probabilidad de sobreajuste.\nMinimizan también la alta varianza o, mejor dicho, se apropian de ella. La naturaleza aleatoria de los bosques aleatorio hace que, aún con los mismos datos, podamos obtener bosques completamente diferentes. ¿Importa? Para nada, porque no vamos a explicar cada bosque de manera individual.\n\nAhora bien, ¿cómo funcionan los bosques aleatorios? No puedo construir diferentes árboles de decisión si tengo los mismos datos de entrenamiento, entonces hay que cambiar eso. La forma en que lo hacemos deriva de dos conceptos (Pal, 2017):\n\nAgregación Bootstrap (“Bagging”): Se genera un conjunto de entrenamiento diferente para cada árbol, utilizando muestreos con reemplazo del conjunto original de datos de entrenamiento\nSelección aleatoria de atributos: Puedes entenderla como una agregación Bootstrap para los predictores , en la que los atributos (variables) considerados en cada nodo son un subconjunto aleatorio de las variables originales.\n\nEs decir, cada árbol en el bosque va a trabajar con su propio subconjunto de observaciones, y en cada partición va a poder seleccionar solo de un subconjunto aleatorio de variables. El resultado es un modelo de aprendizaje supervisado que se considera como uno de los más eficientes (Carvajal, Maucec & Cullick, 2018), pero básta de cháchara y pongámonos manos a la obra.\n\n4.6.2.1 Entrenamiento\nUtilicemos esta vez los datos palmerpenguins::penguins_raw, y aprovechemos también para introducir el uso de tidymodels. Nuestro proceso consiste de básicamente 11 pasos (+1):\n\nGuardemos los datos en un objeto, manteniendo solo las variables numéricas (sin el identificador). El resultado es un conjunto de datos con 7 variables:\n\nLongitud del culmen\nProfundidad del culmen\nLongitud de la aleta\nMasa corporal\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nSpecies, que contiene las etiquetas que queremos predecir\n\n\n\npeng_data <- penguins_raw |>\n             # Seleccionar solo las columnas numéricas y \"Species\"\n             select(where(is.numeric) |\n                    contains(\"Species\")) |>\n             # Extraer la primera palabra de la columna `Species`\n             mutate(Species = str_extract(Species,\n                                          pattern = \"\\\\w*\")) |>\n             # Descartar los datos faltantes\n             drop_na()\nhead(peng_data)\n\n\n\n  \n\n\n\nAhora optimicemos e interpretemos el modelo de bosques\n\nDividir los datos en datos de entrenamiento y prueba. Para esto utilizaremos las funciones rsample::initial_split() y rsample::training(). Además, remuestrearemos los datos de entrenamiento para realizar la validación cruzada y optimizar los hiper-parámetros del modelo con la función rsample::vfold_cv()\n\n\nset.seed(0)\n# Dividimos los datos\npeng_split <- rsample::initial_split(peng_data)\n\n# Extraemos los datos de entrenamiento\npeng_train <- rsample::training(peng_split)\n\n# Datos para validación cruzada\npeng_cv <- vfold_cv(peng_train)\n\n\nPreprocesar los datos. Para esto haremos lo que tidymodels denomina como una receta. Recordarás que los árboles de decisión no requieren prácticamente ningún tipo de preprocesamiento, pero centremos las variables numéricas para ejemplificar.\n\nPasos:\n\nLe damos a la receta (recipe()) la fórmula y los datos de entrenamiento.\nAñadimos un paso adicional para centrar los datos numéricos.\n\nEl objeto peng_prep sigue los pasos que definimos para el procesamiento de los datos (por eso receta) y obtiene los parámetros que se van a utilizar, mientras que peng_juiced contiene los datos ya procesados.\n\n\n\n\n\n\n\n\nNota\n\n\n\nUna lista completa de todos los pasos de pre-procesamiento la puedes encontrar en la documentación correspondiente de tidymodels.\n\n\n\n# Formación de la receta\npeng_rec <- recipe(Species~., data = peng_train) |>\n            update_role(contains(\"Sample\"),\n                        new_role = \"id vars\") |> \n            step_center(all_numeric())\n# Obtener parámetros para preprocesar\npeng_prep <- peng_rec |> prep()\n\n# Preprocesar los datos\npeng_juiced <- peng_prep |> juice()\n\n\nCrear el modelo. Ahora podemos especificar el modelo de bosques aleatorios.\n\nEvidentemente debemos de controlar la complejidad del modelo, lo cual haremos ajustando sus hiperparámetros:\n\nmtry (el número máximo de predictores por árbol)\nmin_n (el número de observaciones necesarias para seguir dividiendo los datos)\ntrees (el número de árboles en el ensemble).\n\nDespués especificamos que es un bosque para clasificación con set_mode, y por último le indicamos en set_engine que utilice la librería ranger para construir el bosque:\n\n\n\n\n\n\n\n\nNota\n\n\n\nUna lista completa de todos los tipos de modelos la puedes encontrar en la documentación correspondiente de tidymodels.\n\n\n\n# Especificar el modelo\n# Los hiperparámetros van a ser ajustados\ntune_spec <- rand_forest(mtry = tune(),\n                         trees = tune(),\n                         min_n = tune()) |> \n             # Es un problema de clasificación\n             set_mode(\"classification\") |>\n             # Se va a resolver con la librería ranger\n             set_engine(\"ranger\")\n\n\n\n\n\n\n\nNota\n\n\n\nUna lista completa de modelos la puedes encontrar aquí\n\n\n\nLuego, y es aquí donde brilla tidymodels, formar un flujo de trabajo (workflow()) que contenga ambos pasos: la receta de preprocesamiento y el modelo.\n\n\n# Se inicia un flujo de trabajo\ntune_wf <- workflow() |> \n           # Sigue la receta para pre-procesar datos\n           add_recipe(peng_rec) |> \n           # Entrena el modelo\n           add_model(tune_spec)\n\n\n\n\n\n\n\nNota\n\n\n\nEn este punto no hemos entrenado el modelo, solo le hemos dicho a tidymodels cómo queremos que se manejen nuestros datos y qué modelo queremos aplicar. ¿Por qué “brilla” entonces tidymodels? Porque separa los pasos de la declaración del modelo de la ejecución, por lo que puedes modificar la especificación del modelo o del preprocesamiento de los datos de manera independiente, y permite que re-entrenemos el modelo simplemente modificando los datos en el paso 1.\n\n\n\nAjustar los hiper-parámetros\n\nEstablecer el procesamiento en paralelo para que el modelo se entrene de manera más eficiente\nEntrenar el modelo, lo cual podemos hacer con fuerza bruta, proponiendo una malla gigantesca de posibles valores, o podemos hacerlo de manera más eficiente escogiendo 20 puntos aleatorios para guiar la búsqueda. Hagamos lo segundo:\n\n\n\nset.seed(123)\n# Asignar 8 núcleos para la computación en paralelo\n# Revisa cuántos puedes disponer tú (más o menos)\ndoParallel::registerDoParallel(cores = 8)\n# Buscar los mejores hiperparámetros de entre 20 valores\ntune_res <- tune_grid(tune_wf,\n                      resamples = peng_cv,\n                      grid = 20)\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\n\nEvaluar primer ajuste. En este punto solo propusimos 20 valores tentativos para los hiper-parámetros del bosque, con el objetivo de guiar la búsqueda y no abusar de la confianza de nuestras computadoras. Pues bien, ahora hay que ver alrededor de qué valores vamos a hacer la optimización. ¿Cómo? Graficando los valores de AUC que le corresponden a cada valor de cada hiperparámetro. En la figura de abajo pareciera que todo es un caos, pero en realidad podemos ver que el AUC está en sus puntos más altos con:\n\nmin_n entre 5 y 10\nmtry con menos de 3\ntrees con alrededor de 1000.\n\n\n\nplot_rf_cv <- function(tune_res){\n  # Graficar los valores de AUC de validación cruzada\n  # Recuperar los valores\n  tune_res |> collect_metrics() |>\n              # Quedarnos solo con los AUC\n              filter(.metric == \"roc_auc\") |>\n              # Extraer las columnas con las medidas\n              select(mean, min_n, mtry, trees) |>\n              # Poner la malla en formato largo\n              pivot_longer(min_n:trees,\n                           values_to = \"value\",\n                           names_to = \"parameter\") |>\n              # Graficar con `ggplot`\n              ggplot(aes(value, mean, color = parameter)) +\n              geom_point(show.legend = F) +\n              facet_wrap(~parameter, scales = \"free_x\") +\n              labs(y = \"AUC\") +\n              theme_bw()\n}\n\nplot_rf_cv(tune_res)\n\n\n\n\n\n\n\n\n\n\nNota\n\n\n\nNota que estamos encadenando el procesamiento de los resultados y su graficado. Eso no tiene nada de sorprendente, a final de cuentas ggplot recibe como primer argumento un data.frame, pero es importante que observes que el operador de encadenamiento pasa de |> a +.\n\n\n\nAjustar el ajuste; es decir, ajustar mejor los hiperparámetros con valores alrededor de esos “óptimos” que encontramos arriba.\n\nEstablecemos una malla de búsqueda más fina con la función grid_regular.\nRe-entrenamos el modelo, pero esta vez de manera más dirigida con rf_grid. El resultdo es que ahora todos los AUCs resultantes están por encima de 0.995.\n\n\n\n# Establecer una malla con los nuevos valores de referencia\nrf_grid <- grid_regular(mtry(range = c(1, 3)),\n                        min_n(range = c(5, 10)),\n                        trees(range = c(700, 1100)),\n                        levels = 5)\n\n# Optimizar los hiperparámetros con esa malla\ntune2_res <- tune_grid(tune_wf,\n                       resamples = peng_cv,\n                       grid = rf_grid)\n\n# Obtener y graficar los resultados\nplot_rf_cv(tune2_res)\n\n\n\n\n\nSeleccionar el mejor modelo. Con el código anterior optimizamos lo más posible los hiper-parámetros, ahora toca seleccionar la mejor combinación de hiperparámetros.\n\nIdentificar el modelo con la función select_best()\nActualizar la especificación original (tune_spec)\n\n\n\n# Seleccionar el modelo con los mejores hiperparámetros\n# (a los que les corresponde el mayor AUC)\nbest_auc <- select_best(tune2_res, \"roc_auc\")\n\n# Actualizar el modelo con los nuevos parámetros\nfinal_rf <- finalize_model(x = tune_spec,\n                           parameters = best_auc)\n\n# Muestra los detalles del modelo\nfinal_rf\n\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  mtry = 1\n  trees = 800\n  min_n = 5\n\nComputational engine: ranger \n\n\n\nRevisar importancia de variables. ¿Qué podemos aprender de este modelo? Las variables más importante para la clasificación son la longitud del culmen, seguida de la longitud de la aleta. ¿Tienen sentido biológico estos resultados? Esa es una discusión para otro momento, pues solo tomamos estos datos para ejemplificar la implementación de los bosques aleatorios.\n\n\n# Graficar la importancia de variables\n\n# Pasa el modelo final\nfinal_rf |> \n  # Fija que se va a extraer la importancia de variables\n  set_engine(\"ranger\", importance = \"permutation\") |>\n  # Estimada desde los datos de entrenamiento (- el identificador)\n  fit(Species ~.,\n      data = juice(peng_prep) |> select(-contains(\"Sample\"))) |>\n  # Calcula y grafica la importancia de variables\n  vip(geom = \"point\") +\n  geom_point(color = \"deepskyblue3\") +\n  see::theme_lucid() +\n  labs(title = \"Importancia de variables\",\n       y = element_blank())\n\n\n\n\nFigura 4.12: Importancia de variables del bosque aleatorio\n\n\n\n\n\n\n\n\n\n\nNota\n\n\n\nSi pones atención te darás cuenta de que estamos obteniendo la importancia de variables utilizando permutaciones, esto es porque es una manera más robusta de obtenerlas que solo utilizando Gini; sin embargo, estas no son las únicas. Tenemos algunas alternativas como valores Shapely, explicaciones SHAP o LIME que tienen algunas ventajas importantes. Desafortunadamente, SHAP y LIME no están implementados en R, solo en Python\n\n\n\nMuy importante, verificar que el modelo no esté sobreajustado. Para esto vamos a crear un último flujo de trabajo, y después ajustar una última vez. ¿Por qué? Porque nuestros modelos hasta ahora se entrenaron en 10 subconjuntos de nuestros datos de entrenamiento (validación cruzada). Para este último ajuste usaremos la función last_fit(), la cual ajusta el modelo final en todos los datos de entrenamiento y evalúa en los datos de prueba. Los valores de ROC-AUC y de exactitud son muy buenos, pero además en línea con lo que esperábamos de la optimización (AUC > 0.995), lo cual quiere decir que el modelo no está sobre-ajustado, sino que es muy bueno.\n\n\n# Flujo de trabajo final\nfinal_wf <- workflow() |> \n            add_recipe(peng_rec) |> \n            add_model(final_rf)\n\n# Último ajuste\nfinal_res <- final_wf |> \n             last_fit(peng_split)\n\n# Resultados de la evaluación en los datos de prueba\nfinal_res |> \n  collect_metrics()\n\n\n\n  \n\n\n# Creación de las curvas ROC\nrf_auc <- final_res |>\n          collect_predictions() |> \n          roc_curve(Species, c(.pred_Adelie,\n                               .pred_Chinstrap,\n                               .pred_Gentoo)) |> \n          mutate(model = \"Random Forest\")\n\nautoplot(rf_auc) +\n  labs(title = \"Curvas ROC\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\nImportante\n\n\n\nEn la función roc_curve del objeto rf_auc estamos pasando un vector con tres objetos ocultos: .pred_Adelie, .pred_Chinstrap, .pred_Gentoo. Estos contienen la probabilidad de pertenencia de cada observación a cada especie, y es con los que se obtiene la curva ROC que le corresponde a cada especie, pues la curva es solo para clasificaciones binarias. ¿Cómo funciona en este caso? La clase positiva es la clase predicha en cada curva, y la negativa es lo demás, básicamente representando un clasificador clase + vs. resto.\n\n\n\nElaborar el reporte ¿Cómo reportamos todo esto? Te recomiendo que le eches un ojo a Enríquez-García et al. (2022) para ver cómo describir el método, pero los resultados podemos resumirlos como:\n\n\nEl bosque aleatorio tuvo un excelente desempeño sin sobreajustar (AUC-ROC = 0.99 en los datos de entrenamiento). Las variables más importantes para la clasificación fueron la longitud del culmen (\\(\\approx 17\\%\\)), seguida de la longitud de la aleta (\\(\\approx 12.5\\%\\); Figura 4.12). El bosque aleatorio optimizado estuvo compuesto por 800 árboles, cada uno formado con un predictor (mtry), considerando al menos 5 observaciones para poder hacer una división (n_min).\n\nEsto fue todo para este (espero) motivante capítulo, donde entramos de lleno al mundo del aprendizaje supervisado. Me hubiera gustado darte un poco menos de teoría, pero aplicar modelos de aprendizaje automatizado es algo que requiere que pongamos muchísima atención a qué es lo que hacemos en cada paso y por qué, pues es muy fácil que resbalemos y que terminemos presumiendo un modelo sobreajustado. Voy a confirmar tus sospechas: también hay que verificar que nuestras regresiones no estén sobreajustadas, pero eso lo dejamos para la siguiente sesión."
  },
  {
    "objectID": "c17_clasif.html#ejercicio",
    "href": "c17_clasif.html#ejercicio",
    "title": "4  Aprendizaje supervisado: Clasificación",
    "section": "4.7 Ejercicio",
    "text": "4.7 Ejercicio\nAplica un clasificador de bosques aleatorios a los datos Medidas.txt que utilizamos en el Capítulo 2. Compara la importancia de variables con las cargas factoriales del ACP y con las variables significativas para la ordenación del NMDS. ¿Hay diferencias?\n\n\n\n\nBreiman L. 2001. Random Forests. Machine Learning 45:5-32. DOI: 10.1023/A:1010933404324.\n\n\nCarvajal G, Maucec M, Cullick S. 2018. Components of artificial intelligence and data analytics. In: Intelligent Digital Oil and Gas Fields. Concepts, Collaboration, and Right-Time Decisions. Cambridge, Massachusetts, USA: Gulf Professional Publishing, 101-148. DOI: 10.1016/B978-0-12-804642-5.00004-9.\n\n\nEnríquez-García AB annd FV-Z, Tripp-Valdez A, Moreno-Sánchez XG, Galván-Magaña F, Elorriaga-Verplancken FR. 2022. Foraging segregation between spotted (Stenella attenuata) and spinner (Stenella longirostris) dolphins in the Mexican South Pacific. Marine Mammal Science 38:1070-1087. DOI: 10.1111/mms.12912.\n\n\nHosmer DWJr, Lemeshow S, Sturdivant RX. 2013. Applied Logistic Regression. John Wiley & Sons, Inc. DOI: 10.1002/9781118548387.\n\n\nKaufman S, Rosset S, Perlich C, Stitelman O. 2012. Leakage in Data Mining: Formulation, Detection, and Avoidance. ACM Transactions on Knowledge Discovery from Data 6. DOI: 10.1145/2382577.2382579.\n\n\nMeyer-Baese A, Schmid V. 2014. Chapter 7 - Foundations of neural networks. In: Meyer-Baese A, Schmid V eds. Pattern recognition and signal analysis in medical imaging. Oxford: Academic Press, 197-243. DOI: 10.1016/B978-0-12-409545-8.00007-8.\n\n\nPal R. 2017. Regression trees, Random Forests, Probabilistic trees, Stacked generalization, Probabilistic Random Forests, Weight optimization. In: Pal R ed. Predictive Modeling of Drug Sensitivity. Academic Press, 149-188. DOI: 10.1016/B978-0-12-805274-7.00007-5.\n\n\nStrobl C, Malley J, Tutz G. 2009. An Introduction to Recursive Partitioning: Rationale, Aplication and Characteristics of Classification and Regression Trees, Bagging and Random Forests. Physiological Methods 14:323-348. DOI: 10.1037/a0016973."
  },
  {
    "objectID": "c18_mvregs.html#librerías",
    "href": "c18_mvregs.html#librerías",
    "title": "5  Regresiones múltiples",
    "section": "5.1 Librerías",
    "text": "5.1 Librerías\n\nlibrary(ggplot2)\nlibrary(GGally)\nlibrary(dplyr)\nlibrary(tidymodels)\nlibrary(performance)\nlibrary(vip)"
  },
  {
    "objectID": "c18_mvregs.html#introducción",
    "href": "c18_mvregs.html#introducción",
    "title": "5  Regresiones múltiples",
    "section": "5.2 Introducción",
    "text": "5.2 Introducción\nEn el ?sec-rls vimos una bastante extensiva introducción al modelo lineal, y en el ?sec-nolin vimos que la regresión polinomial era una extensión de este donde añadíamos “nuevas” pendientes que correspondían con potencias de nuestra variable independiente. Bueno, hoy vamos a introducir pendientes para otros predictores, y hablar de lo que eso implica. También veremos cómo podemos controlar la complejidad de nuestros modelos lineales, y ver cómo juzgar/evaluar adecuadamente un modelo de regresión múltiple, pues es más complejo que solo revisar sus supuestos.\n\n5.2.1 Regresión Lineal Múltiple\nLa regresión lineal múltiple (RLM) es, en escencia, lo mismo que la regresión lineal simple: un modelo de aprendizaje supervisado donde se predice una variable numérica siguiendo un modelo lineal, solo que esta vez añadimos una serie de variables independientes. El modelo lineal múltiple queda dado entonces por:\n\\[\nY = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_nX_n + \\epsilon\n\\]\nO podemos ponerlo en notación matricial, donde \\(X\\) es ahora una matriz que contiene \\(m\\) variables independientes y \\(n\\) observaciones (\\(m \\times n\\)), y \\(\\beta\\) es un vector de \\(m\\) coeficientes.\n\\[\nY = \\alpha + X\\cdot\\beta + \\epsilon\n\\]\n\n\n\n\n\n\nNota\n\n\n\nTanto en la RLS como en la RLM las variables independientes pueden ser categóricas, ordinales y, por supuesto continuas, pero el cómo las integremos al modelo y su interpretación tienen bemoles que veremos más adelante.\n\n\nNo vamos a entrar en los detalles del álgebra lineal correspondientes porque los vimos en el Capítulo 1, lo que sí es “importante” saber es que, mientras en la RLS encontramos una línea que “mapea” los valores de \\(x\\) hacia \\(y\\), en la RLM encontramos un hiper-plano (una sábana) de \\(m\\) dimensiones. Nuestra intución como criaturas tridimensionales solo nos permite ver hasta tres dimensiones, y tendríamos algo como esto:\n\n\n\nFigura 5.1: La regresión lineal múltiple genera un hiperplano que explica la relación \\(Z\\sim X+Y\\) (\\(Y \\sim X_1 + X_2\\)).\n\n\nDicho esto, vayamos a nuestros datos alojados en una dirección web. Estos datos cuentan con las siguientes variables:\n\nLongitud del organismo (L_o)\nDiámetro del organismo (D_o)\nAltura del organimso (A_o)\nPeso entero (P_e)\nPeso sin concha (P_sc)\nPeso de las vísceras (P_v)\nPeso de la cáscara (P_c)\nEdad (años) (Edad)\n\n\n# URL donde se alojan los datos, partida en dos segmentos para acortarla\nabulon_url <- paste0(\"https://storage.googleapis.com/\",\n                     \"download.tensorflow.org/data/abalone_train.csv\")\n\n# Descargar los datos en un objeto\nabulon_dwn <- RCurl::getURL(abulon_url)\n\n# Leer los datos descargados\nabulon_data <- read.csv(text = abulon_dwn,\n                        header = F)\n\n# Nombres de columnas\nvar_names <- c(\"L_o\", \"D_o\", \"A_o\", \"P_e\",\n               \"P_sc\",\"P_v\", \"P_c\", \"Edad\")\ncolnames(abulon_data) <- var_names\n\nhead(abulon_data)\n\n\n\n  \n\n\n\nEl objetivo es construir un modelo de RLM que prediga la edad de los abulones a partir del resto de medidas. Podemos ver la relación entre cada par de variables. Para hacernos la vida más sencilla vamos a utilizar la función GGally::ggpairs():\n\nGGally::ggpairs(abulon_data,\n                progress = F,\n                upper = \"blank\")\n\n\n\n\nSi ponemos atención al último renglon es claro que hay una relación entre Edad y el resto de variables, pero también tenemos dos pequeños “problemas”:\n\nDos individuos que tienen valores de A_o que se alejan considerablemente del resto.\nComo era de esperarse dados los nombres de las variables, algunas están altamente correlacionadas.\n\nPor el momento ignoremos ambas cosas y ajustemos nuestro modelo. Los pasos a seguir son básicamente los mismos que seguimos en el Capítulo 4 con el bosque aleatorio, salvo peculiaridades propias de la regresión lineal. También entonces vamos a aprovechar para introducir formalmente el uso de tidymodels\n\n5.2.1.1 Implementación\nLos pasos a seguir son prácticamente los mismos qu\n\nDividir los datos en datos de entrenamiento y prueba. Si pusiste atención al enlace te darás cuenta de que los datos que descargamos son datos de prueba, por lo que esta vez nos saltaremos este paso.\nPreprocesar los datos. En este caso no es “necesario” que nuestras variables estén en la misma escala, por lo que también vamos a saltarnos este paso; sin embargo, más adelate veremos que es recomendable y que cuando queremos controlar la complejidad del modelo sí se vuelve necesario. De cualquier manera (y para hacernos el hábito) vamos a especificar la receta, aunque no tenga ningún paso más que la fórmula:\n\n\n# Formación de la receta\nabulon_rec <- recipe(Edad~., data = abulon_data)\n\n# Obtener parámetros para preprocesar\nabulon_prep <- abulon_rec |> prep()\n\n# Preprocesar los datos\nabulon_juiced <- abulon_prep |> juice()\n\n\nCrear el modelo. Ahora podemos especificar el modelo de regresión lineal múltiple\n\n\n# Especificación de la regresión lineal\nlm_spec <- linear_reg() |>\n           set_engine(\"lm\") |>\n           set_mode(\"regression\")\n\n\n\n\n\n\n\nNota\n\n\n\nSi te das cuenta las únicas diferencias entre la especificación del bosque aleatorio que construimos en el Capítulo 4 y esta regresión múltiple es que como “motor” estamos utilizando lm, y que el “modo” del modelo es una regresión. En tidymodels solo hay que cambiar estos dos argumentos para aplicar cualquiera de los modelos que tienen una interface, los cuales puedes ver aquí.\n\n\n\nFormar un flujo de trabajo que aplique la receta de preprocesamiento de los datos y se la pase a la especificación del modelo:\n\n\nlm_wf <- workflow() |>\n         add_recipe(abulon_rec) |>\n         add_model(lm_spec)\n\n\nAjustar el modelo. Aquí no tenemos hiperparámetros, por lo que saltaremos este paso, pero más adelante lo vamos a retomar. Por el momento solo ajustemos el modelo con los datos de prueba:\n\n\nlm_fit <- lm_wf |> fit(data = abulon_data)\n\n\nVer el resumen. De nuevo, no tenemos hiperparámetros a optimizar, pero podemos ver la salida de la función lm en el atributo lm_fit$fit$fit$fit (sí, 3 veces fit, no me preguntes por qué lo enterraron hasta allá). Aquí podríamos hablar de qué coeficientes son significativamente diferentes de 0, de si sus relaciones son positivas o negativas, darnos una idea de lo mal ajustado que está el modelo con el \\(R^2\\) y todo lo demás que vimos en el ?sec-rls. Las únicas diferencias en la interpretación es que ahora tenemos más parámetros, que ahora Multiple R-squared tiene sentido y que también vale la pena ver el resultado del ANOVA. La hipótesis de nulidad de ese ANOVA es que todos los parámetros son iguales a 0 (modelo nulo), y la alternativa que al menos uno es diferente. ¿Cuál o cuáles? Eso lo dan las pruebas de \\(t\\) de cada parámetro.\n\n\n# Extraer la salida de la función lm\nlm_form <- lm_fit$fit$fit$fit\nsummary(lm_form)\n\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.4264  -1.3563  -0.3701   0.9374  12.2263 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   3.1004     0.2939  10.551  < 2e-16 ***\nL_o          -1.7735     2.0821  -0.852    0.394    \nD_o          13.5051     2.5404   5.316 1.13e-07 ***\nA_o          11.1089     1.6075   6.911 5.77e-12 ***\nP_e           9.9224     0.8287  11.973  < 2e-16 ***\nP_sc        -20.8791     0.9265 -22.536  < 2e-16 ***\nP_v         -10.7353     1.4591  -7.357 2.35e-13 ***\nP_c           8.0065     1.2375   6.470 1.13e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.184 on 3312 degrees of freedom\nMultiple R-squared:  0.5369,    Adjusted R-squared:  0.5359 \nF-statistic: 548.4 on 7 and 3312 DF,  p-value: < 2.2e-16\n\n\n\nComprobar los supuestos. Aquí llevaríamos el ajuste de los hiperparámetros a mejores valores, pero como no tenemos podemos aprovechar para comprobar los supuestos. Vemos que nuestro posterior predictive check no está del todo bien, el grafico de linealidad tampoco, el de homogeneidad de varianzas menos, tenemos una observación altamente influyente, el gráfico de normalidad está bastante mal, y tenemos un gráfico adicional: el gráfico de colinealidad, que tampoco se ve bien. ¿Qué es la colinealidad? El grado de correlación que hay entre nuestros predictores, pero ahorita vemos más de eso.\n\n\nperformance::check_model(lm_form)\n\n\n\n\n\nRevisar la importancia de variables: A diferencia de lo que vimos en el Capítulo 4 sobre la importancia de variables en bosques aleatorios, en modelos lineales qué variable es más importante usualmente se define por el valor absoluto del estadístico \\(t\\), de modo que entre más grande sea, más importante es la variable:\n\n\nlm_spec |>\n  set_engine(\"lm\") |> \n  fit(Edad~.,\n      data = juice(abulon_prep)) |> \n  vip(geom = \"point\") +\n  geom_point(color = \"dodgerblue4\") +\n  labs(title = \"Importancia de variables\",\n       subtitle = \"Modelo: RLM\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nLa librería vip tiene más funciones que solo vip. Te recomiendo que revises su viñeta para darte una idea de qué puedes hacer.\n\n\n\nVerificar que el modelo no esté sobreajustado. Si bien es cierto que nuestro ajuste general no es muy bueno, ¿qué pasa con los datos de prueba? Como no hicimos la separación entrenamiento-prueba no podemos utilizar el mismo método que usamos con el bosque aleatorio, pero sí que podemos obtener nuestras medidas de ajuste manualmente con ayuda de la función predict. Primero, bajemos los datos de prueba, luego predigamos las edades observadas utilizando nuestro modelo, y luego calculemos el RMSE y el \\(R^2\\) con las funciones yardstick::rmse y yardstick::rsq.\n\n\n# Descargar datos de prueba\nabulon_testd <- paste0(\"https://storage.googleapis.com/\",\n                       \"download.tensorflow.org/data/abalone_test.csv\")\ndownload <- RCurl::getURL(abulon_testd)\nabulon_test <- read.csv(text = download, header = F)\ncolnames(abulon_test) <- var_names\n\n# Predecir edades en los datos\nabulon_test[\"pred\"] <- predict(lm_form, abulon_test)\n\n# Calcular rmse y r2 y juntar los resultados\nrbind(\nabulon_test |> yardstick::rmse(truth = Edad,\n                               estimate = pred),\nabulon_test |> yardstick::rsq(truth = Edad,\n                               estimate = pred)\n)\n\n\n\n  \n\n\n\nEl RMSE es mayor en los datos de prueba que en los de entrenamiento (2.35 vs. 2.18), mientras que el \\(R^2\\) es mayor en los datos de entrenamiento (0.53 vs. 0.48). En cualquiera de los casos no tenemos una diferencia particularmente grande, por lo que no podemos asegurar que el modelo se encuentre sobreajustado (la diferencia es del 5-7% con respecto a los datos de entrenamiento) y podemos decir que el modelo generaliza “bien”. ¿Por qué “bien”? Porque el valor de \\(R^2\\) no es precisamente alentador, pero no hay una diferencia grande entre los datos entre los que fue entrenado y datos que nunca vio; es decir, el modelo está sub ajustado y utilizarlo para predicciones sería arriesgado.\n\n\n\n\n\n\nNota\n\n\n\nPara variar, ¿qué tanto es tantito? Pues resulta que si hacemos una partición entrenamiento prueba, hacemos validación cruzada, etc., etc., nuestro valor para los datos de prueba depende de cómo se haya hecho esa partición. Si tenemos mala suerte podemos terminar con pésimas métricas de evaluación, mientras que si tenemos suerte pueden ser excelentes, solo por un proceso aleatorio. Es por eso que es importante realizar validación cruzada, o tener datos que sean realmente de prueba. De cualquier manera, una diferencia hacia arriba o hacia abajo del 5-10% puede ser dada por “buena”. Qué tan estricto tenga que ser el criterio dependerá, para variar, de tu problema, tus objetivos, y qué consecuencias tenga para ti el cometer un error.\n\n\n¿Y para interpretación? Pues si no podemos hacer buenas predicciones no tiene caso hacer interpretaciones; sin embargo, veamos por qué, aún si tuviera un buen poder predictivo, no podemos hacer una buena interpretación. Volvamos a ver los gráficos diagnósticos:\n\nperformance::check_model(lm_form)\n\n\n\n\nArriba mencionamos todos los problemas que tiene el modelo, evidenciados por todos los gráficos, pero no hemos hablado a detalle de la colinealidad. En la RLM tenemos un supuesto adicional a los de la RLS: No colinealidad de los predictores; es decir, que las variables independientes sean independientes entre sí (no pueden tendría caso que fueran independientes de la variable a predecir, ¿o sí?). La explicación tiene varios bemoles. Bueno, en realidad es uno, pero hay que ver algo de álgebra.\n\n\n5.2.1.2 Supuesto de no colinealidad\nLa multicolinealidad (i.e.,tener dos o más predictores correlacionados) puede ser entendida como darle al modelo información redundante; es decir, que aquellos predictores correlacionados están aportando “la misma” información para predecir la variable de interés. Vuelve al gráfico de pares y verás que hay relaciones aparentes no solo entre Edad y los predictores, sino también entre varios pares de predictores. Esto nos lleva a que cuando el coeficiente de uno de ellos aumente, el otro aumente o disminuye según si la correlación entre ellos es negativa o positiva, pero hagámos el ejercicio algebráico. Nuestro modelo está declarado de la siguiente manera, donde \\(L\\) es la longitud, \\(D\\) el diámetro, \\(P\\) el peso (\\(_E\\): entero, \\(_{SC}\\): sin concha):\n\\[\nEdad = \\alpha + \\beta_1 L_o + \\beta_2 D_o + \\beta_3 P_e + \\beta_4 P_{sc} + \\dots + \\epsilon\n\\]\nAsumamos que dos variables altamente correlacionadas, por ejemplo \\(P_e\\) y \\(P_{sc}\\). Es más, asumamos que no solo son equivalentes, sino matemáticamente idénticas. En ese escenario estas variables podemos resumirlas en una sola que se llame \\(Peso\\), y reescribir el modelo como:\n\\[\nEdad = \\alpha + \\dots + (\\beta_3 + \\beta_4)*Peso + \\dots\n\\]\nResulta entonces que es la suma de \\(\\beta_3\\) y \\(\\beta_4\\) la que afecta a \\(Edad\\), y no sus valores separados. Dicho de otra manera, podemos hacer una de ellas cada vez más pequeña, siempre que tengamos la otra. En la práctica, entonces, no tenemos dos variables y, por lo tanto, tampoco tenemos dos parámetros. Un caso extremo como este, con una correlación prefecta, nos lleva a un modelo indeterminado; es decir, \\(\\beta\\) podría tomar cualquier valor en el intervalo \\((-\\infty, \\infty)\\). Este no es nuestro caso, pero es la razón de ser del “variables independientes”.\n¿Qué implicaciones tiene esto? Depende de cuál sea nuestro interés. Si nos interesa solamente la predicción, ninguna, pues esa correlación entre los parámetros es solamente una consecuencia de nuestros datos y el modelo. La historia cambia si nos interesa explicar el modelo, pues en un modelo de RLM cada parámetro tiene sentido solo en el contexto de los otros. Como te imaginarás, y como vimos arriba, en el mundo real las correlaciones entre los predictores no son extrañas. ¿Qué tan fuerte debe de ser la correlación entre dos o más variables para ser un problema? \\(r = 0.85\\). Nah, mentira, para variar no hay un criterio o número mágico, pero podemos revisar el Factor de Inflación de la Varianza (VIF).\n\n\n\n\n\n\nAdvertencia\n\n\n\nNo importa la correlación “cruda” entre los predictores, sino qué es lo que pasa dentro el modelo. Recuerda, aquí estamos analizando la pertinencia del modelo y, por lo tanto, de los coeficientes que lo conforman. Si cada parámetro/coeficiente tiene sentido solo en el contexto de los otros, no tiene sentido hacer un análisis a priori. Siempre buscamos la colinealidad entre predictores en el contexto del modelo.\n\n\n¿Por qué es esto así? Si el caso extremo de una correlación perfecta entre los predictores es que \\(\\beta\\) pueda estar entre \\((-\\infty, \\infty)\\) podemos inferir que, entre mayor sea la correlación entre los parámetros, mayor es la varianza en su estimación, lo cual se traduce en tamaños de efectos (valores de pendientes, vamos) pequeños y errores estándares grandes. El VIF, entonces, es una medida de qué tanto incrementa la varianza del modelo por tener un parámetro dado; es decir, vamos a tener un VIF por parámetro/coeficiente. Este VIF es un factor, por lo que es un término multiplicativo. Si los predictores son perfectamente independientes entre sí vamos a tener VIFs exactamente iguales a 1, e irán aumentando conforme cada coeficiente esté correlacionado con otro. Aquí sí tenemos una guía de qué tanto es tantito (James et al., 2013): Valores de menos de 5 son correlaciones bajas entre ese predictor y los demás, 5-10 indican correlaciones moderadas, y >10 son correlaciones muy altas e inaceptables. Regresemos a nuestro ejemplo:\n\nrlm_colin <- performance::check_collinearity(lm_form)\nplot(rlm_colin)\n\n\n\n\nVaya, estamos en una situación bastante complicada, pues solamente el coeficiente de la altura carece de una correlación importante con otro(s). Los demás están entre 20 para P_v y hasta más de 100 para P_e, lo cuál nos lleva a la pregunta ¿qué hacemos? Tenemos algunas alternativas:\n\nLa más simple y, por lo tanto, mi primera opción: simplemente eliminar una (o más) variables del análisis. ¿Cuál? No importa, la información es redundante. Puede ser por simple conveniencia, y eliminar aquella(s) de la(s) que sepamos menos, o que sea(n) más difícil(es) de interpretar o medir.\nPosiblemente la que menos me gusta: Crear una nueva variable. Esto puede ser tan simple como obtener el promedio de las variables redundantes (ojo con las escalas), o incluso aplicar una técnica de reducción de dimensionalidad como el ACP. ¿El problema? Que ahora tenemos combinaciones de nuestras variables originales, por lo que la interpretación, que es lo que estábamos buscando, se va por la borda.\nIncluir términos de interacción. Sí, como la interacción que vimos en el ANOVA factorial, pero entraremos en más detalles más adelante.\nAplicar un modelo de regresión regularizado. Estos modelos, que veremos más adelante, permiten penalizar los coeficientes en función de su “importancia” para el modelo, de modo que variables menos informativas van a tener coeficientes más pequeños (regresión Ridge) o incluso hacerse cero y ser eliminadas del modelo (regresión Lasso).\n\nComprobemos lo primero. Por fines de practicidad (y didácticos) utilizando la función lm, y quedémonos únicamente con la altura, el diámetro, y el peso:\n\nlm_nocol <- lm(Edad~ A_o + D_o + P_v, data = abulon_data)\nsummary(lm_nocol)\n\n\nCall:\nlm(formula = Edad ~ A_o + D_o + P_v, data = abulon_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.7115  -1.5972  -0.6611   0.8557  15.4888 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   1.5117     0.2814   5.372 8.34e-08 ***\nA_o          19.5388     1.8630  10.488  < 2e-16 ***\nD_o          15.8126     1.1295  14.000  < 2e-16 ***\nP_v          -4.2458     0.9490  -4.474 7.93e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.577 on 3316 degrees of freedom\nMultiple R-squared:  0.3544,    Adjusted R-squared:  0.3538 \nF-statistic: 606.7 on 3 and 3316 DF,  p-value: < 2.2e-16\n\n\nDe entrada, los errores estándar de D_o y P_v se hicieron más pequeños, y nuestro RSE (2.57) no fue mucho mayor que en el caso anterior (2.18); es decir, ahora nuestro modelo se equivoca (en promedio) 0.39 años más que antes. Esto, en el gran esquema de las cosas puede no ser “tan” grave, o al menos no tanto como podría hacerlo parecer el \\(R^2\\) múltiple de 0.35. ¿Por qué no confiar ciegamente/únicamente en el \\(R^2\\)? Porque tiene una peculiaridad: sigue creciendo conforme se le agregan variables al modelo, independientemente de si estas son informativas o no.\nSi revisamos nuestros VIF tenemos ahora valores con los que podemos vivir:\n\nrlm_nocolin <- performance::check_collinearity(lm_nocol)\nplot(rlm_nocolin)\n\n\n\n\nAhora sí podemos interpretar adecuadamente nuestro modelo, el problema es la capacidad de predicción. Si esta es “suficientemente buena” o no es algo que depende enormemente de nuestro conocimiento del problema. ¿Es un error de 2.57 años grande? ¿Es aceptable? Eso es algo que tú tienes que decidir en función de las consecuencias. Imagina que hay una regulación por edad sobre este recurso, y que organismos que tengan menos de 5 años no pueden ser extraídos. ¿Pesa ese error promedio de 2.57 años? ¿Qué pasa si el límite se va hasta los 11 años? No hay respuestas universales. Decidir si un modelo es útil o no depende de nosotros, y no solo del valor de \\(R^2\\). Dicho esto, vayamos a la siguiente alternativa: incluir interacciones en el modelo.\n\n\n5.2.1.3 Interacciones\nPero antes, ¿qué diablos representa una interacción? En el ?sec-param hablábamos que indicaban que un factor dependía de otro, y aquí es exactamente eso: una interacción representa una relación de dependencia/correlación/moderación entre los predictores. En una RLM sin interacciones un cambio unitario en \\(X_i\\) resulta en un cambio constante (\\(\\beta\\)) en \\(Y\\) al mantener fijos los valores del resto de variables. En casos con multicolinealidad vimos que esto no es el caso, y que puede ser que los cambios en en \\(X_1\\) que afectan a \\(y\\) están siendo modulados por \\(X_2\\). Un ejemplo cotidiano es la ingesta de medicamentos y el consumo de bebidas alcohólicas (no lo hagas). El medicamento puede no tener reaccciones adversas tomado solo, pero tomar bebidas alcoholicas puede hacer que su efecto se reduzca dramáticamente o, incluso, comprometer la salud del paciente.\nHasta el momento en nuestro ejemplo de los abulontes todos nuestros predictores contribuyen de forma aditiva (independiente) a la predicción de la variable dependiente. Si queremos reflejar casos como el ejemplo de los medicamentos y las bebidas alcohólicas es necesario, entonces, incluir términos que no sean aditivos. Usualmente la interacción se incluye con términos multiplicativos (en el Capítulo 6 veremos algunas otras formas de hacerlo), que en un caso con dos variables se vería de la siguiente manera:\n\\[\nY = \\alpha + \\beta_1*x_1 + \\beta_2*x_2 + \\beta_3*(x_1*x_2) + \\epsilon\n\\]\nEs decir, el nuevo coeficiente (\\(\\beta_3\\)) multiplica a una nueva variable que es el producto de dos (o más) variables que interactúan. Aunque incluir estos términos puede ayudar con la capacidad predictiva del modelo, introducen un desafío para su interpretación. Re-escribamos la ecuación de arriba:\n\nPara la pendiente de \\(x_1\\):\n\n\\[\nY = \\alpha + (\\beta_1 + \\beta_3*x2)*x1 + \\beta_2*x_2\n\\]\n\nPara la pendiente de \\(x_2\\):\n\n\\[\nY = \\alpha + \\beta_1*x_1 + (\\beta_2 + \\beta_3*x1)*x_2\n\\]\n\n\n\n\n\n\nNota\n\n\n\n¿De dónde salieron estas ecuaciones? De expandir el término \\(\\beta_3*(x_1*x_2)\\) y luego simplificar para cada variable.\n\n\nEsto nos muestra que: - El término de interacción puede ser entendido como un modelo lineal, por lo que tenemos un modelo lineal dentro de otro. - La interacción es simétrica, de modo que podemos pensar en la pendiente de \\(x_1\\) como una función de \\(x_2\\) y, al mismo tiempo, la pendiente de \\(x_2\\) como una función de \\(x_1\\)\nEstas interacciones tienen otro par de peculiaridades: - En una RLM sin interacciones obtenemos un hiperplano como el que vimos en la Figura 5.1; sin embargo, las interacciones permiten curvar ese espacio, pues las pendientes ya no son constantes, sino funciones de otra variable. - El coeficiente \\(\\beta_1\\) describe la influencia del predictor \\(x_1\\) solo cuando \\(x_2 = 0\\). Esto es porque, en ese caso, \\(\\beta_3*x_2 = 0\\), y el término de \\(x_1\\) se reduce a \\(\\beta_1*x_1\\). Por simetría, ese razonamiento aplica a \\(\\beta_2\\). ¿En Español? No podemos interpretar un efecto de primer orden (sin interacción) si esa variable está interactuando con otra, pues ese efecto en sí mismo solo es verdad cuando la otra variable es 0.\nVeamos qué pasa si incluimos un modelo con todas las interacciones (más adelante veremos cómo incluirlas en tidymodels:\n\nlm_inter <- lm(Edad~L_o*D_o*A_o*P_e*P_sc*P_v*P_c, data = abulon_data)\nsummary(lm_inter)\n\n\nCall:\nlm(formula = Edad ~ L_o * D_o * A_o * P_e * P_sc * P_v * P_c, \n    data = abulon_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.7066  -1.1922  -0.2471   0.8796  11.6235 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                   2.002e+00  4.746e+00   0.422 0.673140    \nL_o                           6.259e+00  5.006e+01   0.125 0.900518    \nD_o                           4.961e+00  7.566e+01   0.066 0.947729    \nA_o                          -3.186e+01  9.720e+01  -0.328 0.743075    \nP_e                          -1.749e+02  1.925e+02  -0.909 0.363549    \nP_sc                          2.291e+02  3.177e+02   0.721 0.470868    \nP_v                           6.777e+02  4.532e+02   1.495 0.134918    \nP_c                          -5.851e+02  3.287e+02  -1.780 0.075180 .  \nL_o:D_o                       3.656e+01  2.132e+02   0.172 0.863823    \nL_o:A_o                       3.183e+01  7.987e+02   0.040 0.968211    \nD_o:A_o                       5.674e+02  1.211e+03   0.469 0.639276    \nL_o:P_e                       2.592e+03  1.167e+03   2.222 0.026336 *  \nD_o:P_e                      -3.299e+03  1.676e+03  -1.969 0.049062 *  \nA_o:P_e                       4.118e+03  2.479e+03   1.661 0.096757 .  \nL_o:P_sc                     -9.840e+02  1.373e+03  -0.717 0.473635    \nD_o:P_sc                      1.054e+03  2.412e+03   0.437 0.662293    \nA_o:P_sc                     -4.279e+03  4.159e+03  -1.029 0.303658    \nP_e:P_sc                      7.105e+02  1.223e+03   0.581 0.561381    \nL_o:P_v                      -5.725e+03  2.343e+03  -2.443 0.014613 *  \nD_o:P_v                       3.355e+03  3.476e+03   0.965 0.334488    \nA_o:P_v                      -1.443e+04  5.663e+03  -2.548 0.010879 *  \nP_e:P_v                       2.118e+03  2.426e+03   0.873 0.382722    \nP_sc:P_v                     -3.144e+03  5.171e+03  -0.608 0.543194    \nL_o:P_c                      -2.114e+03  1.887e+03  -1.120 0.262762    \nD_o:P_c                       8.248e+03  2.488e+03   3.316 0.000925 ***\nA_o:P_c                       5.782e+03  2.368e+03   2.441 0.014684 *  \nP_e:P_c                      -5.773e+03  2.448e+03  -2.358 0.018416 *  \nP_sc:P_c                     -4.737e+02  5.639e+03  -0.084 0.933053    \nP_v:P_c                       1.683e+04  7.571e+03   2.223 0.026257 *  \nL_o:D_o:A_o                  -2.409e+03  2.265e+03  -1.064 0.287583    \nL_o:D_o:P_e                   2.481e+01  1.654e+03   0.015 0.988032    \nL_o:A_o:P_e                  -2.213e+04  8.755e+03  -2.527 0.011545 *  \nD_o:A_o:P_e                   1.819e+04  1.111e+04   1.637 0.101694    \nL_o:D_o:P_sc                 -3.465e+02  2.744e+03  -0.126 0.899513    \nL_o:A_o:P_sc                  1.121e+04  1.238e+04   0.906 0.365124    \nD_o:A_o:P_sc                 -5.928e+03  1.842e+04  -0.322 0.747664    \nL_o:P_e:P_sc                 -3.411e+03  2.320e+03  -1.470 0.141631    \nD_o:P_e:P_sc                  3.812e+03  3.480e+03   1.096 0.273371    \nA_o:P_e:P_sc                 -1.707e+03  8.897e+03  -0.192 0.847848    \nL_o:D_o:P_v                   6.229e+03  4.714e+03   1.321 0.186515    \nL_o:A_o:P_v                   5.777e+04  1.899e+04   3.043 0.002362 ** \nD_o:A_o:P_v                   1.674e+03  2.660e+04   0.063 0.949829    \nL_o:P_e:P_v                  -7.235e+03  6.306e+03  -1.147 0.251330    \nD_o:P_e:P_v                  -8.467e+02  6.236e+03  -0.136 0.891996    \nA_o:P_e:P_v                  -8.950e+03  1.625e+04  -0.551 0.581831    \nL_o:P_sc:P_v                  1.359e+04  1.265e+04   1.074 0.282801    \nD_o:P_sc:P_v                 -8.907e+03  1.474e+04  -0.604 0.545736    \nA_o:P_sc:P_v                  6.483e+03  3.954e+04   0.164 0.869786    \nP_e:P_sc:P_v                 -4.559e+03  3.824e+03  -1.192 0.233322    \nL_o:D_o:P_c                  -7.834e+03  3.172e+03  -2.470 0.013566 *  \nL_o:A_o:P_c                   1.383e+04  1.279e+04   1.081 0.279776    \nD_o:A_o:P_c                  -6.052e+04  1.628e+04  -3.717 0.000205 ***\nL_o:P_e:P_c                   1.186e+04  4.436e+03   2.673 0.007561 ** \nD_o:P_e:P_c                   1.310e+04  6.151e+03   2.130 0.033243 *  \nA_o:P_e:P_c                   1.963e+04  1.428e+04   1.375 0.169352    \nL_o:P_sc:P_c                 -3.123e+03  1.062e+04  -0.294 0.768758    \nD_o:P_sc:P_c                  4.724e+02  1.782e+04   0.027 0.978853    \nA_o:P_sc:P_c                  1.404e+04  3.529e+04   0.398 0.690787    \nP_e:P_sc:P_c                 -9.506e+03  5.105e+03  -1.862 0.062672 .  \nL_o:P_v:P_c                  -1.970e+04  1.775e+04  -1.110 0.267102    \nD_o:P_v:P_c                  -6.260e+04  2.193e+04  -2.854 0.004342 ** \nA_o:P_v:P_c                  -9.779e+04  4.492e+04  -2.177 0.029548 *  \nP_e:P_v:P_c                   8.728e+03  1.055e+04   0.827 0.408347    \nP_sc:P_v:P_c                  5.037e+04  3.579e+04   1.407 0.159463    \nL_o:D_o:A_o:P_e               6.729e+03  1.211e+04   0.556 0.578427    \nL_o:D_o:A_o:P_sc              7.377e+02  2.364e+04   0.031 0.975108    \nL_o:D_o:P_e:P_sc             -5.170e+02  3.764e+03  -0.137 0.890738    \nL_o:A_o:P_e:P_sc              1.599e+04  1.497e+04   1.068 0.285801    \nD_o:A_o:P_e:P_sc             -2.611e+04  2.197e+04  -1.188 0.234740    \nL_o:D_o:A_o:P_v              -8.063e+04  3.351e+04  -2.406 0.016167 *  \nL_o:D_o:P_e:P_v               8.693e+03  7.877e+03   1.104 0.269852    \nL_o:A_o:P_e:P_v               3.979e+04  3.652e+04   1.090 0.275981    \nD_o:A_o:P_e:P_v              -2.009e+04  3.522e+04  -0.570 0.568469    \nL_o:D_o:P_sc:P_v             -6.862e+03  1.847e+04  -0.371 0.710304    \nL_o:A_o:P_sc:P_v             -6.635e+04  8.184e+04  -0.811 0.417548    \nD_o:A_o:P_sc:P_v              7.556e+04  9.739e+04   0.776 0.437867    \nL_o:P_e:P_sc:P_v              8.697e+03  6.391e+03   1.361 0.173682    \nD_o:P_e:P_sc:P_v              7.938e+03  9.316e+03   0.852 0.394202    \nA_o:P_e:P_sc:P_v              2.758e+04  2.407e+04   1.146 0.251952    \nL_o:D_o:A_o:P_c               5.957e+04  1.933e+04   3.082 0.002075 ** \nL_o:D_o:P_e:P_c              -2.399e+04  7.611e+03  -3.151 0.001640 ** \nL_o:A_o:P_e:P_c              -4.321e+04  2.509e+04  -1.722 0.085079 .  \nD_o:A_o:P_e:P_c              -5.657e+04  3.241e+04  -1.745 0.081025 .  \nL_o:D_o:P_sc:P_c              9.188e+03  1.945e+04   0.472 0.636745    \nL_o:A_o:P_sc:P_c             -8.657e+03  6.332e+04  -0.137 0.891261    \nD_o:A_o:P_sc:P_c             -4.986e+03  1.035e+05  -0.048 0.961585    \nL_o:P_e:P_sc:P_c              1.407e+04  6.495e+03   2.166 0.030357 *  \nD_o:P_e:P_sc:P_c              4.604e+03  9.346e+03   0.493 0.622280    \nA_o:P_e:P_sc:P_c              5.181e+04  2.738e+04   1.892 0.058520 .  \nL_o:D_o:P_v:P_c               8.002e+04  2.671e+04   2.996 0.002758 ** \nL_o:A_o:P_v:P_c               6.861e+04  9.871e+04   0.695 0.487053    \nD_o:A_o:P_v:P_c               3.788e+05  1.200e+05   3.158 0.001603 ** \nL_o:P_e:P_v:P_c              -2.347e+04  1.636e+04  -1.434 0.151612    \nD_o:P_e:P_v:P_c               1.123e+03  2.139e+04   0.052 0.958138    \nA_o:P_e:P_v:P_c              -6.974e+03  5.261e+04  -0.133 0.894539    \nL_o:P_sc:P_v:P_c             -5.108e+04  5.640e+04  -0.906 0.365200    \nD_o:P_sc:P_v:P_c             -8.373e+04  8.418e+04  -0.995 0.319948    \nA_o:P_sc:P_v:P_c             -2.833e+05  1.995e+05  -1.421 0.155551    \nP_e:P_sc:P_v:P_c             -5.070e+03  8.550e+03  -0.593 0.553275    \nL_o:D_o:A_o:P_e:P_sc          8.803e+03  2.362e+04   0.373 0.709337    \nL_o:D_o:A_o:P_e:P_v          -2.117e+04  4.495e+04  -0.471 0.637793    \nL_o:D_o:A_o:P_sc:P_v          1.069e+04  1.193e+05   0.090 0.928610    \nL_o:D_o:P_e:P_sc:P_v         -1.512e+04  9.927e+03  -1.524 0.127719    \nL_o:A_o:P_e:P_sc:P_v         -5.207e+04  3.556e+04  -1.464 0.143291    \nD_o:A_o:P_e:P_sc:P_v         -3.552e+04  5.282e+04  -0.672 0.501334    \nL_o:D_o:A_o:P_e:P_c           9.841e+04  4.048e+04   2.431 0.015104 *  \nL_o:D_o:A_o:P_sc:P_c         -3.662e+04  1.119e+05  -0.327 0.743482    \nL_o:D_o:P_e:P_sc:P_c         -9.837e+03  1.029e+04  -0.956 0.339257    \nL_o:A_o:P_e:P_sc:P_c         -7.408e+04  3.453e+04  -2.145 0.032017 *  \nD_o:A_o:P_e:P_sc:P_c         -2.627e+04  4.702e+04  -0.559 0.576452    \nL_o:D_o:A_o:P_v:P_c          -4.137e+05  1.390e+05  -2.976 0.002938 ** \nL_o:D_o:P_e:P_v:P_c           1.777e+04  2.521e+04   0.705 0.480953    \nL_o:A_o:P_e:P_v:P_c           7.169e+04  8.083e+04   0.887 0.375226    \nD_o:A_o:P_e:P_v:P_c          -5.016e+04  1.050e+05  -0.478 0.632954    \nL_o:D_o:P_sc:P_v:P_c          8.043e+04  9.540e+04   0.843 0.399242    \nL_o:A_o:P_sc:P_v:P_c          3.311e+05  3.024e+05   1.095 0.273569    \nD_o:A_o:P_sc:P_v:P_c          4.302e+05  4.464e+05   0.964 0.335307    \nL_o:P_e:P_sc:P_v:P_c          6.111e+03  1.134e+04   0.539 0.590088    \nD_o:P_e:P_sc:P_v:P_c          1.318e+04  1.496e+04   0.881 0.378434    \nA_o:P_e:P_sc:P_v:P_c         -5.125e+03  3.930e+04  -0.130 0.896237    \nL_o:D_o:A_o:P_e:P_sc:P_v      7.409e+04  5.592e+04   1.325 0.185284    \nL_o:D_o:A_o:P_e:P_sc:P_c      5.409e+04  5.251e+04   1.030 0.303035    \nL_o:D_o:A_o:P_e:P_v:P_c      -3.105e+04  1.220e+05  -0.255 0.799030    \nL_o:D_o:A_o:P_sc:P_v:P_c     -4.621e+05  5.015e+05  -0.922 0.356847    \nL_o:D_o:P_e:P_sc:P_v:P_c     -1.408e+04  1.584e+04  -0.889 0.373862    \nL_o:A_o:P_e:P_sc:P_v:P_c      2.837e+03  5.194e+04   0.055 0.956450    \nD_o:A_o:P_e:P_sc:P_v:P_c     -2.743e+04  6.648e+04  -0.413 0.679946    \nL_o:D_o:A_o:P_e:P_sc:P_v:P_c  2.591e+04  7.167e+04   0.362 0.717700    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.013 on 3192 degrees of freedom\nMultiple R-squared:  0.6209,    Adjusted R-squared:  0.6059 \nF-statistic: 41.17 on 127 and 3192 DF,  p-value: < 2.2e-16\n\n\n¡Tenemos un montón de parámetros! No solo eso, sino que coeficientes que antes eran significativos ya no lo son. Pensemos por un momento en lo que hicimos. Si estos términos de interacción corresponden con los productos de las variables involucradas son, entonces, altamente colineales con las variables originales. ¿Importa esto para la predicción? Para nada. Si pones atención, tanto el \\(R^2\\) como el RSE son mejores que en el modelo de efectos de primer orden (principales).\n\n\n\n\n\n\nTip\n\n\n\nTe recomiendo incluir en tu modelo solo interacciones relevantes y no interpretar los efectos de primer orden de las que sean significativas. Si la interacción es significativamente diferente de 0 no podemos interpretar los coeficientes “individuales”. Las razones teóricas de esto las vimos arriba.\n\n\nEsto último me lleva a hablar de algo sumamente importante para cualquier modelo de aprendizaje automatizado: la selección de variables:\n\n\n\n\n\n\nImportante\n\n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nNo porque “podamos” incluir tantas variables querramos (y sus interacciones) quiere decir que debamos hacerlo. Es importante que en nuestros modelos incluyamos variables e interacciones que tengan una justificación teórica y evitar ir a “pescar” efectos significativos; es decir, “a ver qué sale significativo”.\n\n\n\n\nEn nuestro ejemplo tenemos P_e, P_sc, P_v, P_c, que todas indican lo mismo: la masa del individuo, solo que vista de distintas materas. Con la longitud y el diámetro tenemos una historia similar, y tenemos además la correlación inherente entre todas por que representan la talla de un organismo: un abulón más largo seguramente tendrá valores más altos en casi todas las demás variables.\n\n\n\n\n\n\nImportante\n\n\n\nMenos usualmente es más en modelos multivariados (como en muchas otras cosas); es decir, buscamos modelos parsimoniosos.\n\n\nEn el Capítulo 6 vamos a hablar más a profundidad de cómo encontrar el modelo más parsimonioso de entre un conjunto de posibilidades, pero antes de eso veamos cómo podemos penalizar nuestros coeficientes según qué tan importantes sean.\n\n\n\n5.2.2 Regresiones penalizadas\nArriba vimos un modelo lineal que tiene todo lo que no debería de tener, especialmente en el departamento de variables correlacionadas y no informativas. Imaginemos que nuestra teoría justifica que las incluyamos todas, ¿cómo podemos asignarle menos peso a las que no sean tan relevantes? Penalizando nuestros modelos. ¿Cómo hacemos esto? Ajustamos nuestro modelo y luego penalizamos los coeficientes que sean pequeños utilizando un término que puedes encontrar como \\(\\alpha\\) o \\(\\lambda\\), cuya función es minimizar (para variar):\n\nLa suma de cuadrados de nuestras pendientes. Esto hace que los coeficientes se vuelvan más cercanos hacia 0 y hacia los demás. A este tipo de regresión se le conoce como Regresión Ridge, y a su penalización como penalización L2 (L de loss; es decir, pérdida y el 2 hace referencia al cuadrado de la suma). Esta regresión podemos aplicarla cuando tengamos muchos efectos pequeños o medianos, o cuando debamos de forzar la inclusión de todos los términos. Matemáticamente: \\[\nRSS_{Ridge}(\\beta_j, \\beta_0) = \\sum_{i=1}^n(y_i-(\\beta_j*x_i+\\beta_0))^2+\\alpha*\\sum_{j=1}^p\\beta_j^2    \n\\]\nLa suma de absolutos de las pendientes, dando lugar a una penalización L1 y una regresión Lasso. El resultado es que los coeficientes pequeños se hagan 0 y, por lo tanto, sean retirados del modelo. Esta la podemos aplicar cuando tengamos solo unos pocos efectos medianos/grandes, o cuando querramos hacer una selección rigurosa de las variables. Matemáticamente:\n\n\\[\nRSS_{Lasso}(\\beta_j, \\beta_0) = \\sum_{i=1}^n(y_i-(\\beta_j*x_i+\\beta_0))^2+\\alpha*\\sum_{j=1}^p|\\beta_j|\n\\]\n\n\n\n\n\n\nAdvertencia\n\n\n\nEs extremadamente importante que, si vamos a aplicar alguno de estos modelos, escalemos los datos adecuadamente. De no hacerlo, la diferencia en escalas hará que cambien las escalas de los coeficientes y que la regularización no se aplique homogéneamente.\n\n\nEvidentemente, aquí añadimos un hiper-parámetro a nuestro modelo de RLM, el cual hay que optimizar. En R esto está implementado en la librería glmnet, pero podemos seguir utilizando tidymodels para homogeneizar los procesos, pero antes de eso hablemos de cómo podemos escalar nuestras variables y qué efectos tienen esas transformaciones sobre la interpretación de nuestros coeficientes.\n\n5.2.2.1 Re-visitando las transformaciones\nEsta necesidad de escalar los datos me lleva a re-visitar las transformaciones y cómo impactan nuestra interpretación. Me voy a limitar a las dos más comunes en escenarios de RLM: el centrado y la estandarización.\nYa sabemos que centrar una variable consiste en quitarle a cada valor \\(x_i\\) la media de la variable \\(\\overline{x}\\): \\(x' = x- \\overline{x}\\). Como resultado \\(x'\\) estará centrada en 0. Esto tiene dos ventajas, una con nuestras variables directamente y la otra con nuestro intercepto y las pendientes. Si nuestras variables están centradas en 0, entonces todas están en la misma escala, y podemos aplicar modelos penalizados sin preocuparnos por nada. La otra es que en todo modelo lineal hay una correlación implícita entre el intercepto y la pendiente. ¿Por qué? Porque no importa qué línea ajustemos a nuestros datos, todos deben de pasar por un punto: la media de \\(x\\) y la media de \\(y\\); es decir, nuestro ajuste se reduce a girar un lápiz (una línea) alrededor de ese punto, cual rehilete. Si incrementamos la pendiente, debemos forzosamente reducir el intercepto. Si centramos \\(x\\) entonces el intercepto siempre será el valor de \\(y_i\\) que corresponde a la media de \\(x\\), que el modelo ve como 0; es decir, ahora nuestra pendiente puede cambiar libremente sin que se modifique el intercepto. ¿Y si necesitamos reportar los resultados en la escala original? Podemos hacer la siguiente corrección:\n\\[\n\\alpha = \\alpha' - \\beta'x\n\\]\nLa cual se deriva del siguiente razonamiento algebráico:\n\\[\\begin{align*}\ny = \\alpha' + \\beta' x' + \\epsilon \\\\\ny = \\alpha' + \\beta' (x - \\overline{x}) + \\epsilon \\\\\ny = \\alpha' - \\beta' \\overline{x} + \\beta'x + \\epsilon\n\\end{align*}\\]\nEsto también implica que \\(\\beta = \\beta'\\), lo cual tiene todo el sentido del mundo, solo estamos desplazando la distribución de \\(x\\) a tener media 0, no estamos modificando su escala de ninguna manera.\nEn la estandarización la historia es un poco diferente, pues aquí sí que cambiamos la escala tanto de \\(x\\) como de \\(y\\), que ahora tendrán valores \\(Z\\) que representan a cuántas desviaciones estándar está cada valor de la media. La ventaja es que el intercepto siempre está alrededor de 0, y las pendientes en el intervalo \\([-1, 1]\\). La interpretación de la pendiente aquí sí cambia. Si tienes una pendiente de 0.9 en escala \\(Z\\), autamáticamente sé que me voy a mover 0.9 desviaciones estándares en \\(y\\), independientemente de su media o de su desviación estándar. Como ves, la interpretación no es tan simple como en el caso anterior, así que si es importante interpretar las pendientes vamos a preferir el centrado.\nAhora sí, podemos ir a aplicar los modelos de regresión penalizados.\n\n\n5.2.2.2 Regresión Ridge\nEmpecemos con la regresión Ridge (penalización L2). A diferencia del modelo sin penalizar, esta vez sí necesitamos cerciorarnos de que nuestros predictores estén en la misma escala, por lo que hay que añadir un paso de centrado a nuestra receta:\n\nDividir datos en entrenamiento-prueba. No es necesario porque ya tenemos nuestros datos de prueba, pero sí es necesario que establezcamos las divisiones de validación cruzada:\n\n\nabulon_cv <- vfold_cv(abulon_data)\n\n\nPreprocesar los datos\n\n\n# Receta para centrar los predictores\nabulon_rec <- recipe(Edad ~., abulon_data) |>\n              step_center(all_predictors())\n\n# Obtener parámetros para preprocesar\nabulon_prep <- abulon_rec |> prep() \n\n# Preprocesar los datos\nabulon_juiced <- abulon_prep |> juice()\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nEstamos utilizando la función all_predictors() dentro de step_center() para centrar todos los predictores. Si tenemos variables que están codificadas, por ejemplo el sexo como 0 y 1, es importante que las excluyamos pasando los nombres a step_center, por ejemplo: step_center(all_predictors(), -Sexo). Con esto evitamos que nuestra codificación se mueva y que obtengamos resultados erróneos. Más adelante hablaremos de cómo incluir estas variables categóricas en el modelo y cómo interpretar los resultados.\n\n\n\nEspecificar el modelo. Seguimos tratando con una regresión lineal, por lo que también vamos a utilizar linear_reg(); sin embargo, aquí tenemos dos hiper-parámetros: penalty, que es el término de penalización y mixture que define la penalización a aplicar. En nuestro caso estamos interesados en una regresión Ridge, por lo que mixture = 0. Otra diferencia es que el “motor” de la regresión va a cambiar a \"glmnet\":\n\n\nridge_spec <- linear_reg(penalty = tune(),\n                         mixture = 0) |>\n              set_engine(\"glmnet\")\n\n\nFormar el flujo de trabajo. Nuevamente, formamos un flujo donde la receta de preprocesamiento se le pasa al modelo que especificamos:\n\n\ntune_wf <- workflow() |>\n           add_recipe(abulon_rec) |>\n           add_model(ridge_spec)\n\n\nAjustar los hiper-parámetros: Tal y como en el Capítulo 4, establecemos el procesamiento en paralelo y entrenamos un primer modelo con 20 puntos aleatorios para “guiar” nuestra optimización:\n\n\nset.seed(123)\n# Asignar 8 núcleos para la computación en paralelo\n# Revisa cuántos puedes disponer tú (más o menos)\ndoParallel::registerDoParallel(cores = 8)\n\n# Buscar los mejores hiperparámetros\ntune_ridge <- tune_grid(tune_wf,\n                        resamples = abulon_cv,\n                        grid = 50)\n\n\nEvaluar el primer ajuste. Por practicidad hagamos una función que reciba nuestro objeto tune_res y el orden de la validación cruzada, y que nos devuelva un gráfico donde veamos la relación del RMSE y el \\(R^2\\) con la penalizacón. Al ver el gráfico de abajo podemos concluir que valores de penalización muy pequeños son los que a) minimizan el RMSE y b) maximizan el \\(R^2\\), y que tampoco hay mucha diferencia con el ajuste sin penalización. Esto quiere decir que este problema no se beneficia mucho de una regresión ridge; de hecho, entre más queremos penalizar la pendiente “peor” es el ajuste del modelo:\n\n\nplot_optim_res <- function(tune_res, cv_order){\n  tune_res |> collect_metrics() |>\n              select(mean, std_err, penalty, .metric) |> \n              ggplot(aes(penalty, mean, color = .metric)) +\n              geom_line(size = 1.5) +\n              geom_errorbar(aes(ymin = mean - std_err,\n                                ymax = mean + std_err),\n                            alpha = 0.5) +\n              scale_x_log10() +\n              facet_wrap(~.metric,\n                         scales = \"free_y\",\n                         nrow = 2) +\n              labs(x = \"Penalización\",\n                   y = element_blank(),\n                   title = \"Medias y errores estándar de validación cruzada\",\n                   subtitle = paste0(\"Orden: \", cv_order)) +\n              theme_bw() +\n              theme(legend.position = \"none\")\n}\n\nplot_optim_res(tune_ridge, cv_order = 10)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\nNota\n\n\n\nSi pones atención a la función plot_optim_res notarás que estamos añadiendo la capa scale_x_log10(). Esto es porque el valor de penalización que arroja tune_grid debe transformarse a escala logarítmica por el funcionamiento interno de tune_grid y cómo arroja los resultados.\n\n\n\nFinalizar el ajuste: No hubo mucho efecto de la penalización, por lo que podemos directamente seleccionar el mejor modelo:\n\n\nbest_rmse <- select_best(tune_ridge, \"rmse\")\nfinal_ridge <- finalize_workflow(tune_wf,\n                                 best_rmse)\n\n\nRevisar la importancia de variables. Ya sabemos que este modelo lineal no es precisamente bueno, pero igual revisemos la importancia de variables. Vu\n\n\nfinal_fit <- final_ridge |>\n             fit(abulon_data)\n\nfinal_res <- final_fit |> extract_fit_parsnip()\n\nfinal_res |> vip(geom = \"point\") +\n             geom_point(color = \"deepskyblue3\") +\n             theme_bw() +\n             labs(title = \"Importancia de variables\",\n                  y = element_blank())\n\n\n\n\n\nRevisar la capacidad de generalización: Al igual que en la RLM no penalizada/regularizada, obtengamos las medidas de bondad de ajuste con los datos de prueba. En este caso no fueron muy diferentes de la RLM no penalizada, lo cual tiene todo el sentido del mundo: no tenemos una regularización muy fuerte.\n\n\n# Predecir edades en los datos\nabulon_test[\"pred_ridge\"] <- predict(final_res, abulon_test)\n\n# Calcular rmse y r2 y juntar los resultados\nrbind(\nabulon_test |> yardstick::rmse(truth = Edad,\n                               estimate = pred),\nabulon_test |> yardstick::rsq(truth = Edad,\n                               estimate = pred)\n)\n\n\n\n  \n\n\n\n\nPor curiosidad, ¿cómo quedaron nuestros coeficientes?\n\n\nfinal_res |> tidy()\n\n\n\n  \n\n\n\n\n\n5.2.2.3 Regresión Lasso\nVeamos ahora qué pasa con la regresión Lasso. Los pasos son los mismos, por lo que resumiré el código en bloques que lleguen hasta salidas de interés. Vamos a trabajar con los mismos datos, entonces podemos reutilizar tanto nuestras divisiones de validación cruzada como la receta para preprocesar los datos. La especificación del modelo es sumamente sencilla, solamente hay que cambiar el argumento mixture = 1 en linear_reg:\n\nset.seed(123)\n# Especificación del modelo Lasso\n# Optimizar el término de penalización\nlasso_spec <- linear_reg(penalty = tune(),\n                         mixture = 1) |> \n              set_engine(\"glmnet\")\n\n# Flujo de trabajo para el modelo Lasso\ntune_wf <- workflow() |>\n           add_recipe(abulon_rec) |>\n           add_model(lasso_spec)\n\n# Ajuste de hiperparámetros\ndoParallel::registerDoParallel(cores = 8)\ntune_lasso <- tune_grid(tune_wf,\n                        resamples = abulon_cv,\n                        grid = 50)\n\n# Resultados de validación cruzada\nplot_optim_res(tune_lasso, cv_order = 10)\n\n\n\n\nCuriosamente los resultados de la validación cruzada son sumamente similares a los de la regresión Ridge, en el sentido de que las penalizaciones que maximizan la bondad de ajuste son bastante pequeñas. ¿Qué sucede? Que este problema es demasiado complejo para un modelo lineal. Si vuelves a los gráficos diagnósticos de la regresión, especialmente al de residuales/linealidad, es más que obvio que el supuesto que tiene el mismo nombre que el modelo no se cumple, entonces buscar mejorar ese modelo solo quitando o metiendo variables es un sinsentido. Hagamos algo más productivo y utilicemos el modelo con todas las interacciones. Aquí sí que hay que regresar a modificar la receta original o, mejor dicho, añadirle un paso adicional con step_interact(terms = ~RHS), donde RHS es el lado derecho de la fórmula que incluye los términos de interaccón:\n\n\n\n\n\n\nAdvertencia\n\n\n\nNota el uso del operador ~ en el argumento terms.\n\n\n\n# Receta para centrar los predictores\nabulon_inter_rec <- abulon_rec |>\n                    step_interact(terms = ~L_o*D_o*A_o*P_e*P_sc*P_v*P_c)\n\n# Obtener parámetros para preprocesar\nabulon_prep <- abulon_inter_rec |> prep() \n\n# Preprocesar los datos\nabulon_juiced <- abulon_prep |> juice()\n\nAhora sí, ajustemos el flujo de trabajo para incluir esta receta, y optimicemos el modelo Lasso:\n\nset.seed(123)\n\n# Flujo de trabajo para el modelo Lasso\ntune_wf <- workflow() |>\n           add_recipe(abulon_inter_rec) |>\n           add_model(lasso_spec)\n\n# Ajuste de hiperparámetros\ndoParallel::registerDoParallel(cores = 8)\ntune_lasso <- tune_grid(tune_wf,\n                        resamples = abulon_cv,\n                        grid = 50)\n\n# Resultados de validación cruzada\nplot_optim_res(tune_lasso, cv_order = 10)\n\n\n\n\nComo ya habíamos mencionado, un modelo lineal no es suficiente para describir adecuadamente estos datos, incluso en un modelo Lasso con interacciones. ¿Cómo sabemos que funcionó como debía? Podemos buscar los coeficientes que son EXACTAMENTE 0. Por practicidad solo veamos el número de casos en los que se cumple esta igualdad, que fue de 62 de 128 coeficientes del modelo con todas las interacciones sin penalización.\n\nbest_rmse <- select_best(tune_lasso, \"rmse\")\nfinal_lasso <- finalize_workflow(tune_wf,\n                                 best_rmse)\n\nfinal_fit <- final_lasso |>\n             fit(abulon_data)\n\nfit_res <- final_fit |> extract_fit_parsnip()\n\nfit_res |> tidy() |> filter(estimate == 0) |> select(term)\n\n\n\n  \n\n\n\n\n\n5.2.2.4 Consideraciones\n\nEsta propiedad de que algunos términos se hagan exactamente 0 y, por lo tanto, sean eliminados del modelo es lo que hace que la regresión Lasso sea utilizada como un método de selección de variables, mientras que la regresión Ridge es solo una forma de controlar el grado de influencia de todas las variables en el modelo. En este sentido, la regresión Lasso brilla en problemas altamente dimensionales como nuestro modelo con interacciones, mientras que la regresión Ridge tiene más sentido cuando tenemos problemas con menos variables.\nHabrás notado que en estas regresiones penalizadas no hemos hablado de intervalos de confianza o de valores de p. Esto no es porque me de flojera hablar de ellos, no. En realidad, aunque podemos calcular los errores estándares para los coeficientes de manera muy sencilla con Bootstrap, por ejemplo, estos no son muy informativos para estimaciones altamente sesgadas como las que resultan de la implementación de estos modelos penalizados. ¿Por qué? Porque da la casualdidad de que la estimación penalizada reduce la varianza de los estimadores introduciendo un sesgo sustancial, de modo que este se vuelve una parte muy importante de su error cuadrático medio, mientras qeu la varianza puede contribuir solo en una pequeña parte. ¿La qué cosa de quién? En pocas palabras, el error estándar de cada coeficiente ya no representa solo qué tan seguros estamos de la estimación, sino también incluye el cómo fue penalizado; por lo tanto, no podemos confiar ciegamente en lo que nos dicen.\n¿Esto quiere decir que no debo de aplicar estos modelos de regresión? En absoluto, simplemente sé consciente de que no puedes reportar valores de p, que a algunos revisores eso puede no agradarles, y que entonces la recomendación es utilizar una regresión Ridge solo cuando estemos interesados en temas de predicción, y la regresión Lasso solo como un paso previo para seleccionar qué variables eliminar del análisis cuando tengamos demasiadas variables jugando al mismo tiempo.\n\n\n\n\n5.2.3 Predictores categóricos\nLo sé, lo sé, ya fue mucho en esta sesión, pero esto es muy importante. Usualmente nos enseñan que en una regresión lineal múltiple utilizamos predictores numéricos o, cuando menos, ordinales. Y esto es verdad, pero no quiere decir que no podamos hacerle manita de puerco al modelo lineal para incluir predictores categóricos o, tal vez mejor dicho, engañarlo. El proceso es sumamente sencillo, lo único que tenemos que hacer es codificar nuestras variables categóricas en 0 y 1. Nuestros datos de abulones no tienen esta información, así que retomemos los datos de pingüinos de Palmer:\n\nstr(penguins)\n\ntibble [344 × 7] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n\n\nAquí tenemos dos variables que son categóricas con tres niveles (species, island), y una binaria (sex).\n\n5.2.3.1 Predictores binarios\nImaginemos que queremos construir un modelo de regresión lineal simple, donde queremos predecir la masa corporal a partir del sexo. Por el momento olvida la confusión que puede estarte dando esa idea, y mejor escribamos el modelo:\n\\[\nmasa = \\alpha + \\beta*Sexo + \\epsilon\n\\]\nPero el sexo es “hembra” o “macho”. ¿Podemos multiplicar palabras por números? No, pero podemos vernos inteligentes y asignar ARBITRARIAMENTE valores a cada nivel; es decir, no importa a quién le toque qué numero, siempre y cuando sepamos la correspondencia:\n\nmacho: 1\nhembra: 0\n\n¿Qué ganamos con esto? Hagamos rápidamente el ejercicio de regresión y veamos los resultados:\n\nlm_m_sex <- lm(body_mass_g~sex, data = penguins)\nsummary(lm_m_sex)\n\n\nCall:\nlm(formula = body_mass_g ~ sex, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1295.7  -595.7  -237.3   737.7  1754.3 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3862.27      56.83  67.963  < 2e-16 ***\nsexmale       683.41      80.01   8.542  4.9e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 730 on 331 degrees of freedom\n  (11 observations deleted due to missingness)\nMultiple R-squared:  0.1806,    Adjusted R-squared:  0.1781 \nF-statistic: 72.96 on 1 and 331 DF,  p-value: 4.897e-16\n\n\nTenemos nuestro intercepto, que sabemos que es la ordenada al origen, y tenemos también una pendiente, que ahora no es solo sex, sino sexmale. ¿Te das alguna idea de qué representan en términos de nuestros datos? Regresa a cómo es que codificamos las variables, y piensa en por qué la pendiente está como sexmale. Tal vez un gráfico de dispersión te de una pista:\n\nggplot(data = na.omit(penguins), aes(x = sex, y = body_mass_g)) +\n  geom_point(color = \"dodgerblue4\", alpha = 0.5) +\n  theme_bw()\n\n\n\n\nSi imaginas una línea que conecte el centro de ambas columnas de puntos salta que la pendiente representa la diferencia en la variable dependiente entre ambas categorías. ¿Recuerdas que dije que era arbitrario el cómo asignamos la codificación? En este caso R la realiza de forma alfabética: female va antes que male, por lo tanto son 0 y 1. ¿Y el intercepto? Este no es tan evidente, pero representa la media del grupo 0. ¿Por qué representan eso? Recuerda las definiciones que dimos en el ?sec-rls: - La pendiente es la tasa de cambio de \\(x\\) a \\(y\\); es decir, cuántas unidades nos movemos en el eje \\(y\\) por cambio unitario en \\(x\\). En nuestros pingüinos nos movemos, en promedio, 683 g al “pasar” de hembras a machos. - El intercepto es el punto donde la recta corta al eje \\(y\\); es decir, el punto donde \\(x\\) = 0, por lo que si uno de nuestros grupos es 0, el intercepto representa su promedio. En nuestros pingüinos, entonces, la masa promedio de las hembras es de 3862 g.\n¿No me crees? Comprobemos ambas cosas. Primero calculemos la masa promedio de las hembras:\n\nfemale_mass <- mean(penguins$body_mass_g[penguins$sex == \"female\"],\n                    na.rm = T)\nfemale_mass\n\n[1] 3862.273\n\n\nAhora la masa promedio de los machos, y obtengamos la diferencia. La pendiente es positiva, por lo que a la masa de los machos le restaremos la masa de las hembras:\n\nmale_mass <- mean(penguins$body_mass_g[penguins$sex == \"male\"],\n                  na.rm = T)\nmale_mass - female_mass\n\n[1] 683.4118\n\n\n¡MAGIA NEGRA! En absoluto, solo es una consecuencia de cómo introdujimos la variable sex al modelo.\nAlgo importante a notar es que la pendiente es independiente de nuestros grupos, en el sentido de que no importa si les asignamos 0 y 1, 15 y 16, o 100 y 101, siempre que la diferencia entre ellos sea de una unidad. El intercepto, por el contrario, sí que depende del valor de nuestros grupos, pues la recta debe de estirarse para cortar al eje \\(y\\). Es por esto el hincapié en codificar con 0 y 1. Este proceso se denomina en el área de ingenieria de variables como codificación de variables, particularmente como crear dummy variables. Ahora que ya tienes la prueba empírica déjame ser feliz con una demostración matemática muy sencilla, sustituyendo los valores de nuestros grupos en nuestro modelo lineal:\n\\[\\begin{align*}\nmasa = \\alpha + \\beta * sexo + \\epsilon \\\\\n\\therefore \\\\\nmasa = \\left\\{\n  \\begin{array}{lr}\n    \\text{Si } sexo = 0: \\alpha + \\beta*0 + \\epsilon \\Rightarrow Y_0 = \\alpha + \\epsilon \\\\\n    \\text{Si } sexo = 1: \\alpha + \\beta*1 + \\epsilon \\Rightarrow Y_1 = \\alpha + \\beta + \\epsilon\n  \\end{array}\n\\right\\}\n\\end{align*}\\]\nLuego restamos ambas ecuaciones:\n\\[\\begin{align*}\n\\Delta Y = Y_1 - Y_0 = \\alpha - \\alpha + \\beta + \\epsilon - \\epsilon \\\\\n\\therefore \\\\\n\\Delta Y = \\beta\n\\end{align*}\\]\nAhora realiza una prueba \\(t\\) para comparar las medias de masas de machos y hembras, ¿son consistentes los resultados? Bueno, ahora tienes tu respuesta de por qué te decía que la prueba \\(t\\), ANOVA y demás son solo aplicaciones del modelo lineal.\n¿Qué pasa con variables con más de una categoría? Aaaaaah, no desesperres, Pérrez (¿entiendes la referencia?), para allá vamos.\n\n\n5.2.3.2 Predictores con más de dos categorías\nSiguiendo la lógica anterior pensaríamos en un modelo como el siguiente:\n\\[\nmasa = \\alpha + \\beta*Especie + \\epsilon\n\\]\nLa variable “Especie” es categórica nominal con tres niveles. Nuevamente, siguiendo la lógica anterior podemos codificarla de la siguiente manera, ¿no?\n\nAdelie\nChinstrap\nGentoo\n\n¡NO! Si hacemos eso estamos indicando que la variable Isla es categórica ordinal, de modo que Adelie < Chinstrap < Gentoo. Aunque en el caso binario en cierta forma hicimos lo mismo, no importa, pues ya vimos que solamente estamos describiendo la diferencia entre ambos niveles y en realidad se mantiene que ambas categorías tienen la misma importancia (por ello es una variable nominal). ¿Cómo lo hacemos entonces cuando tenemos más de dos predictores? Al igual que en el caso anterior ajustemos el modelo lineal:\n\nlm_m_sp <- lm(body_mass_g~species, data = penguins)\nsummary(lm_m_sp)\n\n\nCall:\nlm(formula = body_mass_g ~ species, data = penguins)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1126.02  -333.09   -33.09   316.91  1223.98 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       3700.66      37.62   98.37   <2e-16 ***\nspeciesChinstrap    32.43      67.51    0.48    0.631    \nspeciesGentoo     1375.35      56.15   24.50   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 462.3 on 339 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.6697,    Adjusted R-squared:  0.6677 \nF-statistic: 343.6 on 2 and 339 DF,  p-value: < 2.2e-16\n\n\nComo era de esperarse, ahora no tenemos solo una pendiente, sino dos. Espera, ¿dos? ¿Qué no son tres especies? Pues resulta que sí formamos tres nuevas variables, una para cada especie, tal que:\n\nspeciesAdelie: 1 si es Adelie, 0 en caso contrario.\nspeciesChinstrap: 1 si es Chinstrap, 0 en caso contrario.\nspeciesGentoo: 1 si es Gentoo, 0 en caso contrario.\n\nAl igual que en el caso anterior, una de ellas sirve de “referencia”. Igual que en el caso anterior, R toma como referencia la primera en orden alfabético. Es decir, el intercepto es el promedio de Adelie:\n\nmean_Adelie <- mean(penguins$body_mass_g[penguins$species == \"Adelie\"],\n                    na.rm = T)\nmean_Adelie\n\n[1] 3700.662\n\n\nY entonces sí, siguiendo la lógica del caso binario, las pendientes son la diferencia entre el nivel de diferencia y el nivel de la pendiente correspondiente. Para Chinstrap:\n\nmean_Chinstrap <- mean(penguins$body_mass_g[penguins$species == \"Chinstrap\"],\n                    na.rm = T)\nmean_Chinstrap - mean_Adelie\n\n[1] 32.42598\n\n\nPara Gentoo:\n\nmean_Gentoo <- mean(penguins$body_mass_g[penguins$species == \"Gentoo\"],\n                    na.rm = T)\nmean_Gentoo - mean_Adelie\n\n[1] 1375.354\n\n\n\n\n\n\n\n\nTip\n\n\n\nEn tidymodels puedes especificar el uso de dummy variables añadiendo el paso step_dummy(var1, var2, ...) a nuestra receta, donde las var son las variables categóricas."
  },
  {
    "objectID": "c18_mvregs.html#corolario",
    "href": "c18_mvregs.html#corolario",
    "title": "5  Regresiones múltiples",
    "section": "5.3 Corolario",
    "text": "5.3 Corolario\nEn esta sesión vimos un montón de detalles y formas de aplicar un modelo lineal. Comenzamos viendo cómo introducir múltiples predictores, el problema de la colinealidad, cómo introducir interacciones (dependencias parciales) entre los predictores, cómo penalizar estos modelos de regresión múltiple, y terminamos con el cómo podemos introducir variables categóricas a nuestros modelos lineales mediante el uso de dummy variables, y qué representan los coeficientes resultantes. Afortunadamente (o tal vez desafortunadamente), las aplicaciones del modelo lineal no terminan aquí. En el Capítulo 6 vamos a hablar de cómo utilizarlo para predecir otro tipo de variables, no solo numéricas."
  },
  {
    "objectID": "c18_mvregs.html#ejercicio",
    "href": "c18_mvregs.html#ejercicio",
    "title": "5  Regresiones múltiples",
    "section": "5.4 Ejercicio",
    "text": "5.4 Ejercicio\nMe encantaría no dejarte un ejercicio porque este tema fue maratónico, pero justo por eso es necesario. Para compensarte te voy a dar dos alternativas:\n\nUtilizando los datos de pingüinos de Palmer aplica, utilizando tidymodels, un modelo de regresión lineal múltiple que prediga la masa corporal a partir del resto de variables. Tu objetivo es encontrar el modelo que minimice el RMSE y que maximice el \\(R^2\\). ¿Cuál fue el modelo final? ¿Es un modelo penalizado? ¿Es un modelo con interacciones? Elabora el reporte correspondiente con todo tu procedimiento.\nUtilizando los datos de abulones realiza una regresión por bosques aleatorios. ¿Mejora el RMSE? (OJO: bosques aleatorios es un modelo no paramétrico y no lineal, entonces aplicar un \\(R^2\\) es pasarse tres pueblos).\n\n\n\n\n\nJames G, Witten D, Hastie T, Tibshirani R (eds.). 2013. New York: Springer."
  },
  {
    "objectID": "c19_glm.html#librerías",
    "href": "c19_glm.html#librerías",
    "title": "6  Modelos Lineales Generalizados",
    "section": "6.1 Librerías",
    "text": "6.1 Librerías\n\nlibrary(ggplot2)\nlibrary(stats4)\nlibrary(brms)\nlibrary(visreg)\nlibrary(pscl)\nlibrary(MASS)\nlibrary(MuMIn)\nlibrary(tidymodels)"
  },
  {
    "objectID": "c19_glm.html#problemas-familiares",
    "href": "c19_glm.html#problemas-familiares",
    "title": "6  Modelos Lineales Generalizados",
    "section": "6.2 Problemas familiares",
    "text": "6.2 Problemas familiares\nEn la sesión anterior mencionamos cómo utilizar combinaciones lineales de variables para predecir una variable continua, pero también recordarás que en la sesión de RLS hablamos de modificar el supuesto de la distribución de nuestros errores para obtener una mejor estimación, lo cual conforma una parte fundamental de los modelos lineales generalizados (GLMs).\nLa modificación puede ser tan “simple” (al menos en términos de intuición) como relajar el supuesto de normalidad, pero podemos también utilizar otras distribuciones que nos permitan modelar otro tipo de información. Bueno, hoy aterrizaremos esa última idea: La estructura principal de un GLM sigue siendo un modelo lineal, aunque nuestro supuesto de la distribución de errores no será exclusivamente normal, sino que puede tomar alguna otra familia de distribuciones, enlazada a nuestros datos con alguna función. En la sesión de hoy revisaremos:\n\nRegresiones robustas, expandiendo un poco la distribución t como distribución de errores y revisando una alternativa.\nFunciones de enlace y enlace inverso.\nRegresiones para conteos: Poisson, Binomial Negativa y sus variantes infladas en zeros.\nSelección de modelos; i.e., cómo seleccionar el mejor de un conjunto de modelos candidatos.\nRegresiones para clases: Regresión logística binomial y multinomial.\n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nOJO: Por practicidad vamos a aplicar los GLMs con R base. Puedes también aplicar GLMs penalizados utilizando básicamente la misma estructura que vimos en el Capítulo 5. ¿Por qué no utilizar tidymodels? Podría verme elegante y decirte que porque ya conocemos el flujo básico de trabajo de tidymodels y las ventajas que nos ofrece, pero la realidad es otra: la implementación de GLMs en tidymodels es bastante limitada aún. Prefiero que entonces nos enfoquemos hoy en la intuición de los GLMs y cómo interpretarlos adecuadamente, más que enfocarnos en la parte técnica. Si tienes dudas sobre el uso de tidymodels te recomiendo que revises Kuhn & Silge (2022). Es un libro web que habla justo sobre los detalles del uso de tidymodels. Recuerda también revisar la propia documentación de tidymodels."
  },
  {
    "objectID": "c19_glm.html#regresiones-robustas",
    "href": "c19_glm.html#regresiones-robustas",
    "title": "6  Modelos Lineales Generalizados",
    "section": "6.3 Regresiones robustas",
    "text": "6.3 Regresiones robustas\nLa idea de una regresión robusta la revisamos en el ?sec-rls; es decir, utilizar una distribución con colas más altas que una distribución normal para poder contender con el efecto de puntos extremos, pero expandamos esa idea. ¿Qué significa el “peso” de las colas de una distribución? Qué tanta densidad (o masa, para distribuciones discretas) de probabilidad está acumulada lejos de la tendencia central. En palabras más sencillas, una distribución con colas ligeras como la normal piensa que la probabilidad de tener valores lejos de la tendencia central es muy baja; por lo tanto, consiidera “todos” los datos como igual de importantes y reacciona moviendo la estimación. ¿No me crees? Veamos un caso extremo, utilizando el tercer conjunto de datos de el cuarteto de Anscombe:\n\n# Almacenados en R como anscombe\nansc <- read.csv(\"datos/anscombe.csv\")\nansc$x <- scale(ansc$x, center = TRUE, scale = FALSE)\nggplot(data = ansc, aes(x = x, y = y, color = conjunto)) +\n  geom_point() +\n  facet_wrap(~conjunto) +\n  theme_bw() +\n  labs(title = \"Cuarteto de Anscombe\") +\n  scale_color_manual(values = c(\"gray70\", \"gray70\", \"#1f77b4\", \"gray70\"))\n\n\n\n\nSi vemos su distribución de y notaremos que no es exactamente normal, debido a ese punto extremo en 12.5:\n\nansc_iii = ansc[ansc$conjunto == \"III\",]\nggplot(data = ansc_iii, aes(x = y)) +\n  geom_density(color = \"#1f77b4\", fill = NA) +\n  theme_bw() +\n  labs(title = \"Densidad de y del conjunto III de Anscombe\")\n\n\n\n\nAjustemos entonces nuestra regresión lineal simple:\n\nansciii_lm <- lm(y~x, data = ansc_iii)\nsummary(ansciii_lm)\n\n\nCall:\nlm(formula = y ~ x, data = ansc_iii)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   7.5000     0.3728  20.120 8.61e-09 ***\nx             0.4997     0.1179   4.239  0.00218 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\n\nGráficamente:\n\nggplot(data = ansc_iii,\n       aes(x = x, y = y)) +\n  geom_point(color = \"#1f77b4\") +\n  geom_smooth(method = \"lm\",\n              color = \"gray50\",\n              fill = NA) +\n  theme_bw() +\n  labs(title = \"RLS con supuesto de normalidad\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nNo se ve mal; sin embargo, es claro que el punto extremo está influenciando la estimación.Podemos aplicar algún criterio de detección de valores extremos (o deformar nuestros datos) para cumplir con el supuesto de normalidad; sin embargo, más que parchar nuestros datos, es preferible modificar nuestro modelo. Cambiemos entonces a una regresión con una verosimilitud t de Student:\n\nLLt <- function(b0, b1, df, sigma){\n  # Encontrar los residuales. Modelo a ajustar (lineal)\n  R = ansc_iii$y - ansc_iii$x*b1 - b0\n  \n  # Calcular la verosimilitud. Residuales con distribución t de student\n  \n  R = suppressWarnings(brms::dstudent_t(R, df = df,\n                                        mu = 0, sigma = sigma))\n  \n  # Sumar el logaritmo de las verosimilitudes\n  # para todos los puntos de datos.\n  -sum(R, log = TRUE)\n}\n\nmlet_fit <- mle(LLt, \n                start = list(b0 = 0, b1 = 0, df = 2, sigma = 1),\n                nobs = length(ansc_iii$y),\n                lower = list(b0 = -20, b1 = -12, df = 1, sigma = 0.1),\n                upper = list(b0 = 20, b1 = 12, df = 30, sigma = 10))\nsummary(mlet_fit)\n\nWarning in sqrt(diag(object@vcov)): NaNs produced\n\n\nMaximum likelihood estimation\n\nCall:\nmle(minuslogl = LLt, start = list(b0 = 0, b1 = 0, df = 2, sigma = 1), \n    nobs = length(ansc_iii$y), lower = list(b0 = -20, b1 = -12, \n        df = 1, sigma = 0.1), upper = list(b0 = 20, b1 = 12, \n        df = 30, sigma = 10))\n\nCoefficients:\n        Estimate   Std. Error\nb0     7.1141555  0.015785311\nb1     0.3453896  0.005152567\ndf    30.0000000 36.983300378\nsigma  0.1000000          NaN\n\n-2 log L: -81.09539 \n\n\nAhora empatemos ambas regresiones en un mismo gráfico:\n\ncoefs_t <- coef(mlet_fit)\nfitted <- coefs_t[1] + coefs_t[2]*ansc_iii$x\nggplot(data = ansc_iii,\n       aes(x = x, y = y)) +\n  geom_line(aes(x = x, y = fitted),\n            color = \"#ff7f0e\",\n            size = 1, alpha = 0.7) +\n  geom_smooth(method = \"lm\",\n              color = \"gray50\",\n              fill = NA) +\n  geom_point(color = \"#1f77b4\") +\n  labs(title = \"Regresión robusta\",\n       subtitle = \"OLS (gris) vs. t de Student (naranja)\") +\n  theme_bw()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nEn este caso el punto extremo ya no influenció la estimación de la regresión, lo cual en la mayoría de los casos es algo deseable. Aunque esta es una forma de realizar una regresión robusta, existen otras. Una de ellas es modificar la función de pérdida, como por ejemplo con la Regresión con pérdida Huber. Los detalles matemáticos los dejaré para tu investigación, lo realmente importante es entender que los errores (residuales) son ponderados diferencialmente en función de su magnitud; es decir, se resta importancia a aquellos residuales que sean grandes y, de hecho, si están por encima de cierto límite, son descartados por completo. Su implementación es sumamente sencilla, pues lo único que tenemos que hacer es modificar lm por la función rlm de la librería MASS:\n\nrr_huber <- MASS::rlm(y~x, data = ansc_iii)\nsummary(rr_huber)\n\n\nCall: rlm(formula = y ~ x, data = ansc_iii)\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0049962 -0.0028591 -0.0007219  0.0028667  4.2421008 \n\nCoefficients:\n            Value     Std. Error t value  \n(Intercept)    7.1150    0.0013  5309.3547\nx              0.3457    0.0004   815.8284\n\nResidual standard error: 0.005248 on 9 degrees of freedom\n\n\n¿Notas algo interesante? Los resultados son los mismos que en la regresión t de Student, aunque aquí podemos ver la ponderación dada a cada punto:\n\nh_weights <- data.frame(x = ansc_iii$x,\n                        resid = rr_huber$residuals,\n                        weight = rr_huber$w)\nh_weights\n\n\n\n  \n\n\n\nGráficamente:\n\nggplot(data = ansc_iii,\n       aes(x = x, y = y)) +\n  geom_smooth(method = MASS::rlm,\n             color = \"#ff7f0e\",\n             size = 1, alpha = 0.7,\n             fill = \"blue\") +\n  geom_smooth(method = \"lm\",\n              color = \"gray50\",\n              fill = NA) +\n  geom_point(aes(color = h_weights$weight)) +\n  labs(title = \"Regresión robusta\",\n       subtitle = \"OLS (gris) vs. Huber (naranja)\") +\n  scale_color_gradient(name = \"Peso\",\n                       low = \"firebrick\",\n                       high = \"#1f77b4\") +\n  theme_bw()\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nComo era de esperarse, los resultados son los mismos que los de la regresión t de Student. ¿Qué tiene que ver esto con los modelos lineales generalizados? Veamos primero los tres elementos clave que definen un GLM:\n\nEl modelo lineal, que es el mismo en ambos casos.\nUna familia para el error, que no modificamos en la regresión Huber; sin embargo la familia sería Gaussiana (Normal).\nUna función de enlace (o enlace inverso), que en ambos casos sería una función de identidad; es decir, ninguna modificación para pasar de nuestros datos a la distribución del error.\n\nEs decir, en un sentido amplio, ambas aproximaciones son GLMs, aunque usualmente nos referimos a GLMs cuando la distribución del error es diferente a una distribución normal. ¿Cuál aplicar? Ya que el resultado es el mismo, puedes escoger una u otra, solo ten en cuenta que la implementación por máxima verosimilitud de un modelo t de Student puede tener muchos bemoles al momento de optimizarse, además de que su salida es incompatible con algunas otras funciones, incluyendo el cálculo de los intervalos de confianza (para los coeficientes de la regresión Huber puedes utilizar la función confint.default(rr.huber))."
  },
  {
    "objectID": "c19_glm.html#funciones-de-enlace",
    "href": "c19_glm.html#funciones-de-enlace",
    "title": "6  Modelos Lineales Generalizados",
    "section": "6.4 Funciones de enlace",
    "text": "6.4 Funciones de enlace\nEste es un buen momento para hablar de un tema que a veces causa bastante confusión: las funciones de enlace o las funciones de enlace inverso. Estas son funciones “arbitrarias” (ojo a las comillas) que tienen una sola función (valga la redundancia): poner la salida de nuestro modelo lineal en los “requerimientos” de la familia de nuestro error. En el caso anterior, la distribución t es una distribución continua de probabilidad que está centrada en 0, como esperaríamos de nuestros residuales, por lo que la función de enlace es una función de identidad; es decir, no hacemos nada a la salida del modelo para poder obtener residuales continuos centrados en 0. Pero este no siempre es el caso; de hecho, las aplicaciones más comunes de GLM siempre requieren de algún enlace. ¿Y los enlaces inversos? Son simple y sencillamente el inverso de la función de enlace aplicados al lado contrario de la igualdad. Matemáticamente es más claro:\nEn un GLM con una función de enlace tendríamos la siguiente estructura para nuestro modelo lineal:\n\\[\nf(y) = \\beta_0 + \\beta_1*x\n\\]\nEs decir, modificamos la salida (\\(y\\)) de nuestro modelo lineal utilizando una función \\(f\\). Si es una función logarítmica, por ejemplo, se vería de la siguiente manera:\n\\[\nlog(y) = \\beta_0 + \\beta_1*x\n\\]\nPero obtendríamos exactamente lo mismo si utilizamos un poco de álgebra y resolvemos para \\(y\\), aplicando un exponencial a ambos lados de la igualdad:\n\\[\\begin{align*}\ne^{log(y))} = e^{(\\beta_0 + \\beta_1*x)} \\\\\n\\therefore \\\\\ny = e^{(\\beta_0 + \\beta_1 *x)}\n\\end{align*}\\]\nEl apelativo “inversa” es simplemente para indicar el lado dónde se está aplicando el enlace. Habiendo dicho esto, vayamos a una de las aplicaciones más comunes para GLM: la regresión para conteos."
  },
  {
    "objectID": "c19_glm.html#regresiones-para-conteos",
    "href": "c19_glm.html#regresiones-para-conteos",
    "title": "6  Modelos Lineales Generalizados",
    "section": "6.5 Regresiones para conteos",
    "text": "6.5 Regresiones para conteos\nTe preguntarás qué tienen de especial los conteos, y la respuesta es muy simple: son valores enteros mayores o iguales a 0. Esto quiere decir que una distribución normal (t, o cualquier otra distribución continua) NO es adecuada para modelar los datos. ¿Qué hacemos? Utilizamos alguna distribución discreta que nos permita tratar con el número de veces en que algo sucede.\n\n6.5.1 Regresión Poisson\nMuy posiblemente esto te suene a ensayos de Bernoulli o ejercicios con distribuciones Poisson (¿cuántos autos rojos pasan en una hora por un punto determinado?, por ejemplo). Pues justamente podemos utilizar esa misma distribución (Poisson). Esta distribución tiene un par de peculiaridades. La primera es que asume que los eventos ocurren de manera independiente entre sí, a un intervalo fijo de espacio o tiempo. La segunda es que su único parámetro (\\(\\lambda\\)) representa tanto la media como la varianza de la distribución (más adelante hablaremos de las implicaciones de esto), por lo que DEBE ser positivo. ¿El problema? Nuestros residuales pueden ser negativos. ¿Qué podemos hacer? Aplicar una función de enlace inverso que nos permita restringir nuestro predictor a valores positivos, justo como la función exponencial, por lo que nuestro modelo se expresaría de la siguiente forma:\n\\[\\begin{align*}\n\\lambda = e^{(\\beta_0 + \\beta_1*x)} \\\\\ny \\sim Poisson(\\lambda)\n\\end{align*}\\]\nPara aplicarlo resolvamos un problema en el cuál trataremos de predecir el número de peces capturados en un lago por un pescador, considerando el número de hijos y si llevan o no un camper:\n\ncsvurl <- \"https://stats.idre.ucla.edu/stat/data/fish.csv\"\nfish <- read.csv(csvurl)[,c(\"child\", \"camper\", \"count\")]\nhead(fish)\n\n\n\n  \n\n\n\nExploremos nuestros datos. Al tratarse de una variable discreta podemos, sin ningún problema, utilizar un gráfico de frecuencias (¿cuál otro utilizarías?):\n\nggplot(aes(x = count), data = fish) +\n  geom_bar(stat = \"count\", color = NA, fill = \"dodgerblue4\") +\n  theme_bw() +\n  labs(title = \"Frecuencia de peces capturados en un lago\",\n       x = element_blank(),\n       y = element_blank())\n\n\n\n\nPor otra parte, habrás notado un par de cosas: a) hay una gran cantidad de ceros y b) tenemos algunos puntos “extremos”; i.e., algunos pescadores que tuvieron demasiada suerte y que capturaron demasiados peces en comparación con el resto. Este tipo de distribuciones no son extrañas en la naturaleza, y tienen un par de bemoles de los cuales hablaremos después. Por lo pronto, construyamos nuestro GLM. Podemos construirlo con la función glm de R base, cuyo uso es sumamente similar al de la función lm, salvo que indicaremos la familia como un argumento adicional:\n\npoiss <- glm(count~., data = fish, family = \"poisson\")\nsummary(poiss)\n\n\nCall:\nglm(formula = count ~ ., family = \"poisson\", data = fish)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.7736  -2.2293  -1.2024  -0.3498  24.9492  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  0.91026    0.08119   11.21   <2e-16 ***\nchild       -1.23476    0.08029  -15.38   <2e-16 ***\ncamper       1.05267    0.08871   11.87   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2958.4  on 249  degrees of freedom\nResidual deviance: 2380.1  on 247  degrees of freedom\nAIC: 2723.2\n\nNumber of Fisher Scoring iterations: 6\n\n\nLa salida es muy similar a otras que hemos visto. Cómo llamamos a la función glm, un descriptor de los residuales, los coeficientes con sus respectivas pruebas de nulidad (después hablaremos de su interpretación), seguidas de algunos elementos propios de la función GLM. Primero tenemos una nota sobre un parámetro de dispersión, que se asumió como 1. Esto quiere decir que estamos asumiendo que la media es igual a la varianza, lo cual podemos tomar solo como un recordatorio para que revisemos dicho supuesto. Después tenemos información sobre la devianza del modelo. Podemos utilizar la devianza residual para realizar una prueba de bondad de ajuste para el modelo global. Esta es la diferencia entre la devianza del modelo y la máxima devianza de un modelo ideal, donde los valores predichos son idénticos a los observados (devianza nula). Por lo tanto, buscamos valores pequeños de la devianza residual. Dicha prueba podemos realizarla de la siguiente manera:\n\nwith(data = poiss,\n     expr = cbind(res.deviance = deviance,\n                  df = df.residual,\n                  p = pchisq(deviance, df.residual,\n                             lower.tail = F)))\n\n     res.deviance  df p\n[1,]      2380.12 247 0\n\n\n\n\n\n\n\n\nNota\n\n\n\nLa función with(data, expr) evalúa una expresión de R en un ambiente construido desde los datos. En el bloque de arriba nos ahorra el tener que extraeer manualmente poiss$deviance y poiss$df.residual.\n\n\nTenemos un valor de p sumamente pequeño, lo cual sugiere que el modelo no se encuentra bien ajustado. ¿Alguna idea de por qué? Como te imaginarás, tiene que ver con la distribución de nuestros datos, eso que mencionamos sobre muchos ceros y algunos pescadores con mucha suerte. De hecho, cada una de estas características es un problema en sí mismo, así que abordemoslos uno por uno. Una pregunta que puedes estarte haciendo es ¿y la función de enlace? Va implícita en la familia. En este caso, es una función de enlace logarítmica, que es el equivalente a la función de enlace inverso que revisamos antes.\n\nstr(poisson()[1:5])\n\nList of 5\n $ family  : chr \"poisson\"\n $ link    : chr \"log\"\n $ linkfun :function (mu)  \n $ linkinv :function (eta)  \n $ variance:function (mu)  \n\n\n\n\n6.5.2 Exceso de ceros: Regresión Poisson Inflada en Cero\nLa primera peculiaridad de nuestros datos es que hay una cantidad enorme de ceros. Aunque esto puede suceder de manera natural, la distribución Poisson no es capaz de contender adecuadamente con estos casos. Afortunadamente, hay una manera de extender el modelo Poisson para permitirnos arreglar esto. En su forma más fundamental, asumiremos que tenemos dos procesos:\n\nUno modelado con una distribución Poisson.\nUno generando ceros adicionales.\n\nEs decir, cuando hablemos de modelos “inflados en cero” estamos hablando de una situación en la que tenemos ceros “falsos” o, mejor dicho, extras a los ceros verdaderos que nos podemos encontrar. Veamos qué pasa al ajustar este modelo a nuestros datos. Para este modelo necesitaremos de la función zeroinfl() de la librería pscl:\n\nzi_poiss <- pscl::zeroinfl(count~child+camper, data = fish)\nsummary(zi_poiss)\n\n\nCall:\npscl::zeroinfl(formula = count ~ child + camper, data = fish)\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-1.2395 -0.8340 -0.4694 -0.1764 24.1051 \n\nCount model coefficients (poisson with log link):\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  1.64535    0.08278  19.877   <2e-16 ***\nchild       -0.77272    0.09103  -8.489   <2e-16 ***\ncamper       0.75526    0.09112   8.289   <2e-16 ***\n\nZero-inflation model coefficients (binomial with logit link):\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   0.0424     0.2426   0.175   0.8613    \nchild         1.0244     0.2200   4.656 3.22e-06 ***\ncamper       -0.7085     0.2926  -2.422   0.0155 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 10 \nLog-likelihood: -1025 on 6 Df\n\n\nLa salida es similar al caso anterior, solo tenemos coeficientes para la regresión logística para clasificar ceros verdaderos de falsos y los coeficientes del modelo Poisson sin el exceso de ceros; sin embargo, notarás que no hay ningún indicativo sobre si este modelo es mejor a nuestro modelo Poisson, por lo que podemos compararlos. Para ello podemos utilizar distintas alternativas: una prueba de Vuong (función vuong(mod_1, mod_2) de pscl) o utilizar una aproximación multi-modelo para la selección de modelos. Optaremos por esa última vía, la cual exploraremos a detalle más adelante. Por lo pronto, es suficiente que sepas que utilizaremos una medida llamada Criterio de Información de Akaike (AIC), y el mejor modelo será aquel que tenga el menor valor de AIC:\n\nAIC(poiss, zi_poiss)\n\n\n\n  \n\n\n\nComo era de esperarse, el modelo inflado en cero es un mejor candidato; sin embargo, tenemos un problema pendiente: nuestros pescadores muy suertudos.\n\n\n6.5.3 Sobre-dispersión: Regresión Binomial Negativa\nEsos pescadores muy suertudos pueden hacer lo mismo que nuestro punto extremo en el ejemplo de regresión robusta; es decir, jalar nuestras estimaciones hacia ellas y alejarlas de la estimación “real”, solo que aquí es un tanto diferente y tiene que ver con el supuesto de nuestra distribución Poisson: la media y la varianza son iguales. Este supuesto, evidentemente, no se sostiene cuando tenemos una dispersión muy grande de nuestros datos (varianza > media), lo cual genera el problema de la sobre dispersión de nuestro modelo (no de los datos). Una estrategia es cambiar nuestra verosimilitud a una distribución Binomial Negativa, la cual tiene un parámetro adicional a la distribución Poisson. Este parámetro modela, justamente, la dispersión de nuestros datos. Apliquemos entonces nuestra regresión binomial negativa:\n\nbineg <- MASS::glm.nb(count~., data = fish, trace = F)\nsummary(bineg)\n\n\nCall:\nMASS::glm.nb(formula = count ~ ., data = fish, trace = F, init.theta = 0.2552931119, \n    link = log)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.3141  -1.0361  -0.7266  -0.1720   4.0163  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   1.0727     0.2425   4.424 9.69e-06 ***\nchild        -1.3753     0.1958  -7.025 2.14e-12 ***\ncamper        0.9094     0.2836   3.206  0.00135 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(0.2553) family taken to be 1)\n\n    Null deviance: 258.93  on 249  degrees of freedom\nResidual deviance: 201.89  on 247  degrees of freedom\nAIC: 887.42\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  0.2553 \n          Std. Err.:  0.0329 \n\n 2 x log-likelihood:  -879.4210 \n\n\nNotarás que esta salida es prácticamente la misma que la que tuvimos en nuestro GLM Poisson, salvo que ahora nos da el valor del parámetro de sobredispersión. Te estarás preguntando: ¿Cómo sé si, en efecto, mis modelos están sobre-dispersos? Para eso podemos utilizar una prueba de razón de verosimilitud, en la cual compararemos la verosimilitud de ambos modelos (binomial negativa y Poisson) y veremos si se ajustan a la misma distribución; es decir, la prueba de razón de verosimilitud es una prueba de bondad de ajuste, con distribución \\(\\chi^2\\). ¿Qué es lo que estamos comparando? Si el parámetro adicional ayuda a que el ajuste del modelo mejore significativamente. Para aplicarla podemos utilizar la función odTest(mod_bn) de la librería pscl:\n\npscl::odTest(bineg)\n\nLikelihood ratio test of H0: Poisson, as restricted NB model:\nn.b., the distribution of the test-statistic under H0 is non-standard\ne.g., see help(odTest) for details/references\n\nCritical value of test statistic at the alpha= 0.05 level: 2.7055 \nChi-Square Test Statistic =  1837.7652 p-value = < 2.2e-16 \n\n\nA ojo de buen cubero era más que evidente que nuestro modelo estaba sobre disperso, por lo que estos resultados no son sorprendentes. Algo que puedes pensar es “si estoy comparando qué modelo está mejor ajustado, ¿puedo entonces utilizar el AIC?” Y la respuesta es, por supuesto:\n\nAIC(poiss, zi_poiss, bineg)\n\n\n\n  \n\n\n\nY los resultados son, como debe de ser, consistentes. Llegados a este punto podrías preguntarme: “Ok, Arturo, ya corregimos para el exceso de ceros y para la sobre dispersión, pero lo hicimos de manera independiente. ¿Hay alguna manera de hacer ambas cosas al mismo tiempo?” En efecto, y es justo lo siguiente que vamos a revisar.\n\n\n6.5.4 Exceso de ceros y sobre dispersión: Regresión Binomial Negativa Inflada en Cero\nY, justamente, es una combinación de ambas; es decir, utilizaremos una distribución de error binomial negativa inflada en cero. La lógica es, entonces, una combinación de ambas aproximaciones; es decir, modelaremos a los ceros verdaderos y luego construiremos el modelo de regresión binomial negativa. Para hacerlo utilizaremos la función zeroinfl() que vimos antes, solo que cambiaremos la familia a negbin:\n\nzi_bineg <- pscl::zeroinfl(count~., data = fish, dist = \"negbin\")\nsummary(zi_bineg)\n\n\nCall:\npscl::zeroinfl(formula = count ~ ., data = fish, dist = \"negbin\")\n\nPearson residuals:\n      Min        1Q    Median        3Q       Max \n-0.512182 -0.497136 -0.325130 -0.003367 13.978082 \n\nCount model coefficients (negbin with log link):\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   1.0515     0.2700   3.895 9.84e-05 ***\nchild        -0.9113     0.2851  -3.196  0.00139 ** \ncamper        0.7976     0.3054   2.611  0.00902 ** \nLog(theta)   -1.2960     0.1316  -9.849  < 2e-16 ***\n\nZero-inflation model coefficients (binomial with logit link):\n            Estimate Std. Error z value Pr(>|z|)\n(Intercept)  -11.499     55.699  -0.206    0.836\nchild         10.483     55.659   0.188    0.851\ncamper        -9.501     55.663  -0.171    0.864\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nTheta = 0.2736 \nNumber of iterations in BFGS optimization: 84 \nLog-likelihood: -434.9 on 7 Df\n\n\nFinalmente, podemos comparar nuestros cuatro modelos candidatos para encontrar el más adecuado:\n\nAIC(poiss, zi_poiss, bineg, zi_bineg)\n\n\n\n  \n\n\n\nVemos que los AIC de los modelos con distribución de error binomial negativa tienen los menores valores; por lo tanto seleccionaremos a alguno de los dos. ¿Cuál? En la siguiente sección hablaremos de las peculiaridades. Por lo pronto, sigamos con nuestro criterio de seleccionar el que tenga el menor AIC, que corresponde a la distribución binomial negativa inflada en cero. Ahora sí, podemos interpretar nuestros coeficientes.\n\n\n6.5.5 Interpretación\nDesafortunadamente, la interpretación no es tan simple como en la RLM o RLS, debido a la función de enlace que utilizamos.\nPara facilitarnos la existencia, planteemos un modelo Poisson con un solo predictor, y la función de enlace logarítmica:\n\\[\nY \\sim Poisson(\\theta) \\\\\nlog(\\theta) = \\alpha + \\beta x \\\\\n\\therefore \\\\\n\\theta = e^{\\alpha + \\beta x}\n\\]\nPero, por las leyes de los exponentes, podemos reescribir la última ecuación como:\n\\[\n\\theta = e^{a}e^{\\beta x}\n\\] Esto quiere decir que los coeficientes no son aditivos, sino multiplicativos:\n\nIntercepto: \\(e^\\alpha\\), valor de \\(\\theta\\) cuando \\(x = 0\\). Si este parámetro es o no de interés depende totalmente del problema.\nPendiente(s): \\(e^\\beta\\).\n\n\nSi \\(\\beta = 0\\), entonces \\(e^\\beta = 1\\); es decir, no hay un efecto del predictor.\nSi \\(\\beta > 0\\), entonces \\(e^\\beta > 1\\); es decir, el predictor incrementa el valor de \\(\\theta\\) a una tasa de \\(e^\\beta\\) por cada incremento unitario en \\(x\\)\nSi \\(\\beta < 0\\), entonces \\(e^\\beta < 1\\); es decir, el predictor disminuye el valor de \\(\\theta\\) a una tasa de \\(e^\\beta\\) por cada incremento unitario en \\(x\\).\n\nRecuperemos los coeficientes de nuestra regresión binomial negativa inflada en cero y exponenciémoslos:\n\nexp(coef(zi_bineg)[1:3])\n\ncount_(Intercept)       count_child      count_camper \n        2.8620079         0.4019951         2.2202680 \n\n\nPongamos atención solo a aquellos coeficientes con count_, pues son los que realmente nos interesan. La interpretación entonces sería:\n\nIntercepto: Cuando el pescador no tiene hijos y no lleva un camper, el promedio de peces capturados es de 2.86\nPendientes:\n\n\nChild: El promedio de peces capturados disminuye 1-0.4 veces por cada hijo adicional.\nCamper: Si el pescador tiene un camper, el promedio de peces capturados incrementa 2.2 veces.\n\n\n\n\n\n\n\nImportante\n\n\n\nCorolario: La interpretación depende totalmente de la función de enlace que utilicemos, y siempre es necesario aplicar el enlace para poder interpretarlos.\n\n\nPara construir un gráfico podemos generar una línea con valores predichos o, mejor dicho, dos líneas: una para cada nivel de camper:\n\nfish$pred <- predict(zi_bineg, type = \"response\")\n\nggplot(data = fish, aes(x = child, y = count,\n                        color = factor(camper))) +\n  geom_point() +\n  geom_line(aes(y = pred)) +\n  labs(title = \"GLM Binomial negativo\",\n       x = \"Número de hijos\",\n       y = \"Peces capturados\") +\n  scale_color_discrete(name = \"Camper\",\n                       labels = c(\"NO\", \"SI\")) +\n  scale_y_continuous(limits = c(0, 12),\n                     label = scales::label_comma(accuracy = 1)) +\n  theme_bw()\n\n\n\n\nO utilizando visreg:\n\npartial_plots <- visreg::visreg(bineg,\n                                scale = \"response\",\n                                ylab = \"Capturas\",\n                                gg = TRUE)\n\nfor (plt in partial_plots) {\n  plot(plt + theme_bw() +\n         scale_y_continuous(limits = c(0, 12),\n                            label = scales::label_comma(accuracy = 1)) +\n         scale_x_discrete(limits = c(0, max(plt$data$x))) +\n         labs(x = plt$labels$x))\n}\n\n\n\n\n\n\n\nAntes de cambiar la hoja, es necesario aclarar que aquí NO hay supuesto de normalidad. ¿Cuál es el punto de cambiar la distribución del error si vamos a seguir casados buscando normalidad estadística?\nY ahora podemos hablar de la selección de modelos."
  },
  {
    "objectID": "c19_glm.html#selección-de-modelos",
    "href": "c19_glm.html#selección-de-modelos",
    "title": "6  Modelos Lineales Generalizados",
    "section": "6.6 Selección de modelos",
    "text": "6.6 Selección de modelos\nEl tener varios varios modelos candidatos no es algo extraño, entonces es necesario tener algún tipo de criterio que nos permita comparar entre ellos. Una aproximación es la que hemos utilizado hasta el momento; es decir, utilizar el Criterio de Información de Akaike (AIC) y seleccionar el menor. ¿Qué es el AIC y con qué se come?\n\n6.6.1 Criterio de Información de Akaike\nEl AIC es un criterio basado en teoría de la información, particularmente en la divergencia Kullback-Leibler. Ese detalle matemático va más allá del alcance de este curso; sin embargo, podemos entender que está basado en la verosimilitud de un modelo dado; es decir, qué tan verosímil es que ese modelo haya generado los datos. Un detalle es que, como hemos visto en sesiones anteriores, el “ajuste” de un modelo es directamente proporcional a su complejidad (al menos vs. los datos de entrenamiento). Es, entonces, necesario penalizar de alguna manera el número de parámetros en el modelo, para no comernos un “gol” con un modelo excesivamente complejo. Puesto en una ecuación, el AIC queda de la siguiente manera:\n\\[\nAIC = -2ln(L) + 2k\n\\]\nDonde \\(L\\) es la verosimilitud del modelo y \\(k\\) el número de parámetros. Si nuestro modelo tiene un gran número de parámetros, el valor de AIC se hará más grande, mientras que, si tiene un menor número, se hará más pequeño. ¿Qué nos dice un AIC en sí mismo? NADA, absolutamente nada. Si yo te digo que un modelo tiene un AIC de 800 no puedes saber si es bueno o malo, pues no hay una referencia. Esto nos lleva a hablar sobre algunas consideraciones que debemos de tener al utilizar el AIC:\n\nValores más bajos indican modelos más parsimoniosos.\nEs una medida relativa de la parsimonia de un modelo, por lo que solo tiene sentido cuando comparamos AIC para hipótesis (modelos) alternativas.\nPodemos comparar modelos no anidados. De hecho, podríamos comparar un modelo lineal con uno no lineal.\nLas comparaciones son válidas SOLO para modelos ajustados con los mismos valores de respuesta; i.e., mismos valores de \\(y\\).\nComparar muchos modelos con AIC es una mala idea, pues caemos en el mismo problema de las comparaciones múltiples, donde podemos encontrar por azar un modelo con el valor más bajo de AIC, cuando en realidad no es el modelo más apropiado.\nPara variar, cuando tratamos con tamaños de muestra pequeños (n/k < 40) el AIC pierde confiabilidad, por lo que hay que aplicar una corrección:\n\n\\(AIC_c = AIC + \\frac{2k(k+1)}{n-k-1}\\)\nDado que conforme incrementa n, el \\(AIC_c\\) se aproxima al \\(AIC\\), es una buena idea utilizar \\(AIC_c\\).\n\nPodemos encontrar múltiples modelos que tengan AICs similares, esto solo sugiere que estas hipótesis alternativas tienen soportes similares. ¿Qué tanto es tantito? Esa respuesta es un poco más compleja, y requiere que presentemos el \\(\\Delta{AIC}\\) (también lo puedes encontrar como \\(\\Delta_i\\)):\n\n\\(\\Delta AIC = AIC_i - AIC_{min}\\); es decir, la diferencia de cada AIC respecto al valor mínimo de AIC entre los modelos candidatos. Esta transformación forza al “mejor” modelo a tener un \\(\\Delta AIC = 0\\), y representa la pérdida de información si utilizamos un modelo candidato \\(m_i\\) en vez de \\(m_{min}\\).\nModelos con un \\(\\Delta_i \\leq 2\\) tienen soporte substancial (evidencia),\nModelos con un \\(4 \\leq \\Delta_i \\leq 7\\) tienen considerablemente menos soporte y\nModelos con \\(\\Delta_i > 10\\) carecen, escencialmente, de soporte.\n\n\nRecuperemos nuestra comparación anterior, calculemos los \\(AIC_c\\) (con la función AICc de la librería MuMIn) y calculemos los \\(\\Delta_i\\). Como mencionábamos antes, los modelos con distribuciones binomial negativas son los que tienen el mayor soporte, mientras que los Poisson carecen de cualquier soporte (dados estos datos y modelos candidatos). También podemos ver que pesa más la sobredispersión que el exceso de ceros, pues el modelo inflado en cero es marginalmente mejor que aquel no inflado.\n\nAICs <- MuMIn::AICc(poiss, zi_poiss, bineg, zi_bineg)\nAICs$Delta <- AICs$AIC - min(AICs$AIC)\nAICs"
  },
  {
    "objectID": "c19_glm.html#regresiones-para-clases",
    "href": "c19_glm.html#regresiones-para-clases",
    "title": "6  Modelos Lineales Generalizados",
    "section": "6.7 Regresiones para clases",
    "text": "6.7 Regresiones para clases\nEn el Capítulo 4 hablamos sobre los problemas de clasificación, y aplicamos un bosque aleatorio, pero mencionamos que había otro clasificador llamado regresión logística, sin dar más detalles. Aprovechemos, entonces, para revisar un poco más a profundidad la intuición detrás de ella.\n\n6.7.1 Regresión logística binaria\nLa regresión logística es un modelo lineal al cual aplicamos una función logística, lo que nos permite restringir nuestra salida al intervalo \\([0,1]\\) y, por lo tanto, predecir la probabilidad de pertenencia a una clase dada. Esa definición es correcta para resumir lo más posible la técnica; sin embargo, los detalles son un poco más complejos.\nLa regresión logística forma parte de los GLMs; por lo tanto, consta de un predictor lineal, una familia de distribución del error y una función de enlace. La familia de distribución del error es binomial; es decir, está en términos de la probabilidad de éxitos vs. la probabilidad de fracasos. Es por esto que la regresión logística tradicional solo nos permite clasificar entre dos clases. Su función de enlace es la función logit:\n\\[\nlogit(z) = \\frac{1}{1 + e^{-z}}\n\\]\nEsta función tiene la peculiaridad de que, independientemente de los valores de \\(z\\) (el predictor lineal), el resultado siempre estará contenido entre 0 y 1, el cual es, convenientemente, el mismo que el dominio del parámetro \\(p\\) de la distribución binomial (la probabilidad de éxito). Expresado matemáticamente:\n\\[\\begin{align*}\n\\theta = logistic(\\alpha + \\beta x) \\\\\ny \\sim Binom(\\theta)\n\\end{align*}\\]\nApliquemos entonces una regresión logística para clasificar entre Adelie y Gentoo de los pingüinos de Palmer, solo para ilustrar cómo interpretar los coeficientes. Primero, filtremos los datos:\n\n# Copia de los datos originales\npeng_dat <- na.omit(palmerpenguins::penguins)\n# Mantener solo las especies de interés\npeng_dat <- subset(peng_dat,\n                   island == \"Biscoe\" | island == \"Dream\")\n\npeng_dat <- peng_dat |>\n            # Centrar las variables numéricas\n            mutate(across(where(is.numeric), ~c(scale(., scale = F)))) |>\n            # Eliminar covariables inútiles para la clasificación\n            dplyr::select(!c(species, sex, year))\n\nAhora ajustemos el modelo. No te olvides de dividir en entrenamiento-prueba (o, mejor aún, realizar validación cruzada), considerar si es necesario escalar los datos, y todos los demás pasos que vimos en el Capítulo 4:\n\nlogit_reg <- glm(island~., data = peng_dat, family = \"binomial\")\nsummary(logit_reg)\n\n\nCall:\nglm(formula = island ~ ., family = \"binomial\", data = peng_dat)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.2376  -0.3120  -0.1451   0.5213   2.0174  \n\nCoefficients:\n                    Estimate Std. Error z value Pr(>|z|)    \n(Intercept)       -0.7876798  0.2465237  -3.195   0.0014 ** \nbill_length_mm     0.2285915  0.0512441   4.461 8.16e-06 ***\nbill_depth_mm      0.6530103  0.1368932   4.770 1.84e-06 ***\nflipper_length_mm -0.0122859  0.0312904  -0.393   0.6946    \nbody_mass_g       -0.0026338  0.0005288  -4.981 6.33e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 390.87  on 285  degrees of freedom\nResidual deviance: 187.62  on 281  degrees of freedom\nAIC: 197.62\n\nNumber of Fisher Scoring iterations: 6\n\n\nY obtengámos los gráficos parciales, utilizando la función visreg(mod, scale = \"response\"), donde mod es el modelo aujustado:\n\npartial_plots <- visreg::visreg(logit_reg, scale = \"response\",\n                                ylab = \"P(Isla)\",\n                                line.par = c(col = \"dodgerblue4\"),\n                                fill.par = c(fill = \"gray90\"),\n                                gg = TRUE)\n\nfor (plt in partial_plots) {\n  plot(plt +\n         scale_y_continuous(n.breaks = 3,\n                            labels = c(\"Biscoe\", 0.5, \"Dream\")) +\n         theme_bw())\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Cómo interpretamos los coeficientes? Antes de pasar a eso, es necesario que pongamos atención al argumento scale de visreg, el cual indicamos como response. Esto lo que hizo fue poner nuestra salida en lo que nos interesa: la probabilidad de pertenencia a una especie, dadas las medidas de cada variable. Veamos qué pasa si retiramos ese argumento:\n\npartial_plots <- visreg::visreg(logit_reg, ylab = \"log-odds(Isla)\", gg = TRUE)\n\nfor (plt in partial_plots) {\n  plot(plt + theme_bw())\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\nAhora nuestros gráficos están en la escala de nuestro modelo lineal; es decir, con response estamos introduciendo la función de enlace y graficando el GLM completo; sin embargo, esto no está presente en los coeficientes “crudos” arrojados por la función GLM, por lo que toca aplicar el álgebra correspondiente.\nEl modelo básico es:\n\\[\n\\theta = logistic(\\alpha + \\beta x)\n\\]\nEl inverso de la función logística es la función logit, dada por:\n\\[\nlogit(z) = log \\left( \\frac{z}{1-z} \\right)\n\\]\nPor lo que si tomamos la primera ecuación y aplicamos la función logit a ambos términos, obtenemos esta ecuación:\n\\[\nlogit(\\theta) = \\alpha + \\beta x\n\\]\nO, de manera equivalente:\n\\[\nlog \\left( \\frac{\\theta}{1-\\theta} \\right) = \\alpha + \\beta x\n\\]\nAhora, recordemos que \\(\\theta\\) en nuestro modelo es \\(p(y = 1)\\) (la probabilidad de “éxito”, o de ser virginica):\n\\[\nlog \\left(\\frac{p(y = 1)}{1 - p(y = 1)} \\right) = \\alpha + \\beta x\n\\]\nLa cantidad \\(\\frac{p(y = 1)}{1 - p(y = 1)}\\) se conoce como los odds, que representan la probabilidad de éxito sobre la probabilidad de fracaso. Mientras que la probabilidad de obtener 2 al lanzar un dado es de 1/6, los odds para el mismo evento son \\(\\frac{1/6}{5/6} \\approx 0.2\\), o un éxito a cinco fracasos. En una regresión logística, \\(\\beta\\) representa el incremento en log-odds por incremento unitario en \\(x\\), no en la probabilidad de pertenencia a una clase, aunque la relación entre odds y probabilidad es monótona; es decir, conforme incrementa una, la otra también.\n\n\n\n\n\n\nTip\n\n\n\nSi te interesa ver directamente el cambio en probabilidad por incremento unitario de las variables, puedes utilizar una pariente cercana: la regresión probit.\n\n\n\n\n6.7.2 Regresión multinomial\nAl igual que en el caso anterior, simplemente extenderemos aquellos detalles que no se aterrizaron por completo, particularmente el utilizar una red neuronal como análogo a una regresión logística multinomial. Como acabamos de ver, una regresión logística binaria nos permite predecir la probabilidad de éxito; i.e., de pertenecer a una sola clase. ¿Cómo lo extendemos a más de dos clases? Podemos construir modelos una clase vs. las demás, podemos utilizar una regresión softmax, o podemos utilizar una red neuronal. Una red neuronal está formada por capas, las cuales están conectadas entre sí tal cual neuronas:\n\n\n\nRed neuronal\n\n\nTenemos una capa de entrada, correspondiente a nuestros valores, seguida de una o más capas ocultas, compuestas por neuronas (perceptrones) que tienen funciones de activación, las cuales están conectadas por constantes multiplicadoras (pesos o weights) a las cuales se les añade una constante (sesgo o bias), cuyo resultado, finalmente, se envía a la capa de salida \\(y\\) (nuestras clases objetivo), resultando en la siguiente forma:\n\\[\ny = f(bias + \\sum(weight*input))\n\\]\nGráficamente:\n\n\n\nNeurona\n\n\n¿Suena familiar? Con solo una capa oculta y una función (\\(f\\)) de identidad tendríamos un modelo lineal cualquiera, solo que se ajusta mediante descenso estocástico de gradiente (fuera de esta discusión) en vez de mínimos cuadrados o máxima verosimilitud. Si esa función \\(f\\) la hacemos una función sigmoide (logística), tenemos entonces una regresión logística para más de dos clases. En un sentido estricto, esta aproximación no es un GLM (no tenemos una familia de distribución del error per-se), pero se puede considerar una generalización a más de dos clases. Desafortunadamente aquí tendré que romper la homogeneidad de la sesión e introducir tidymodels, porque hay un par de hiperparámetros a ajustar, pero no te preocupes, voy a asignar valores fijos para saltarnos el proceso de validación cruzada y optimización que ya conoces. Primero, hagamos una receta para preprocesar los datos:\n\npenguins_na <- na.omit(palmerpenguins::penguins)\n# Formación de la receta\npeng_rec <- recipe(island~.,\n                   data = penguins_na) |>\n            #update_role(island, role = \"outcome\") |> \n            step_select(!c(species, sex, year)) |> \n            step_normalize(all_numeric_predictors()) |> \n            step_dummy(all_nominal_predictors())\n            \n# Obtener parámetros para preprocesar\npeng_prep <- peng_rec |> prep()\n\n# Preprocesar los datos\npeng_juiced <- peng_prep |> juice()\n\nAhora especifiquemos el modelo. Aquí el argumento penalty especifica el grado de penalización del modelo, y mixture el tipo de penalización (Ridge o Lasso), tal y como vimos en las regresiones penalizadas en el Capítulo 5. De hecho, también utilizaremos la librería glmnet. Algo que no mencioné en ese capítulo fue que mixture puede tomar cualquier valor entre 0 y 1, y que cuando 0 < mixture < 1 tenemos un modelo de “red elástica”. ¿Qué es eso? Simplemente una mezcla entre ambas penalizaciones donde, evidentemente, entre más nos acerquemos a 1 más “Lasso” será el modelo. Obviamente debemos de optimizar al menos uno de los dos, pero eso ya vimos como hacerlo ;).\n\nmnom_reg <- multinom_reg(penalty = 1,\n                         mixture = 0) |>\n            set_engine(\"glmnet\")\n\nmnom_wf <- workflow() |>\n           add_recipe(peng_rec) |>\n           add_model(mnom_reg)\n\nmnom_fit <- mnom_wf |> fit(data = penguins_na)\n\nmnom_res <- mnom_fit |> extract_fit_parsnip()\nmnom_res |> tidy() |>\n  pivot_wider(names_from = term,\n              values_from = estimate)\n\n\n\n  \n\n\n\nAlgo importante a tener en cuenta es que al ajustar este modelo tomamos una clase como referencia (igual que con un predictor categórico en Capítulo 5) y entonces esa no tiene coeficientes. ¿Es un problema? Sí y no. Usualmente solo nos interesa saber qué variables son más importantes para la clasificación; es decir, qué variables son “más diferentes” entre nuestras clases, para lo cual podemos utilizar vip como ya habíamos visto en el Capítulo 4:\n\nmnom_res |> vip::vip(geom = \"point\") +\n             geom_point(color = \"deepskyblue3\") +\n             theme_bw() +\n             labs(title = \"Importancia de variables\",\n                  y = element_blank())\n\n\n\n\nEsto sería todo para esta clase de GLM, aunque no quiere decir que sean los únicos. Si te interesa modelar el tiempo entre eventos puedes utilizar un modelo Gamma, puedes cambiar la relación entre la media y la varianza de la regresión para conteos utilizando un modelo Quasi-Poisson en vez de un modelo con distribución binomial negativa (ver lecturas recomendadas), entre otros. Puedes también, como vimos en el último ejemplo, aplicar GLMs penalizados utilizando glmnet, solo hay que especificar la familia correspondiente.\n\n\n\n\nKuhn M, Silge J. 2022. Tidy Modeling with R. O’Reilly."
  },
  {
    "objectID": "s06_despedida.html",
    "href": "s06_despedida.html",
    "title": "7  Despedida",
    "section": "",
    "text": "Con esto llegamos al final del curso. ¡Muchas felicidades! Acabas de pasar por un camino bastante escabroso, con algunas cosas bastante digeribles, y otras que son un dolor de cabeza, pero espero que el contenido haya sido de tu agrado, que te hayas llevado algo y, sobre todo, que lo aprendido te sea útil. En este momento cuentas con bases sólidas tanto para adentrarte más en cualquiera de los temas aquí vistos como para incursionar en otros temas. Una recomendación personal es revisar un paradigma de inferencia diferente: la Inferencia Bayesiana. Puedes también adentrarte más en el área del aprendizaje automatizado, pues es un mundo con un potencial enorme de aplicación a problemas biológicos.\nPor último, recuerda que no porque podamos hacer algo quiere decir que debamos de hacerlo. Nada me daría más gusto que saber que el curso te motivó a aplicar en tus análisis alguna de las técnicas que aquí vimos, pero siempre pregúntate si es pertinente para responder la pregunta que quieres responder.\nSin más que agregar, te deseo lo mejor, hoy y siempre."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Referencias",
    "section": "",
    "text": "Anderson MJ. 2005. Distance-based tests for homogeneity of multivariate\ndispersions. Biometrics 62:245–253. DOI: 10.1111/j.1541-0420.2005.00440.x.\n\n\nArmhein V, Greenland S, McShane B. 2019. Scientists rise up against\nstatistical significance. Nature 567:305–307. DOI: 10.1038/d41586-019-00857-9.\n\n\nBaak M, Koopman R, Snoek H, Klous S. 2019. A new correlation coefficient\nbetween categorical, ordinal and interval variables with\nPearson characteristics. ArXiV. DOI: 10.48550/arxiv.1811.11440.\n\n\nBotta S, Secchi ER, Muelbert MMC, Danilewicz D, Negri MF, Cappozzo HL,\nHohn AA. 2010. Age and growth of franciscana dolphins, pontoporia\nblainvillei (cetacea: Pontoporiidae) incidentally caught off\nsouthern brazil and northern argentina. Journal of the Marine\nBiological Association of the United Kingdom 90:1492–1500. DOI: 10.1017/S0025315410001141.\n\n\nBox GE. 1976. Science and statistics. Journal of the American\nStatistical Association 71:791–799. DOI: 10.1080/01621459.1976.10480949.\n\n\nBox GE. 1979. Science and statistics. ournal of the American\nStatistical Association 71:791–799. DOI: 10.1080/01621459.1976.10480949.\n\n\nBreiman L. 2001. Random forests. Machine Learning 45:5–32. DOI:\n10.1023/A:1010933404324.\n\n\nBreusch TS, Pagan AR. 1979. A simple test for\nheteroscedasticity and random coefficient variation.\nEconometrica 47:1287–1294.\n\n\nCailliet GM, Smith WD, Mollet HF, Goldman KJ. 2006. Age and growth\nstudies of chondrichthyan fishes: The need for consistency in\nterminology, verification, validation, and growth function fitting.\nEnvironmental Biology of Fishes 77:211–228. DOI: 10.1007/s10641-006-9105-5.\n\n\nCairo A. 2012. The functional art. Berkeley, USA: New Riders,\nPearson Education.\n\n\nCarvajal G, Maucec M, Cullick S. 2018. Components\nof artificial intelligence and data analytics. In:\nIntelligent digital oil and gas fields. Concepts, collaboration, and\nright-time decisions. Cambridge, Massachusetts, USA: Gulf\nProfessional Publishing, 101–148. DOI: 10.1016/B978-0-12-804642-5.00004-9.\n\n\nChen S, Watanabe S. 1989. Age dependence of natural mortality\ncoefficient in fish population dynamics. NIPPON SUISAN\nGAKKAISHI 55:205–208. DOI: 10.2331/suisan.55.205.\n\n\nEnríquez-García AB annd FV-Z, Tripp-Valdez A, Moreno-Sánchez XG,\nGalván-Magaña F, Elorriaga-Verplancken FR. 2022. Foraging segregation\nbetween spotted (stenella attenuata) and spinner (stenella\nlongirostris) dolphins in the mexican south pacific. Marine\nMammal Science 38:1070–1087. DOI: 10.1111/mms.12912.\n\n\nGerrodette T. 2011. Inference without significance: Measuring support\nfor hypotheses rather than rejecting them. Marine Ecology\n32:404–418. DOI: 10.1111/j.1439-0485.2011.00466.x.\n\n\nGompertz B. 1832. On the nature of the function expressive of\nthe law of human mortality, and on a new mode of determining the value\nof life contingencies. Phylosophical Transcriptions of the Royal\nSociety of London 123:513–585.\n\n\nHaig BD. 2010. What is a spurious corelation? Understanding\nStatistics 2:125–132. DOI: 10.1207/S15328031US0202_03.\n\n\nHöfer T, Przyrembel H, Verleger S. 2004. New evidence for the\ntheory of the stork. Paediatric and Perinatal Epidemiology\n18:88–92.\n\n\nHosmer DWJr, Lemeshow S, Sturdivant RX. 2013. Applied logistic\nregression. John Wiley & Sons, Inc. DOI: 10.1002/9781118548387.\n\n\nJames G, Witten D, Hastie T, Tibshirani R (eds.). 2013. New York:\nSpringer.\n\n\nKaufman S, Rosset S, Perlich C, Stitelman O. 2012. Leakage in data\nmining: Formulation, detection, and avoidance. ACM Transactions on\nKnowledge Discovery from Data 6. DOI: 10.1145/2382577.2382579.\n\n\nKing AP, Eckersley RJ. 2019. Chapter 8 - inferential statistics v:\nMultiple and multivariate hypothesis testing. In: King AP, Eckersley RJ\neds. Statistics for biomedical engineers and scientists.\nAcademic Press, 173–199. DOI: https://doi.org/10.1016/B978-0-08-102939-8.00017-7.\n\n\nKnuth DE. 1984. Literate programming. The Computer Journal\n27:97–111. DOI: 10.1093/comjnl/27.2.97.\n\n\nKuhn M, Silge J. 2022. Tidy modeling\nwith r. O’Reilly.\n\n\nMatloff N. 2020. Teaching\nR in a kinder, gentler, more effective manner: Teach\nbase-R, not just the tidyverse.\n\n\nMeyer-Baese A, Schmid V. 2014. Chapter 7 - Foundations of\nneural networks. In: Meyer-Baese A, Schmid V eds. Pattern\nrecognition and signal analysis in medical imaging. Oxford:\nAcademic Press, 197–243. DOI: 10.1016/B978-0-12-409545-8.00007-8.\n\n\nPal R. 2017. Regression trees, random forests, probabilistic trees,\nstacked generalization, probabilistic random forests, weight\noptimization. In: Pal R ed. Predictive modeling of drug\nsensitivity. Academic Press, 149–188. DOI: 10.1016/B978-0-12-805274-7.00007-5.\n\n\nPerneger TV. 1998. What’s wrong with bonferroni adjustments.\nBMJ 316:1236–1238. DOI: 10.1136/bmj.316.7139.1236.\n\n\nR Core Team. 2022. R: A\nlanguage and environment for statistical computing. Vienna,\nAustria: R Foundation for Statistical Computing.\n\n\nReshef DN, Reshef YA, Finucane HK, Grossman SR, McVean G, Turnbaugh PJ,\nLander ES, Mitzenmacher M, Sabeti PC. 2011. Detecting novel associations\nin large datasets. Science 334:1518–1524. DOI: 10.1126/science.1205438.\n\n\nRicker WE. 1979. Growth rates and models. In: Hoar WS, Randall DJ, Brett\nJR eds. Fish physiology. 677–747.\n\n\nRougier NP, Droettboom M, Bourne PE. 2014. Ten simple rules for better\nfigures. PLoS computational biology 10:e1003833–7. DOI: 10.1371/journal.pcbi.1003833.\n\n\nSavage LJ. 1954. The foundations of statistics. John Wiley\n& Sons.\n\n\nSawyer SF. 2013. Analysis of Variance: The\nFundamental Concepts. Journal of Manual & Manipulative\nTherapy 17:27E–38E. DOI: 10.1179/jmt.2009.17.2.27e.\n\n\nShirkhorshidi AS, Aghabozorgi S, Wah TY. 2015. A comparison study on\nsimilarity and dissimilarity measusres in clustering continuous data.\nPLoS ONE 10:e0144059. DOI: 10.1371/journal.pone.0144059.\n\n\nSies H. 1988. A new parameter for sex education.\nNature 332:495.\n\n\nStrobl C, Malley J, Tutz G. 2009. An introduction to recursive\npartitioning: Rationale, aplication and characteristics of\nclassification and regression trees, bagging and random forests.\nPhysiological Methods 14:323–348. DOI: 10.1037/a0016973.\n\n\nTufte E. 1983. The visual display of quantitative information.\nCheshire, Connecticut: Graphics Press.\n\n\nWilkinson L. 2005. The grammar of graphics. USA: Springer.\n\n\nWolter K, Timlin MS. 1998. Measuring the strength of ENSO\nevents: How does 1997/98 rank. Weather 53:315–324.\n\n\nZar JH. 2010. Biostatistical Analysis. Prentice\nHall.\n\n\nZuur AF, Ieno EN, Walker N, Saveliev AA, Smith GM. 2009. Mixed effects models and extensions in ecology with\nR. Springer. DOI: 10.1007/978-0-387-87458-6."
  }
]