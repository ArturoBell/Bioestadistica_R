[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bioestadística Aplicada con R y RStudio",
    "section": "",
    "text": "Prefacio\n¡Hola! Te doy la bienvenida al curso Bioestadística Aplicada con R y RStudio, ofertado por Dr. Plancton.\nEste es el libro de acompañamiento del curso. Aquí encontrarás tanto la teoría como el código que se aborda en el curso, dispuestos en una manera que facilita su lectura, y cuyo objetivo es simplemente proveer una versión lista para ser revisada en cualquier momento, sin necesidad de iniciar RStudio. Cada sección del libro tiene enlaces a los videos correspondientes, en caso de que prefieras ver y escuchar la explicación.\nTen en cuenta que puede existir un desfase entre el material del libro y los videos. La razón es que el material se actualizará para mejorar el contenido, la entrega o la explicación siempre que sea posible, lo cual es fácil de hacer en el libro y las libretas con el código, pero no en los videos; sin embargo, en el momento en el que el desfase sea lo suficientemente grande, también se actualizarán los videos de las secciones correspondientes."
  },
  {
    "objectID": "index.html#objetivos-de-aprendizaje",
    "href": "index.html#objetivos-de-aprendizaje",
    "title": "Bioestadística Aplicada con R y RStudio",
    "section": "Objetivos de aprendizaje",
    "text": "Objetivos de aprendizaje\nEl objetivo de este curso es que seas capaz no solo de implementar distintas técnicas de análisis de datos utilizando R, sino que también puedas ser crítico con tus resultados y que minimices, en la medida de lo posible, el sesgo algorítmico. En este curso aprenderás los fundamentos detrás de las pruebas vistas, en donde abordaremos la teoría desde un punto de vista práctico, buscando que puedas formarte una intuición propia. También trataremos de desmitificar el valor de p y, sobre todo, cómo no interpretarlo. En las partes más abstractas te adentrarás en el aprendizaje automatizado, y verás que hay vida más allá del \\(R^2\\) (regresiones) y la exactitud (clasificaciones). Adicionalmente, y no por ello menos importante, te adentrarás en la visualización de datos y aprenderás a realizar reportes que faciliten compartir y leer tu trabajo.\nUno de los errores más comunes al enseñar estadística con R es tratar de enseñar ambas cosas al mismo tiempo. En este curso, R es solo un medio y no un fin; es decir, no es un curso de programación en R tanto como es un curso de ciencia de datos aplicada; sin embargo, hay una amplia introducción al lenguaje al inicio, y espero que el explicar línea por línea el código te permita familiarizarte con el lenguaje. Dicho esto, el curso fue diseñado para que personas con nulo o muy poco conocimiento de programación en general puedan seguirlo, y siempre podrás contactarnos en caso de que no hayamos explicado algo adecuadamente."
  },
  {
    "objectID": "index.html#discord-y-acceso-al-material",
    "href": "index.html#discord-y-acceso-al-material",
    "title": "Bioestadística Aplicada con R y RStudio",
    "section": "Discord y acceso al material",
    "text": "Discord y acceso al material\nSi bien es cierto que puedes utilizar y acceder a todo el material del curso desde esta página, te recomiendo encarecidamente unirte al servidor de Discord utilizando tu enlace único (enviado a tu correo al registrarte), pues ahí podrás interactuar no solamente con tus compañeros y compañeras, sino también con tu profesor. Un último comentario, NO es necesario que instales el cliente de Discord en tu computadora o dispositivo móvil, aunque si lo haces podrás recibir las notificaciones sobre modificaciones que se hagan al material."
  },
  {
    "objectID": "index.html#estructura-del-libro",
    "href": "index.html#estructura-del-libro",
    "title": "Bioestadística Aplicada con R y RStudio",
    "section": "Estructura del libro",
    "text": "Estructura del libro\nEste libro es, como ya mencioné, el material utilizado en el curso y, por lo tanto, está dividido en distintas secciones, cada una formada por distintos capítulos. Esta división trata de seguir un órden lógico, desde los conceptos y temas más fundamentales hasta los procedimientos más abstractos y, por lo tanto, todos los temas están conectados: cada capítulo asume que ya no tienes ningún problema con los anteriores.\nEn cuanto a su uso, en la esquina superior izquierda hay una barra de búsqueda, por si deseas realizar alguna consulta rápida. Debajo tienes todas las secciones y capítulos, en la porción central el contenido y a la derecha la tabla de contenidos del tema que estás leyendo. Referente al contenido, al inicio de cada capítulo tienes el video al tema correspondiete (ojo al desfase), mientras que en el texto encontrarás algunas anotaciones con información relevante al tema:\n\n\n\n\n\n\nNota\n\n\n\nAquí habrá información adicional al tema que se está tratando.\n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nAquí habrá puntos/conceptos/procedimientos con los cuales hay que tener mucho cuidado y la razón.\n\n\n\n\n\n\n\n\nImportante\n\n\n\nAquí habrá información que es sumamente importante que tengas presente.\n\n\nTambién encontrarás resaltados especiales para diferenciar claramente si estoy haciendo referencia a código en texto, funciones, librerías o software o a algunos conceptos clave. Todo el texto que veas con formato de hipervínculo te llevará a la referencia correspondiente: una figura, un capítulo, un enlace externo o una referencia bibliográfica."
  },
  {
    "objectID": "index.html#compartir-o-no-compartir-he-ahí-el-dilema",
    "href": "index.html#compartir-o-no-compartir-he-ahí-el-dilema",
    "title": "Bioestadística Aplicada con R y RStudio",
    "section": "¿Compartir o no compartir? He ahí el dilema",
    "text": "¿Compartir o no compartir? He ahí el dilema\nNo hay nada que te impida compartir este libro y con ello todo el curso. De hecho, nada me daría más gusto que saber que el curso te fue lo suficientemente agradable y útil como para dirigirlo a alguien más, pero te pido que lo hagas lo menos posible. Tu acceso al curso es vitalicio, pero ni el contenido ni el material son estáticos, por lo que dependemos de las nuevas ventas para poder seguir mejorándolo y mantenerlo en línea. Si quieres compartirlo puedes utilizar tu código de referido, el cuál da un descuento adicional a cualquier otro descuento o promoción.\nDicho esto, te doy nuevamente la bienvenida y espero que el curso cubra con tus expectativas (y un poco más)."
  },
  {
    "objectID": "s0_preparacion.html#r",
    "href": "s0_preparacion.html#r",
    "title": "Preparación",
    "section": "R",
    "text": "R\nEvidentemente, lo primero que deberás instalar es R. Puedes encontrar el instalador de la versión más reciente para tu sistema operativo en CRAN (The Comprehensive R Archive Network).\nSimplemente da click en el enlace al servidor más cercano a tu ubicación, por ejemplo https://cran.itam.mx/.\nDespués simplemente descarga la versión que corresponde a tu sistema operativo.\nIndependientemente del sistema operativo que utilices, la instalación consiste en ejecutar el instalador y seguir los pasos indicados en su ventana, dando click en aceptar y continuar según sea necesario."
  },
  {
    "objectID": "s0_preparacion.html#rstudio",
    "href": "s0_preparacion.html#rstudio",
    "title": "Preparación",
    "section": "RStudio",
    "text": "RStudio\nUna vez instalado R podemos instalar RStudio. Más adelante veremos cuál es la diferencia entre ambos, pero por el momento piensa en RStudio como una ventana para R. Primero, dirígete a https://posit.co/download/rstudio-desktop/.\n\n\n\n\n\n\nNota\n\n\n\n¿Por qué el enlace para descargar RStudio es de posit? En octubre del 2022 la empresa se volvió posit. Aquí puedes leer más al respecto, pero el resumen es que el nombre de la empresa cambió, pero el nombre de su ambiente integrado de desarrollo (IDE, RStudio) se mantendrá igual.\n\n\nEn la sección All Installers ubica tu sistema operativo y descarga el instalador correspondiente\nUna vez descargado, ejecuta el instalador y sigue los pasos que aparecen en pantalla."
  },
  {
    "objectID": "s0_preparacion.html#quarto-y-tinytex",
    "href": "s0_preparacion.html#quarto-y-tinytex",
    "title": "Preparación",
    "section": "Quarto y tinytex",
    "text": "Quarto y tinytex\nCon estas dos instalaciones sería más que suficiente para empezar a trabajar, sin embargo, podemos ir más allá y utilizar RStudio para crear reportes, artículos científicos, libros (incluyendo tesis), páginas web, presentaciones, y más. Esto solía (y puede) hacerse con librerías como rmarkdown, distilled o bookdown; sin embargo, tenemos la siguiente generación: Quarto. En el curso veremos una introducción para hacer reportes y, de hecho, este material fue escrito en documentos Quarto. Al igual que en los casos anteriores, dirígete a la página de descargas y descarga e instala la versión correspondiente a tu sistema operativo.\n\n\n\n\n\n\nImportante\n\n\n\nNO verás un ejecutable (acceso directo) después de que se haya realizado la instalación. Para utilizar Quarto necesitaremos de a) la línea de comandos o b) una interface, que en nuestro caso es RStudio. Debajo de los enlaces de descarga hay más información sobre cómo utilizarlo, incluyendo detalles sobre su uso en VS Code, Jupyter y en un editor de textos (línea de comandos). Puedes explorarlos, aunque aquí verás los detalles correspondientes para RStudio.\n\n\nAdicional a Quarto, y si deseas exportar tus documentos a archivos PDF, es necesario instalar una distribución de LaTeX. Si ya cuentas con alguna, puedes saltarte este paso, de lo contrario puedes o realizar la instalación completa o simplemente instalar TinyTeX desde aquí. Si optas por instalar TinyTeX y no tienes nada de experiencia con R, te recomiendo seguir los pasos de su página una vez que hayas pasado por el tema Bases de R. No te preocupes, hay un recordatorio al final de la sesión."
  },
  {
    "objectID": "s0_preparacion.html#instalaciones-opcionales",
    "href": "s0_preparacion.html#instalaciones-opcionales",
    "title": "Preparación",
    "section": "Instalaciones “opcionales”",
    "text": "Instalaciones “opcionales”\nAdicionalmente, te recomiendo encarecidamente (por no decir te solicito) que instales, dependiendo de tu sistema operativo, algunas herramientas. Tienes la descripción de cada una, así como desde donde realizar su instalación:\n\nWindows: Rtools. Esta es una serie de herramientas que nunca vas a ver cuando se utilicen, pero que son un dolor de cabeza si no cuentas con ellas y tienes la pésima fortuna de necesitar compilar un paquete desde su código fuente, o la dicha de querer publicar un paquete. A final de cuentas permiten justamente eso, administrar esas ejecuciones. Solía ser parte de la instalación de R en Windows, pero ya no es así, por lo que recomiendo la instales siguiendo las instrucciones oficiales (selecciona la versión más nueva de Rtools disponible).\nmacOS: XQuartz. R es un lenguaje con un enfoque muy fuerte hacia la generación de gráficos, lo cual permite hacer visualizaciones de muy alta calidad. Desafortunadamente, algunas cosas se apoyan del sistema de ventanas X.Org X Window System y que, de no estar disponible, pueden dar algunos dolores de cabeza. Apple solía incluir una implementación en la aplicación X11 en las versiones de OS X 10.5 a 10.7, pero ya no en sistemas más modernos. Es ahí donde entra el proyecto XQuartz. El instalador está disponible en la página del proyecto.\nmacOS: Xcode Command Line Tools. Si bien es cierto que macOS juega un papel importante en algunos círculos de desarrollo de software, no incluye algunas herramientas necesarias para el mismo objetivo que Rtools en Windows: compilar desde fuente. En este caso, hay dos formas de instalar lo que necesitamos, ambas provistas por Apple: a) instalar el entorno de desarrollo Xcode (40 GB) o, mi recomendación si no desarrollas aplicaciones para SOs de Apple, b) instalar las herramientas de línea de comando. Xcode lo puedes instalar directamente desde tu App Store, mientras que las herramientas de línea de comando desde el Terminal con el comando xcode-select --install. Puedes abrir el terminal utilizando Spotlight (CMD + espacio), o ejecutando su aplicación, ubicada en Aplicaciones -> Utilidades.\n\nUna vez instalado todo esto estás más que listo para avanzar con el programa del curso. Recuerda que si tuviste algún problema siempre puedes contactarme en el servidor de Discord del curso."
  },
  {
    "objectID": "s01_biolcdatos.html#objetivo-de-aprendizaje",
    "href": "s01_biolcdatos.html#objetivo-de-aprendizaje",
    "title": "Biología, Ciencia de Datos y R",
    "section": "Objetivo de aprendizaje",
    "text": "Objetivo de aprendizaje\nEn esta sección del curso comenzarás reflexionando sobre el cómo se han aplicado tradicionalmente las técnicas estadísticas a los problemas biológicos. Después, te introducirás a RStudio, la elaboración de reportes con Quarto y posteriormente a las bases del lenguaje R y su dialecto tidy."
  },
  {
    "objectID": "c01_biolcdatos.html",
    "href": "c01_biolcdatos.html",
    "title": "1  Ciencia de datos y biología",
    "section": "",
    "text": "¡Hola! En esta primera clase del curso Bioestadística Aplicada con R y RStudio abriremos con la relación emergente entre la biología y la ciencia de datos.\nPara comenzar, hablaremos un poco sobre lo que conocemos como estadística. El origen de esta disciplina se encuentra en la necesidad de los gobiernos de conocer cuál es el estado de su población, de ahí su raíz etimológica “relativo al Estado”. Sin embargo, en la actualidad, existen diversas definiciones y opiniones sobre lo que representa. Hay quienes mencionan que es la primera de las ciencias inexactas, mientras otras personas la consideran como la ciencia que nos permite cambiar nuestras ideas ante la incertidumbre. Si bien es cierto que estas visiones son en apariencia muy diferentes, ambas son perspectivas bastante válidas: la primera hace referencia a la relativa facilidad con la que se pueden manipular los datos o las pruebas (intencionalmente o no) para llegar al resultado que nosotros deseemos y la segunda a que nos permite tomar decisiones aún sin conocer en su totalidad un fenómeno. Si deseas leer sobre algunos mitos y realidades de la estadística, sigue este enlace.\nUna definición más formal es la de la Real Academia de la Lengua Española: “Rama de la matemática que utiliza grandes conjuntos de datos numéricos para obtener inferencias basadas en el cálculo de probabilidades”. De esta definición podemos retomar algunas ideas claves: la primera es que es una rama de la matemática, por lo cual nuestra resolución de problemas debe basarse en un razonamiento lógico bajo la cobertura de las matemáticas, por lo que los procedimientos y resultados deben de ser expresados sin ambigüedades. La siguiente es que requiere de grandes conjuntos de datos, lo cual hace referencia a un tema que será abordado más adelante en el curso: la representatividad. Por último, podemos también reconocer su objetivo, el cual es permitirnos obtener conclusiones a partir de este conjunto de datos. A partir de esta definición podemos considerar a la bioestadística como la aplicación de la estadística a problemas biológicos, desde la estimación del tamaño poblacional de una especie, comparaciones de tallas entre sitios de muestreo, modelar el crecimiento corporal, entre muchas otras.\n\n\n\n\n\n\nNota\n\n\n\nDebido a que la bioestadística está formada por una gran cantidad de procedimientos, muchos de ellos específicos a áreas particulares del conocimiento, en este curso abordaremos los fundamentos básicos de la estadística y técnicas de uso general.\n\n\nLlegados a este punto, quiero introducir otro concepto: la ciencia de datos. En pocas palabras, esta estudia los métodos para extraer información sobre los datos y, en última instancia, facilitar la toma de decisiones. Sus objetivos son i) describir los datos, comparar entre grupos/poblaciones/clases, ordenar o clasificar observaciones y, en última instancia, predecir un resultado futuro a partir de los datos con las que se cuenta.\nAl igual que con la definición de estadística, existen distintas definiciones y visiones, pero por el momento analicemos este diagrama que nos habla sobre las habilidades de un individuo realizando un procedimiento a sus datos y cuál sería el resultado:\n\n\n\nFigura 1.1: Ciencia de datos\n\n\n\nConsideremos las habilidades de hackeo como habilidades de programación y el conocimiento técnico necesario para la aplicación de distintas técnicas de análisis de datos, a la experiencia como el conocimiento que el individuo tenga sobre el fenómeno que está analizando y al conocimiento sobre matemáticas y estadística como el conocimiento teórico sobre las técnicas que está aplicando.\nEn la intersección de las habilidades de hackeo y la experiencia se encuentra una “zona de peligro”, la cual también podemos definir como el área de la “caja negra”, es decir, el área en la que se aplican pruebas y métodos sin conocer sus fundamentos y las conclusiones o inferencias están en función de la experiencia y los prejuicios de quien las realice, sin considerar su pertinencia.\nPor otra parte, en la intersección de la experiencia y el conocimiento de estadística se encuentra la investigación tradicional; es decir, existe un conocimiento teórico sobre las pruebas que se están aplicando y la experiencia para poder realizar inferencias sobre los datos considerando las limitaciones, fortalezas y debilidades de las técnicas; sin embargo, la visión sobre el problema se encuentra normalmente limitada a las pruebas tradicionales, lo cual a su vez limita el tipo de análisis y preguntas que se puedan resolver.\nEn la intersección de las habilidades de hackeo y el conocimiento sobre estadística se encuentra la disciplina del “aprendizaje automatizado”, algo que discutiremos más profundamente en la sección de Técnicas Multivariadas, pero que hace referencia al extraer relaciones entre los datos de manera eficiente, independientemente de si estas relaciones son causales o no.\nFinalmente, en el centro, recibiendo entradas de las tres áres encontramos a la ciencia de datos.\n\nTomando esto en consideración, te propongo hacer un ejercicio de reflexión sobre la pertinencia de aproximarnos a los problemas biológicos de una manera más flexible, eficiente, informada y que considere también la experiencia del investigador, más allá del modo tradicional e inamovible que nos ha llevado a pensar de manera incorrecta que la falta de significancia estadística es falta de significancia biológica o viceversa. ¿Es esto un problema muy grave? Esa pregunta la dejaremos para un tema donde es más adecuado abordarla: pruebas de hipótesis, en especial al hablar sobre los usos y abusos del famosísimo (¿infame?) valor de p.\nCon esta idea terminamos la primera clase del curso. Nos vemos en la siguiente para familiarizarnos con RStudio y cómo elaborar reportes utilizando Quarto."
  },
  {
    "objectID": "c02_intro_rs.html",
    "href": "c02_intro_rs.html",
    "title": "2  Introducción a RStudio y Quarto",
    "section": "",
    "text": "3 Entra a Quarto\nAhora sí, podemos dedicarle nuestra atención a Quarto. Antes mencioné que era un sistema para la creación de documentos científicos y técnicos, que se instala aparte de R y RStudio (lo hicimos ya en la sesión anterior), y que es sumamente flexible. Vimos también un ejemplo de cómo crear un documento Quarto y un poco de cómo interactuar con él. Ahora entremos a todos los detalles. Retomemos nuestro documento de ejemplo (Figura 2.11)."
  },
  {
    "objectID": "c02_intro_rs.html#r-vs.-rstudio",
    "href": "c02_intro_rs.html#r-vs.-rstudio",
    "title": "2  Introducción a RStudio y Quarto",
    "section": "2.1 R vs. RStudio",
    "text": "2.1 R vs. RStudio\nAntes de volar y pretender formar un sitio web o una tesis, comencemos hablando de RStudio. En la sección de preparación instalamos tres cosas diferentes: R, RStudio, y Quarto. Olvidemos a este último por un momento y centrémonos en los dos primeros. R es un lenguaje de programación. Como tal, se ejecuta en consola, ya sea el terminal (macOS/Linux) o el interpretador de comandos cmd en Windows. Lo único que veremos si abrimos/ejecutamos R per-se es una ventana como la siguiente:\n\n\n\nFigura 2.1: Consola de R (R GUI)\n\n\nEs decir, solamente veremos nuestra consola, compuesta por una descripción de la versión de R que estamos utilizando y un prompt (el símbolo >) que nos presiona a darle a la computadora una instrucción. Más allá de ser una interfaz extremadamente simple, no está pensada para el desarrollo de reportes como los que nosotros realizamos. No podemos escribir texto libre, ni tampoco podemos guardar nuestro progreso. Para eso habría que abrir un script, pero hay una mejor alternativa que nos permite hacer eso y mucho más: RStudio. Por el momento hasta aquí vamos a llegar con R, pero no te preocupes, le vamos a dedicar mucho más tiempo posteriormente."
  },
  {
    "objectID": "c02_intro_rs.html#el-ide-rstudio",
    "href": "c02_intro_rs.html#el-ide-rstudio",
    "title": "2  Introducción a RStudio y Quarto",
    "section": "2.2 El IDE RStudio",
    "text": "2.2 El IDE RStudio\nMientras que R es un lenguaje de programación ejecutable en consola, RStudio es un ambiente gráfico de desarrollo (IDE). ¿Qué significa? Que es una interfaz gráfica que nos permite no solo ejecutar nuestro código línea a línea, sino que también incluye otros páneles que nos facilitan enormemente la existencia y, además, abre otras puertas para la creación de documentos como este libro. Vayamos por partes.\n\n2.2.1 La ventana de RStudio\nAl abrir RStudio por primera vez te vas a topar con una ventana como la siguiente:\n\n\n\nFigura 2.2: Ventana de RStudio\n\n\nYa sé, ya sé, no se ve mucho más amable que la ventana de R. Es más, se ve mucho más intimidante porque ahora tenemos la consola y otros espacios. Al ser un IDE, RStudio incluye elementos gráficos para todo lo que pudiéramos llegar a necesitar mientras desarrollamos nuestros análisis, entonces vamos a descomponer esta ventana panel por panel, de arriba a abajo y de izquierda a derecha.\n\n\n\n\n\n\nImportante\n\n\n\nSi no te aparecen los cuatro páneles no te preocupes, simplemente ve a File -> New File -> R script. Con eso se abrirá el panel faltante.\n\n\n\n2.2.1.1 El editor\nEl primer panel es el editor:\n\n\n\nFigura 2.3: Editor en RStudio\n\n\nEste es, como el nombre sugiere, un editor de textos, que no debemos de confundir con procesador de palabras (i.e., Word o similares). En él vamos a poder escribir sripts o libretas que contengan la serie de pasos que realizamos durante nuestro análisis. Cada una de las pestañas en este panel es siempre un documento de texto simple, independientemente de si es un script o una libreta. Esto tiene varias ventajas, pero la más importante es que nos podemos llevar esos archivos a cualquier computadora y estar bastante seguros de que podremos, cuando menos, ver su contenido y editarlo sin preocuparnos por problemas de compatibilidad entre versiones del software o, peor aún, sistemas operativos (*ejem* Word *ejem*). Estos archivos de texto simple pueden, dependiendo del tipo de archivo, enviar instrucciones a R.\n\n\n2.2.1.2 La consola\nEl siguiente panel es la consola:\n\n\n\nFigura 2.4: Consola en RStudio\n\n\nEste panel es, literalmente, lo que veíamos al abrir R por sí solo; es decir, un espacio donde tenemos nuestro prompt y donde se ejecutarán nuestras instrucciones o líneas de comandos. Notarás que hay otras tres pestañas: una llamada Terminal, y otra llamada Background Jobs. Estas son interfaces a la terminal del sistema y a los trabajos que estemos ejecutando en segundo plano. Nuestra interacción con estas dos pestañas tiende a ser limitada (o incluso nula), salvo que realicemos algo muy especializado.\n\n\n2.2.1.3 Scripts: qué, cómo, cuándo y por qué.\nAntes de pasar al siguiente panel es importante hablar de los scripts, las libretas, e intentar hacer un poco de labor de convencimiento. Si has tenido un acercamiento previo a R/RStudio, es bastante probable que trabajar con scripts te sea familiar. Si no es así, un script de R es un archivo de texto con extensión .R en el que ponemos nuestro código línea a línea. Pensemos en un ejercicio en el que queremos primero sumar 1 y 1, y luego 2 y 2. Nuestro script en RStudio se vería así:\n\n\n\nFigura 2.5: Script en el editor\n\n\nSolo tenemos el código, no tenemos los resultados. ¿La razón? Aún no le hemos dicho a la computadora que queremos que las ejecute. ¿Cómo le decimos? Tenemos dos formas:\n\nEjecutar el script línea a línea, para lo que debemos posicionar nuestro cursor (dar click) sobre la línea a ejecutar, utilizar el atajo de teclado CMD + R en macOS/Linux o CTRL + R en Windows, o dar click sobre el botón Run que está cerca de la esquina superior derecha del panel.\nEjecutar el script completo, para lo que seleccionaríamos todo su contenido y utilizaríamos el mismo atajo de teclado o botón que antes.\n\nSea cual sea la opción que hayas escogido, la salida (el resultado) aparecerá en la consola:\n\n\n\nFigura 2.6: Script ejecutado en consola\n\n\n¿Cuál es el problema? El primero es que en el momento en el que cerremos RStudio esos resultados se van a perder, salvo que los hayamos guardado manualmente en algún lugar. El segundo tiene que ver con una falta de unificación: el código (las sumas) están por un lado, mientras que los resultados están en otro que, además, es volátil. El tercero se deriva de los dos anteriores: falta de legibilidad y reproducibilidad. No podemos hacer el reporte al mismo tiempo en que analizamos los datos y, si en algún momento volvemos al script, debemos de ejecutarlo todo nuevamente para ver los resultados. Suena engorroso, ¿no? Un cuarto problema es que no tenemos descripciones de nuestros resultados. Si ya has trabajado con estos archivos me vas a decir “para eso existen los comentarios”, a lo que yo te respondería que no, los comentarios no son para eso. Si no has trabajado con R ni ningún otro lenguaje de programación te preguntarás qué es un comentario. Bien, un comentario es un fragmento de texto no ejecutable; es decir, es algo que podemos escribir y pasarle a la consola pero que no se va a ejecutar. En R estos están dados por el operador #. Agreguemos un comentario a nuestro script con la palabra “Sumas” y ejecutémoslo todo nuevamente:\n\n\n\nFigura 2.7: Script con comentarios ejecutado en consola\n\n\nComo esperábamos, en la consola no hay una salida asociada a la instrucción # Sumas, por lo tanto puedo usar esos comentarios para describir mis resultados, ¿no? La respuesta es, como en muchas otras cosas, depende. O, mejor dicho, de que se puede, se puede, que debamos hacerlo, es otra historia. Los comentarios tienen la función de describir muy brevemente qué intención tiene el código, no escribir párrafos completos con el reporte de los resultados. Comentarios válidos son agregar al inicio del script quién lo escribió, qué hace el código contenido en él, un medio de contacto, y breves descripciones de qué se hace en cada línea, sin repetir el código en texto simple (no decir # Suma 1 y 1 si el código es 1+1, por ejemplo). Existe otro gran problema el cuál no es obvio en este ejercicio, pero que tiene que ver con la carga de datos en archivos dentro de nuestra computadora (archivos .csv o .xlsx, por ejemplo), pero eso lo veremos en un tema posterior. Por el momento veamos una alternativa que resuelve todos estos problemas.\n\n\n2.2.1.4 Libretas y reportes: Quarto\nAquí es donde entran Quarto y las libretas. Al instalar Quarto no instalamos un programa per-se, sino que instalamos una extensión a RStudio que es, y cito textualmente, “un sistema de publicación científica y técnica de código abierto construido sobre Pandoc”, que permite, citando nuevamente: i) crear contenido dinámico no solo con R sino con otros lenguajes de programación; ii) escribir documentos como texto plano; iii) publicar artículos, reportes, presentaciones, sitios web, blogs y libros de alta calidad en formatos HTML, PDF, MS Word, ePUB; y iv) escribir con markdown científico, incluyendo ecuaciones, citas, referencias cruzadas, páneles de figuras, anotaciones, diseños avanzados y más. ¿A que ya suena mejor que los scripts? Sin ir más lejos, todo el material que utilizaremos en este curso fue escrito en RStudio utilizando Quarto, y puedes ver la versión final en el sitio web de acompañamiento. Debido a que explicar Quarto es un tema que merece le dediquemos tiempo y estar más arriba que un subtema de IDE RStudio, vamos a dejarlo de lado por el momento, solo revisemos cómo crear un nuevo documento y las diferencias fundamentales con los scritps. Para crear un documento podemos ir a la barra de herramientas -> File -> New file -> Quarto document, o utilizar el botón correspondiente en la ventana de RStudio:\n\n\n\nFigura 2.8: Nuevo documento Quarto\n\n\nAl darle click nos va a aparecer la siguiente ventana para personalizar el documento:\n\n\n\nFigura 2.9: Opciones documento Quarto\n\n\nAquí añadirás el título de tu documento y opcionalmente el autor. Por el momento dejaremos todo lo demás tal y como está y daremos click en Create, lo que abrirá una pestaña nueva en el editor:\n\n\n\nFigura 2.10: Opciones documento Quarto\n\n\nLa pestaña se parece al contenido de un script, la única diferencia es un texto contenido entre ---. A esta parte la podemos identificar como el preámbulo del documento, y es un fragmento de nuestro documento escrito en formato YAML ¿Qué es eso y con qué se come? No te preocupes ahorita por eso, solo necesitas saber ahorita que vamos a desarrollar nuestro trabajo debajo del preámbulo. En la Figura 2.11 puedes ver un ejemplo básico de un documento Quarto con el código de nuestras sumas.\n\n\n\nFigura 2.11: Un documento Quarto básico\n\n\nUn ejemplo más claro del potencial de Quarto es, nuevamente, el material de este curso. Pero sigamos explorando la ventana de RStudio\n\n\n\n2.2.2 El ambiente de trabajo\nEl siguiente panel es lo que se conoce como el ambiente de trabajo (workspace), que nos da un listado de las cosas que le hemos dado a R para que recuerde (objetos). En la siguiente sesión hablaremos largo y tendido de esto, pero veamos un ejemplo donde, utilizando la consola, le digamos a R que recuerde el resultado de la suma 1 + 1, asignándolo a una referencia que arbitráriamente llamaré suma:\n\n\n\nFigura 2.12: Guardar un resultado en la consola\n\n\nAquí hay dos cosas a tener en cuenta: 1. La asignación se hizo con el operador <-. Esto es sumamente importante: en R guardamos cosas utilizando el operador <- y no =. 2. Al ejecutar la línea no obtuvimos el resultado. Esto es porque solo le dijimos que lo recordara, que lo anotara en un post-it, si quieres, no que nos lo mostrara. Si queremos que nos lo muestre solo tenemos que llamarlo por su nombre (ejecutar en la consola):\n\n\n\nFigura 2.13: Imprimir el resultado\n\n\n¿Qué tiene que ver esto con el ambiente de trabajo? Pues ahora ya no está vacío, ya tenemos una entrada en la lista, en donde se muestra el nombre del objeto y su valor. Conforme vayamos creando más objetos, más entradas tendrá esa lista. Prueba a crear un objeto que contenga el texto \"Hola mundo\" (ojo a las comillas) y dar click en su nombre en el ambiente de trabajo. ¿Qué ocurre?\n\n\n2.2.3 El ambiente gráfico\nEl último panel corresponde al ambiente gráfico. Este panel junta varios elementos a los que es más fácil acceder visualmente. La primera pestaña es un explorador de archivos. puedes dar click a cada carpeta para ver sus elementos, crear nuevas carpetas, y mucho más. En este curso no lo utilizaremos más que como una referencia visual de dónde están ubicados nuestros archivos.\n\n\n\nFigura 2.14: Explorador de archivos\n\n\nLa siguiente pestaña nos muestra la serie de gráficos que hemos ido generando. Podemos ejecutar en la consola el comando plot(cars) y verás que el gráfico se muestra en esta pestaña.\n\n\n\nFigura 2.15: Gráfico en el ambiente gráfico\n\n\nEn la siguiente pestaña encontraremos un listado de las librerías/paqueterías que tenemos instaladas, sus versiones, una breve descripción de para qué son, así como botones para instalarlas o actualizarlas.\n\n\n\nFigura 2.16: Paquetes\n\n\nDespués tenemos la pestaña de ayuda. Aquí podemos ver, valga la redundancia, ayuda sobre R, pero también sobre funciones en las que estemos interesados. Si queremos ver la ayuda de una función FUN podemos o utilizar el comando ?FUN Figura 2.17 o utilizar la barra de búsqueda de la pestaña\n\n\n\nFigura 2.17: ¡Ayuda!\n\n\nEn la siguiente pestaña tenemos un visor (Viewer), donde tendremos vistas previas de los documentos RMarkdown o Quarto con los que estemos trabajando. Tomando como ejemplo el documento Quarto que generamos antes, da click al botón Render con la flecha azul. ¿Qué obtienes?\nLa última pestaña es un visor de presentaciones, el cual no utilizaremos en este curso.\n\n\n2.2.4 Personalizando RStudio\nComo todo IDE, podemos personalizar la apariencia de RStudio. Para hacerlo simplemente ve a la barra de herramientas, luego en Tools -> Global Options. Te aparecerá la siguiente ventana\n\n\n\nFigura 2.18: Ventana de ajustes\n\n\nVamos a revisar algunas de las opciones que te recomiendo tener presentes. Puedes hacer los cambios y aplicarlos todos juntos al final con el botón Apply:\n\nGeneral\n\nBasic\n\nWorkspace: Restore .RData into workspace at startup & Save workspace to .RData on exit. ¿Quieres que cada que cierres RStudio todos los objetos de tu espacio de trabajo se guarden en un archivo, y que esos mismos se carguen la siguiente vez que abras RStudio? Personalmente no es algo con lo que esté de acuerdo, porque más frecuentemente que no vas a querer un ambiente limpio, por lo que estas opciones están desmarcada y en Never, respectivamente.\nHistory: La misma historia (je) que en el caso anterior. ¿Quieres que tu historial de comandos ejecutados se guarde, aún si no guardas el .RData? ¿Quieres que se remuevan los duplicados? Por las mismas razones que antes también las tengo desmarcadas.\n\nGraphics\n\nBackend a Cairo. Simplemente es con qué se están graficando las cosas. ¡Es INDISPENSABLE que hayas instalado XQuartz si estás en macOS!\n\n\nAppearance\n\nZoom: ¿Qué tan grandes quieres todos los elementos de la ventana? Para mi trabajo personal, y dependiendo de la resolución del monitor donde se encuentre la ventana, esta oscila entre 100% y 125%, para este curso está en 175-200%\nEditor font y Editor font size: Tipo y tamaño de letra. Personalmente recomiendo no cambiar el tipo de letra.\nEditor theme: Cambia el color de fondo y los colores de realce de la sintaxis. Usualmente trabajo por las noches, por lo que prefiero un tema con fondo obscuro como Tomorrow Night Bright, pero puedes buscar el que tú quieras.\n\nPane Layout\n\nAquí se muestran los cuatro paneles de la ventana de RStudio. Personalmente prefiero tener los dos elementos que más utilizo lado a lado y no uno encima del otro. ¿La razón? El código crece hacia abajo, entonces el espacio vertical tiende a ser más importante que el espacio horizontal. Es decir, que en el panel superior derecho pongo la consola, y en el panel inferior izquierdo el ambiente de trabajo.\n\n\nEl resto de opciones son más específicas, por lo que recomiendo no tocarlas salvo que sepas qué estás moviendo y con qué objetivo. Todo se puede revertir, pero no hay necesidad de buscar dolores de cabeza."
  },
  {
    "objectID": "c02_intro_rs.html#preámbulo",
    "href": "c02_intro_rs.html#preámbulo",
    "title": "2  Introducción a RStudio y Quarto",
    "section": "3.1 Preámbulo",
    "text": "3.1 Preámbulo\nEl primer elemento es el preámbulo, que mencionamos está en formato YAML y que está contenido entre ---. YAML es un formato de serialización de datos legible por humanos. ¿En castellano? Una lista con niveles de pares de claves:valores que definen los metadatos de nuestro documento. En nuestro ejemplo tenemos:\n---\ntitle: \"Untitled\"\n---\nEs decir, el título que aparecerá en el reporte es \"Untitled\", tal y como vimos en la vista previa. Esto no tiene mucho sentido, así que cambiémoslo por \"Mi primerQuarto\" y añadamos una nueva entrada para el autor, tal que:\n---\ntitle: \"Mi primer `Quarto`\"\nauthor: \"Tu nombre\"\n---\nOtro elemento que usualmente se agrega en el preámbulo es el formato de salida del documento renderizado; es decir, ya presentado para compartir/imprimir. Mi recomendación es exportar a archivos HTML, salvo que vayas a imprimir el documento (PDF), necesites paginación (PDF de nuevo) o que por alguna desafortunada razón necesites un archivo MS Word. Un HTML lo declaramos tal que:\n---\ntitle: \"Mi primer `Quarto`\"\nauthor: \"Tu nombre\"\nformat:\n  html:\n    code-fold: true\n---\nNotarás algunas cosas. La primera es que html está indentado; es decir, no comienza en la misma posición que format. Esto es para indicar que html pertenece a format, al igual que code-fold pertenece a html. La siguiente es, justamente, que agregamos a la lista la entrada code-fold. Esta es una opción que indica si queremos que el código sea colapsable mediante un botón en el archivo final. En este caso, la indicamos como true, por lo que así será. Si no lo quisiéramos así indicaríamos false. Si renderizamos nuestro documento ahora tendremos:\n\n\n\nFigura 3.1: Primer Quarto renderizado\n\n\nSi tienes curiosidad por saber qué características YAML dieron lugar al libro de acompañamiento, puedes revisar el archivo _Quarto.yml."
  },
  {
    "objectID": "c02_intro_rs.html#markdown",
    "href": "c02_intro_rs.html#markdown",
    "title": "2  Introducción a RStudio y Quarto",
    "section": "3.2 Markdown",
    "text": "3.2 Markdown\nPasando el preámbulo tenemos una sección de texto libre, con algunas anotaciones para el formato del texto. Estas anotaciones están hechas en lenguaje markdown. Markdown es un “lenguaje de programación para textos” y permite hacer cosas bastante interesantes. Las anotaciones más básicas son:\n\nEncabezados y secciones: #, ##, ###, ####, etc.\n*Itálicas* : Itálicas\n**Itálicas**: Negritas\n`Código`: Código\nHipervínculos: [Google](https://www.google.com): Google\n$y_i = \\alpha + \\beta*x_i + \\epsilon$: \\(y_i = \\alpha + \\beta*x_i + \\epsilon\\)\n\nPuedes hacer listas numeradas, como la anterior, o listas sin numerar:\n\nElemento\nOtro elemento\n\nE, incluso, puedes hacer listas anidadas añadiendo una indentación de doble tabulación a los elementos anidados:\n\nIntroducción a RStudio y Quarto\n\nR vs. RStudio\nIDE RStudio\n\n\nEn el archivo .qmd este capítulo puedes ver cómo añadí las capturas de la ventana de RStudio. Añadir imágenes es básicamente el mismo procedimiento que con un enlace, solo añadiendo el operador ! antes. Por ejemplo, la siguiente línea añade el logo de R desde su dirección oficial, le asigna el pie de foto “Logo R” y una etiqueta interna que se puede utilizar para referencias cruzadas (Figura 3.2) con @fig-logoR :\n![Logo `R`](https://www.r-project.org/logo/Rlogo.png){#fig-logoR}\n\n\n\nFigura 3.2: Logo R"
  },
  {
    "objectID": "c02_intro_rs.html#referencias-y-citas",
    "href": "c02_intro_rs.html#referencias-y-citas",
    "title": "2  Introducción a RStudio y Quarto",
    "section": "3.3 Referencias y citas",
    "text": "3.3 Referencias y citas\nPodemos agregar referencias, siempre y cuando estas estén contenidas en un archivo .bib como el archivo references.bib que está en este directorio y, por supuesto, referenciarlas en el texto (e.g. Knuth, 1984). Si estás viendo la página web con este material, pasa tu cursor sobre la cita y notarás como aparece la información bibliográfica completa. Este archivo .bib está compuesto por entradas en formato bibTeX, heredado del hermano mayor de Markdown: LaTeX. La sintaxis básica es la siguiente:\n@TIPO{CLAVE,\n      author = {},\n      year = {},\n      title = {}}\nLos campos adicionales dependerán del TIPO de referencia que se esté añadiendo. Si quieres ver algunas de las opciones más comunes, te recomiendo revisar esta página. Para incluir una referencia cruzada lo único que tienes que hacer es: @CLAVE. Si tomamos como ejemplo el artículo de Knuth de 1984 sobre la programación literal sería @Knuth_1984 para una referencia en el texto, Knuth (1984), o [@Knuth_1984] para una referencia dentro de paréntesis (Knuth, 1984). En cualquiera de los dos casos, si estás viendo el material renderizado, asegúrate de pasar el cursor sobre las citas, y verás que aparece la referencia bibliográfica completa. Para agregar la lista de referencias al final del texto, debes de agregar el div #refs y tener un encabezado de referencias al final del documento, tal que:\n# Referencias{.unnumbered}\n::: {#ref}\n:::\n\n\n\n\n\n\nImportante\n\n\n\nLa clave del divisor en el bloque mostrado es #ref, pero debería de ser #refs. Esto es para evitar que Quarto se confunda y ponga aquí las referencias en vez de en la sección correspondiente. También puedes cambiar el nombre del encabezado, si así lo deseas, o retirar la anotación {.unnumbered} si quieres que la sección esté numerada.\n\n\n\n\n\n\n\n\nNota\n\n\n\nYo construyo mis archivos .bib a mano, pero no es necesario. Si quieres pasar de un listado de referencias que ya tienes en Word puedes considerar text2bib o Edifix (1. OJO con las opciones; 2. Edifix es de pago). Si quieres una interfaz gráfica para manejar tus archivos .bib, puedes considerar JabRef. Finalmente, gestores de referencias como Mendeley o ReadCube Papers permiten exportar las referencias en formato bib."
  },
  {
    "objectID": "c02_intro_rs.html#consideraciones-sobre-quarto",
    "href": "c02_intro_rs.html#consideraciones-sobre-quarto",
    "title": "2  Introducción a RStudio y Quarto",
    "section": "3.4 Consideraciones sobre Quarto",
    "text": "3.4 Consideraciones sobre Quarto\nAunque Quarto es extremadamente potente y flexible, es importante tener presente algunas cosas. La primera es que NO hay manera de que en una sola sesión yo pueda explicarte con lujo de detalle todas sus funciones y posibilidades, para eso prefiero dirigirte a la (bastante completa) guía de Quarto. Otra consideración es que, aunque puedes exportar tus documentos como PDF, Word o ePUB u otros, mi recomendación es que siempre que tengas la libertad exportes a un HTML, que es un poco más permisivo con líneas de código muy largas, o al mostrar tablas con muchas columnas. Si NECESITAS de un PDF, asegúrate de tener instalada alguna distribución de LaTeX o, si no quieres la instalación completa, cuando menos asegurarte de haber instalado TinyTeX como sugerimos en la sesión de preparación, de lo contrario NO podrás exportar tus reportes a PDF."
  },
  {
    "objectID": "c02_intro_rs.html#ejercicio",
    "href": "c02_intro_rs.html#ejercicio",
    "title": "2  Introducción a RStudio y Quarto",
    "section": "3.5 Ejercicio",
    "text": "3.5 Ejercicio\nUtilizando Quarto genera un documento HTML con tabla de contenidos en el que te presentes. Las características son:\n\nTítulo: tu nombre\nIncluye tu correo electrónico en el preámbulo\nIncluye las secciones:\n\nGrado académico e institución de procedencia\nMotivación para tomar el curso\nExpectativas sobre el curso (¿hay alguna técnica particular que quieras aprender/revisar?)\nLibro(s) favorito(s) (como cita(s) en el texto, incluyendo la(s) referencia(s) completa(s) al final del documento)\nUna captura de pantalla con tu ventana de RStudio. No importa si es con los ajustes por defecto, si la pusiste igual a la mía, o si pusiste un tema con colores estridentes."
  },
  {
    "objectID": "c02_intro_rs.html#qué-sigue",
    "href": "c02_intro_rs.html#qué-sigue",
    "title": "2  Introducción a RStudio y Quarto",
    "section": "3.6 ¿Qué sigue?",
    "text": "3.6 ¿Qué sigue?\nEn esta sesión revisamos muy someramente el poder de RStudio y Quarto, limitándonos a las funciones más indispensables y que se utilizan de manera cotidiana, pero hay mucho más por ver. Proyectos dentro de RStudio, elaborar libros/páginas web/tesis o incluso artículos científicos con formato editorial según los requerimientos de algunas revistas, solo por mencionar algunos.\nPor otra parte, una de las ventajas poco conocidas de trabajar con archivos de texto simple (.R, .Rmd o .qmd), al menos en el área de ciencias biológicas, es el poder aprovechar al máximo sistemas de gestión de versiones, incluyendo por supuesto al más famoso: git. Muy posiblemente te hayas encontrado en algún momento con GitHub, que no era otra cosa mas que un almacén público de repositorios git. Ahora es un servidor capaz de mantener servicios simples en ejecución o, incluso, alojar páginas web (la página web de acompañamiento de este curso es un ejemplo), por lo que te recomiendo que le eches un ojo al funcionamiento de ambos (git y GitHub), y que revises la integración de git en RStudio. Personalmente utilizo solo la línea de comandos, pero es una herramienta más dentro de nuestro IDE y que puede resultarte más intuitiva.\n\n\n\n\nKnuth DE. 1984. Literate Programming. The Computer Journal 27:97-111. DOI: 10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "c03_bases_r.html#el-lenguaje-de-programación-r",
    "href": "c03_bases_r.html#el-lenguaje-de-programación-r",
    "title": "3  Bases de R",
    "section": "3.1 El lenguaje de programación R",
    "text": "3.1 El lenguaje de programación R\nEn la sesión anterior hablamos de RStudio como una interfaz gráfica a R, pero no fuimos más allá de decir que R es un lenguaje de programación que se ejecuta en consola. Pues bien, R es, y cito textualmente, “un lenguaje y ambiente para la computación estadística y la creación de gráficos” (R Core Team, 2022). ¿En castellano? Es un lenguaje creado especialmente para procedimientos estadísticos y el graficado de datos. Es un software libre, por lo que además de ser gratuito es auditable (i.e., cualquiera puede revisar el código fuente), “cualquiera” puede contribuir (ojo a las comillas). En su código fuente base (R base) incluye diversos algoritmos, modelos y distribuciones de probabilidad, aunque también es fácilmente extensible por medio de paquetes o librerías.\nEn este punto me dirás “Ok, Arturo, de acuerdo con lo que dices, pero ¿por qué se ha vuelto tan popular en análisis bioestadísticos?” Pues tiene que ver con varios factores. El primero es que es un lenguaje de alto nivel; es decir, está hecho para ser leído por humanos y no por computadoras. El segundo es que es interactivo, por lo que podemos ir modificando el código y ver su salida en tiempo real sin necesidad de compilar primero el código. El tercero tiene que ver su extensibilidad. Hay una gran variedad de librerías que agregan funcionalidades particulares, que van desde funcionalidades muy generales hasta la implementaciones muy particulares dependientes del área de investigación. ¿Algunos ejemplos? En tidyverse tenemos un dialecto enfocado a la ciencia de datos y la programación funcional. En ggplot2 tenemos un potente graficador. En vegan tenemos una gran cantidad de funciones para problemas de ecología de comunidades. En SIBER tenemos una forma de estimar tamaños de nichos isotópicos. En STAN (rstan, para ser más preciso) tenemos un ambiente general para inferencia Bayesiana. En fin, hay tantas librerías como problemas a resolver, y continuamente se añaden nuevas, pero antes de saltar a utilizarlas necesitamos entender las bases de R base (je)."
  },
  {
    "objectID": "c03_bases_r.html#objetos",
    "href": "c03_bases_r.html#objetos",
    "title": "3  Bases de R",
    "section": "3.2 Objetos",
    "text": "3.2 Objetos\nR es un lenguaje en el cual domina el paradigma de la programación orientada a objetos; i.e., en R todo es un objeto. El método de creación es el mismo para todos los casos: utilizar el símbolo de asignación <- (atajo de teclado alt -). Como ejemplo, creemos un objeto que contenga el texto “¡Hola Mundo!”:\n\ntexto <- \"¡Hola mundo!\"\n\nNotarás que no hubo salida ni en la libreta ni en la consola. Esto quiere decir que el objeto fue creado satisfactoriamente, y ahora podemos acceder o utilizar ese texto llamando al objeto texto:\n\ntexto\n\n[1] \"¡Hola mundo!\"\n\n\n\n3.2.1 Consideraciones\nAunque podemos poner virtualmente cualquier nombre a nuestros objetos, es necesario que tengamos algunas cosas en cuenta:\n\nNo empezar nombres de objetos con números.\nNo utilizar nombres de funciones u otros objetos creados anteriormente: enmascaramiento (funciones) o sobre-escritura (variables)\nEvitar empezar con punto (.), pues el objeto queda oculto del ambiente de trabajo\nUtilizar nombres cortos, pero lo más descriptivos posibles (amundsen_plot >> plot)\nSe pueden utilizar _ o . dentro del nombre (como separadores, por ejemplo). La guía de estilo de R sugiere el uso de _, aunque la guía de estilo de Google para R sugiere el uso de CamelCase (AmundsenPlot). Lo más importante para uso personal/interno es ser consistente y evitar mezclar estilos."
  },
  {
    "objectID": "c03_bases_r.html#librerías-y-funciones",
    "href": "c03_bases_r.html#librerías-y-funciones",
    "title": "3  Bases de R",
    "section": "3.3 Librerías y funciones",
    "text": "3.3 Librerías y funciones\nAunque en R predomina el paradigma de la programación orientada a objetos, también podemos hacer uso del paradigma funcional de la programación; es decir, podemos construir nuestros programas mediante la aplicación y construcción de funciones. Aunque en este momento te suene poco intuitivo o como si estuviéramos comenzando por el final, vamos a comenzar con las funciones, para después hablar de los tipos de objetos que tenemos, pues resulta que para crear cierto tipo de objetos necesitamos utilizar funciones.\n\n3.3.1 Funciones\nLas funciones representan una serie de métodos para obtener un resultado, para utilizarlas emplearemos la estructura fun(arg1, arg2, ..., argn), donde arg*representa un argumento; es decir, un elemento “pasado” a la función para regular sus procesos. El ejemplo más simple, y con el cuál ya hemos estado en contacto, es la función print():\n\nprint(\"¡Hola Mundo!\")\n\n[1] \"¡Hola Mundo!\"\n\n\nEsta función “imprime” un resultado en pantalla, pero puede ser utilizada para mucho más que para imprimir texto. A final de cuentas, este mismo resultado lo podemos obtener simplemente poniendo el texto en la consola. ¿Qué puede hacer la función? Eso lo podemos ver consultando la ayuda de la función, utilizando el operador ? antes del nombre, tal que:\n\n?print\n\nTe darás cuenta que no tenemos una salida en la libreta, lo cuál es normal, pues la ayuda tiene su propio espacio en el ambiente gráfico. Si estuviéramos trabajando solo con la línea de comandos en el terminal, ahí sí veríamos la salida en la consola. Una de las partes más importantes de la documentación de ayuda es que tenemos los argumentos de la función; es decir, qué necesita la función para realizar su trabajo, así como el papel de cada uno de estos elementos.\nPor otra parte, cuando declaramos una función los llamaremos parámetros. Para declarar una función generaremos una variable cuyo nombre será el nombre “llamable” de la función, a la cual asignaremos el cuerpo de la función utilizando function(parámetros){cuerpo}. Para ejemplificar, creemos una función para calcular la media aritmética de un conjunto de números x:\n\nmedia.arit <- function(x){\n  # Mejora: Trabajo con NAs\n  suma <- sum(x)\n  n <- length(x)\n  return(suma/n)\n}\n\nmedia.arit(1:5)\n\n[1] 3\n\n\n¿Cuándo declarar una función? Cuando tengamos un flujo de trabajo que consista de exactamente los mismos pasos con posibles pequeñas variaciones, el cuál aplicaremos de manera continua. De hecho, uno de los productos que obtendremos de estas reuniones será un script con las funciones relacionadas con los análisis tróficos IIR, PSIRI, gráficos de Amundsen, etc. para que puedan ser utilizados de manera sencilla, repetitiva, y consistente.\nPor último, debido a que un gran número de funciones son altamente regulables (cuentan con un gran número de parámetros), te recomiendo hacer uso extensivo (excesivo) de la ayuda (?fun) para que obtengas de primera mano el conocimiento sobre su objetivo, sus entradas, su salida y, en consecuencia, ayudarte a prevenir o solucionar errores.\n\n\n3.3.2 Librerías\nAfortunadamente para nostros, muchas de las técnicas o procedimientos que realizamos ya fueron programados por alguien con más experiencia, y usualmente compilados en una librería de R. Hay una gran cantidad de librerías disponibles, algunas instalables directamente desde CRAN (R), mientras que otras son instalables desde repositorios públicos como GitHub. Por lo general, la gran mayoría las instalaremos desde R, utilizando la función install.packages(\"package\", dependencies = T). Esta función buscará la versión más reciente del paquete solicitado y la descargará e instalará para que pueda ser utilizada por nosotros. Un ejemplo:\ninstall.packages(\"tidyverse\", dependencies = T)\nEsta línea descargará el paquete tidyverse, que es un “súper” paquete formado por muchos otros paquetes que forman un “dialecto” dentro de R. Por el momento no te preocupes por eso, solo lo instalamos para evitarnos muchas descargas independientes de paquetes que pueden llegar a serte extremadamente útiles, tal como ggplot2. Es importante mencionar que una cosa es instalar la librería y otra cosa es utilizar la librería, para lo cual necesitamos de la función library(package), tal que:\n\nlibrary(ggplot2)\n\nUna vez que hicimos esto ya podemos utilizar TODAS las funciones que forman parte de la librería. ¿Y si solo quiero utilizar una función particular? En ese caso puedes utilizar el operador ::. Probemos viendo la ayuda de la función str_extract de la librería stringr que forma parte del tidyverse:\n\n?stringr::str_extract\n\nAhora que sabemos qué es una función, qué es una librería y cómo utilizarlos, vayamos a explorar el otro tipo de objetos: las variables.\n\n\n\n\n\n\nNota\n\n\n\nSi te interesa ver la referencia de una librería puedes hacerlo con la función citation(\"librería\")."
  },
  {
    "objectID": "c03_bases_r.html#variables",
    "href": "c03_bases_r.html#variables",
    "title": "3  Bases de R",
    "section": "3.4 Variables",
    "text": "3.4 Variables\nA diferencia de una función en la cual almacenamos series de pasos para obtener un resultado, una variable nos permite almacenar todo lo demás: resultados, números, texto, tablas, e incluso otras variables. Su declaración la vimos arriba: con el símbolo de asignación (<-). Para imprimir el resultado en pantalla podemos llamar a la variable o utilizar la función print(var):\n\nvar <- 1:5\nprint(var)\n\n[1] 1 2 3 4 5"
  },
  {
    "objectID": "c03_bases_r.html#tipos-de-variables",
    "href": "c03_bases_r.html#tipos-de-variables",
    "title": "3  Bases de R",
    "section": "3.5 Tipos de variables",
    "text": "3.5 Tipos de variables\nExisten dos tipos de variables, los cuales a su vez se subdividen en otros tipos. Para conocer el tipo de una variable utilizamos la función typeof(var), mientras que las funciones is.*() nos permiten probar si una variable es de un tipo en específico (e.g. is.character(var)).\n\n3.5.1 Datos\nLas variables que contienen un solo elemento se conocen como datos:\n\nCharacter: Cadena de caracteres, indicadas por comillas dobles o sencillas:\n\n\nchar <- \"a\"\ntypeof(char)\n\n[1] \"character\"\n\n\n\nInteger: Números enteros, indicados por la letra “L” después del número:\n\n\ninteger <- 5L\ntypeof(integer)\n\n[1] \"integer\"\n\n\n\nDouble: Fracciones, también conocidos como floating points:\n\n\ndbl <- 7/5\ntypeof(dbl)\n\n[1] \"double\"\n\n\n\nLogical: Valor lógico o booleano. Solo puede tomar dos valores: TRUE o FALSE o sus abreviaturas T o F\n\n\nbool <- is.double(dbl)\nprint(paste('valor: ', bool))\n\n[1] \"valor:  TRUE\"\n\nprint(paste('tipo: ', typeof(bool)))\n\n[1] \"tipo:  logical\"\n\n\n\nComplex: Números complejos, con una parte real y una imaginaria:\n\n\ncomp.n <- 8+3i\ntypeof(comp.n)\n\n[1] \"complex\"\n\n\n\n\n3.5.2 Estructuras/arreglos\nLas estructuras son colecciones de valores, cada una con sus propiedades y sus métodos de acceso a los valores que las conforman (indexación/indización):\n\n3.5.2.1 Vector\nLa estructura más básica. Una colección unidimensional de elementos. Las funciones para crearlos son: c(), la cual combina una serie de elementos en un vector (mismo tipo) o una lista (diferentes tipos); vector(mode, length): genera un vector “vacío” con longitud (número de elementos) length y tipo de datos 'mode'.\n\nvect_1 <- c(1:5)\nvect_2 <- vector(mode = 'double', length = 5)\nprint(vect_1)\n\n[1] 1 2 3 4 5\n\nprint(vect_2)\n\n[1] 0 0 0 0 0\n\n\nPara indexar un vector utilizamos: var[i], donde i representa la posición del (los) elemento(s) de interés:\n\nprint(vect_1[4])\n\n[1] 4\n\nprint(vect_1[2:3])\n\n[1] 2 3\n\n\n\n\n3.5.2.2 Factor\nRepresentan variables categoricas. Contienen los valores de la variable así como los valores posibles que puede tomar (niveles). Se crean con la función factor(x, levels, labels), donde x representa los valores de la variable, levels representa los posibles niveles y labels (opcional) representan etiquetas de cada nivel:\n\nfact.1 <- factor(x = c('a', 'b', 'c'), \n                 levels = c('a', 'b', 'c', 'd', 'e'))\nfact.1\n\n[1] a b c\nLevels: a b c d e\n\n\n\n\n3.5.2.3 Matrix\nUna estructura bidimensional (columnas/renglones). Se generan utilizando la función matrix(data, nrow, ncol, byrow), donde data representa la colección de objetos que formarán la matriz, nrow y ncol el número de renglones y columnas, respectivamente, y byrow si se llenará por renglones (FALSE) o por columnas (TRUE, por defecto)\n\nmat_1 <- matrix(c(T, F, F, T), nrow = 2, ncol = 2)\nmat_1\n\n      [,1]  [,2]\n[1,]  TRUE FALSE\n[2,] FALSE  TRUE\n\n\nPara indexar una matriz utilizaremos también corchetes; sin embargo, indicaremos el par renglón,columna donde se ubica el elemento:\n\nprint(mat_1[1,1])\n\n[1] TRUE\n\nprint(mat_1[2,1])\n\n[1] FALSE\n\n\nSi quisieramos indexar toda una dimensión (renglón o columna), utilizaríamos el mismo método, dejando en blanco la dimensión contraria; es decir, si nos interesa una columna, dejaremos en blanco el número de renglón y si nos interesa un renglón dejaremos en blanco el número de columna:\n\nprint(mat_1[,2])\n\n[1] FALSE  TRUE\n\nprint(mat_1[1,])\n\n[1]  TRUE FALSE\n\n\nOJO: print(mat_1[c(1,1)]) NO da la diagonal de la matriz, esa la obtenemos con diag(mat1), sino que repite 2 veces el primer elemento de la matriz\n\nmat_1[c(2,1),1]\n\n[1] FALSE  TRUE\n\n\n\n\n3.5.2.4 DataFrame\nEl DataFrame es la estructura con la que más comúnmente estaremos en contacto. Es una tabla completa que, a diferencia de la matriz, contiene nombres de columnas. Tiene dos particularidades que hay que considerar: 1) todos los elementos que forman a cada columna deberán ser del mismo tipo y 2) El número de renglones de todas las columnas debe de ser el mismo. Se crean utilizando la función data.frame(col_name = data, ...):\n\ndf_1 <- data.frame(col_a = c(0:5), \n                   col_b = c(20:25), \n                   col_c = c(15:20))\n# Nota: Si no se indica el nombre de las columnas este será asignado automáticamente\ndf_1\n\n\n\n  \n\n\n\nExisten distintos modos de indexar un DataFrame. El primero de ellos var$col_name:\n\ndf_1$col_a\n\n[1] 0 1 2 3 4 5\n\n\nEjercicio renglones 1 y 4 de la columna c:\n\ndf_1$col_c[c(1,4)]\n\n[1] 15 18\n\n\nComo vemos, este modo de indexación extrae la columna completa en forma de un vector, por lo que si queremos accesar un valor en particular solo habrá que utilizar ese método de indexación:\n\ndf_1$col_b[4]\n\n[1] 23\n\n\nFinalmente, también podemos utilizar el método de indexación de matrices, recordando que se especifica el par renglón, columna:\n\ndf_1[4,2]\n\n[1] 23\n\n\nEsta es la estructura con la que más debemos de familiarizarnos, pues la mayor parte de nuestros datos los representamos en ella. ¿Siempre debemos de ingresar los datos manualmente? Para nada, tenemos todo un abanico de funciones que nos permiten cargar datos directamente de archivos, pero eso lo veremos más adelante.\n\n\n3.5.2.5 List\nLas listas son una colección de cualquier combinación de datos o estructuras, incluyendo otras listas:\n\nl_1 <- list(df_1, mat_1, vect_1)\nprint(l_1)\n\n[[1]]\n  col_a col_b col_c\n1     0    20    15\n2     1    21    16\n3     2    22    17\n4     3    23    18\n5     4    24    19\n6     5    25    20\n\n[[2]]\n      [,1]  [,2]\n[1,]  TRUE FALSE\n[2,] FALSE  TRUE\n\n[[3]]\n[1] 1 2 3 4 5\n\n\nEn la salida de arriba vemos el método de indexación: var[[i]][j,k], donde i representa el número de objeto en la lista y j,k el par renglón,columna (de aplicar). En el caso de DataFrames podemos seguir utilizando el operador $ para utilizar los noombres de columnas:\n\nl_1[[1]]$col_a[6]\n\n[1] 5\n\n\nAhora que hemos hablado de todos los tipos de estructuras, y antes de encaminarnos hacia los procesos de automatización, hablemos de cómo cargar nuestros datos en R.\nEjercicios:\n\nl_1[[2]][2,2]\n\n[1] TRUE\n\nl_1[[3]][5]\n\n[1] 5\n\nl_1[[1]]$col_b[3:4]\n\n[1] 22 23\n\nl_1[[1]]$col_a[c(6,1)]\n\n[1] 5 0"
  },
  {
    "objectID": "c03_bases_r.html#carga-de-datos",
    "href": "c03_bases_r.html#carga-de-datos",
    "title": "3  Bases de R",
    "section": "3.6 Carga de datos",
    "text": "3.6 Carga de datos\nEl cómo carguemos nuestros datos depende de varios factores: a) el formato del archivo en el que estén archivados, b) el cómo esté acomodada la información, c) qué necesitemos para hacer los análisis posteriores. El ejemplo más simple es cargar un archivo de texto separado por comas, en el cuál las comas separan las columnas y los saltos de línea los renglones. Tomemos como ejemplo el archivo \"datos1.csv\":\n\ndatos1 <- read.table(\"datos/datos1.csv\", sep = \",\", header = T)\n\nPodemos verificar la información obteniendo el encabezado del data.frame:\n\nhead(datos1)\n\n\n\n  \n\n\n\nLos archivos separados por comas son uno de los formatos más comunes, por lo que R cuenta con una función dedicada (la función read.table() con valores predefinidos):\n\ndatos1 <- read.csv(\"datos/datos1.csv\")\nhead(datos1)\n\n\n\n  \n\n\n\nAquí todo se cargó sin ningún problema porque el archivo estaba listo para ser leído, pero esto no siempre es el caso. Por ejemplo, los datos pueden estar en la segunda hoja de un archivo Excel, la cuál tiene 5 renglones de encabezado dando una descripción de los datos y en el renglón 6 están dispuestos los nombres de las variables. Además, sabemos que vamos a realizar un análisis de agrupamientos jerárquicos (clúster), el cuál requiere que los nombres de los individuos estén marcados en los nombres de los renglones (Ver archivo datos2.xlsx):\n\ndatos2 <- read.table(\"datos/datos2.xlsx\")\n\nEsto, evidentemente, da un error, pues le dimos a la función read.table() un archivo que no es de texto simple, sino un Excel.\nVamos entonces por partes:\n\nFormato: es un archivo Excel, por lo que hay que utilizar una función que permita leer ese tipo de archivo. En nuestro caso utilizaremos la función readxl::read_xlsx(). Aquí no obtendremos ningún error, pues el tipo de archivo es el correcto. Lo único que obtenemos es un mensaje (New names:) que nos diría a qué columnas se les asignó nombres nuevos (y cuáles).\n\n\ndatos2 <- readxl::read_xlsx(\"datos/datos2.xlsx\")\n\nNew names:\n• `Lp14-C` -> `Lp14-C...94`\n• `Lp14-C` -> `Lp14-C...109`\n\n\nPero, ¿qué pasa si leemos el encabezado? Resulta que la función cargó la primera hoja del excel, cuando en realidad nosotros queríamos la segunda\n\nhead(datos2)\n\n\n\n  \n\n\n\nNecesitamos entonces indicar explícitamente que queremos se cargue la segunda hoja:\n\ndatos2 <- readxl::read_xlsx(\"datos/datos2.xlsx\", sheet = 2)\n\nNew names:\n• `` -> `...2`\n• `` -> `...3`\n• `` -> `...4`\n• `` -> `...5`\n• `` -> `...6`\n• `` -> `...7`\n• `` -> `...8`\n• `` -> `...9`\n• `` -> `...10`\n• `` -> `...11`\n• `` -> `...12`\n• `` -> `...13`\n• `` -> `...14`\n• `` -> `...15`\n• `` -> `...16`\n• `` -> `...17`\n• `` -> `...18`\n• `` -> `...19`\n• `` -> `...20`\n• `` -> `...21`\n• `` -> `...22`\n• `` -> `...23`\n• `` -> `...24`\n• `` -> `...25`\n• `` -> `...26`\n• `` -> `...27`\n• `` -> `...28`\n• `` -> `...29`\n• `` -> `...30`\n• `` -> `...31`\n• `` -> `...32`\n• `` -> `...33`\n• `` -> `...34`\n• `` -> `...35`\n• `` -> `...36`\n• `` -> `...37`\n• `` -> `...38`\n• `` -> `...39`\n• `` -> `...40`\n• `` -> `...41`\n• `` -> `...42`\n• `` -> `...43`\n• `` -> `...44`\n• `` -> `...45`\n• `` -> `...46`\n• `` -> `...47`\n• `` -> `...48`\n• `` -> `...49`\n• `` -> `...50`\n\nhead(datos2)\n\n\n\n  \n\n\n\n\nSaltar renglones: Ya tenemos la hoja que nos interesa, el problema es que cargó el encabezado como renglones con observaciones, por lo que hay que saltarlos:\n\n\ndatos2 <- readxl::read_xlsx(\"datos/datos2.xlsx\",\n                            sheet = 2,\n                            skip = 5)\nhead(datos2)"
  },
  {
    "objectID": "c03_bases_r.html#operaciones-comunes",
    "href": "c03_bases_r.html#operaciones-comunes",
    "title": "3  Bases de R",
    "section": "3.7 Operaciones comunes",
    "text": "3.7 Operaciones comunes\nComo ya vimos, no siempre vamos a obtener la información en el formato que necesitamos. Aunque podemos solventar algunas de estas carencias durante la carga de los archivos, a veces necesitamos “masajear” los datos o “manipularlos” para llevarlos a lo que las funciones que nos interesan nos piden. Tomemos como ejemplo los datos de la hoja número 1 del archivo datos2.xlsx:\n\ndatos3 <- readxl::read_xlsx(\"datos/datos2.xlsx\", sheet = 1)\n\nNew names:\n• `Lp14-C` -> `Lp14-C...94`\n• `Lp14-C` -> `Lp14-C...109`\n\nhead(datos3)\n\n\n\n  \n\n\n\n\n3.7.1 Transposición\nEn estos datos las presas están en los renglones, y los individuos de los depredadores en las columnas. Aunque esta disposición no tiene fundamentalmente nada de malo, normalmente las instancias (observaciones individuales/réplicas) están en los renglones, y las variables (presas) en las columnas. Es necesario entonces tranponer los datos. Esto lo podemos hacer de manera sencilla con la función t():\n\nhead(t(datos3))\n\n      [,1]              [,2]          [,3]           [,4]       \nPrey  \"Alpheus_lottini\" \"Alpheus_spp\" \"Alpheus_umbo\" \"Amphipods\"\nCu1-C \"0\"               \"0\"           \"0\"            \"0\"        \nCu2-C \"0\"               \"4\"           \"0\"            \"0\"        \nCu3-C \"0\"               \"0\"           \"0\"            \"0\"        \nCz1-C \" 0\"              \" 0\"          \" 0\"           \" 6\"       \nAr1-C \"0\"               \"0\"           \"0\"            \"0\"        \n      [,5]                [,6]               [,7]                   \nPrey  \"Apogon_retrosella\" \"Appendicularians\" \"Axoclinus_nigricaudis\"\nCu1-C \"0\"                 \"0\"                \"0\"                    \nCu2-C \"0\"                 \"0\"                \"0\"                    \nCu3-C \"0\"                 \"0\"                \"0\"                    \nCz1-C \" 0\"                \" 0\"               \" 0\"                   \nAr1-C \"0\"                 \"0\"                \"0\"                    \n      [,8]                   [,9]           [,10]                            \nPrey  \"Bittium_cerralvoense\" \"Chaetognaths\" \"Cirripedia_Chthamalus_anisopoma\"\nCu1-C \"0\"                    \"0\"            \"0\"                              \nCu2-C \"0\"                    \"0\"            \"0\"                              \nCu3-C \"0\"                    \"0\"            \"0\"                              \nCz1-C \" 0\"                   \" 0\"           \" 0\"                             \nAr1-C \"0\"                    \"0\"            \"0\"                              \n      [,11]                           [,12]                               \nPrey  \"Cladocerans_Penila_avirostris\" \"Cladocerans_Pseudovadne_tergestina\"\nCu1-C \"0\"                             \"0\"                                 \nCu2-C \"0\"                             \"0\"                                 \nCu3-C \"0\"                             \"0\"                                 \nCz1-C \" 0\"                            \" 0\"                                \nAr1-C \"0\"                             \"0\"                                 \n      [,13]                     [,14]                        [,15]            \nPrey  \"Copepods_Acartia_clausi\" \"Copepods_Calanus_pacificus\" \"Epitonium_canna\"\nCu1-C \"9\"                       \"0\"                          \"0\"              \nCu2-C \"0\"                       \"0\"                          \"0\"              \nCu3-C \"0\"                       \"0\"                          \"0\"              \nCz1-C \" 8\"                      \" 0\"                         \" 0\"             \nAr1-C \"0\"                       \"0\"                          \"0\"              \n      [,16]       [,17]          [,18]                     [,19]     \nPrey  \"Fish_eggs\" \"foraminifera\" \"Gnathophyllum_panamense\" \"Hidrozoa\"\nCu1-C \"0\"         \"0\"            \"0\"                       \"0\"       \nCu2-C \"0\"         \"0\"            \"2\"                       \"0\"       \nCu3-C \"0\"         \"0\"            \"0\"                       \"0\"       \nCz1-C \"13\"        \" 0\"           \" 0\"                      \" 0\"      \nAr1-C \"0\"         \"0\"            \"0\"                       \"0\"       \n      [,20]            [,21]                         [,22]                    \nPrey  \"Ichtyoplankton\" \"Larvae_crustaceans_megalopa\" \"larvae_crustaceans_zoea\"\nCu1-C \"6\"              \"0\"                           \"0\"                      \nCu2-C \"0\"              \"0\"                           \"0\"                      \nCu3-C \"0\"              \"0\"                           \"0\"                      \nCz1-C \" 0\"             \" 0\"                          \" 0\"                     \nAr1-C \"0\"              \"0\"                           \"0\"                      \n      [,23]                [,24]                 [,25]   [,26]               \nPrey  \"Liomera_cinctimana\" \"Litiopa_melanostoma\" \"Mysid\" \"Mytella_arciformis\"\nCu1-C \"0\"                  \"0\"                   \"0\"     \"0\"                 \nCu2-C \"0\"                  \"0\"                   \"0\"     \"0\"                 \nCu3-C \"0\"                  \"0\"                   \"0\"     \"0\"                 \nCz1-C \" 0\"                 \" 0\"                  \" 0\"    \" 0\"                \nAr1-C \"0\"                  \"0\"                   \"0\"     \"0\"                 \n      [,27]                 [,28]                 [,29]                \nPrey  \"Mytella_tumbezensis\" \"Nanocassiope_polita\" \"Nyctiphanes_simplex\"\nCu1-C \"0\"                   \"0\"                   \"0\"                  \nCu2-C \"0\"                   \"0\"                   \"0\"                  \nCu3-C \"0\"                   \"0\"                   \"0\"                  \nCz1-C \" 0\"                  \" 0\"                  \" 0\"                 \nAr1-C \"0\"                   \"0\"                   \"0\"                  \n      [,30]       [,31]      [,32]             [,33]               \nPrey  \"Ostracods\" \"Otoliths\" \"Palaemon_ritter\" \"Panopeus_purpureus\"\nCu1-C \"0\"         \"0\"        \"0\"               \"0\"                 \nCu2-C \"0\"         \"0\"        \"0\"               \"0\"                 \nCu3-C \"0\"         \"0\"        \"2\"               \"0\"                 \nCz1-C \" 0\"        \" 0\"       \" 0\"              \" 0\"                \nAr1-C \"0\"         \"0\"        \"0\"               \"0\"                 \n      [,34]                      [,35]              [,36]           \nPrey  \"Parviturbo_acuticostatus\" \"Parviturbo_erici\" \"Parviturbo_spp\"\nCu1-C \"0\"                        \"0\"                \"0\"             \nCu2-C \"0\"                        \"0\"                \"0\"             \nCu3-C \"0\"                        \"0\"                \"0\"             \nCz1-C \" 0\"                       \" 0\"               \" 0\"            \nAr1-C \"0\"                        \"0\"                \"0\"             \n      [,37]       [,38]              [,39]                   [,40]            \nPrey  \"Pteropods\" \"Quadrella_nitida\" \"Tagelus_californianus\" \"Tegula_globulus\"\nCu1-C \"0\"         \"0\"                \"0\"                     \"0\"              \nCu2-C \"0\"         \"0\"                \"0\"                     \"0\"              \nCu3-C \"0\"         \"0\"                \"0\"                     \"0\"              \nCz1-C \" 0\"        \" 0\"               \" 0\"                    \" 0\"             \nAr1-C \"0\"         \"0\"                \"0\"                     \"0\"              \n      [,41]            [,42]           [,43]                [,44]             \nPrey  \"Tegula_mariana\" \"Tellina_coani\" \"Trapezia_bidentata\" \"Trapezia_formosa\"\nCu1-C \"0\"              \"0\"             \"0\"                  \"0\"               \nCu2-C \"0\"              \"0\"             \"0\"                  \"0\"               \nCu3-C \"0\"              \"0\"             \"0\"                  \"0\"               \nCz1-C \" 0\"             \" 0\"            \" 0\"                 \" 0\"              \nAr1-C \"2\"              \"0\"             \"0\"                  \"0\"               \n      [,45]          [,46]              [,47]          [,48]     \nPrey  \"Trapezia_spp\" \"Ulva_dactylifera\" \"Ulva_lactuca\" \"Ulva_spp\"\nCu1-C \"0\"            \"0\"                \"0\"            \"0\"       \nCu2-C \"0\"            \"0\"                \"0\"            \"0\"       \nCu3-C \"0\"            \"0\"                \"0\"            \"0\"       \nCz1-C \" 0\"           \" 0\"               \" 0\"           \" 0\"      \nAr1-C \"0\"            \"0\"                \"0\"            \"0\"       \n      [,49]                                \nPrey  \"UOM (Unidentified Organic Material)\"\nCu1-C \"0\"                                  \nCu2-C \"0\"                                  \nCu3-C \"0\"                                  \nCz1-C \" 0\"                                 \nAr1-C \"0\"                                  \n\n\nEsto logró nuestro objetivo, aunque con un pequeño gran problema: toda la información es texto. ¿Por qué? Resulta que las columnas solo pueden contener datos de un solo tipo, por lo que al tener el texto de las especies presa todas las columnas son transformadas a cadenas de caracter. ¿Qué podemos hacer? Transponer los datos en tres pasos.\n\n\n3.7.2 “Rebanadas” (slices)\nEl primer paso es separar los nombres de las presas de los datos de los depredadores:\n\nprey <- datos3$Prey\nhead(prey)\n\n[1] \"Alpheus_lottini\"   \"Alpheus_spp\"       \"Alpheus_umbo\"     \n[4] \"Amphipods\"         \"Apogon_retrosella\" \"Appendicularians\" \n\ncounts <- datos3[,2:ncol(datos3)]\nhead(counts)\n\n\n\n  \n\n\n\nTransponer la matriz de conteos:\n\ncountst <- t(counts)\nhead(countst)\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]\nCu1-C    0    0    0    0    0    0    0    0    0     0     0     0     9\nCu2-C    0    4    0    0    0    0    0    0    0     0     0     0     0\nCu3-C    0    0    0    0    0    0    0    0    0     0     0     0     0\nCz1-C    0    0    0    6    0    0    0    0    0     0     0     0     8\nAr1-C    0    0    0    0    0    0    0    0    0     0     0     0     0\nPs1-C    0    0    0    0    0    0    0    0    0     0     0     0     0\n      [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25]\nCu1-C     0     0     0     0     0     0     6     0     0     0     0     0\nCu2-C     0     0     0     0     2     0     0     0     0     0     0     0\nCu3-C     0     0     0     0     0     0     0     0     0     0     0     0\nCz1-C     0     0    13     0     0     0     0     0     0     0     0     0\nAr1-C     0     0     0     0     0     0     0     0     0     0     0     0\nPs1-C     0     0     0     0     0     0     0     0     0     0     0     0\n      [,26] [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37]\nCu1-C     0     0     0     0     0     0     0     0     0     0     0     0\nCu2-C     0     0     0     0     0     0     0     0     0     0     0     0\nCu3-C     0     0     0     0     0     0     2     0     0     0     0     0\nCz1-C     0     0     0     0     0     0     0     0     0     0     0     0\nAr1-C     0     0     0     0     0     0     0     0     0     0     0     0\nPs1-C     0     0     0    15     0     0     0     0     0     0     0     0\n      [,38] [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49]\nCu1-C     0     0     0     0     0     0     0     0     0     0     0     0\nCu2-C     0     0     0     0     0     0     0     0     0     0     0     0\nCu3-C     0     0     0     0     0     0     0     0     0     0     0     0\nCz1-C     0     0     0     0     0     0     0     0     0     0     0     0\nAr1-C     0     0     0     2     0     0     0     0     0     0     0     0\nPs1-C     0     0     0     0     0     0     0     0     0     0     0     0\n\n\n\n\n3.7.3 Cambiar nombres de columnas\nPara acceder o asignar los nombres de las columnas o renglones de arreglos bidimensionales podemos utilizar los atributos colnames(data) y rownames(data):\n\ncolnames(countst) <- prey\nhead(countst)\n\n      Alpheus_lottini Alpheus_spp Alpheus_umbo Amphipods Apogon_retrosella\nCu1-C               0           0            0         0                 0\nCu2-C               0           4            0         0                 0\nCu3-C               0           0            0         0                 0\nCz1-C               0           0            0         6                 0\nAr1-C               0           0            0         0                 0\nPs1-C               0           0            0         0                 0\n      Appendicularians Axoclinus_nigricaudis Bittium_cerralvoense Chaetognaths\nCu1-C                0                     0                    0            0\nCu2-C                0                     0                    0            0\nCu3-C                0                     0                    0            0\nCz1-C                0                     0                    0            0\nAr1-C                0                     0                    0            0\nPs1-C                0                     0                    0            0\n      Cirripedia_Chthamalus_anisopoma Cladocerans_Penila_avirostris\nCu1-C                               0                             0\nCu2-C                               0                             0\nCu3-C                               0                             0\nCz1-C                               0                             0\nAr1-C                               0                             0\nPs1-C                               0                             0\n      Cladocerans_Pseudovadne_tergestina Copepods_Acartia_clausi\nCu1-C                                  0                       9\nCu2-C                                  0                       0\nCu3-C                                  0                       0\nCz1-C                                  0                       8\nAr1-C                                  0                       0\nPs1-C                                  0                       0\n      Copepods_Calanus_pacificus Epitonium_canna Fish_eggs foraminifera\nCu1-C                          0               0         0            0\nCu2-C                          0               0         0            0\nCu3-C                          0               0         0            0\nCz1-C                          0               0        13            0\nAr1-C                          0               0         0            0\nPs1-C                          0               0         0            0\n      Gnathophyllum_panamense Hidrozoa Ichtyoplankton\nCu1-C                       0        0              6\nCu2-C                       2        0              0\nCu3-C                       0        0              0\nCz1-C                       0        0              0\nAr1-C                       0        0              0\nPs1-C                       0        0              0\n      Larvae_crustaceans_megalopa larvae_crustaceans_zoea Liomera_cinctimana\nCu1-C                           0                       0                  0\nCu2-C                           0                       0                  0\nCu3-C                           0                       0                  0\nCz1-C                           0                       0                  0\nAr1-C                           0                       0                  0\nPs1-C                           0                       0                  0\n      Litiopa_melanostoma Mysid Mytella_arciformis Mytella_tumbezensis\nCu1-C                   0     0                  0                   0\nCu2-C                   0     0                  0                   0\nCu3-C                   0     0                  0                   0\nCz1-C                   0     0                  0                   0\nAr1-C                   0     0                  0                   0\nPs1-C                   0     0                  0                   0\n      Nanocassiope_polita Nyctiphanes_simplex Ostracods Otoliths\nCu1-C                   0                   0         0        0\nCu2-C                   0                   0         0        0\nCu3-C                   0                   0         0        0\nCz1-C                   0                   0         0        0\nAr1-C                   0                   0         0        0\nPs1-C                   0                  15         0        0\n      Palaemon_ritter Panopeus_purpureus Parviturbo_acuticostatus\nCu1-C               0                  0                        0\nCu2-C               0                  0                        0\nCu3-C               2                  0                        0\nCz1-C               0                  0                        0\nAr1-C               0                  0                        0\nPs1-C               0                  0                        0\n      Parviturbo_erici Parviturbo_spp Pteropods Quadrella_nitida\nCu1-C                0              0         0                0\nCu2-C                0              0         0                0\nCu3-C                0              0         0                0\nCz1-C                0              0         0                0\nAr1-C                0              0         0                0\nPs1-C                0              0         0                0\n      Tagelus_californianus Tegula_globulus Tegula_mariana Tellina_coani\nCu1-C                     0               0              0             0\nCu2-C                     0               0              0             0\nCu3-C                     0               0              0             0\nCz1-C                     0               0              0             0\nAr1-C                     0               0              2             0\nPs1-C                     0               0              0             0\n      Trapezia_bidentata Trapezia_formosa Trapezia_spp Ulva_dactylifera\nCu1-C                  0                0            0                0\nCu2-C                  0                0            0                0\nCu3-C                  0                0            0                0\nCz1-C                  0                0            0                0\nAr1-C                  0                0            0                0\nPs1-C                  0                0            0                0\n      Ulva_lactuca Ulva_spp UOM (Unidentified Organic Material)\nCu1-C            0        0                                   0\nCu2-C            0        0                                   0\nCu3-C            0        0                                   0\nCz1-C            0        0                                   0\nAr1-C            0        0                                   0\nPs1-C            0        0                                   0\n\n\n\n\n3.7.4 Transformaciones\nEl resultado de las operaciones anteriores es una matriz; sin embargo, podemos pasarlo a un data.frame:\n\ncountst <- as.data.frame(countst)\nhead(countst)\n\n\n\n  \n\n\n\n\n\n3.7.5 Añadir vectores como columnas\nAhora tenemos un data.frame; sin embargo, tenemos las claves como nombres de los renglones y, según qué querramos realizar, podemos necesitar que estas formen su propia columna. Una forma de hacerlo es: 1) extraer los nombres de los renglones y 2) añadirlos como una columna adicional:\n\nkeys <- rownames(countst)\ncountst <- cbind(keys, countst)\nhead(countst)"
  },
  {
    "objectID": "c03_bases_r.html#operadores-lógicos",
    "href": "c03_bases_r.html#operadores-lógicos",
    "title": "3  Bases de R",
    "section": "3.8 Operadores lógicos",
    "text": "3.8 Operadores lógicos\nLos operadores lógicos nos sirven para hacer comparaciones y obtener un resultado booleano (T o F). Los más comunes son: 1. cond1|cond2: Condicional “O”. T si se cumple alguna de las dos condiciones\n\nc <- 5L\nis.integer(c)|is.double(c)\n\n[1] TRUE\n\n\n\ncond1&cond2: Condicional “Y”. T si se cumplen ambas condiciones\n\n\nis.integer(c)&(c>3)\n\n[1] TRUE\n\n\n\n<, >: Comparaciones, menor qué o mayor qué\n\n\nprint(c<10)\n\n[1] TRUE\n\nprint(c>5)\n\n[1] FALSE\n\n\n\n<=, >=: Comparaciones, menor o igual qué; mayor o igual qué.\na!=b: Desigualdad, T si a es diferente de b\n\n\nc!=5\n\n[1] FALSE"
  },
  {
    "objectID": "c03_bases_r.html#automatización",
    "href": "c03_bases_r.html#automatización",
    "title": "3  Bases de R",
    "section": "3.9 Automatización",
    "text": "3.9 Automatización\nEl primer paso de la automatización es generar funciones que te permitan realizar la misma acción múltiples veces y no cometer algún error en dichas repeticiones. Esta práctica se deriva de una de las máximas más importantes en programación: “Don’t repeat yourself” (DRY, no te repitas a ti mismo); es decir, dejar que la computadora haga las repeticiones por sí mismas. En la práctica, esto implica no estar copiando y pegando el mismo bloque de código una y otra vez y luego modificarlo manualmente, sino que escribirlo solo una vez y luego decirle a la computadora que repita esa acción n veces, modificando algún(os) argumento(s), o que cambie el comportamiento en función de si se cumple o no una condición en nuestros datos. En ese caso, las estructuras de control son nuestras mejores aliadas.\n\n3.9.1 Ciclos for\nHay distintas formas de realizar la repetición de acciones, pero hoy introduciremos únicamente los ciclos for por ser los más probables a ser requeridos. Un ciclo for consta de cuatro elementos:\nfor (variable in vector) {action}\n\nLa estructura de control for, evidentemente\nUna variable que hace las veces de un marcador de posición; es decir, la utilizaremos para indicar en dónde se van a sustituir los valores que queremos ciclar\nLos valores que ciclaremos, contenidos en un vector\nLa acción a realizar\n\nComo muchas otras cosas, es más fácil entenderlo utilizando algunos ejemplos. El primero de ellos es simplemente imprimir la secuencia de números del 1 al 10. Aunque podemos escribir 10 veces la función print e ir cambiando el número, es mucho más sencillo:\n\nfor (i in 1:10) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\n¿Qué fue lo que hizo la computadora? Utilizó la función print() para mostrarnos el contenido de i, el cuál es el iésimo elemento de la secuencia 1:10. En otras palabras, si es la segunda vuelta que da, imprimirá el número 2, si es la séptima imprimirá 7, y así hasta que termine con todos los elementos en la secuencia. Ahora, sumemos 2 a cada número de la secuencia:\n\nfor (i in 1:10) {\n  print(i+2)\n}\n\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n[1] 11\n[1] 12\n\n\nAl igual que en el caso anterior, tomó uno por uno los valores (de forma secuencial), le añadió 2 y luego mostró el resultado en pantalla. Este tipo de estructuras son sumamente útiles, pues no solo nos ahorran errores, sino también tiempo de ejecución. Sobra decir que no es la única manera de hacer este tipo de ciclos, ni tampoco es la más rápida. Tenemos algo que se conocen como funciones vectorizadas.\n\n\n3.9.2 Familia de funciones apply()\nPodemos entender la vectorización como la aplicación de una función a cada elemento de un vector (igual que el ciclo for), aunque procesando todo el vector “al mismo tiempo”. Esta última parte no es estrictamente verdad, aunque lo cierto es que son más rápidas que los ciclos for tradicionales. En R tenemos toda una gama de funciones que hacen justo eso, la familia apply y sus relacionadas, compuesta por las funciones:\n\napply\nlapply\nsapply\ntapply\nmapply\nby\naggregate\n\nTodas estas funciones manipulan porciones de datos como matrices, arreglos, listas o data frames de forma repetitiva. Básicamente nos permiten evitar el uso explícito de un ciclo for. Toman como argumento una lista, matriz o data frame y le aplican una función con uno o más argumentos adicionales. Esta función puede ser:\n\nUna función de agregación, por ejemplo la media o la suma\nFunciones de transformaciones o para extraer sub-conjuntos\nOtras funciones vectorizadas, que dan como resultado estructuras más complejas como listas, vectores, matrices o arreglos. Pero basta de cháchara, ¿cómo y cuándo debemos de utilizarlas? La respuesta depende totalmente de la estructura de los datos y el formato de salida que se necesite. Veamos algunos casos de uso:\n\n\n3.9.2.1 apply()\nEsta es la función “madre” de las demás, la cual opera sobre arreglos. Para simplicidad, vamos a limitarnos a arreglos bi-dimensionales como las matrices o data.frames. La sintaxis para su uso es apply(X, MARGIN, FUN, ...), donde X es el arreglo, MARGIN es el márgen sobre el cuál va a actuar la función; es decir, si queremos que la aplique a cada renglón (MARGIN = 1) o a cada columna (MARGIN = 2), y FUN es el nombre de la función a aplicar (puede ser cualquiera, incluso una función definida por nosotros).\nPodemos pensar en obtener la suma o el promedio de cada columna de nuestro data.frame con las presas utilizando un ciclo, o podemos utilizar apply, tal que:\n\nsums <- apply(datos2[,2:ncol(datos2)], 2, sum)\nsums\n\n                    Alpheus_lottini                         Alpheus_spp \n                                  4                                  12 \n                       Alpheus_umbo                           Amphipods \n                                  9                                  44 \n                  Apogon_retrosella                    Appendicularians \n                                  5                                   5 \n              Axoclinus_nigricaudis                Bittium_cerralvoense \n                                  2                                  35 \n                       Chaetognaths     Cirripedia_Chthamalus_anisopoma \n                                 50                                   5 \n      Cladocerans_Penila_avirostris  Cladocerans_Pseudovadne_tergestina \n                                 34                                 171 \n            Copepods_Acartia_clausi          Copepods_Calanus_pacificus \n                                390                                  50 \n                    Epitonium_canna                           Fish_eggs \n                                  3                                  96 \n                       foraminifera             Gnathophyllum_panamense \n                                205                                  11 \n                           Hidrozoa                      Ichtyoplankton \n                                  1                                 153 \n        Larvae_crustaceans_megalopa             larvae_crustaceans_zoea \n                                 22                                  17 \n                 Liomera_cinctimana                 Litiopa_melanostoma \n                                 13                                  13 \n                              Mysid                  Mytella_arciformis \n                                  5                                   2 \n                Mytella_tumbezensis                 Nanocassiope_polita \n                                  1                                   2 \n                Nyctiphanes_simplex                           Ostracods \n                                549                                  76 \n                           Otoliths                     Palaemon_ritter \n                                  2                                   2 \n                 Panopeus_purpureus            Parviturbo_acuticostatus \n                                  4                                   1 \n                   Parviturbo_erici                      Parviturbo_spp \n                                  3                                   2 \n                          Pteropods                    Quadrella_nitida \n                                  8                                   1 \n              Tagelus_californianus                     Tegula_globulus \n                                  2                                  22 \n                     Tegula_mariana                       Tellina_coani \n                                 20                                   3 \n                 Trapezia_bidentata                    Trapezia_formosa \n                                  2                                   6 \n                       Trapezia_spp                    Ulva_dactylifera \n                                  4                                  35 \n                       Ulva_lactuca                            Ulva_spp \n                                158                                  78 \nUOM (Unidentified Organic Material) \n                                 20 \n\n\n\n\n3.9.2.2 aggregate()\nOtra función extremadamente útil es la función aggregate(). Esta función nos permite aplicar una función a distintos grupos. Un escenario clásico es obtener el promedio de una variable para cada grupo, por ejemplo el promedio de conteos de quetognatos. La forma “tradicional” es: aggregate(x, by, FUN), donde x es el objeto R a agrupar, by es una lista con los grupos y FUN es la función a aplicar; sin embargo, podemos utilizar una notación más compacta utilizando una formula: aggregate(forumla, data, FUN). Las fórmulas en R son objetos sumamente útiles y que se utilizan para una gran diversidad de cosas. Su estructura es: Y ~ X, y se lee “Y con respecto a X”. En nuestro caso particular:\n\naggregate(Chaetognaths~sp, data = datos1, FUN = mean)\n\n\n\n  \n\n\n\n\n\n\n3.9.3 Condicionales\nOk, ahora conocemos una manera de aplicar una función a una serie de elementos, pero que pasa si queremos aplicarla de manera condicionada; es decir, si queremos solo imprimir los números mayores a 5, por ejemplo. Eso es justo de lo que se tratan los condicionales, particularmente if, else, e ifelse. La lógica detrás de ellos es sumanmente simple: si se cumple una condición, realiza una acción, si no se cumple, realiza otra (o no realices nada). Comencemos con if. Su estructura es: if(condition){action T}, que notarás es básicamente la descripción que dimos, solo que sin una acción en caso de que no se cumpla la condición. Un ejemplo sería proporcionar un número y que nos diga si es mayor a 5:\n\nx <- 6\nif (x>5) {print(\"x es mayor a 5\")}\n\n[1] \"x es mayor a 5\"\n\n\n¿Y si no se cumple la condición? Veamos qué pasa:\n\nx <- 1\nif (x>5) {print(\"x es mayor a 5\")}\n\nR no nos da ninguna salida, pues no sabe qué hacer. Una forma de decirle es utilizando el complemento de if, else, que nos permite establecer una acción secundaria:\n\nif (x>5) {print(\"x es mayor a 5\")\n  }else{print(\"x no es mayor a 5\")}\n\n[1] \"x no es mayor a 5\"\n\n\nUna notación mucho más compacta para este tipo de casos es utilizar la función ifelse(condition, action T, action F):\n\nx <- 4\nifelse(x > 5,\n       \"x es mayor a 5\",\n       \"x no es mayor a 5\")\n\n[1] \"x no es mayor a 5\"\n\n\nEl resultado fue el mismo que el anterior, pero qué pasa si tenemos más de dos escenarios, que, por ejemplo, nos interesara decir si el número es mayor, menor o igual a 5. Veamos primero lo que sucede si establecemos que x sea 5:\n\nx <- 5\nifelse(x > 5,\n       \"x es mayor a 5\",\n       \"x no es mayor a 5\")\n\n[1] \"x no es mayor a 5\"\n\n\nLa computadora no hizo nada mal, 5 no es mayor a 5. En otros lenguajes de programación añadiríamos una estructura llamada elif, pero en R solo hay que añadir otro(s) if. Mientras que else aplica para todos los casos donde la condición no se cumpla, estos if secundarios nos permiten establecer condiciones adicionales para cuando la condición principal no se cumpla. Volviendo a nuestro problema con x = 5:\n\nx <- 4\nif (x > 5) {\n  print(\"x es mayor a 5\")\n}\n\nif (x == 5) {\n  print(\"x es 5\")\n}else{print(\"x es menor a 5\")}\n\n[1] \"x es menor a 5\"\n\n\nAhora sí cubrimos todas nuestras bases para este problema de comparación. Como te imaginarás, el siguiente paso lógico es mezclar for e if o, mejor dicho, anidarlos:\n\nx <- 1:10\n\nfor (i in x) {\n  if (i > 5) {\n    print(\"x es mayor a 5\")\n  }\n  if (i == 5) {\n    print(\"x es 5\")\n  }else{print(\"x es menor a 5\")}\n}\n\n[1] \"x es menor a 5\"\n[1] \"x es menor a 5\"\n[1] \"x es menor a 5\"\n[1] \"x es menor a 5\"\n[1] \"x es 5\"\n[1] \"x es mayor a 5\"\n[1] \"x es menor a 5\"\n[1] \"x es mayor a 5\"\n[1] \"x es menor a 5\"\n[1] \"x es mayor a 5\"\n[1] \"x es menor a 5\"\n[1] \"x es mayor a 5\"\n[1] \"x es menor a 5\"\n[1] \"x es mayor a 5\"\n[1] \"x es menor a 5\"\n\n\nCon esto también quiero decir que podemos anidar ciclos for, tal que:\n\nx <- 1:5\ny <- 100:105\n\nfor (i in x) {\n  for (j in y) {\n    print(c(i,j))\n  }\n}\n\n[1]   1 100\n[1]   1 101\n[1]   1 102\n[1]   1 103\n[1]   1 104\n[1]   1 105\n[1]   2 100\n[1]   2 101\n[1]   2 102\n[1]   2 103\n[1]   2 104\n[1]   2 105\n[1]   3 100\n[1]   3 101\n[1]   3 102\n[1]   3 103\n[1]   3 104\n[1]   3 105\n[1]   4 100\n[1]   4 101\n[1]   4 102\n[1]   4 103\n[1]   4 104\n[1]   4 105\n[1]   5 100\n[1]   5 101\n[1]   5 102\n[1]   5 103\n[1]   5 104\n[1]   5 105"
  },
  {
    "objectID": "c03_bases_r.html#ejercicios",
    "href": "c03_bases_r.html#ejercicios",
    "title": "3  Bases de R",
    "section": "3.10 Ejercicios",
    "text": "3.10 Ejercicios\n\n¿Qué es un objeto?\n¿Cuál es la diferencia entre una variable y una función?\n¿Qué es una librería? ¿Qué pasa si utilizas la función ggplot() sin haber cargado la librería ggplot2?\n¿Cuál es la diferencia entre un dato y una estructura?\nCarga los datos1.csv y añade un renglón adicional con los totales para cada columna.\nCarga los datos1.csv y obten una tabla con los conteos promedio de cada presa para cada especie (aggregate/apply o ciclos).\nCombina el ciclo for anidado del último ejemplo con un condicional, en el cuál se imprima si la suma de ambos números (i, j) es mayor, menor o igual a 105.\nEjecuta el siguiente código y responde ¿qué hace cada línea? Recuerda que ante la duda siempre puedes revisar la ayuda de las funciones.\n\n```{r}\npkgs <- c(\"tidyverse\", \"ggtext\", \"Rmisc\", \"rcompanion\",\n          \"gap\", \"brms\", \"stats4\", \"Metrics\",\n          \"performance\", \"ggdendro\", \"dendextend\",\n          \"factoextra\", \"cluster\", \"NbClust\",\n          \"FactoMineR\", \"MASS\", \"klaR\", \"vegan\",\n          \"tidymodels\", \"MVN\", \"Hotelling\",\n          \"gridExtra\", \"GGally\", \"car\", \"FSA\",\n          \"randomcoloR\", \"gganimate\", \"reshape2\",\n          \"DescTools\", \"PerformanceAnalytics\",\n          \"corrplot\", \"ggrepel\", \"pROC\", \"RCurl\",\n          \"glmnet\", \"pcsl\", \"MuMIn\", \"nlraa\")\n\ninstall.packages(pkgs, dependencies = T)\n```\n\n\n\n\n\n\nImportante\n\n\n\nSi tienes algún problema con la ejecución por favor házmelo saber. Esto cubre la instalación de las librerías que utilizaremos durante el curso, por lo que es sumamente importante que se ejecuten correctamente.\nSi en algún momento de la instalación R te pregunta si quieres instalar desde la fuente dile que NO. Aunque las versiones fuente están más actualizadas pueden llegar a romper la compatibilidad con otras librerías.\n\n\n\n\n\n\nR Core Team. 2022. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing."
  },
  {
    "objectID": "c04_tidyverse.html#un-dialecto-dentro-de-r",
    "href": "c04_tidyverse.html#un-dialecto-dentro-de-r",
    "title": "4  Introducción a tidyverse",
    "section": "4.1 Un dialecto dentro de R",
    "text": "4.1 Un dialecto dentro de R\nEn la sesión anterior mencionamos que hay un “paquete de paquetes” que da lugar a un “dialecto” dentro de R: tidyverse, pero no especifiqué a qué me refería con ello. Pues bien, mientras que en R base los procedimientos los realizamos línea a línea, generando a veces una gran cantidad de objetos intermedios o sobreescribiendo los existentes, tidyverse está basado en el paradigma funcional de la programación, con una “gramática” (sintaxis) distinta, muy similar a lo que veremos en ggplot2. De hecho, ggplot2 es un paquete del tidyverse, por lo que el “dialecto” tidy comparte la filosofía declarativa y fomenta la “encadenación” de comandos. A muchas personas les gusta más la forma tidy, a otras les gusta más trabajar con R base. En lo personal soy partidario de que utilices lo que más te acomode, siempre y cuando lo que hagas tenga sentido, pero más de esto cerca del final de la sesión.\nEste nuevo dialecto fue creado con un objetivo en particular: la ciencia de datos. Como tal, cuenta con una gran cantidad de librerías (y por lo tanto funciones) especializadas para realizar operaciones rutinarias. ¿Quieres realizar gráficos? En la siguiente sesión hablaremos de ggplot2. ¿Quieres hacer “manipulación” (ojo, no cuchareo) de datos? Aquí veremos algnas funciones de dplyr. ¿Quieres trabajar con procesamiento de cadenas de caractér? Para esto está stringr. ¿Quieres herramientas para programación funcional? Ve hacia purrr (sí, triple r). readxl es otra librería con la que ya estás familiarizado y que forma parte del tidyverse. ¿Tienes un problema en el que que necesitas manejar fechas? lubridate puede ser una opción. Puedes conocer todos los paquetes que forman el tidyverse con:\n\ntidyverse::tidyverse_packages(include_self = T)\n\n [1] \"broom\"         \"cli\"           \"crayon\"        \"dbplyr\"       \n [5] \"dplyr\"         \"dtplyr\"        \"forcats\"       \"ggplot2\"      \n [9] \"googledrive\"   \"googlesheets4\" \"haven\"         \"hms\"          \n[13] \"httr\"          \"jsonlite\"      \"lubridate\"     \"magrittr\"     \n[17] \"modelr\"        \"pillar\"        \"purrr\"         \"readr\"        \n[21] \"readxl\"        \"reprex\"        \"rlang\"         \"rstudioapi\"   \n[25] \"rvest\"         \"stringr\"       \"tibble\"        \"tidyr\"        \n[29] \"xml2\"          \"tidyverse\"    \n\n\nA partir de aquí, el cómo aprovecharlos depende mucho de el problema que tengas entre manos, pero la idea general es la misma: utilizar una gramática declarativa para llegar a la solución. Veamos en qué consiste con un ejemplo cotidiano: obtener el promedio de una variable para varios grupos.\n\n4.1.1 Promedios de grupos: aggregate vs group_by() |> summarise()\nVeamos un ejemplo en el cual calcularemos la longitud de pico promedio para cada especie de pingüino, según los datos de palmerpenguins. Primero, carguemos los datos:\n\ndatos1 <- palmerpenguins::penguins\n\nAhora, obtengamos los promedios con la función aggregate, tal y como vimos en la sesión anterior:\n\naggregate(bill_length_mm~species, data = datos1, FUN = mean)\n\n\n\n  \n\n\n\nAhora repliquémoslo con tidyverse, particularmente con las funciones group_by y summarise (o summarize) de la librería dplyr:\n\nlibrary(dplyr)\n\ndatos1 |>\n  group_by(species) |>\n  summarise(mbill_l = mean(bill_length_mm, na.rm = T))\n\n\n\n  \n\n\n\nNotarás que el resultado es exactamente el mismo, aunque la forma de hacerlo es diferente. Descompongámosla paso a paso para ver qué es lo que está pasando:\n\nlibrary(dplyr): cargamos la librería dplyr, la cual contiene las funciones que nos interesa aplicar: group_by y summarise.\nLlamamos directamente a nuestros datos (datos1) y utilizamos un operador que no habíamos visto: |>. Este es el operador pipe, el cual pasa lo que está a la izquierda de él como el primer argumento de lo que está a la derecha de él. Esto puede sonar confuso, pero la instrucción datos1 |> group_by(sp) es equivalente a group_by(datos1, sp). Podemos ver al operador pipe como su nombre sugiere: una tubería que manda la información de un lado hacia otro.\ngroup_by(sp): Como te mencionaba, tidy es un poco más explícito que R base. Mientras que el argumento con la fórmula en aggregate indica cómo se van a agrupar los datos, aquí primero los agrupamos y después aplicamos la función que nos interesa. group_by hace justamente eso, agrupar nuestros datos, nada más, nada menos. El argumento principal de esta función es la(s) columnas bajo las cuales queremos agrupar nuestros datos. En este caso solo es una (sp), por lo que la pasamos directamente.\nNuevamente utilizamos el operador |>. Hasta este punto hemos pasado los datos1 a la función group_by(sp), por lo que ya están agrupados por especie, pero falta aplicar la función mean() para obtener el promedio, entonces volvemos a encadenar hacia la función summarise(). Esta función recibe una serie de pares nombres de columnas y funciones a aplicar. En este caso, estamos generando una columna llamada mbill_l que contiene los promedios de la columna bill_length_mm de los datos1.\n\n\n\n\n\n\n\nImportante\n\n\n\nSi revisas documentación “antigua” sobre tidyverse (previa a R 4.1, de hecho), notarás que el operador pipe es %>% en vez de |>. El resultado es el mismo y, de hecho, a partir de R 4.1 puedes seleccionar cuál utilizar en las preferencias de RStudio. En el curso utilizaremos |> por ser el operador pipe nativo de R, el cual fue introducido (como te imaginarás) en R 4.1.\n\n\n¿Abstracto? Sin duda. ¿Útil? También. ¿Más explícito que R base con aggregate? Debatible. Lo que no es debatible es que esta notación brilla especialmente en cierto tipo de problemas. Pensemos que nos interesa conocer el promedio de las longitudes de picos por especie para cada isla. Intenta hacerlo con R base y te darás cuenta de que no es tan intuitivo, salvo que estés familiarizado con el uso de fórmulas o ciclos y condcionales. ¿En tidy? Solamente hay que agregar la nueva variable de agrupamiento a group_by:\n\ndatos1 |>\n  group_by(species, island) |>\n  summarise(mbill_l = mean(bill_length_mm,\n                           na.rm = T))\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n\n\n  \n\n\n\nAhora tenemos un tibble (funcionalmente equivalente a un data.frame) con tres columnas, en donde se da el promedio para cada combinación de las variables de agrupamiento. ¿A que es más sencillo que intentar hacerlo con R base?. Esto último no es del todo cierto, pues hay una forma muy sencilla de hacerlo con aggregate(), pero encontrar una manera es parte de tu tarea, así que no arruinaré la diversión.\n\n\n\n\n\n\nNota\n\n\n\nCon esto no quiero deciro que tidy sea mejor que R base o viceversa, solamente que son dos aproximaciones, cada una con sus propias ventajas y desventajas. Por un lado, R base puede llegar a ser más compacto, pero tidy tiende a ser más explícito. Por supuesto, también hay casos en los que lo contrario es verdad.\n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nHay ocasiones en las cuales querrás evitar el uso de tidyverse, y una de ellas es al crear nuevas librerías. ¿La razón? Puedes crear un conflicto de dependencias si no lo manejas con cuidado. Otra es que es sumamente complicado depurar errores.\n\n\nTe habrás dado cuenta de que hasta este momento no hemos asignado nuestros resultados a ningún objeto. Esto se debe a dos razones. La primera, y tal vez la que pasa por tu cabeza, es que de esta manera podemos mostrar los resultados más rápidamente, y sí, pero el trasfondo está en la segunda razón: La asignación sigue un sentido opuesto al encadenamiento. Mientras que con |> la información fluye de izquierda a derecha, con <- la información fluye de derecha a izquierda. Quise, entonces, que primero te acostumbraras al flujo de información con pipe, pues asignar el resultado a un objeto es lo mismo que hemos hecho hasta ahora:\n\ngmeans <- datos1 |>\n          group_by(species, island) |>\n          summarise(mbill_l = mean(bill_length_mm,\n                                   na.rm = T))\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\ngmeans\n\n\n\n  \n\n\n\n\n\n4.1.2 Subconjuntos de datos\nEn la sesión anterior nos familiarizamos con máscaras booleanas y la función subset. tidy tiene su propia aproximación. Pensemos que queremos quedarnos solo con los pingüinos provenientes de Biscoe. Con subset:\n\nsubset(datos1, island == \"Biscoe\")\n\n\n\n  \n\n\n\nMientras que con tidy:\n\ndatos1 |> filter(island == \"Biscoe\")\n\n\n\n  \n\n\n\nLa notación no es tan diferente como en el caso anterior, y el resultado es el mismo. ¿Cuál utilizar? Depende totalmente de la preferencia de cada quien. A diferencia del caso anterior, subset no se vuelve tan compleja conforme vamos escalando en complejidad, y la equivalencia entre aproximaciones con filter se mantiene. Veamos qué pasa si obtenemos SOLO los pingüinos Adelie de Biscoe. Nuevamente, con subset:\n\nsubset(datos1, species == \"Adelie\" & island == \"Biscoe\")\n\n\n\n  \n\n\n\nCon tidy:\n\ndatos1 |> filter(species == \"Adelie\" & island == \"Biscoe\")\n\n\n\n  \n\n\n\n¿Por qué empezar con un ejemplo tan “complejo” como el caso anterior? Para dejar “lo peor” al inicio, y a partir de ahí las cosas puedan fluir un poco mejor.\n\n\n4.1.3 Añadir o modificar columnas\nOtra tarea cotidiana que vimos en la sesión anterior fue el añadir nuevas columnas a nuestro data.frame. Pensemos que tiene sentido obtener el “área” que utiliza el pico, la cual obtendríamos multiplicando bill_length_mm y bill_depth_mm. Este producto lo almacenaríamos en una nueva columna llamada bill_area. En R base:\n\ndatos1[\"bill_area\"] <- datos1$bill_length_mm * datos1$bill_depth_mm\ndatos1$bill_area\n\n  [1]  731.17  687.30  725.40      NA  708.31  809.58  692.42  768.32  617.21\n [10]  848.40  646.38  653.94  723.36  818.32  730.06  651.48  735.30  879.75\n [19]  632.96  989.00  691.74  704.99  689.28  691.42  667.36  667.17  755.16\n [28]  724.95  704.94  765.45  659.65  673.32  703.10  773.01  618.80  827.12\n [37]  776.00  780.70  725.68  760.18  657.00  750.72  666.00  868.77  625.30\n [46]  744.48  780.90  708.75  644.40  896.76  700.92  757.89  626.50  819.00\n [55]  624.45  770.04  682.50  763.28  605.90  718.16  603.33  871.43  639.20\n [64]  748.02  622.44  748.80  575.10  785.01  595.94  810.92  636.50  730.48\n [73]  681.12  865.62  621.25  791.80  687.12  721.68  582.82  804.11  595.12\n [82]  755.04  689.96  680.94  663.94  838.39  707.85  686.34  735.36  731.32\n [91]  642.60  743.91  581.40  716.76  626.26  771.12  708.66  745.55  532.91\n[100]  799.20  626.50  820.00  603.20  756.00  704.94  750.33  663.92  764.00\n[109]  647.70  820.80  628.65  925.68  702.69  822.90  819.72  781.41  656.20\n[118]  764.65  606.90  764.46  622.64  746.46  683.40  765.90  559.68  771.40\n[127]  682.88  759.45  666.90  793.80  689.15  827.52  680.80  693.75  670.56\n[136]  719.25  623.00  808.02  610.50  710.63  687.42  698.32  497.55  691.90\n[145]  626.64  729.30  729.12  673.44  640.80  684.18  615.60  767.75  608.52\n[154]  815.00  686.67  760.00  690.20  627.75  662.84  714.51  580.22  720.72\n[163]  560.33  788.90  623.35  706.64  668.68  774.01  567.00  747.84  669.90\n[172]  735.37  717.86  653.95  674.25  731.54  561.99  696.11  636.35  717.00\n[181]  689.26  765.00  723.69  607.76  653.95 1013.20  726.68  788.92  583.62\n[190]  768.12  598.40  764.59  584.99  793.60  620.61  744.00  802.95  606.04\n[199]  632.45  802.95  597.17  714.16  661.72  683.85  649.44  751.50  669.60\n[208]  693.00  608.82  682.50  626.40  771.12  625.14  688.38  635.23  852.51\n[217]  650.36  836.64  665.28  801.90  617.70  760.50  715.50  723.84  751.92\n[226]  688.20  696.00  777.60  674.50  832.93  623.76  741.28  711.95  819.00\n[235]  692.04  795.00  619.62  878.84  624.96  728.46  665.00  885.70  712.50\n[244]  892.62  659.75  796.95  654.15  797.56  780.52  684.74  696.96  843.15\n[253]  727.50  950.30  731.60  736.50  652.74  753.48  612.99  843.72  606.20\n[262]  726.31  767.60  791.82  661.20  839.45  651.42  881.60  698.65  790.56\n[271]  646.64      NA  669.24  791.28  668.96  803.39  832.35  975.00  984.96\n[280]  848.98 1043.46  804.56  839.02  933.66  869.40 1020.87  829.48 1049.51\n[289]  813.10  941.20  784.89  989.80 1006.00 1032.40  863.04  895.44  733.52\n[298]  848.75  717.12  981.64  835.93  988.00  929.20  940.50  825.92 1056.00\n[307]  678.94 1127.36  709.75  958.80  924.42  798.00  871.08 1076.40  778.54\n[316] 1064.65  955.50  808.50  972.19  773.50  911.11  939.80  896.79  960.40\n[325]  963.05  861.54  788.84  976.60  790.61  998.79  735.25  981.36  750.32\n[334]  981.07  943.76  884.64 1012.05  772.20  776.90 1104.84  787.35  902.72\n[343]  965.20  938.74\n\n\nAhora hagámoslo con tidy, pero primero reestablezcamos el objeto datos1:\n\ndatos1 <- palmerpenguins::penguins\n\nY ahora hagamos la operación con tidy:\n\ndatos1 <- datos1 |> \n          mutate(bill_area = bill_length_mm * bill_depth_mm)\ndatos1 |> select(bill_area)\n\n\n\n  \n\n\n\nEvidentemente, los resultados son los mismos, lo cual me lleva directamente a la siguiente sección."
  },
  {
    "objectID": "c04_tidyverse.html#por-qué-no-enseñar-tidy-desde-el-inicio",
    "href": "c04_tidyverse.html#por-qué-no-enseñar-tidy-desde-el-inicio",
    "title": "4  Introducción a tidyverse",
    "section": "4.2 ¿Por qué no enseñar tidy desde el inicio?",
    "text": "4.2 ¿Por qué no enseñar tidy desde el inicio?\nSi tidy se acomodó a tu forma de ver las cosas, o si se te hizo más fácil de leer, es probable que te preguntes por qué no me salté R base para entrar directamente a tidy. Además de incrementar las horas del curso (broma), fue porque (pedagógicamente) tidy puede introducir ciertas barreras para quienes se van introduciendo a R. Matloff (2020) hace una excelente y extensiva recopilación de las razones por las cuales enseñar solo tidy (o solo R base) es una mala idea, así que te recomiendo leas su opinión; sin embargo, me gustaría darte mi perspectiva. Te adelanto: si no tienes experiencia en programación, el paradigma de tidy supone una curva de aprendizaje más alta que solo R base (solo R a partir de aquí) y, tal vez más importante, no es necesario casarse con uno u otro.\n\n4.2.1 tidy es más abstracto\nComo habrás notado en esta sesión, tidy es, escencialmente, más complejo que R. Esto no es una falla en el diseño, sino que tiene que ver con la filosofía de la aproximación: generar código que sea más fácilmente leíble por seres humanos. Sin duda alguna, el utilizar data |> group_by() |> summarise() puede parecer menos críptico o más “entendible” que solo aggregate(formula, data, FUN); sin embargo, esto se debe a que ya conocías qué es una librería y cómo cargarla, qué es una función y qué es un argumento, pero aún así hubo que explicar qué es encadenamiento de operaciones/funciones y el operador pipe y también tuvimos que entender que la información fluye de izquierda a derecha al encadenar y de derecha a izquierda al asignar.\nSi esto no fuera suficiente, el utilizar pipes puede complicar demasiado las cosas al querer depurar errores. ¿La razón? Es una capa más de abstracción. Mientras que cuando aprendemos R es común generar objetos intermedios con los resultados y ver sus salidas (o detectar errores en cada paso), en tidy esto tiende a no ser el caso. ¿Qué obtienes si solo aplicas group_by() en el ejemplo anterior? (i.e., no aplicas summarise()). El ejemplo que vimos es relativamente sencillo pero, en la medida que los problemas se van haciendo más complejos, es fácil perder la pista de qué sale de una función y entra a otra.\nEn mi opinión esto no es un problema TAN grande como pudiera parecer. Tomemos de ejemplo a Python, el lenguaje de programación reconocido como el más intuitivo. Al utilizarlo, el encadenamiento y uso de pipes es cotidiano y, aún más, preferido. En este sentido, tidyverse me recuerda mucho a la funcionalidad que de pandas en Python pero, al igual que aquí, lo correcto es aproximarse primero a Python base y luego pensar en aprender pandas. En mi opinión, el problema real (quitando la capa de abstracción) es en realidad dos problemas: uno relacionado con la filosofía de tidy y otro con quienes enseñamos R.\n\n\n4.2.2 tidy limita tus opciones\nUn ejemplo de esto está en esta misma clase. ¿Cómo obtendríamos los promedios de datos agrupados a dos niveles? Dejé el ejercicio “en el tintero” (es parte de tu tarea para esta sesión) pero, si solo te hubiera enseñado tidy, no podrías pensar en ciclos, el operador $ o cualquier otra forma de indización. ¿La razón? tidy no está enfocado a tratar con vectores individuales, aborrece el uso de ciclos y tampoco sigue las notaciones básicas de indización. Recuerda: en programación siempre es mejor tener más herramientas a tu disposición. Esta limitación la podemos probar rápidamente si queremos extraer los elementos 10:25 de la columna sex de los datos1. Con R base:\n\ndatos1$sex[10:25]\n\n [1] <NA>   <NA>   <NA>   female male   male   female female male   female\n[11] male   female male   female male   male  \nLevels: female male\n\n\n¿Con tidy? Realmente no hay una función que permita hacerlo. Podemos extraer la columna sex utilizando la función select:\n\ndatos1 |> select(sex)\n\n\n\n  \n\n\n\nPero si queremos indizar el resultante obtenemos un error:\n\ndatos1 |> select(sex)[10:25]\n\nError: function '[' not supported in RHS call of a pipe\n\n\n\n\n\n\n\n\nNota\n\n\n\nRHS es el acrónimo de “Right Hand Side”; es decir, el operador [ no está soportado a la derecha de |>.\n\n\n¿La alternativa? Primero indizar datos1 y luego utilizar select. Esto, como ves aquí abajo, funciona, pero hubiera sido mucho más fácil solo utilizar R base, sin mencionar que en tidy “puro” esta solución no es aceptable. ¿La razón? En tidy predomina el paradigma funcional de la programación; es decir, la salida depende únicamente de los argumentos pasados a la función.\n\ndatos1[10:25,] |> select(sex)\n\n\n\n  \n\n\n\nPor otra parte, hay una falta de consistencia interna derivada de una estrategia publicitaria (tal vez) un poco mal llevada. ggplot2 no surgió dentro del tidyverse, sino que fue incluído después. Si bien es cierto que la filosofía es similar (i.e., una estructura declarativa), el cómo funcionan es completamente diferente. Una de las máximas de tidyverse (sin ggplot2) es que todo lo que entra o lo que sale es un data.frame (o tibble), mientras que en ggplot2 entra un data.frame y sale una lista. De hecho, más adelante veremos cómo podemos utilizar ciclos para automatizar la generación de gráficos, pero esto va en contra de la filosofía de tidy y no sería posible si nos hubiéramos enfocado únicamente en ella.\n\n\n4.2.3 Los ponentes somos necios\nEl mayor problema al que nos enfrentamos al aprender algún lenguaje de programación (y en muchas otras cosas) es que estamos sujetos a los prejuicios y preferencias de la persona que nos está enseñando. Al aprender a manejar nuestro tío amante de los autos nos va a decir que la transmisión manual (estándar) es mejor que la automática, pero nuestro papá, quien ve los carros solo como un medio de transporte, nos va a decir que con la automática es suficiente. ¿Cuál es mejor? Para variar, la respuesta es: “depende”. ¿De qué? De la situación en la que nos encontremos. En ciudad tener una transmisión manual puede ser muy cansado, pero puede darnos un mayor control en una carretera con un descenso empinado y muchas curvas.\nEn el problema R base vs. tidy es lo mismo. Hay ponentes “puristas” en ambos sentidos: personas que creen que tidy debería considerarse sacrilegio, y personas que creen que R base es obsoleto, arcaico, y que debería de caer en desuso. No te conviertas en ninguno de ellos y mejor toma lo mejor de ambos.\n\n\n4.2.4 tidy homogeneiza procesos\nEn mi opinión, lo que te acabo de exponer son los problemas principales de enseñar solo tidy y, de acuerdo con Matloff (2020), las razones por las cuales enseñar una mezcla de ambos es la mejor opción. Aprender R base nos permite resolver problemas que en tidy sería muy largo, mientras que tidy nos permite simplificar procedimientos que serían más complicados en R base. Otra ventaja de tidy es que permite unificar procesos bajo una misma sintaxis.\nUn ejemplo de esto lo tenemos en el aspecto de aprendizaje automatizado. Si te pones a revisar tutoriales/referencias sobre aprendizaje automatizado es muy probable que te encuentres con un montón de librerías (una por problema), funciones dedicadas y sintáxis que son específicas a la técnica que quieras aplicar. Teníamos/tenemos un excelente intento de solventar este problema: caret. Funcionaba bien en el sentido de que permitía conjuntar una gran diversidad de técnicas de aprendizaje automatizado en un mismo entorno, unificadas en un mismo estilo. Utilizando solo caret podríamos ir desde el preprocesamiento de los datos hasta el entrenamiento y validación de nuestros modelos. ¿El problema? Las pocas funciones que forman su esqueleto se volvieron sumamente complejas, algunas de ellas con 30 o más argumentos. El equipo de posit se ha puesto el mismo desafío, y su solución es tidymodels. A diferencia de caret es altamente modular, pero mantiene la intención de unificar el flujo de trabajo en una misma estructura. Tal vez yendo en contra de nuestro consejo, la mayor parte de nuestros procedimientos de aprendizaje automatizado los realizaremos bajo tidymodels, pero haremos referencia a las librerías y funciones involucradas en cada paso. Para ejemplificar, realicemos una regresión lineal simple entre la longitud y la profundidad del pico de los pingüinos.\nEn R base lo podemos hacer en una sola línea, llamando a la función lm con una fórmula y unos datos:\n\nlm(bill_length_mm~bill_depth_mm, data = datos1)\n\n\nCall:\nlm(formula = bill_length_mm ~ bill_depth_mm, data = datos1)\n\nCoefficients:\n  (Intercept)  bill_depth_mm  \n      55.0674        -0.6498  \n\n\nEn tidymodels es un poco más complejo:\n\nlibrary(tidymodels)\n\nlinear_reg() |>\n  set_engine(\"lm\") |> \n  set_mode(\"regression\") |> \n  fit(bill_length_mm~bill_depth_mm, data = datos1)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = bill_length_mm ~ bill_depth_mm, data = data)\n\nCoefficients:\n  (Intercept)  bill_depth_mm  \n      55.0674        -0.6498  \n\n\n¿Por qué utilizar tidymodels entonces? Eso lo dejaremos para las sesiones en las que hablemos de aprendizaje automatizado, pero verás que toma mucho sentido en el momento en el que empiezas a hacer particiones entrenamiento/prueba, optimización mediante validación cruzada, preprocesamiento de datos, evaluación, y demás tareas necesarias. Lo único que te diré en este punto es que el poder homogeneizar todos estos procedimientos en un solo estilo de trabajo simplifica las cosas."
  },
  {
    "objectID": "c04_tidyverse.html#conclusión",
    "href": "c04_tidyverse.html#conclusión",
    "title": "4  Introducción a tidyverse",
    "section": "4.3 Conclusión",
    "text": "4.3 Conclusión\nAunque tidy puede llegar a verse más elegante o moderno que R base, no es un substituto total. Aunque R base te permite resolver los mismos problemas que tidy, a veces no es tan intuitivo. ¿Solución? No casarse con ninguno de los dos y exprimirlos lo mejor posible. ¿Tu problema se resuelve más rápidamente con tidy? Úsalo. ¿R base se presta mejor? Aprovéchalo. Recuerda, R (como cualquier otro lenguaje de programación) es una caja de herramientas en la cual debes de buscar la que mejor se adapte al problema o pregunta que quieras responder.\nEsto es todo para esta sesión, nos vemos en la siguiente para hablar sobre teoría y buenas prácticas para la visualización de datos."
  },
  {
    "objectID": "c04_tidyverse.html#ejercicio",
    "href": "c04_tidyverse.html#ejercicio",
    "title": "4  Introducción a tidyverse",
    "section": "4.4 Ejercicio",
    "text": "4.4 Ejercicio\n\nUtilizando R base obtén los promedios de las longitudes de picos de los pingüinos de palmerpenguins para cada especie en cada isla. OJO: Hay al menos dos formas de hacerlo, una muy simple y una más rebuscada. No importa cuál realices, el objetivo es que te rompas la cabeza un rato ;).\nUtilizando tidy (dplyr), y en una sola cadena, filtra los datos para la isla Biscoe, crea una columna que tenga cada valor de la masa corporal menos la media global de esa columna, y luego obtén el promedio de esta columna para cada especie.\nRealiza la misma operación del punto 2 con R base. Algunas funciones que puedes tomar en cuenta para estos dos puntos son subset, filter, mutate, aggregate, summarise y/o for.\nOpcionalmente puedes intentar mezclar ambos procedimientos para llegar al mismo resultado.\n¿Qué opción se te hizo más sencilla? ¿tidy, R base o una combinación de las dos?\n\n\n\n\n\nMatloff N. 2020. Teaching R in a Kinder, Gentler, More Effective Manner: Teach Base-R, Not Just the Tidyverse."
  },
  {
    "objectID": "s02_fundan.html#objetivo-de-aprendizaje",
    "href": "s02_fundan.html#objetivo-de-aprendizaje",
    "title": "Fundamentos del análisis de datos",
    "section": "Objetivo de aprendizaje",
    "text": "Objetivo de aprendizaje\nEn esta sección del curso comenzarás a adentrarte al análisis de datos. Comenzarás con una introducción a la teoría de la visualización de datos y después aborarás el concepto más importante: la probabilidad. Posteriormente escalarás a la teoría del muestreo, revisarás cómo describir tus datos (tanto numérica como gráficamente) y por último cómo probar si tu evidencia puede dejar en ridículo a una (a veces ridícula) hipótesis de nulidad."
  },
  {
    "objectID": "c05_ggplot2.html#representaciones-de-la-realidad",
    "href": "c05_ggplot2.html#representaciones-de-la-realidad",
    "title": "5  Principios de visualización de datos y ggplot2",
    "section": "5.1 Representaciones de la realidad",
    "text": "5.1 Representaciones de la realidad\nComo seres humanos, con una tendencia a encontrar el camino que ofrezca una menor resistencia (otra forma de decir que somos flojos), usualmente resumimos una realidad altamente compleja utilizando distintas estrategias. Si yo te pido que me digas “qué es una orca” puedes darme una descripción textual del tipo “las orcas son mamíferos del orden Cetartiodactyla”, una descripción numérica en forma de mediciones (Longitud: 6-8 m) o medidas de tendencia central/dispersión (Peso máximo: 5.5 toneladas), pero usualmente preferimos medios audiovisuales como un dibujo, un video o, en el caso de la investigación y el tema principal de hoy, gráficos."
  },
  {
    "objectID": "c05_ggplot2.html#visualización-de-datos",
    "href": "c05_ggplot2.html#visualización-de-datos",
    "title": "5  Principios de visualización de datos y ggplot2",
    "section": "5.2 Visualización de datos",
    "text": "5.2 Visualización de datos\n\n\n\nFigura 5.1: Calidad de gráficos en artículos científicos (Fuente: xkcd)\n\n\nTenemos una gran variedad de razones y objetivos para los cuales necesitamos o recurrimos a visualizaciones de datos (solo visualizaciones a partir de aquí), algunas de ellas son:\n\nExplorar nuestros datos\nElaborar reportes con nuestros análisis\nComunicar hallazgos gráficamente\n\nSoportar resultados en una publicación\nPresentar a un público no especializado\n\n\n\n\n\n\n\n\nImportante\n\n\n\nIndependientemente de para qué hagamos la visualización, lo cierto es que es algo que merece mucha dedicación y que, en realidad, va más allá de hacer un simple gráfico: la visualización de datos nos permite contar una historia.\n\n\nSi obviamos no tenemos cuidado al hacer nuestras visualizaciones procedimiento podemos terminar con un gráfico como el siguiente:\n\n\n\nFigura 5.2: Gráfico “por defecto” en Statistica\n\n\n¿Qué tiene de malo y cómo podemos mejorarlo? Esa es la pregunta que vamos a responder en esta sesión, replanteándola como ¿qué debo tener en cuenta para hacer una buena visualización?, y para lo cual seguiremos algunas heurísticas que nos ayudarán a ser conscientes de qué elementos incluiremos en el gráfico y cuáles no.\n\n\n\n\n\n\nNota\n\n\n\n¿Qué es una heurística? Es una guía que vamos a seguir hasta que encontremos algo mejor; es decir, las recomendaciones que vamos a revisar son, al final del día, eso: recomendaciones. No tienes ninguna obligación de seguirlas, ni mi objetivo es imponer estas ideas, sino hacer que seas consciente de todo lo que implica una visualización de datos.\n\n\n\n5.2.1 Cairo y su rueda\nLa primera heurística que vamos a revisar es la rueda de Cairo (2012). Alberto es una de las figuras más relevantes en el área de la visualización de datos, y una de las muchas heurísticas que propone es considerar la siguiente rueda:\n\n\n\nFigura 5.3: Rueda de Cairo\n\n\nEsta rueda no es una guía tal cual, sino más bien una herramienta que nos permite evaluar las compensaciones que debemos hacer al realizar nuestras visualizaciones. La rueda nos muestra seis ejes que hacen referencia a distintas características de una visualización:\n\nAbstracción-Figuración: Este primer eje refiere a qué tipo de gráfico estamos utilizando. ¿Tenemos un gráfico abstracto como un gráfico de barras, dispersión, etc.? O, por el contrario, tenemos un dibujo que describe exactamente el proceso (gráfico figurativo).\nFuncionalidad-Decoración: Esta es bastante auto-explicativa, y hace referencia a qué tantos elementos decorativos forman la visualización.\nDensidad-Ligereza: Refiere al número de elementos que conforman la visualización o, en otras palabras, a qué tan cargado está nuestro gráfico.\nDimensionalidad: Refiere al número de variables que se incluyen en el gráfico o, puesto de otra forma, a cuántas partes de la historia queremos narrar con un mismo gráfico. No es lo mismo hacer un gráfico de dispersión en el que los puntos sean todos del mismo color a hacer uno donde los puntos estén coloreados según una tercera variable.\nOriginalidad-Familiaridad: Refiere a si los elementos utilizados son familiares para el observador o son elementos nuevos. Una forma fácil de entender este eje es poner lado a lado un gráfico de frecuencias y un gráfico de densidad. Ambos cumplen con el mismo objetivo (uno es más adecuado para variables continuas), pero es bastante probable que los gráficos de frecuencias te sean más familiares.\nRedundancia-Novedad: Referente al número de elementos que ayudan a soportar una misma parte de la historia. Un ejemplo sería tener un gráfico de frecuencias para distintas clases, donde cada barra tiene un color distinto (como en la Figura 5.7). Ahí tanto las clases en el eje x como el color de las barras nos indican que son cosas diferentes.\n\nSi analizamos un poco la distribución de las características, los gráficos que tiendan más a estar en la mitad superior de la rueda son más complejos y profundos que los de la parte baja. De hecho, el mismo Cairo menciona que los científicos e ingenieros prefieren una rueda como esta:\n\n\n\nFigura 5.4: Rueda preferida por científicos e ingenieros\n\n\nMientras que los artistas, diseñadores gráficos y periodistas preferirían una esta otra:\n\n\n\nFigura 5.5: Rueda preferida por diseñadores gráficos\n\n\n¿Esto quiere decir que forzosamente debamos de hacer gráficos abstractos, con muchos elementos “originales” pero mínimas decoraciones, que representen múltiples variables y que no sean redundantes? PARA NADA. Recuerda, esto es solo una heurística, y el hacia dónde te inclines en cada eje dependerá de qué objetivo tengas con tu visualización, a quién vaya dirigida e, incluso, en qué medio va a ser observada.\n\n5.2.1.1 Minard y la (fallida) invasión Napoleónica\nPara cerrar con esta heurística házme un favor y observa con atención el siguiente gráfico:\n\n\n\nFigura 5.6: Gráfico de Minard sobre la invasión Napoleónica a Rusia\n\n\nEvidentemente es un gráfico muy cargado de información, pero es también considerado por muchos como la “mejor visualización que se ha creado”. ¿Cuántos elementos lo conforman y qué parte de la historia cuentan?\n\n\n\n5.2.2 Tufte y sus tintas\nAhora hablemos de una heurística propuesta por una de las figuras seminales en la visualización de datos: Edward Tufte. Él propone que una buena visualización debería tener una alta proporción de tinta de datos a tinta total (Tufte, 1983), donde la tinta de datos es la tinta gastada para imprimir el núcleo del gráfico; es decir, la información mínima necesaria para transmitir el mensaje, mientras que la tinta total es, como el nombre sugiere, la cantidad de tinta empleada para imprimir el gráfico completo. ¿Qué implica una alta proporción de tinta de datos a tinta total? Que reduzcamos lo más posible el número de elementos en el gráfico.\n\n5.2.2.1 Darkhorse Analytics: Data looks better naked\nPara esta parte sigamos el ejemplo de Darkhorse Analytics, en el cual se mejora la siguiente figura:\n\n\n\nFigura 5.7: Gráfico aparentemente inofensivo (Calorías por 100g de alimento)\n\n\nEl gráfico así como está presentado puede encajar en mayor o menor medida con tus gustos, pero lo cierto es que es un gráfico con una gran cantidad de elementos. Bajo la heurística de las tintas de Tufte quitar es mejorar, así que vayamos elemento a elemento. El primero es el color de fondo. No entraré en el debate de si el color es bonito o no, sino más bien quiero que te preguntes ¿qué me dice el color de fondo sobre lo que se está graficando? La respuesta es: nada; por lo tanto, hay que eliminarlo:\n\n\n\nFigura 5.8: ¿Realmente es necesario el color de fondo?\n\n\nEl siguiente elemento son, en realidad, varios. La visualización muestra el número de calorías por 100g de distintos alimentos. Eso aparece repetido en el título, el título del eje x, el título del eje \\(y\\) y, tal vez de forma menos obvia, en la acotación de colores y las etiquetas del eje x. ¿Qué hacer? Una posible solución Es reducir el título a “calorías por 100g”. ¿100 g de qué? de lo que tenemos en las etiquetas del eje x. Esto nos permite no solo reducir el texto en el título, sino también eliminar los títulos de ambos ejes y la acotación:\n\n\n\nFigura 5.9: ¿Calorías por 100 g de qué?\n\n\nYa “adelgazamos” nuestro gráfico, pero aún podemos continuar. El siguiente elemento son las cuadrículas que delimitan el área del gráfico y el área de graficado. Nuevamente la pregunta es ¿nos dicen algo sobre las calorías de los alimentos? Y nuevamente la respuesta es no. Aunado a esto, desde un punto de vista psicológico, el tener esas delimitaciones puede “limita” la imaginación del observador. Estés o no de acuerdo con este último punto, no cambia el que no aportan nada, así que podemos quitarlas:\n\n\n\nFigura 5.10: ¿Libertad?\n\n\nPuede que estés pensando que hasta este punto ya eliminamos muchas cosas, pero aún hay un par de elementos más. El primero son los colores de las barras. Al igual que la acotación son un elemento redundante que manda el mensaje de que estamos tratando con cosas diferentes, pero eso ya está definido claramente en el eje x. En este caso sería más interesante resaltar alguna categoría en particular, tal vez para responder a la pregunta ¿qué tan calóricos son estos alimentos en relación al tocino?, lo que nos permitiría resaltar al tocino en rojo si ponemos a las demás en un color “neutro” como el gris:\n\n\n\nFigura 5.11: ¿Más o menos calorías que el tocino?\n\n\n¿Esto es todo? Aún no. Tenemos en las barras un elemento derivado de las décadas de los 80s-90s cuando se masificó el uso de las computadoras y los modelos tridimensionales: las sombras y el volumen de las barras. Realmente no aportan nada a la narrativa, sino que solo “cargan” más el gráfico, así que también las eliminamos:\n\n\n\nFigura 5.12: Un look más minimalista\n\n\nEn este punto hay algo que destaca tanto o incluso más que las barras: el texto del título y de las etiquetas del eje \\(y\\). Evidentemente no podemos eliminarlos porque ya no sabríamos qué es lo que representa el gráfico, pero lo que sí podemos hacer es cambiar las negritas por gris claro, con lo cual cambiamos el punto de anclaje de la figura a la barra roja con el tocino:\n\n\n\nFigura 5.13: ¡Tocino!\n\n\nAhora ya estamos muy cerca, pero hay algo que se ve fuera de lugar. Ese “algo” está relacionado con el eje \\(y\\): las etiquetas y las líneas guía. En este punto seguramente me dirás “si quitamos esos elementos ya no vamos a saber”qué tanto es tantito” o cuántas calorías tiene cada cosa”, y tendrías la razón, pero podemos sustituirlo poniendo directamente el número de calorías por 100g de cada alimento sobre su barra, lo cual resulta en un gráfico no solo más simple sino también más preciso:\n\n\n\nFigura 5.14: ¿Cuántas calorías tienes?\n\n\nSi comparas esta última visualización con la Figura 5.7 notarás que hubo un cambio notable. El gráfico nuevo es más simple, más fácil de leer e incluso más preciso que el primero, con todo y que tiene muchos menos elementos.\n¿Con esto quiero decir que siempre debamos de tomar esta aproximación minimalista? No, para nada. De hecho, hay casos en los cuales el tener muchos elementos visuales puede ayudar a llamar la atención del lector, a expensas de la precisión del gráfico. Tomemos el siguiente ejemplo:\n\n\n\nFigura 5.15: Costos monstruosos\n\n\nAmbos gráficos presentan exactamente la misma información, pero estarás de acuerdo conmigo en que el dibujo con el monstruo es mucho más llamativo que el simple gráfico de barras, lo cual lo hace más adecuado para el medio en el que fue distribuido: una revista/periódico. Evidentemente no es compatible con una publicación científica, pero si presentas el segundo gráfico es bastante probable que la gente no voltee a verlo.\n\n\n\n5.2.3 Cairo y sus principios\nEsta siguiente heurística también fue propuesta por Cairo, y son cinco sencillos principios que debemos de seguir para llevar a una visualización altamente efectiva. El primero es el principio de funcionalidad, en el que los elementos del gráfico deben de ayudar a transmitir la historia que estamos contando. Volvamos a la Figura 5.7. Las barras, sus colores y la acotación eran funcionales, pero no lo eran el color del fondo o las cuadrículas de graficado.\nEl segundo principio es el de veracidad. Este es auto-explicativo, pero es posiblemente el más importante de todos: que los datos presentados sean veraces, y que sean presentados de una forma veraz o, en otras palabras: NO CUCHAREAR DATOS NI SU APARIENCIA. Ejemplos de visualizaciones donde este principio no se cumple abundan en los medios de comunicación, algunas veces sin que sea el objetivo, pero muchas otras intencionalmente para forzar una narrativa. Un caso particular es la Figura 5.16, en donde tenemos el gráfico presentado en los medios de comunicación durante las elecciones entre Nicolás Maduro y Henrique Capriles. En apariencia la diferencia en votos era gigantesca, acrecentado no solo por la diferencia de alturas de los cilindros, sino también por ser figuras tridimensionales. Si los seres humanos somos malos juzgando áreas (razón por la que no se recomienda hacer gráficos de pastel), somos peores aún juzgando volúmenes.\n\n\n\nFigura 5.16: A que no me alcanzas\n\n\nVolviendo a la diferencia de alturas, aquí hay una “trampa” más: el eje \\(y\\) se encuentra truncado. ¿Qué tanto? Lo suficiente para que una diferencia de 1.59% se vea como una ventaja abrumadora de Maduro sobre Capriles. ¿Cómo se vería el gráfico si se presentara de forma veraz? Bastante menos dramático, eso es seguro:\n\n\n\nFigura 5.17: Tú y yo no somos tan diferentes\n\n\nRecuerda: no porque estemos contando una historia tenemos porque forzar una narrativa o solo mostrar lo que nos conviene. Nuevamente, ejemplos como este abundan en los medios de comunicación, y aquí puedes ver algunos otros.\nEl tercer principio es el de belleza. También es autoexplicativo: que el gráfico sea “bonito”. El problema con este principio es que es sumamente subjetivo. Lo que puede ser bonito para mi puede no serlo para ti, y viceversa, pero podemos valernos de las dos heurísticas anteriores para llegar a un gráfico que sea atractivo y, sobre todo, legible.\nSi estos tres principios se cumplen y van de la mano, llegamos de forma automática al cuarto: el principio de comprensibilidad. Cairo menciona que una visualización comprensible es aquella que permite que el lector/observador pueda ver el gráfico, analizarlo y que pueda tener un momento de “¡EUREKA!”, sin que nosotros describamos el gráfico.\n\n\n5.2.4 Del dicho al hecho hay mucho trecho\nEstas son algunas de las consideraciones más básicas que debemos de tener al realizar una visualización de datos, pero no son las únicas. Te recomiendo leer el artículo de Rougier, Droettboom & Bourne (2014), el cual tiene otras guías para mejorar nuestras figuras. Ahora bien, una cosa es conocer la teoría, pero de nada sirve si no podemos llevarlo a la práctica, y es aquí donde entra ggplot2."
  },
  {
    "objectID": "c05_ggplot2.html#introducción-a-ggplot2",
    "href": "c05_ggplot2.html#introducción-a-ggplot2",
    "title": "5  Principios de visualización de datos y ggplot2",
    "section": "5.3 Introducción a ggplot2",
    "text": "5.3 Introducción a ggplot2\nRecordarás de las primeras sesiones que un componente muy importante de R es la realización de gráficos, lo cual quiere decir que R base nos permite realizar visualizaciones de datos. De hecho, el graficador por defecto es sumamente potente, pero desafortunadamente no es precisamente intuitivo. En este sentido, la librería ggplot2 es una (¿mejor?) alternativa en la que los gráficos se crean de manera declarativa, y está basada en el libro “Grammar of Graphics” (Wilkinson, 2005). Digo “¿mejor?” porque, como en todo, es una cuestión de gustos y costumbre; sin embargo, pedagógicamente es bastante amable. La creación de gráficos se realiza mediante capas, donde tenemos nuestros datos, en un data.frame, luego indicamos qué queremos que grafique y en dónde, luego cómo queremos que lo grafique, y luego cómo queremos los demás elementos. En código tendríamos algo así:\n```{r}\nggplot(data = datos, aes(x, y)) + geom_*() + ...\n```\nPodría explicarte qué es cada cosa aquí, o podemos mejor aprender haciendo y crear una visualización. Antes de comenzar una visualización es necesario saber qué queremos responder con ella. En este caso, utilizaremos la base de datos mpg incluída en ggplot2. El primer paso es, entonces, conocer la información que contiene. Para ello guardaremos la base en una variable que llamaremos df1:\n\nlibrary(ggplot2)\n\ndf1 <- ggplot2::mpg\ndf1\n\n\n\n  \n\n\n\nUna manera rápida de tener una idea de cómo está dispuesta una base de datos es utilizando la función head(var). Esta nos mostrará solo las primeras instancias (renglones) del data.frame que estemos analizando. En la tabla inferior podemos ver que se trata de una base de datos sobre automóviles y que las columnas representan: el fabricante, el modelo, el desplazamiento de combustible (litros), el año del modelo, el número de cilindros, el tipo de transmisión, el tipo de tracción, los consumos en ciudad y autopista (en millas por galón, mpg), el tipo de combustible que utilizan y la clase a la que pertenencen. También nos plantearemos el objetivo de eliminar la mayor cantidad de elementos posibles hasta solo tener el esqueleto y de ahí agregar algunos elementos que favorezcan la interpretación.\n\nhead(df1)\n\n\n\n  \n\n\n\n\n5.3.1 ggplot() + ...\nA partir de esta información podemos tratar de responder si existe una relación apreciable entre el consumo de combustible (por ejemplo en autopista) y el desplazamiento del motor, considerando la clase del vehículo. Para atender a esta pregunta utilizaremos un gráfico de dispersión, con el desplazamiento en el eje x, el consumo en el eje \\(y\\) y la clase indicada por los colores de los puntos. Ahora que tenemos claro qué queremos visualizar y cómo lo vamos a visualizar podemos empezar a graficar. El primer paso es inicializar el espacio de graficado con la función ggplot() y pasarle los parámetros estéticos utilizando la función aes(x, y, colour). Es importante mencionar que en este momento aparecerá únicamente el espacio de graficado en blanco. Esto es normal, ya que únicamente definimos el “qué”, pero no el “cómo”.\n\nggplot(data = df1, aes(x = displ, y = cty, colour = class))\n\n\n\n\nYa que inicializamos el espacio gráfico podemos agregar la información que nos interesa. Para facilitar la construcción paso a paso y evitar el repetir código innecesariamente podemos almacenar la gráfica completa en una variable (por ejemplo plot2) e ir añadiendo capas (operador +) posteriormente. Para ver un gráfico guardado en una variable simplemente hay que llamar a esa variable. La primera capa que agregaremos será la que indicará el tipo de gráfico que deseamos (nombrados como geom_*), en este caso un gráfico de dispersión:\n\nplot1 <- ggplot(data = df1, aes(x = displ, y = hwy, colour = class)) +\n         geom_point()\nplot1 # Imprime el gráfico\n\n\n\n\nAhora sí tenemos la información que necesitamos y podríamos comenzar a describir el gráfico, pero en realidad hay demasiados elementos que son innecesarios y otros que son poco informativos en su estado actual (etiquetas de ejes), entonces trabajemos uno por uno. Para modificar las etiquetas de los ejes podemos utilizar las funciones xlab() y ylab() como capas separadas; sin embargo, podemos modificar todas las etiquetas y títulos en un mismo paso utilizando la función labs(title, x, y, caption, colour, ...).\n\nplot2 <- plot1 + labs(x = \"Desplazamiento (l)\",\n                      y = 'Consumo (mpg)',\n                      colour = 'Clase',\n                      title = \n                        'Tamaño del motor y Rendimiento de combustible',\n                      subtitle = 'Consumo en carretera',\n                      caption = 'Datos: ggplot2::mpg')\nplot2\n\n\n\n\n\n\n5.3.2 Tema de ggplot2\nAhora que está claro cuáles son las variables que estamos mostrando podemos empezar a modificar la estética. Recordemos que debemos mantener la relación datos/tinta lo más alta posible, y uno de los elementos más prevalentes del gráfico es el fondo gris con todo y cuadrículas. Para modificar esos elementos tenemos que modificar el “tema” de la gráfica, que no es otra cosa mas que utilizar una función que nos permita modificar en una sola línea la estética general del gráfico. Los temas se encuentran señalados con el nombre theme_*. Probemos con theme_minimal():\n\nplot2 + theme_minimal()\n\n\n\n\nLogramos eliminar el fondo gris y de paso las “espinas” (líneas de los ejes) y ahora el gráfico está en mucho mejor condición para ser presentado; sin embargo aún podemos ir más lejos. El objetivo de esta gráfica no es ver los detalles precisos de la información, si no extraer la información más relevante, por lo que la cuadrícula es un elemento que no aporta nada a la visualización. Para retirarla utilizaremos la función theme(), la cual permite modificar el aspecto de todos los elementos del gráfico. En realidad, las funciones theme_*() son aplicaciones de theme() con diferentes valores por defecto, por lo que podemos replicar el efecto de theme_minimal() e incluir otras modificaciones. Otra función muy útil para este procedimiento es la función element_blank(), la cual le indica a ggplot2 que no debe mostrar ese elemento. Otra cuestión importante que debemos de considerar es la relación de aspecto. Debido a que esta puede modificar enormemente la percepción de los datos, su selección no es algo trivial. En general, la proporción áurea (1:1.61) es un buen punto de partida y en series de tiempo es la proporción que menos deforma los datos. Una proporción cuadrada tiene sentido únicamente en aquellos casos en los que ambos ejes tengan la misma magnitud de variación y procuraremos que el eje más largo sea aquel con la variación más pequeña. En este caso, la variación del eje \\(y\\) (5 a 45) es mucho mayor que la del eje x (1.5 a 7), por lo cual una proporción cuadrada no sería una buena alternativa. En su lugar, utilicemos la proporción áurea. El último elemento que eliminaremos aquí son las marcas de los ejes, ya que realmente no aportan demasiada información.\n\nplot2 <- plot2 + \n         # Eliminamos la cuadrícula menor\n         theme(panel.grid.minor = element_blank(),\n               # Eliminamos la cuadrícula mayor\n               panel.grid.major = element_blank(),\n               # Eliminamos el color de fondo\n               panel.background = element_blank(),\n               # Eliminamos las líneas de los ejes\n               axis.line = element_blank(),\n               # Eliminamos la leyenda\n               legend.key = element_blank(),\n               # Cambiamos la relación de aspecto\n               aspect.ratio = 1/1.61,\n               # Eliminamos las marcas de los ejes\n               axis.ticks = element_blank()\n                       )\nplot2\n\n\n\n\n\n\n5.3.3 Personalizar los ejes\nAhora que nos deshicimos del fondo, la cuadrícula y las líneas y marcas de los ejes podemos trabajar en los valores de los ejes. Una de las mejores maneras de hacerlo es utilizando las funciones scale_x_*() o scale_y_*(), sustituyendo el * por continuous o discrete dependiendo del tipo de variable con el que estemos trabajando. En este caso, eliminaremos por completo las marcas del eje \\(y\\) y dejaremos únicamente los desplazamientos más comunes en el eje x.\n\nplot2 <- plot2 + scale_x_continuous(breaks = c(1.8, 2.5, 5, 7)) +\n                 scale_y_continuous(breaks = NULL)\nplot2\n\n\n\n\n\n\n5.3.4 Añadir líneas de referencia\nAhora que nos deshicimos de los valores del eje la gráfica ya no es entendible debido a que no sabemos cuál es la orientación o la escala de los datos. Una alternativa es añadir un par de líneas de referencia. Esto lo haremos con la función geom_hline(), la cual nos permite añadir líneas horizontales a través de todo el gráfico que cruzan al eje \\(y\\) en una posición que nosotros determinamos:\n\n# Valores de referencia como el mínimo, la media y\n# el máximo de los consumos\n\nrefs <- c(round(min(df1$hwy),0),\n          round(mean(df1$hwy),0),\n          round(max(df1$hwy),0))\n\n# Líneas de referencia, una verde para el mejor consumo,\n# una gris para el consumo promedio y una roja para el peor consumo\nplot2 <- plot2 + geom_hline(yintercept = refs[1],\n                            colour = 'firebrick', alpha = 0.5,\n                            linetype = 'dashed') +\n                 geom_hline(yintercept = refs[2],\n                            colour = 'lightslategrey', alpha = 0.5,\n                            linetype = 'dashed') +\n                 geom_hline(yintercept = refs[3],\n                            colour = 'forestgreen', alpha = 0.5,\n                            linetype = 'dashed')\nplot2\n\n\n\n\nAhora el gráifico ya cuenta nuevamente con un sentido de dimensión, pero no tenemos los valores de referencia, entonces habrá que poner esas anotaciones con la función geom_text(), utilizando como valores de posición en y los mismos que las líneas de referencia + un pequeño valor:\n\n# Líneas de referencia con los mismos colores\nplot2 <- plot2 + annotate('text', x = 1.3, y = refs[1]+1, \n                          label = as.character(refs[1]),\n                          colour = 'firebrick') +\n                 annotate('text', x = 1.3, y = refs[2]+1,\n                          label = as.character(refs[2]),\n                          colour = 'lightslategrey') +\n                 annotate('text', x = 1.3, y = refs[3]+1,\n                          label = as.character(refs[3]),\n                          colour = 'forestgreen')\nplot2\n\n\n\n\nCon esta última modificación terminamos de explorar algunas de las funciones más básicas e importantes para personalizar los elementos que más impactan en una visualización, pero antes de terminar de discutir este punto me gustaría terminar el objetivo que nos propusimos al inicio de sacar información de la gráfica. En general, existe una tendencia a que el consumo de combustible incremente conforme incrementa el desplazamiento, lo cual es de esperarse, ya que el desplazamiento es una medida de el volumen máximo de combustible que puede entrar al motor en un momento dado; sin embargo, podemos también observar que, independientemente del desplazamiento, las SUVs y pickups tienden a tener los peores rendimientos de combustible, mientras que los subcompactos tienden al otro extremo. Podemos también analizar a los vehículos de dos plazas y ver que aún cuando tienen desplazamientos altos, sus rendimientos son mejores que los de las SUVs.\n\n\n5.3.5 Conclusión y ejercicio\nEn cuanto a la parte visual, se podría argumentar que esta visualización final no es tan precisa como la primera, que algún elemento podría embellecerse, o que podriamos eliminar la leyenda y poner etiquetas de texto en algunos puntos para indicar las clases. Todos estos argumentos y muchos otros serían válidos ya que la estética es algo subjetivo; sin embargo, las decisiones que tomemos deberán estar en función del medio de distribución de la visualización (no es lo mismo una página web que en un medio impreso, por ejemplo) y sobre todo del público objetivo. Esta visualización en particular funciona para los fines didácticos que tenía en mente, es adecuada para una presentación de resultados de manera electrónica como este video, pero no es una visualización adecuada para una publicación científica. Arreglar eso será tu ejercicio para esta sesión.\nPara finalizar con el objetivo principal de la clase te presento la visualización inicial y la final, una junto a la otra, para ver en dónde comenzamos, dónde terminamos y cómo llegamos hasta aquí. También te sugiero revises y descargues el PDF de esta página, que es un acordeón donde se encuentran los gráficos y funciones más comunes. Más adelante revisaremos algunos de ellos pero es un recurso que vale la pena tener a la mano.\n\n\n\n\n\n\n\n\n\n\n\n5.3.5.1 Gráfico final\nY también el código necesario para el gráfico final, todo en un solo bloque de código.\n\n# Valores de referencia para utilizar en la gráfica\nrefs <- c(round(min(df1$hwy),0),  # Valor mínimo = peor consumo\n          round(mean(df1$hwy),0), # Valor promedio\n          round(max(df1$hwy),0))  # Valor máximo = mejor consumo\n\n# Objeto con todos los pasos para llegar a la gráfica final\n# Inicializamos el espacio gráfico\nfinal.plot <- ggplot(data = df1, aes(x = displ, y = hwy,\n                                     colour = class)) +\n              # Gráfico de dispersión\n              geom_point() +\n              # Establecemos los títulos, subtítulos y un pie de foto\n              labs(x = 'Desplazamiento (l)',\n                   y = 'Consumo (mpg)',\n                   colour = 'Clase',\n                   title = 'Tamaño del motor y Rendimiento de combustible',\n                   subtitle = 'Consumo en carretera',\n                   caption = 'Datos: ggplot2::mpg'\n                   ) +\n              #Eliminamos la cuadrícula menor\n              theme(panel.grid.minor = element_blank(),\n                    #Eliminamos la cuadrícula mayor\n                    panel.grid.major = element_blank(),\n                    #Eliminamos el color de fondo\n                    panel.background = element_blank(),\n                    #Eliminamos las líneas de ejes\n                    axis.line = element_blank(),\n                    #Eliminamos el fondo de la leyenda\n                    legend.key = element_blank(),\n                    #Establecemos la rel. de aspecto\n                    aspect.ratio = 1/1.61,\n                    #Eliminamos las marcas de los ejes\n                    axis.ticks = element_blank(),\n                    #Cambiamos el tipo de letra\n                    text = element_text(family = 'Times',\n                                        colour = 'gray50')\n                    ) + \n              # Reducimos las divisiones del eje ex a 4 valores\n              scale_x_continuous(breaks = c(1.8, 2.5, 5, 7)) +\n              # Eliminamos las divisiones del eje $y$\n              scale_y_continuous(breaks = NULL) +\n              # Añadimos una línea roja en el peor consumo\n              geom_hline(yintercept = refs[1],\n                         colour = 'firebrick', alpha = 0.5, \n                         linetype = 'dashed') +\n              # Añadimos una línea gris en el consumo promedio\n              geom_hline(yintercept = refs[2],\n                         colour = 'lightslategrey', alpha = 0.5, \n                         linetype = 'dashed') +\n              # Añadimos una línea verde en el mejor consumo\n              geom_hline(yintercept = refs[3],\n                         colour = 'forestgreen', alpha = 0.5,\n                         linetype = 'dashed') +\n              # Etiqueta del peor consumo\n              annotate('text', x = 1.3, y = refs[1]+1,\n                       label = as.character(refs[1]),\n                       colour = 'firebrick') +\n              #Etiqueta del consumo promedio\n              annotate('text', x = 1.3, y = refs[2]+1,\n                       label = as.character(refs[2]),\n                       colour = 'lightslategrey') +\n              # Etiqueta del mejor consumo\n              annotate('text', x = 1.3, y = refs[3]+1,\n                       label = as.character(refs[3]),\n                       colour = 'forestgreen') \n              \nfinal.plot\n\n\n\n\n\n\n\n5.3.6 Extras\nAunque estas modificaciones no necesariamente forman parte del proceso necesario para la visualización que era de nuestro interés, sí que son rutinarias, por lo que vale la pena echarles un ojo.\n\n5.3.6.1 Colores de puntos\nModificar los colores de los puntos. Podemos utilizar la función randomColor(n) de la librería con el mismo nombre. Esta función solamente recibe el número de colores que queremos y los generará de manera aleatoria:\n\naleat <- randomcoloR::randomColor(7)\nfinal.plot + scale_color_manual(name = \"Clase\", values = aleat)\n\n\n\n\nPodemos también especificar una paleta predefinida, utilizando la capa scale_color_brewer():\n\nfinal.plot + scale_color_brewer(type = \"seq\", palette = \"Paired\")\n\n\n\n\nOtra opción es directamente pasar un vector con los nombres de los colores que sean de nuestro interés:\n\ncolor_names <- c(\"red\", \"blue\", \"yellow\", \"black\",\n                 \"dodgerblue\", \"pink\", \"gray\")\nfinal.plot + scale_color_manual(name = \"Clase\", values = color_names)\n\n\n\n\n\n\n5.3.6.2 Tamaño de los puntos\nPara modificar el tamaño de los puntos solamente hay que agregar el argumento size a la capa geom_point, en el cuál indicaremos qué tamaños tomarán los puntos. Puede ser un solo valor:\n\nfinal.plot + geom_point(size = 0.1)\n\n\n\n\nO también a partir de una columna de la base de datos (dividida entre 5 para no obtener únicamente “manchas”):\n\nfinal.plot + geom_point(size = df1$hwy/5)\n\n\n\n\n\n\n5.3.6.3 Tipografías y Exportación de gráficos\nEl manejo de las tipografías en R es un poco especial, por ello usualmente recomiendo generar el gráfico en R, exportarlo como PDF (cairo_pdf(\"filename.pdf\", width, height, family)) y agregar las cursivas donde sea necesario; sin embargo, un paquete que puede resultar especialmente útil es ggtext. Este añade un nuevo tipo de “elemento” de texto que recibe formato Markdown (element_markdown()); es decir, podemos agregar itálicas o negritas. Para poder utilizarlo, sin embargo, es necesario modificar ligeramente nuestros datos de antemano. Para facilitarnos las cosas agregaremos una nueva columna a df1 que contenga las clases en itálicas y extraeremos los valores únicos (algo más eficiente sería hacerlo al revés, pero es más lógico de esta manera):\n\ndf1$clase <- paste0(\"*\",df1$class,\"*\")\nclases <- unique(df1$clase)\n\nFinalmente lo agregaremos a la gráfica. ¡OJO! Es necesario modificar el tema para que entienda el formato markdown:\n\nif(!require(ggtext)) {install.packages(\"ggtext\", dependencies = T)}\n\nLoading required package: ggtext\n\nfinal.plot + scale_color_discrete(name = \"Clase\",labels = clases) +\n             theme(legend.text = ggtext::element_markdown())\n\n\n\n\nCon este elemento podemos modificar también fracciones de cualquier texto de nuestra gráfica, por ejemplo carretera en negritas:\n\nfinal.plot + labs(subtitle = \"Consumo en **carretera**\") +\n             theme(plot.subtitle = ggtext::element_markdown())\n\n\n\n\nMezclando ambas modificaciones:\n\nfinal.plot + scale_color_discrete(name = \"Clase\",labels = clases) +\n             labs(subtitle = \"Consumo en **carretera**\") +\n             theme(plot.subtitle = ggtext::element_markdown(),\n                   legend.text = ggtext::element_markdown())\n\n\n\n\nAhora sí, esto es todo para esta clase. ¡Nos vemos en la siguiente!"
  },
  {
    "objectID": "c05_ggplot2.html#ejercicio",
    "href": "c05_ggplot2.html#ejercicio",
    "title": "5  Principios de visualización de datos y ggplot2",
    "section": "5.4 Ejercicio",
    "text": "5.4 Ejercicio\nAjusta la visualización de los datos mpg para que pueda ser publicable en una revista científica (de tu interés) y responde:\n\n¿Qué elementos quitarías?\n¿Qué elementos cambiarías?\n¿Qué elementos agregarías?\n¿Crees que en su estado actual cumple con los criterios de Tufte y Cairo que revisamos en clase? (Explica tu respuesta).\n\nNOTA: En vez de los datos mpg puedes utilizar datos propios o los datos de pingüinos de Palmer.\n\n\n\n\nCairo A. 2012. The functional art. Berkeley, USA: New Riders, Pearson Education.\n\n\nRougier NP, Droettboom M, Bourne PE. 2014. Ten Simple Rules for Better Figures. PLoS computational biology 10:e1003833-7. DOI: 10.1371/journal.pcbi.1003833.\n\n\nTufte E. 1983. The Visual Display of Quantitative Information. Cheshire, Connecticut: Graphics Press.\n\n\nWilkinson L. 2005. The Grammar of Graphics. USA: Springer."
  },
  {
    "objectID": "c06_prob.html#definiciones-básicas",
    "href": "c06_prob.html#definiciones-básicas",
    "title": "6  Probabilidad",
    "section": "6.1 Definiciones básicas",
    "text": "6.1 Definiciones básicas\nHabrás notado que he evitado utilizar la palabra probabilidad hasta este momento. Esto es porque la probabilidad es justamente el asignarle un número a las posibilidades y, por lo tanto, podemos pensar en la probabilidad com una medida de incertidumbre. ¿Cómo la expresamos numéricamente? La manera más sencilla de entender a la probabilidad es desde un punto de vista geométrico; es decir, como una proporción o una frecuencia relativa, de la forma ¿cuántas veces ha ocurrido B en relación al número de veces que han ocurrido tanto A como B? Pero vayamos paso a paso.\nPrimero, algunas (tediosas y obligadas) definiciones:\n\nExperimento aleatorio: Es, como su nombre lo indica, un experimento, prueba (o como quieras llamarlo) en el cual no puedes saber con certeza cuál va a ser el resultado. Un resultado es, entonces, la “salida” o lo que pasa al realizar el experimento una sola vez, y aquí seguramente te perdí, entonces un ejemplo: Si lanzar una moneda es nuestro experimento, sus resultados son cara y cruz. Imagina que lanzo una moneda al aire, ¿cuál crees que haya sido el resultado?\nEspacio muestreal: Son todos y cada uno de los posibles resultados de un experimento.\nEvento: Podemos pensar en un evento como un conjunto de datos/resultados, por lo tanto, el espacio muestreal es un evento.\n\nOtros eventos son:\n\nUniverso: Que incluye al espacio muestreal y el conjunto vacío (\\(\\varnothing\\)). Como te imaginarás, este conjunto está vacío, no tiene nada.\nUnión: Denotada como \\(\\cup\\) (diferente de u y U), representa la unión de dos conjuntos/eventos; es decir, el caso donde nos interesa encontrar A o B.\nIntersección: Denotada como \\(\\cap\\) (diferente de n), representa el traslape entre dos conjuntos/eventos; es decir, el caso donde nos interesa encontrar A y B.\nComplemento: Denotado como \\(\\tilde{A}\\), representa lo que no es ese conjunto, en este caso, lo que NO es A, que es el área restante de B y el conjunto vacío.\n\n\n\n\nFigura 6.2: Eventos y diagramas de Venn"
  },
  {
    "objectID": "c06_prob.html#axiomas-de-la-probabilidad",
    "href": "c06_prob.html#axiomas-de-la-probabilidad",
    "title": "6  Probabilidad",
    "section": "6.2 Axiomas de la probabilidad",
    "text": "6.2 Axiomas de la probabilidad\n¿Qué tiene que ver esto con probabilidad? Pues que estas denominaciones dan lugar sus leyes/reglas; es decir los Axiomas de la Probabilidad:\n\n\\(P(A) \\geq 0\\); es decir, todo evento tiene una probabilidad positiva y no mayor a 1. No necesita mayor explicación, simplemente ¿cómo interpretarías una probabilidad negativa?\n\\(P(U) = 1\\); es decir, la probabilidad de nuestro espacio muestreal y el conjunto vacío es 1. Eso tiene sentido, si hacemos un experimento aleatorio vamos a tener un resultado, independientemente de cuál sea, lo cual está relacionado con el tercer axioma:\nSi A y B son mútuamente excluyentes: \\(P(A \\cup B) = P(A) + P(B)\\). Básicamente, si nos interesa saber cuál es la probabilidad de que ocurran dos resultados, en donde si ocurre uno ya no ocurre el otro, lo único que tenemos que hacer es sumar la probabilidad de cada uno de ellos. Si lanzo una moneda al aire, la probabilidad de que caiga cara es del 50% y la probabilidad de que caiga cruz es del 50%, pero la probabilidad de que caiga es del 100%.\n\nAdicionales a estos tres axiomas tenemos dos casos especiales y una generalización:\n\n\\(P(\\varnothing) = 0\\); el cual es autoexplicativo, la probabilidad de que ocurra el evento vacío es 0.\n\\(P(\\tilde{A}) = 1 - P(A)\\); es decir, si \\(\\tilde{A}\\) representa lo que no es A y dado que \\(P(U) = 1\\), solo debemos de restarle a ese universo la probabilidad de A.\n\\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\). Te darás cuenta que este se parece al tercer axioma y es porque es una generalización al caso donde A y B no son mútuamente excluyentes. ¿De dónde sale la resta? De que si A y B se encuentran unidos (el diagrama de Venn inferior) y sumamos el área de A con el área de B terminamos sumando dos veces la zona en la que están sobrelapados (intersección); por lo tanto, tenemos que quitarlo una vez para no sobre estimar. A esta generalización se le conoce como la regla aditiva de la probabilidad.\n\n\n\n\nFigura 6.3: Axiomas de la probabilidad"
  },
  {
    "objectID": "c06_prob.html#probabilidades-marginales-y-condicionales",
    "href": "c06_prob.html#probabilidades-marginales-y-condicionales",
    "title": "6  Probabilidad",
    "section": "6.3 Probabilidades marginales y condicionales",
    "text": "6.3 Probabilidades marginales y condicionales\nAhora entendimos que podemos hacer operaciones con la probabilidad, y eso nos lleva a los siguientes dos conceptos que son sumamente importantes: las probabilidades marginal y condicional.\nHablamos de una probabilidad marginal cuando nos interesa la P(A) cuando \\(A \\cup B\\). En este caso, podemos expresar al conjunto A como \\(A = (A \\cap B) \\cup (A \\cup \\tilde{B})\\). ¿En Español? El conjunto A está dado por la unión de la intersección de A y B y la intersección de A con el complemento de B. ¿Aún menos rebuscado? Es la suma de las partes no unidas. El procedimiento aquí es justamente un caso similar a la regla aditiva de la probabilidad. Estamos sumando la zona traslapada entre A y B con lo que no es B, que nos deja únicamente con A. Te preguntarás por qué se denomina marginal. Para esto primero necesitamos definir una tabla de contingencia. Esta es simplemente una tabla en la que cada renglón tiene frecuencias relativas de los distintos niveles de una variable categórica y las columnas tienen las frecuencias relativas de cada nivel de otra variable categórica. En los márgenes de la tabla tenemos los totales para cada nivel (evento o conjunto) y de ahí viene el nombre.\nPor otra parte, la probabilidad condicional nos permite responder a la pregunta ¿cuál es la P(A) si ya sé que B ocurrió?. Matemáticamente la representamos como \\(P(A|B)\\) (probabilidad de A dado B), y es una razón de la probabilidad conjunta de A y B (\\(P(A,B)\\) o \\(P(A \\cap B)\\)) y la probabilidad marginal de B (\\(P(B)\\)); es decir: \\(P(A|B) = \\frac{P(A,B)}{P(B)}\\). La probabilidad conjunta representa la probabilidad de que dos eventos ocurran al mismo tiempo y puede llegar a ser un poco problemática. Si ambos eventos son independientes, obtenerla es sencillo: \\(P(A,B) = P(A)P(B)\\). El problema surge si A y B no son independientes, en cuyo caso: \\(P(A,B) = P(A)P(B|A)\\), lo cual nos lleva a una referencia cruzada. Por practicidad, y porque el interés del curso no es que sepas hacer estas cosas a mano, obtengamos la probabilidad conjunta desde su posición en la tabla de contingencia; es decir, cada una de sus celdas.\n\n\n\nFigura 6.4: Probabilidades marginales y condicionales\n\n\nEn el ejemplo de la diapositiva (OJO: los datos no son representativos de ninguna población) calculamos la \\(P(Blond|Blue)\\); es decir, la probabilidad de que alguien sea rubio si sabemos que tiene los ojos azules, dada por la división de la probabilidad conjunta \\(P(Blond,Blue) = 0.16\\) y la marginal \\(P(B) = 0.36\\) que resulta en \\(P(Blond|Blue) \\approx 0.44\\). ¿Cuál sería entonces la \\(P(Green|Red)\\)?"
  },
  {
    "objectID": "c06_prob.html#distribuciones-de-probabilidad",
    "href": "c06_prob.html#distribuciones-de-probabilidad",
    "title": "6  Probabilidad",
    "section": "6.4 Distribuciones de probabilidad",
    "text": "6.4 Distribuciones de probabilidad\n\n\n\nFigura 6.5: Estadísticos, probabilidades y distribuciones (Fuente: xkcd)\n\n\nDejando los memes de las viñetas (la inferior es bastante trágica), hablemos de cómo escalar de valores puntuales a algo más aplicado a la investigación. Podemos utilizar nuestra intución de probabilidad de manera cotidiana (e.g., probabilidad de lluvia), pero en cuestiones académicas tenemos una hipótesis de trabajo, la cual trasladamos a pruebas de significancia para realizar inferencias. Eso es algo que abordaremos más a detalle en la siguiente sección; sin embargo, vamos a tener múltiples datos, cuya distribución probabilidad es lo que va a moldear nuestros análisis. Es necesario, entonces, definir qué es una distribución de probabilidad.\nEn pocas palabras, una distribución de probabilidad es una lista con todos los resultados de un evento y sus probabilidades correspondientes. Hay una gran diversidad de distribuciones teóricas de probabilidad, cada una con sus peculiaridades, parámetros, momentos y lugares para utilizarlas. No te preocupes por aprenderlas todas, hablaremos de las distribuciones relevantes para cada modelo que apliquemos. Por ahora solo es importante que conozcas que, si hablamos de distribuciones discretas, hablamos entonces de la probabilidad de cada resultado. Si tenemos una distribución continua, podemos partirla en intervalos para discretizarla y hablar de la probabilidad de que una observación pertenezca a ese intervalo. Sea cual sea el caso, estas son masas de probabilidad, las cuales suman a 1, tal que:\n\\[\n\\sum_{i = 1}^n P(x_i) = 1\n\\]\n\n\n\nFigura 6.6: Algunas distribuciones de probabilidad y sus relaciones. ¿Te gusta ver símbolos? La imagen de la izquierda va a ser de tu agrado. ¿Eres más visual? Toma la de la derecha.\n\n\n\n\n\n\n\n\nNota\n\n\n\n¿A qué me refiero con discreta o continua? A los valores que pueden tomar los resultados que dan forma a una distribución. Una distribución de probabilidad discreta solo toma valores enteros o categóricos, mientras que una continua puede tomar valores decimales. La relación de esto con nuestros datos la veremos en la siguiente sesión: muestreo.\n\n\nPero volvamos al tema de las distribuciones continuas, porque tienen una cualidad bastante interesante. Resulta que en una distribución continua la probabilidad de cada valor (\\(P(x)\\)) es 0. ¿Por qué? Porque, por definición, todos los valores en el intervalo de la distribución son posibles y hay una cantidad infinita de ellos (de aquí sale también el problema de la precisión de punto flotante, pero esa es otra historia). ¿Cómo contender con esto? Podemos discretizar la distribución y hablar de masas de probabilidad de los intervalos resultantes (regla de Sturgess, por ejemplo); sin embargo, estos intervalos son, por mucho apellido de autor que lleven, arbitrarios. ¿Entonces? Podemos hacerlos infinitesimalmente pequeños; es decir, aproximar la amplitud de los intervalos a 0 (pero no exactamente 0) y entonces tenemos densidades de probabilidad. ¿Por qué densidad? Porque dividimos la masa de ese intervalo infinitesimalmente pequeño entre su amplitud, lo cual nos deja con una definición similar a \\(densidad = \\frac{masa}{área}\\). Si hacemos eso, nuestras densidades pueden ser mayores a 1, lo cual indica que tenemos una alta masa en relación a la escala. El otro cambio es que, como recordarás de tus clases de cálculo, al pasar de una variable discreta a una continua pasamos de una sumatoria a una integral:\n\\[\n\\sum_{i = 1}^n \\frac{p([x_i, i_i+\\Delta x])}{\\Delta x} \\Rightarrow \\int dxp(x) = 1\n\\]\nNo es necesario que memorices esto, solo que tengas en cuenta la diferencia entre masas y densidades de probabilidad. Como añadido, este mismo problema es lo que causa que un gráfico de frecuencias (histograma) no sea la mejor solución para ver la distribución de una variable continua. En su lugar podemos utilizar gráficos de densidad, los cuales hacen lo que acabamos de mencionar (al menos en escencia). ¿Cómo los hacemos? Eso lo veremos más adelante.\nDejando las ecuaciones de lado, la selección de la distribución de probabilidad que utilizaremos depende del problema. ¿Tienes datos de conteos? Puedes utilizar las distribuciones Poisson o binomial negativa. ¿Tienes datos continuos en el intervalo 0-1? Vale la pena echarle un ojo a la distribución Beta. ¿Tienes datos binarios? Deberías voltear hacia la distribución binomial y, de hecho, vamos a explorarla un poco para entender cómo funcionan las distribuciones de probabiliad y cómo podemos aprovechar sus implementaciones en R.\n\n\n\n\n\n\nNota\n\n\n\nAunque en el curso vamos a ver distintas ecuaciones y funciones matemáticas, NO es necesario que las memorices. Veremos solo las más indispensables y solo en los momentos en los que sean útiles para dar sentido a los procedimientos que estamos realizando. En general, prefiero utilizar pruebas visuales para que desarrolles una intuición sobre qué es lo que se está haciendo, por qué y para qué, aunque no tengas tatuadas las ecuaciones y resuelvas los problemas a mano. Todos hemos utilizado un microondas y sabemos que no debemos de meterle objetos metálicos pero ¿sabes exactamente cómo ensamblar uno?"
  },
  {
    "objectID": "c06_prob.html#distribuciones-de-probabilidad-y-volados",
    "href": "c06_prob.html#distribuciones-de-probabilidad-y-volados",
    "title": "6  Probabilidad",
    "section": "6.5 Distribuciones de probabilidad y volados",
    "text": "6.5 Distribuciones de probabilidad y volados\nEsta no sería una sesión de probabilidad si no habláramos de volados, así que hagamos justo eso. ¿Qué es un volado? Un experimento en el cual lanzamos una moneda y obtenemos uno de dos resultados: cara o cruz. Este resultado podemos verlo de otra manera: éxito (cara) o fracaso (cruz). Si estamos en un escenario de este tipo, en el cual solo podemos tener dos resultados, estamos hablando de ensayos de Bernoulli, y su formalización matemática se conoce como proceso Bernoulli. No voy a entrar en esos detalles porque es innecesario para los fines del curso, solo es un breviario cultural, lo que no es un breviario cultural es que también se les conoce como ensayos binomiales. Sí, binomial como en la distribución que mencioné antes, así que formulémosla paso a paso.\n\n\n\n\n\n\nNota\n\n\n\nDependiendo del software o referencia que estés consultando, a la distribución binomial también se le conoce como distribución Bernoulli.\n\n\nRetomemos la definición de una distribución de probabilidades: “una lista de resultados posibles y sus probabilidades correspondientes”. Entonces la distribución binomial tiene exactamente dos resultados posibles y (por lo tanto) dos probabilidades correspondientes. Ni una más, ni una menos. Si pensamos en un volado tendríamos lo siguiente:\n\nDistribución de probabilidades de un volado.\n\n\nResultado\nProbabilidad\n\n\n\n\nCara\n0.5\n\n\nCruz\n0.5\n\n\n\nPero si pensamos un poco más a profundidad en lo que implica la tabla anterior, es decir, solo dos resultados posibles mutuamente excluyentes, y recordamos nuestros axiomas de la probabilidad, podemos generalizarla de la siguiente manera:\n\nGeneralización a la distribución binomial.\n\n\nResultado\nProbabilidad\n\n\n\n\nÉxito\n\\(p\\)\n\n\nFracaso\n\\(q = 1 - p\\)\n\n\n\nEn otras palabras, dado que solo tenemos dos resultados posibles, la probabilidad de fracaso \\(q\\) siempre es el complementario del éxito \\(p\\) (\\(1 - p\\)). Por esta razón, la distribución binomial tiene un solo parámetro, que representa la probabilidad de éxito.\n\n\n\n\n\n\nNota\n\n\n\n¿Qué es un parámetro de una distribución? Un número que controla su forma. Piensa en ellos como perillas que cambian la distribución como las perillas de la estufa cambian la intensidad de la flama.\n\n\nAhora bien, recordarás que la probabilidad vista desde un punto de vista geométrico representa una proporción; por lo tanto, realmente (y formalmente) tenemos otro parámetro: el número de experimentos (\\(n\\)). Una forma común de representar textualmente las distribuciones es utilizando notación probabilística:\n\\[\nY \\sim Binom(n, p)\n\\]\nEsto se lee: \\(Y\\) es una variable aleatoria con distribución (sí, todo eso está contenido en el \\(\\sim\\)) binomial (\\(Binom\\)), con parámetros \\(n\\) y \\(p\\). Si sustituimos nuestros volados:\n\\[\nY \\sim Binom(n, p = 0.5)\n\\]\nTe estarás preguntando: ¿y la \\(n\\)? Pues en realidad puede ser cualquier número entero > 0. Podemos pensar en 4 volados, y esto quedaría de la siguiente manera:\n\\[\nY \\sim Binom(n = 4, p = 0.5)\n\\]\nSi lanzáramos los cuatro volados esperaríamos obtener dos caras y dos cruces, pero la realidad es demasiado caprichosa como para ajustarse a la teoría Podemos decirle a R que exprese ese capricho, utilizando la función rbinom(n, size, prob), donde rbinom es un acrónimo para “random binomial” (binomial aleatoria). Como el nombre sugiere, esta función permite generar variantes aleatorias de cierta cantidad de experimentos aleatorios (size), obtenidos de una distribución binomial con parámetros n y prob. En nuestro caso, nos interesa generar 4 volados (n) con una probabilidad de cara (éxito, prob) de 0.5. ¿Y size? En nuestro ejemplo es 1, pues hicimos un solo experimento donde “lanzamos” 4 volados:\n\nrbinom(n = 4, size = 1, prob = 0.5)\n\n[1] 1 0 0 1\n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nVoy a presentarte la expresión matemática de la distribución binomial. No te preocupes, no hay que resolver nada a mano, ni es necesario que te la aprendas, solo lo hago como recurso didáctico.\n\n\nDependiendo de tu suerte habrás obtenido exactamente dos caras (unos) y dos cruces (ceros), o un poco más de alguno. ¿La razón? Una variable aleatoria es una función que transforma la realidad a números. En el caso de nuestra distribución binomial está en términos de obtener la probabilidad de obtener EXACTAMENTE \\(k\\) éxitos en \\(n\\) ensayos de Bernoulli independientes, dada por la función de masas de probabilidad:\n\\[\nP(k|n,p) = P(X = k) = \\binom{n}{k}p^k(1-p)^{n-k}\n\\]\npara \\(k = 0, 1, 2, \\dots, n\\), donde \\(\\binom{n}{k}\\) representa el coeficiente binomial:\n\\[\n\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\n\\]\n\n\n\n\n\n\nNota\n\n\n\n¿Variante aleatoria es lo mismo que variable aleatoria? NO, pero sí están relacionadas. Una variable aleatoria es, como dije arriba, una función que transforma la realidad a números o, en otras palabras, una variable cuyo valor numérico depende de la salida de un fenómeno aleatorio (lanzar un volado), mientras que una variante aleatoria es un resultado particular obtenido de esa función (cara o cruz).\n\n\n¿Cómo traducimos esto al Español? Nuestros \\(k\\) éxitos suceden con probabilidad \\(p^k\\) y \\(n-k\\) fracasos suceden con probabilidad \\((1-p)^{n-k}\\); sin embargo, estos \\(k\\) éxitos pueden ocurrir en cualquiera de nuestros \\(n\\) experimentos, y hay \\(\\binom{n}{k}\\) formas diferentes (combinaciones) de distribuir esos \\(k\\) éxitos en la secuencia de \\(n\\) experimentos. Vale, no es tan tangible como me hubiera gustado, y mi objetivo no es saturarte de símbolos, solo quiero que veas que estas ecuaciones, por más abstractas que parezcan, tienen un sentido lógico, y cada elemento que las conforma nos dice algo sobre la intuición detrás de ellas.\nAhora bien, en este caso hemos asumido que la moneda que hemos estado lanzando es justa; es decir, que la probabilidad de que caiga cara o cruz es la misma, pero ¿qué pasa si la moneda está cargada hacia que caiga más de alguna manera? Es ahí donde entran las frecuencias a largo plazo.\n\n6.5.1 Frecuencia a largo plazo\nRecordarás que uno de los parámetros de la distribución binomial es \\(p\\), la probabilidad de éxito, y ese es el parámetro que queremos aproximar. Si bien es cierto que hay distintos métodos para abordar este tipo de problemas (estimación de parámetros), podemos aproximarlo con un poco de voluntad y fuerza bruta. ¿Cómo? Repitiendo el experimento que nos interesa una cantidad suficiente de veces. En nuestro caso queremos evaluar si una moneda es justa o no (si \\(p ≈ 0.5\\)), para lo cual realizaremos una serie de volados y ver cuál es la probabilidad de que caiga cara (\\(P(H)\\)), lo cual es lo mismo que la proporción caras (H) a cruces (T; H:T) final. Pongamos también en práctica nuestro R y generemos una función que reciba un número \\(N\\) de volados a realizar y que regrese un data.frame con el registro de cada resultado:\n\nvolado <- function(N){\n  \n  set.seed(0)\n  \n  prob <-  round(runif(n = 1), 2)\n  \n  # Realizar N lanzamientos\n  sec <- rbinom(n = N,\n                size = 1,\n                prob = prob)\n  \n  # Suma acumulativa:\n  # Si solo obtenemos 1s el resultado final será 500.\n  # Si solo obtenemos 0s el resultado final será 0.\n  suma_acum <- cumsum(sec)\n  \n  # Generamos un identificador para cada lanzamiento\n  lanzamiento <- 1:N\n  \n  # Calculamos la proporción H:T acumulada a cada lanzamiento\n  prop_acum <- suma_acum/lanzamiento\n  \n  # data.frame con resultados\n  resultados <- data.frame(lanzamiento, sec, prop_acum)\n  \n  return(list(prob, prop_acum, resultados))\n}\n\nEsta función tiene una pequeña “trampa”, o un pequeño “truco”, según como quieras verlo. Si pones atención, la primera línea es prob <- runif(n = 1), y ese objeto es el que se pasa a la función rbinom como argumento para prob. Estas funciones se ven sospechosamente similares, y con justa razón: ambas codifican distribuciones de probabilidad. Mientras que la función rbinom nos permite obtener variantes aleatorias de una distribución bionomial, la función runif nos permite obtener variantes aleatorias de una distribución uniforme. Una de las aplicaciones prácticas de esta distribución es la generación de números aleatorios, por lo que podemos generar una una “carga” aleatoria (prob) para nuestra “moneda virtual” (rbinom).\n\n\n\n\n\n\nNota\n\n\n\n¿Qué es una distribución uniforme? Una distribución en la que todos los elementos tienen la misma probabilidad. La función runif tiene tres argumentos: runif(n, min, max), donde n es el número de variantes a obtener, min es el valor mínimo y max el valor máximo. Por defecto da un número aleatorio entre 0 y 1.\n\n\nHabiendo dicho esto, lancemos 500 volados y veamos qué “carga” tiene la moneda, utilizando un gráfico de líneas:\n\nlibrary(ggplot2)\n\n# 500 lanzamientos\nN <- 500\n# Realizar los lanzamientos\nvolados <- volado(N = N)\n# Extraemos los resultados\n## P(H) \"real\"\nprob <- volados[[1]]\n## Data.frame\nres_volados <- volados[[3]]\n\n\n# Inicialización del espacio gráfico\nprop_plot <- ggplot(data = res_volados,\n                    aes(x = lanzamiento,\n                        y = prop_acum)) +\n              # Gráfico de líneas\n              geom_line(colour = \"deepskyblue4\",\n                        linetype = \"solid\",\n                        size = 0.7) +\n              # Marcador en los puntos\n              geom_point(colour = \"deepskyblue4\",\n                         alpha = 0.1,\n                         fill = NA,\n                         shape = \"circle\",\n                         stroke = 1,\n                         size = 4) +\n              # Modificar etiquetas\n              labs(x = \"# de lanzamiento\",\n                   y = element_blank(), \n                   title = \"Frecuencia a largo plazo\",\n                   subtitle = paste(\"P(H) final:\",\n                                    round(res_volados$prop_acum[N], 2)),\n                   caption = \"\") +\n              # Cambiar el tema por defecto\n              theme_bw() +\n              # Escala del eje y con tres valores de interés\n              scale_y_continuous(breaks = c(0, 0.5, 1),\n                                 labels = c(\"T\", \"50%\", \"H\")) +\n              # Cinco etiquetas en el eje x\n              scale_x_continuous(n.breaks = 5) +\n              # Líneas de referencia en los puntos de interés\n              geom_hline(yintercept = 1,\n                         colour = rgb(118,78,144, maxColorValue = 255),\n                         alpha = 0.9, linetype = \"dashed\") +\n              geom_hline(yintercept = 0.5,\n                         colour = \"lightslategray\",\n                         alpha = 0.9, linetype = \"dashed\") +\n              geom_hline(yintercept = 0,\n                         colour = rgb(118,78,144, maxColorValue = 255),\n                         alpha = 0.9, linetype = \"dashed\") +\n              # Línea de referencia con el valor \"real\"\n              geom_hline(yintercept = volados[[1]],\n                         colour = \"forestgreen\",\n                         alpha = 0.9, linetype = \"dashed\") +\n              # Anotación con el valor \"real\"\n              annotate(\"text\",\n                       x = N*0.9,\n                       y = ifelse(volados[[1]] > 0.5,\n                                  volados[[1]] - 0.05,\n                                  volados[[1]] + 0.05),\n                       label = paste(\"P(H) = \", volados[[1]]),\n                       colour = \"forestgreen\")\nprop_plot\n\n\n\n\nFigura 6.7: Frecuencia a largo plazo de 500 volados para aproximar la probabilidad de obtener una cara en un volado (\\(P(H)\\)) con una moneda con “carga” (sesgo) aleatorio.\n\n\n\n\nTe darás cuenta de que nuestro valor final fue muy cercano al esperado. Ahora intenta hacer esto mismo, pero con 5 lanzamientos. ¿Qué obtuviste? Es bastante posible que la \\(P(H)\\) haya quedado bastante lejos de el objetivo. Esto resalta la importancia de algo que abordaremos más a profundidad la siguiente sesión: la representatividad.\nVeamos ahora la distribución de probabiliades de manera gráfica. Esta es una distribución discreta, por lo que utilizaremos un gráfico de frecuencias:\n\n# Inicialización del espacio gráfico\nfreq_binom <- ggplot(data = res_volados,\n                     aes(x = as.character(sec))) +\n              # Graficar las frecuencias absolutas\n              geom_bar(colour = \"deepskyblue4\",\n                       fill = \"deepskyblue4\") +\n              # Cambiar el tema por defecto\n              theme_bw() +\n              # Transformar el eje x a categórico y asignar etiquetas\n              scale_x_discrete(name = \"Resultado\",\n                               labels = c(\"1\" = \"H\",\n                                          \"0\" = \"T\")) +\n              # Modificar el resto de las etiquetas del gráfico\n              labs(title = \"Resultados de volados\",\n                   y  = \"Frecuencia\")\n\nfreq_binom\n\n\n\n\nFigura 6.8: Distribución de frecuencias de los volados con la moneda con carga aleatoria.\n\n\n\n\n\n\n\n\n\n\nImportante\n\n\n\nUna forma relativamente común de representar proporciones es utilizando gráficos de “pie” o “rebanadas”. EVITA hacerlo lo más que puedas. Los seres humanos somos, en general, bastante malos para evaluar diferencias de áreas, además de que la misma información puede ser representada en un gráfico de frecuencias, sean absolutas o relativas.\n\n\n¿Qué concluimos de estos resultados? Eso dependerá del “humor” de tu generador de números aleatorios. Puede que tu moneda sí haya sido justa (\\(P(H) ≈ 0.5\\)), o puede que haya estado fuertemente “cargada”. Independientemente de esto podemos hacer un ejercicio para calcular la probabilidad de que la moneda sea justa, considerando un márgen de error del 5% alrededor del 50%; es decir, si obtenemos una \\(P(H)\\) entre 45% y 55% la vamos a dar por “justa”, y esto me lleva a hablar de distribuciones continuas."
  },
  {
    "objectID": "c06_prob.html#distribuciones-continuas-y-pingüinos",
    "href": "c06_prob.html#distribuciones-continuas-y-pingüinos",
    "title": "6  Probabilidad",
    "section": "6.6 Distribuciones continuas y pingüinos",
    "text": "6.6 Distribuciones continuas y pingüinos\nEntonces, ¿cómo calculamos la probabilidad? Muy sencillo, el “sesgo” de la moneda proviene de una distribución uniforme, por lo que podemos partir el intervalo \\([0,1]\\) en partes iguales de \\(0.05\\) (5%):\n\ninterv <- seq(0, 1, 0.05)\ninterv\n\n [1] 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70\n[16] 0.75 0.80 0.85 0.90 0.95 1.00\n\n\nLuego podemos simplemente sacar la proporción de los casos que caen en nuestro intervalo aceptable con respecto al total: Si partimos en “pasos” de 5% entonces solamente 0.45, 0.5 y 0.55 son aceptables, por lo que sería 3 con respecto al número total de intervalos:\n\n3/length(interv)\n\n[1] 0.1428571\n\n\n\n\n\n\n\n\nTip\n\n\n\nRecuerda: la función length() nos da el número de elementos en objeto dado.\n\n\n\n\n\n\n\n\nNota\n\n\n\nResponde: ¿Qué tipo de probabilidad estamos calculando aquí? ¿Qué axioma estamos utilizando?\n\n\nEs decir que la probabilidad de que nuestra moneda sea justa es del 14.2% Si reflexionas un poco, esto no tiene sentido. ¿Cómo es posible que un 10% de una distribución uniforme tenga un 14% de probabilidad de ocurrir? Recuerda, discretizar una distribución continua resulta en intervalos arbitrarios y, aún cuando nuestro valor de “sesgo” viene de una distribución uniforme, no es lo mismo partirla en 20 pedazos a partirla en 100. De hecho, comprobémoslo: partamos nuestro intervalo \\([0,1]\\) en 100, y obtengamos la proporción de los intervalos que son mayores o iguales a 0.45 y menores o iguales a 0.55:\n\ninterv_100 <- seq(from = 0, to = 1, length.out = 100)\nlength(interv_100[interv_100 >= 0.45 & interv_100 <= 0.55])/length(interv_100)\n\n[1] 0.1\n\n\n\n\n\n\n\n\nTip\n\n\n\nSi tienes duda de qué hace length.out de diferente a lo que hicimos en los intervalos de 0.05% recuerda que siempre puedes darte una vuelta por la documentación de la función seq.\n\n\nAhora sí, nuestro intervalo de “aceptación” del 10% se ve reflejado en una probabilidad del 10%. Esto nos abre la puerta a un par de ejercicios interesantes. Primero, retomemos nuestros datos de pingüinos y quedémonos solo con los datos de los pingüinos Adelie:\n\npenguins <- subset(palmerpenguins::penguins, species == \"Adelie\")\npenguins <- na.omit(penguins)\n\nAhora veamos la distribución de la masa corporal en gramos:\n\nggplot(data = penguins, aes(x = bill_length_mm)) +\n  geom_density(colour = \"deepskyblue4\") +\n  theme_bw() +\n  labs(title = \"Distribución de la longitud del pico (mm)\",\n       subtitle = \"Pingüinos Adelie\",\n       x = element_blank(),\n       y = \"Densidad\")\n\n\n\n\nFigura 6.9: Distribución empírica (observada) de la distribución del pico de pingüinos Adelie.\n\n\n\n\n\n\n\n\n\n\nNota\n\n\n\nEn la sesión de estadística descriptiva ahondaremos un poco más en la diferencia entre un gráfico de densidad y un gráfico de frecuencias; sin embargo, si recuerdas la definición de densidad de probabilidad te será fácil saber que entre más alta sea la densidad en un punto dado, mayor es su probabilidad.\n\n\nAhora imaginemos que estamos en campo, medimos un individuo de 44 mm, y nos interesa saber cuál es la probabilidad de encontrar un al menos tan grande como él. Pues hacemos exactamente lo mismo que en el caso anterior:\n\nbill_lengths <- penguins$bill_length_mm\nlength(bill_lengths[bill_lengths >= 44])/length(bill_lengths)\n\n[1] 0.03424658\n\n\nLo cual nos da una probabilidad del 3.4%. Esto que hicimos aquí es el fundamento de las pruebas de significancia (pruebas de hipótesis), solo que hay una pequeña diferencia: el contraste no lo hacemos con la distribución de nuestros datos, sino con una distribución teórica (usualmente la distribución normal). En la sesión de técnicas paramétricas hablaremos duro y tendido de la distribución normal, sus características y cómo aprovecharla para probar “hipótesis” (vamos a ver también cuál hipótesis es la que estamos probando, porque posiblemente no sea la que tienes en mente). Veamos entonces, asumiendo una distribución normal, cuál es la probabilidad de encontrarnos con un idividuo de al menos 44 mm. Para esto utilizaremos la función pnorm(q, mean, sd, lower.tail), la cual calcula la función de densidad acumulada para los parámetros dados. ¿Y eso con qué se come? Pues bien, es ahí donde entra la acumulación: estamos integrando (sumando, vamos) las densidades hasta un punto (cuantil, más adelante hablaremos de esto) dado, considerando o no la cola (el lado) inferior (izquierda) o superior (derecha) de la distribución. Si nos interesan preguntas de “menor a” utilizamos lower.tail = TRUE, pues nos interesa toda la distribución hasta ese punto. Si nos interesa “superior a” utilizamos lower.tail = FALSE, pues nos interesa la distribución de ese punto en adelante:\n\npnorm(q = 44,\n      mean = mean(bill_lengths),\n      sd = sd(bill_lengths),\n      lower.tail = FALSE)\n\n[1] 0.0259491\n\n\n\n\n\n\n\n\nNota\n\n\n\nEl fundamento de esto es que si integramos nuestra curva de densidad (acumulamos/sumamos el área bajo la curva) obtenemos la probabilidad correspondiente.\n\n\n¡Es más baja! Veamos cómo quedaría nuestro individuo de 44 mm bajo el supuesto de una distribución normal:\n\nggplot() +\n   stat_function(fun = dnorm, n = 100,\n                 args = list(mean = mean(bill_lengths),\n                             sd = sd(bill_lengths)),\n                 colour = \"deepskyblue4\") +\n  xlim(min(bill_lengths),\n       max(bill_lengths)) +\n  geom_vline(xintercept = 44,\n             linetype = \"dashed\",\n             colour = \"firebrick\") +\n  theme_bw() +\n  labs(title = \"Distribución de longitudes de picos\",\n       subtitle = \"Pingüinos Adelie, distribución normal\",\n       x = element_blank(),\n       y = \"Densidad\")\n\n\n\n\nFigura 6.10: Distribución normal teórica de la distribución del pico de pingüinos Adelie, con media y desviación estándar de los datos observados. La línea vertical muestra la posición de un individuo con un pico de 44 mm en la distribución teórica.\n\n\n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nEsto es solo un ejemplo de cómo trabajamos con distribuciones continuas, y cómo podemos utilizar R para calcular probabilidades bajo esas distribuciones. Si bien es la misma intuición detrás de las pruebas de significancia, no estamos haciendo una.\n\n\nEn efecto, se encuentra muy cerca del límite de la distribución. Ahora puede que te estés preguntando el por qué del “un individuo de al menos 44 mm”. Recuerda que la probabilidad de un valor individual en una distribución continua es 0, por lo que hay que a) establecer un intervalo alrededor de ese valor, o b) poner la pregunta en términos de si es mayor o menor a ese valor. En la sesión de pruebas de hipótesis veremos cómo se relaciona esto con los valores de p, que no están muy lejos de lo que hicimos aquí. Ahora cambiemos la pregunta para ver cuál es la probabilidad de encontrar un individuo menor a 44 mm:\n\npnorm(q = 44,\n      mean = mean(bill_lengths),\n      sd = sd(bill_lengths),\n      lower.tail = TRUE)\n\n[1] 0.9740509\n\n\n¡Es altísima! El complementario del caso anterior, de hecho, pues solamente volteamos la pregunta a responder, pero tenía que hacerla de emoción :P.\nHasta este punto creo que ya te he torturado lo suficiente con la teoría alrededor de la probabilidad y las distribuciones de probabilidad. Podríamos seguir y hablar de las más comunes/utilizadas, pero creo que te será más digerible que en cada tema hablemos de las distribuciones de interés, especialmente de la distribución normal.\nEsto sería todo para esta sesión (en compensación por lo tedioso de la teoría no hay ejercicio). ¡Nos vemos en la siguiente!"
  },
  {
    "objectID": "c07_muestreo.html#datos-y-variables",
    "href": "c07_muestreo.html#datos-y-variables",
    "title": "7  Introducción al muestreo",
    "section": "7.1 Datos y variables",
    "text": "7.1 Datos y variables\nEn una investigación reunimos datos con el objetivo de obtener alguna conclusión o predicción. Pues bien comencemos definiendo un dato como una representación puntual de la realidad. Son un solo valor, sea una sola medición, un promedio, una desviación estándar, una proporción de sexos, etc, y el conjunto de datos de un mismo atributo medidos en distintos individuos nos dan una variable. Es decir, en nuestros conjuntos de datos cada valor es un dato, y cada columna (usualmente) es una variable. ¿Por qué es importante conocer esto? Porque hay distintos tipos de variables, los cuales definen cómo es que vamos a graficar y tratar esos datos:\n\nCualitativas: hacen referencia a las cualidades de nuestros individuos, y tienen dos escalas:\n\nNominal: hace referencia a categorías en las que no hay un orden o distintas importancias. Ejemplos pueden el sexo o el color.\nOrdinal: aquí hay un órden, y un ejemplo muy claro son las encuestas: 0 es nunca, 1 es casi nunca, 2 es ocasionalmente, 3 es casi siempre y 4 es siempre. Aunque son categorías bien definidas, 2 < 3 y 3 < 4. No son cuantitativas porque las respuestas están sujetas a la interpretación personal, pero descartar el órden en el análisis sería un error.\n\nCuantitativas, que hacen referencia a atributos cuantificables de manera objetiva. Hay dos tipos, cada uno con dos escalas.\n\nTipos:\n\nDiscretas: Son solo números enteros. Un ejemplo cotidiano es la edad, que usualmente la expresamos en años. No vamos por la vida diciendo tengo 18.5 años o 18 años con 6 meses, solo decimos tengo 18 años.\nContinuas: Es el caso contrario, son números fraccionarios. Se les denomina continuas porque hay un número infinito de valores posibles entre un valor y el otro, un ejemplo es la temperatura (35.1ºC, 100 K, etc.)\n\nEscalas:\n\nIntervalo: La escala de intervalo es aquella en donde el 0 NO es absoluto o, mejor dicho, donde el 0 es arbitrario. La temperatura expresada en grados centígrados es un ejemplo claro, 0ºC no indica ausencia de movimiento molecular, solo toma como referencia arbitraria el punto de congelación del agua.\nRazón: Aquí el 0 sí es absoluto y representa la ausencia del atributo en cuestión. La longitud es un ejemplo, si algo tiene longitud 0 más bien no tiene longitud, o si algo tiene una temperatura de 0 K quiere decir que no tiene movimiento molecular (~-273.15ºC).\n\n\n\n\n\n\nFigura 7.1: Tipos y escalas de variables"
  },
  {
    "objectID": "c07_muestreo.html#qué-datos-obtener",
    "href": "c07_muestreo.html#qué-datos-obtener",
    "title": "7  Introducción al muestreo",
    "section": "7.2 ¿Qué datos obtener?",
    "text": "7.2 ¿Qué datos obtener?\nAlgo que es muy importante tener siempre bien presente es que, aún cuando existen herramientas y técnicas que nos permiten procesar múltiples variables, en cualquier procedimiento de ciencia de datos es INDISPENSABLE que los datos sean de excelente calidad y, sobre todo, que sean adecuados para responder la pregunta que nos interesa, lo cual nos debe de llevar, invariablemente, a preguntarnos “¿qué datos debo de obtener?” O, en otras palabras, “¿qué debo medir?” Una frase que se me quedó marcada de mis clases de la licenciatura es “La investigación inicia y termina en el escritorio del investigador”; es decir, no salimos a hacer trabajo de campo y a registrar todo lo que se nos atraviese, caso contrario podemos terminar en una conclusión como “los bebés son traídos por cigüeñas”, o podemos tomar una decisión equivocada.\n\n7.2.1 Coincidencias\nSituémonos en la Alemania de 1960-1980, cuando estaban pasando por una crisis de natalidad. Sies (1988) quiso encontrar una medida que permitiera entender el problema y salió a buscar respuestas. En esta búsqueda se encontró con algo que le pareció sumamente interesante: había una relación notablemente alta entre la cantidad de pares de cigüeñas reproductoras y la cantidad de bebés nacidos.\n\n\n\nFigura 7.2: Bebes, cigüeñas y casualidad\n\n\nPor muy inverosímil que esto pueda parecernos, lo cierto es que este tipo de relaciones altas entre variables que no están relacionadas existen y en ocasiones puede ser muy difícil identificar si en efecto la relación es causal, casual, o si obedece a que ambas dependen de una tercera variable latente (no observada). Sobre este tema te recomiendo el artículo de Höfer, Przyrembel & Verleger (2004) sobre la “teoría de la cigüeña”, el de Haig (2010) sobre qué es una correlación espuria y también revisar esta página de internet con otras correlaciones curiosas.\n\n\n\n\n\n\nNota\n\n\n\nSi bien es cierto que las correlaciones espurias es algo que debíamos de ver en el tema de correlación, es importante reconocer que un muestreo bien planeado, basado en ciencia, es lo que minimiza la probabilidad de que describamos una relación de este tipo. Matemáticamente es prácticamente identificar si la relación que tenemos es causal o casual, por lo que es mejor evitarlas con un poco de planeación y sentido común.\n\n\n\n\n7.2.2 Contradicciones\nOtro ejemplo de la importancia de la selección de variables es el “Sesgo de supervivencia”. Situémonos ahora en la Segunda Guerra Mundial, en los cuarteles de la Fuerza Aérea de Estados Unidos. A un grupo de matemáticos le fue dada la tarea de designar qué partes de los aviones debían ser reforzadas para incrementar la tasa de supervivencia. El grupo entonces analizó los aviones que volvían a la base y generaron un diagrama como este, en el cual los puntos rojos representan las áreas con mayores daños balísticos.\n\n\n\nFigura 7.3: ¿Dónde reforzamos?\n\n\nLa primera aproximación que viene a la cabeza es reforzar esas zonas, pero Abraham Wald tuvo la suficiente visión para darse cuenta de un problema fundamental con lo que estaban midiendo: los aviones que volvían; es decir, aquellos que no habían sido derribados. Entonces propuso que en su lugar se reforzaran aquellas zonas donde NO había daños, ya que esos aviones fueron los que no volvieron a su base. El resultado: se incrementó la supervivencia como se había solicitado."
  },
  {
    "objectID": "c07_muestreo.html#un-último-paréntesis",
    "href": "c07_muestreo.html#un-último-paréntesis",
    "title": "7  Introducción al muestreo",
    "section": "7.3 Un último paréntesis",
    "text": "7.3 Un último paréntesis\nHablemos ahora sobre el uso de las palabras colecta y recolecta en el contexto biológico, que en ocasiones de utilizan de manera intercambiable al referirse a la obtención de muestras. Si revisamos la definición de colectar (RAE) veremos que esta es “recaudar (cobrar dinero)”, mientras que la definición de recolectar es “Recoger los frutos de una cosecha” o “Reunir cosas o personas de procedencia diversa”. Bajo esta luz, está claro que la terminología correcta es recolectar; sin embargo, el problema no termina ahí. En México, los permisos para reunir muestras con fines científicos tienen el nombre legal de “Licencias de colecta científica” (NOM-126-SEMARNAT-2000) y en el artículo 3 de la Ley General de Vida Silvestre se nombra a “la extracción de ejemplares, partes o derivados de vida silvestre del hábitat en que se encuentran” como colecta. Entonces, ¿cuál utilizar? Aunque pueda parecer algo trivial, lo correcto es utilizar cada una en su contexto, recolectar en la descripción del método de muestreo y colecta al declarar que a) se realizó el trámite correspondiente de acuerdo con el marco legal y b) los números de las licencias de colecta. Aunque pudiera parecer que esta discusión NO está relacionada con el curso, considero importante también el no olvidarnos del marco legal (y ético) involucrado en el desarrollo de la investigación, desde el muestreo hasta el análisis de los datos y el reporte de los resultados."
  },
  {
    "objectID": "c07_muestreo.html#muestreemos-orcas",
    "href": "c07_muestreo.html#muestreemos-orcas",
    "title": "7  Introducción al muestreo",
    "section": "7.4 Muestreemos orcas",
    "text": "7.4 Muestreemos orcas\n\n\n\n\n\n\nNota\n\n\n\nEl diseño experimental y el diseño de muestreo son temas lo suficientemente grandes como para formar un curso con ellos. Únicamente revisaremos los conceptos más importantes, de manera que puedas reflexionar un poco más sobre qué debes de tomar en cuenta en tus diseños.\n\n\nAhora sí, podemos entrar a la teoría del muestreo. Lo primero es introducir algunos conceptos básicos, aunque intentemos hacerlo de una manera más dinámica que solo dar sus definiciones. Imaginemos que el siguiente escenario. Queremos saber cuál es la longitud promedio de las orcas de cierta localidad. ¿Para qué? Para facilitar la explicación, por supuesto, aunque esto puede ser fácilmente extrapolado a cualquier investigación, sea experimental u observacional.\nBien, entonces estudiaremos la longitud de las orcas de la costa central de Oaxaca. Esto conforma nuestra población objetivo o población estadística; es decir, el conjunto de individuos que vamos a estudiar. Nota que estos individuos están bien delimitados; es decir, no vamos a considerar otras especies de delfines, ni fijarnos en el peso o en orcas de otra localidad. Únicamente vamos a medir orcas que nos encontremos al navegar en la costa central de Oaxaca. Formalmente:\n\nPoblación estadística: Conjunto de todos los individuos a estudiar. Esta puede empatar o no con una población biológica. En este caso, lo más probable es que las orcas de la CCO sean parte de una población de orcas transeúntes; sin embargo, forman nuestra población estadística.\nIndividuo: Objeto, ente, planta, animal, quimera, célula o cualquier unidad sobre la cual se realiza la observación y que, consecuentemente, tiene el atributo que queremos medir.\n\n\n\n\n\n\n\n\n(a) Individuo\n\n\n\n\n\n\n\n\n\n(b) Población estadística y población biológica\n\n\n\n\nFigura 7.4: De individuo a población.\n\n\n\n\n\n\n\n\nTip\n\n\n\nLa Figura 7.4 (b) tiene algunos símbolos que no hemos mencionado, todos referentes a la simbología/notación de conjuntos:\n\n\\(\\{\\}\\) Indican que es un conjunto\n\n\\(\\lor\\) es el “ó lógico” (uno u otro).\n\\(\\subset\\) indica que lo que está a la izquierda es un subconjunto de lo que está a la derecha.\n\n\n\nAmbos conceptos van de la mano para definir las generalidades de nuestros métodos en campo o laboratorio, pues llevan la información que delimita nuestro esfuerzo y que, entonces, define nuestro marco de muestreo. No tiene ningún sentido medir orcas de La Paz, BCS, si nuestro interés son las de la CCO, Formalmente:\n\nMarco de muestreo: Lista de todas las unidades de muestreo, donde una unidad de muestreo es la unidad básica para la captación de información de la población de interés. En palabras menos rebuscadas: el conjunto de qué vamos a medir y a quién (o a qué) se lo vamos a medir.\n\nUna vez definimos nuestro marco de muestreo podemos salir a campo. En un mundo ideal tendríamos acceso a la población completa, por lo que la estimación de la longitud promedio que realizaramos sería correcta pra ese momento. ¿Por qué solo en ese momento? Puede que los individuos juveniles crezcan, que los más viejos mueran, que nazcan nuevos, que se vayan algunos, etc., y que, entonces el parámetro poblacional (la longitud promedio) que queremos medir cambie. Esto complica un poco las cosas, pues es logísticamente imposible acceder a todas las orcas de la CCO. Si lo hiciéramos tendríamos un censo, de lo contrario tenemos un muestreo; es decir, vamos a medir únicamente una fracción de la población estadística, la cual va a conformar nuestra muestra y cuyos estadísticos van a ser nuestros estimadores de los parámetros poblacionales. Formalmente (Figura 7.5):\n\nParámetro poblacional: Función definida sobre los valores de las características medibles de una población. En palabras ménos técnicas: el valor real de lo que queremos saber de la población. Representados usualmente con letras griegas o mayúsculas (\\(\\mu\\) o \\(M\\), \\(\\sigma\\) o S, por ejemplo).\nCenso: Medición de toda la población. En otras palabras: un muestreo de toda la población en un momento determinado en el tiempo.\nMuestreo: Medición de un atributo en individuos de una población.\nMuestra: Conjunto de individuos de la población que son medidos (formalmente hace referencia a las mediciones en sí mismas).\nEstadístico: Función definida sobre los valores medibles de una muestra. En otras palabras, la estimación del atributo de interés a partir de la muestra. Representados con letras latinas minúsculas (\\(\\bar{x}\\), \\(s\\)).\n\n\n\n\nFigura 7.5: Estadístico es a muestra como parámetro es a población."
  },
  {
    "objectID": "c07_muestreo.html#un-buen-muestreo",
    "href": "c07_muestreo.html#un-buen-muestreo",
    "title": "7  Introducción al muestreo",
    "section": "7.5 Un buen muestreo",
    "text": "7.5 Un buen muestreo\nVolvamos a nuestro ejemplo con las orcas. Realizamos cierto número de navegaciones en la temporada seca (tal vez por limitaciones logísticas), en las cuales encontramos y medimos 50 orcas. Ese número de individuos medidos es nuestro tamaño de muestra. Ese número es sumamente importante, pues define la representatividad del muestreo; es decir, qué tan buen “resumen” de la población es nuestra muestra. OJO: Esto solo aplica si el muestreo tuvo ciertas cualidades:\n\nAleatorio: Es decir, nuestro diseño de muestreo/experimental debe de permitir que todos los individuos de nuestra población sean medidos con la misma probabilidad. Esto es sumamente difícil de conseguir y podemos entrar a detalles filosófico de qué podemos considerar aleatorio y qué no, pero sí podemos tratar de hacerlo:\n\n\nExhaustivo: Muy de la mano (de hecho una consecuencia) de la aleatoriedad. El muestreo debe de considerar todos los posibles valores o atributos de la variable a medir. En nuestro ejemplo esto se reduce a que midamos orcas de todos los tamaños y que no sesguemos el muestreo a individuos únicamente grandes o pequeños, salvo que así lo definiéramos en nuestro marco de muestreo.\n\n\nExclusivo: Atributos o valores de un indicador deben ser mutuamente excluyentes; es decir, que no tengamos un individuo que mida 8 y 9.3 m, por ejemplo. Esto es un poco redundante en este escenario, pero si fuéramos a establecer rangos de edad como cría, juvenil, hembra adulta y macho adulto, que todos estén en una sola categoría, por lo que hay que decidir qué hacer con un juvenil con características de adulto. También tiene que ver con tener un marco de muestreo bien definido.\nPreciso: Tratar de tener el mayor número de distinciones posibles para tener una mejor descripción. En nuestras orcas es mejor medir en 830 cm que redondear a 8 m.\n\n\n7.5.1 Errores de muestreo/medición\nEsa “mejor descripción” me lleva también a hablar sobre el tema de los errores de muestreo o medición. Por más que querramos evitarlo, siempre habrá errores dentro de nuestro muestreo o dentro de nuestra medición. Tal vez utilizamos una regla de 30 cm para medir organismos que miden más de 30 cm, o tal vez utilizamos una regla de metal que se contrae si hace frío y se expande si hace calor. De cualquier manera, siempre vamos a ser “víctimas” de alguno de dos tipos de errores (Figura 7.6):\n\nError sistemático: Un error que ocurre cada que realizamos la medición. Es constante y consistente. Si no somos conscientes de este error, nuestros resultados son inválidos. Pensemos en nuestras orcas, y que estamos utilizando fotogrametría aérea (fotos de dron) para hacer nuestros registros. Si calibramos nuestra medición al ángulo de visión que obtenemos a 10 m de altura y tomamos todas nuestras fotos a 11 m, el error va a ser constante y podemos corregirlo con una re-calibración. El problema está cuando no sabemos que este error está sucediendo; es decir, asumir que nuestras fotos se tomaron a 10 m y hacer la estimación con esa calibración.\nError aleatorio: Un error que no es constante. Volviendo a la fotogrametría, pensemos que obtuvimos un dron cuyo altímetro salió defectuoso, que no lo sabemos y que aunque marque 10 m de altura puede estar a ± 1m de ahí. Ese error es aleatorio. No podemos predecir si en un momento dado va a sobre o subestimar la altura. En este caso, los resultados son no confiables.\n\n\n\n\nFigura 7.6: Tipos de errores representados con dianas de tiro.\n\n\nPosiblemente esos términos de no confiable o inválido te suenen alarmantes. La clave para poder contender con ellos es a) saber que existen y b) ser conscientes de su magnitud. Si sabemos qué tanto estamos subestimando por la diferencia de alturas de 10 a 11 m, podemos corregir el valor. Si sabemos cuál es la distribución del error de medición del altímetro, podemos modelarlo (la inferencia Bayesiana se presta muy bien para eso). ¿Con eso quiero decir que entonces no hay que cuidar estos detalles? PARA NADA, por el contrario, hay que cuidarlos tanto que hay que saber cómo remediarlos o incluirlos en nuestros análisis de datos.\n\n\n\n\n\n\nAdvertencia\n\n\n\nRecuerda que, desde un punto de vista práctico, la estadística es muy simple: le das un conjunto de datos y te regresa algunos números. Que lo resultante sea confiable no depende solo de utilizar una técnica adecuada, sino de que la materia prima (los datos) sea buena. ¿Has probado un jugo de naranja hecho con naranjas “pasadas”?"
  },
  {
    "objectID": "c07_muestreo.html#tipos-de-muestreo-y-representatividad",
    "href": "c07_muestreo.html#tipos-de-muestreo-y-representatividad",
    "title": "7  Introducción al muestreo",
    "section": "7.6 Tipos de muestreo y representatividad",
    "text": "7.6 Tipos de muestreo y representatividad\nEn nuestro ejemplo tenemos mediciones de 50 orcas, asumamos que controlamos nuestros errores y que, por lo tanto, esos valores son válidos y confiables. ¿Podemos estar seguros de que esas 50 orcas nos dan una representación adecuada de la población? En otras palabras, nuestra muestra es representativa? Para explicar esto, generemos una población hipotética de 1000 orcas, en la cual hay 500 hembras y 500 machos y, para simplificarnos la existencia, solo hay individuos adultos. Para seguir con la simplificación, asumiremos que la longitud en ambos sexos tiene una distribución normal, con un promedio de 9.6 m para machos y 8.2 m para hembras y una dispersión de 0.1 m (más adelante hablaremos de todos estos detalles). En este escenario, podemos asumir que la media poblacional (el valor que queremos inferir) es el promedio de los promedios de ambos sexos:\n\nmachos <- rnorm(500, mean = 9.6, sd = 0.1)\nhembras <- rnorm(500, mean = 8.2, sd = 0.1)\nmedia_real <- mean(c(9.6, 8.2))\npobl <- data.frame(sexo = c(rep(\"M\", 500), rep(\"H\", 500)),\n                   lt = c(machos, hembras))\n\nSiempre es más fácil ver un gráfico:\n\nlibrary(ggplot2)\npobl_dens <- ggplot(data = pobl, aes(x = lt)) +\n             geom_density(color = \"dodgerblue4\") +\n             theme_bw() +\n             geom_vline(xintercept = media_real,\n                        color = \"forestgreen\") +\n             labs(title = \"Densidad de la LT de orcas\",\n                  x = element_blank(),\n                  y = element_blank())\npobl_dens\n\n\n\n\nY ahora hablemos de los diseños probabilísticos de muestreo, partiendo de este ejemplo.\n\n7.6.1 Muestreo Aleatorio Simple\nEs, como el nombre lo indica, el más simple de los diseños. La idea es que haremos el muestreo de manera que la probabilidad de muestrear cualquier individuo de la población es uniforme para todos los individuos; es decir, la misma. PREGUNTA: ¿Esto se sostiene para nuestro ejemplo? Cuando tenemos diseños observacionales como este, siempre vamos a infringir, de una manera u otra, con la parte aleatoria. Estamos sujetos a si encontramos o no a los animales, si la “curiosidad” por la embarcación es variable, etc., etc., etc., por lo que, dependiendo de lo que estemos realizando, podemos solo “asumir” que el muestreo fue aleatorio. En este caso de ejemplo, por fortuna, podemos darnos el “lujo” de hacerlo de manera adecuada; es decir:\n\nDefinir la población estadística (hecho)\nDefinir un conjunto de muestras con la misma probabilidad de ser escogidas. Son nuestras 50 muestras a obtener, para lo cual\nSeleccionaremos una de las muestras utilizando números aleatorios.\n\nHagamos entonces el ejercicio para una muestra y luego dejemos que R haga lo demás:\n\nind <- round(runif(1, min = 1, max = 1000))\nind\n\n[1] 467\n\npobl$lt[ind]\n\n[1] 9.42255\n\n\nAquí lo que hicimos fue tomar un individuo de nuestra población de manera uniformemente “aleatoria”, en este caso el número 267 y medirlo. Ahora tenemos que repetir esa acción otras 49 veces, una a una, o podemos decirle a R que lo haga en un solo paso (imaginemos que somos nosotros quienes lo hacemos):\n\nmuestras <- sample(pobl$lt, size = 50, replace = FALSE)\nhead(muestras)\n\n[1] 8.167183 8.196013 9.676545 9.725936 8.158998 9.627604\n\n\nNotarás dos cosas: 1) no sabemos a qué individuos corresponden esas muestras, lo cual sería problemático si no tuviéramos 2) el argumento replace = FALSE. Con esto lo que hicimos fue un muestreo sin reemplazo; es decir, una vez que un individuo fue medido se “retiró” de la población, en el sentido de que no puede ser vuelto a medir. Este tipo de muestreo es más preciso, pues no tenemos duplicados o pseudo-réplicas. Por otra parte, el muestreo con reemplazo es útil en otro tipo de situaciones, por ejemplo cuando queremos aproximar frecuencias a largo plazo como hicimos en la sesión anterior.\n\n\n\nFigura 7.7: ¿Con o sin reemplazo?\n\n\nEn nuestro ejemplo de las orcas es correcto asumir que esto puede ser así, pues podemos saber quién es quien con fotos de sus aletas dorsales y, con ello, descartar las mediciones duplicadas. Desafortunadamente, esto no siempre es así, y es algo que debemos de considerar al diseñar nuestro muestreo. Dicho esto, ya tenemos nuestras 50 muestras, entonces podemos realizar la estimación de nuestro promedio (media):\n\nmean(muestras)\n\n[1] 8.93053\n\n\nPongámosla en nuestro gráfico inicial como una línea vertical para ver en dónde quedó:\n\npobl_dens + geom_vline(xintercept = mean(muestras),\n                       color = \"firebrick\")\n\n\n\n\nNo está exactamente donde debería de estar. Para complicar más las cosas, añadamos un pequeño sesgo a nuestro muestreo: No es completamente aleatorio, sino que hubo algún factor que hiciera que muestreáramos preferencialmente a los machos. Tal vez eran más curiosos y era más fácil tomar una foto con la embarcación como referencia. Simulemos ese escenario. Nuestra muestra seguirá siendo de 50, pero esta vez tendremos 35 machos y 15 hembras:\n\nmuestras <- c(sample(machos, size = 35, replace = F),\n              sample(hembras, size = 15, replace = F))\npobl_dens + geom_vline(xintercept = mean(muestras),\n                       color = \"Firebrick\")\n\n\n\n\nAquí la estimación ya se aleja mucho más de nuestra media “real”. La razón es, sin duda, el sesgo que añadimos hacia los machos.\n\n\n7.6.2 Muestreo Aleatorio Estratificado\nEn este caso, donde la población se encuentra sub-dividida en estratos no traslapados y estos son muestreados de manera independiente, estamos hablando de un Muestreo Aleatorio Estratificado. Este es simplemente una “mezcla” de MAS, en el sentido que haremos un muestreo aleatorio simple dentro de cada estrato, aunque es importante conocer su tamaño de antemano. Para obtener la media poblacional solo obtenemos el promedio de ambos promedios:\n\npobl_dens + geom_vline(xintercept = mean(c(mean(muestras[1:35]),\n                                           mean(muestras[36:50]))),\n                       color = \"firebrick\")\n\n\n\n\nMatemáticamente la ecuación es más complicada, primero obtendríamos el peso de cada estrato (\\(W_s\\)), donde \\(n_s\\) es el tamaño del estrato \\(s\\) y \\(N\\) es el tamaño poblacional total:\n\\[\nW_s = \\frac{n_s}{N}\n\\]\nLuego, la media de la población estratificada está dada por:\n\\[\n\\mu = \\frac{\\sum_{s = 1}^{S} n_s*\\bar{x}_s}{N} \\\\\n\\therefore \\\\\n\\mu = \\sum_{s = 1}^{S} W_s*\\bar{x}_s\n\\]\nComprobémoslo:\n\n# Peso de cada estrato:\nw <- c((500/1000), (500/1000))\nm <- c(mean(muestras[1:35]), mean(muestras[36:50]))\n\npobl_dens + geom_vline(xintercept = sum(w*m), color = \"Firebrick\")\n\n\n\n\nLa igualdad podría demostrarse matemáticamente (solo aplica con estratos de igual tamaño), pero eso queda como ejercicio si te da curiosidad. OJO: Hacer esta estimación de la media total solo es válida si a) tiene sentido recuperarla o estimarla, lo cual depende totalmente de la pregunta de investigación (puede que tenga más sentido estimar la media para cada grupo) y b) si el muestreo es representativo. Veamos qué pasa si muestreamos únicamente 10 y 5 individuos:\n\nmuestras <- c(sample(machos, size = 10, replace = F),\n              sample(hembras, size = 5, replace = F))\n\npobl_dens + geom_vline(xintercept = mean(c(muestras[1:10],\n                                           muestras[11:15])),\n                       color = \"Firebrick\")\n\n\n\n\nComo era de esperarse, la estimación está sumamente sesgada. Esto es por el problema de la representatividad; es decir, nuestra muestra es muy pequeña (especialmente para las hembras) y, por lo tanto, no es una buena imagen de lo que pasa a nivel población. Como ejercicio, estima la media de la población estratificada con las ecuaciones que vimos arriba. ¿Cambió el resultado?\nEste problema tiene una “solución”: muestrear más. ¿Qué tanto? Depende de la precisión (grado de error) que querramos. La relación entre el tamaño de muestra, la Varianza poblacional y la precisión está dada por:\n\\[\nn = \\frac{1.96^2 S^2}{d^2}\n\\]\n¿De dónde sale ese 1.96? Es el límite al 95% de confianza (en pruebas de hipótesis hablaremos de qué es eso) de una distribución normal, dado en términos de desviaciones estándar, por lo que podríamos cambiarlo por cualquier otro número para representar cualquier intervalo de confianza. ¿Qué pasa si la población no tiene una distribución normal? A ese 1.96 tenemos aproximadamente el 75% de los datos poblacionales (Desigualdad de Chebyshev), independientemente de su distribución. No entraré en esos detalles, lo que es realmente importante es que debemos de conocer la varianza poblacional, lo cuál es un problema y de los grandes. Gigante, de hecho. Podemos estimarla a partir de la muestra, pero resulta que, si no es representativa, nuestra estimación de la varianza tampoco es confiable. En este caso, para tener un muestreo representativo al 95% de confianza, considerando la varianza poblacional de \\(0.1^2\\) y con una precisión de 0.05m, necesitaríamos un tamaño de muestra de mínimo 15 individuos (por sexo).\n\nn <- ((1.96^2)*(0.1^2))/0.05^2\nround(n)\n\n[1] 15\n\n\nO podemos ver cuál es la precisión de la estimación de la media de las hembras con nuestra muestra de 5 individuos:\n\nd <- sqrt(((1.96^2)*(0.1^2))/5)\nd\n\n[1] 0.08765386\n\n\nEs decir, tenemos un márgen de error aproximado de 0.87m para la media de las hembras. A esto lo acompaña algo que se conoce como pruebas de potencia, sobre lo cual encontrarás referencias en la sección de lecturas recomendadas del servidor de Discord. También es importante mencionar que en la sesión de pruebas de hipótesis hablaremos de cómo representar la incertidumbre en nuestras estimaciones (intervalos de confianza). Por el momento basta que te lleves el mensaje de que es importante considerar qué factores pueden estar generando “ruido” en tu diseño. Si tu pregunta se puede responder mediante un experimento en condiciones controladas, controla toda posible fuente de variación externa al factor que te interesa, de manera que lo que midas refleje únicamente lo que quieras responder. Si es un estudio observacional, acota tu marco de muestreo lo más posible. En ambos casos, y en la medida de lo posible, mide todas las fuentes de variación que pudieran estar influenciando tus observaciones e inclúyelas en tus análisis.\nEsto sería todo para esta sesión. Espero que haya sido de tu agrado y, sobre todo, que aunque no haya podido resolver todas tus incertidumbres, te haya motivado a reflexionar sobre la importancia de un buen muestreo. La inferencia estadística NO es magia negra que pueda resolver nuestros problemas, simplemente nos ayuda a tomar decisiones en situaciones de incertidumbre pero, que esas decisiones sean buenas (o no) depende totalmente de los datos que tengamos disponibles (y un poco de suerte).\n\n\n\n\nHaig BD. 2010. What Is a Spurious Corelation? Understanding Statistics 2:125-132. DOI: 10.1207/S15328031US0202_03.\n\n\nHöfer T, Przyrembel H, Verleger S. 2004. New evidence for the Theory of the Stork. Paediatric and Perinatal Epidemiology 18:88-92.\n\n\nSies H. 1988. A new parameter for sex education. Nature 332:495."
  },
  {
    "objectID": "s03_basics.html#objetivo-de-aprendizaje",
    "href": "s03_basics.html#objetivo-de-aprendizaje",
    "title": "Técnicas básicas",
    "section": "Objetivo de aprendizaje",
    "text": "Objetivo de aprendizaje\nEl objetivo de esta sección es que te familiarices con el análisis de datos, comenzando con la estadística descriptiva. Posteriormente que ahondes en las técnicas (posiblemente) más comunes y socorridas en análisis bioestadísticos: \\(t\\) de Student y ANOVA, no solo con su aplicación, sino también con sus fundamentos e intuiciones. Revisarás qué es son los intervalos de confianza y valor de p, cómo y cómo no interpretarlos, revisarás los supuestos de normalidad y homogeneidad de varianzas, desde una perspectiva teórica y con pruebas visuales para facilitar su entendimiento."
  },
  {
    "objectID": "c08_descriptiva.html#descripciones-o-exploraciones",
    "href": "c08_descriptiva.html#descripciones-o-exploraciones",
    "title": "8  Estadística descriptiva",
    "section": "8.1 Descripciones o exploraciones",
    "text": "8.1 Descripciones o exploraciones\nYa fuimos a campo o al laboratorio, realizamos nuestro muestreo o experimiento, y tenemos nuestros datos. ¿Ya podemos empezar a hacer pruebas de significancia y regresiones, no? Técnicamente sí, pero paremos nuestro tren y no querramos correr antes de saber caminar. Al momento de realizar el análisis de nuestros datos es “sano” que nos familiaricemos con ellos antes de realizar cualquier proceso de estadística inferencial. El objetivo de la estadística descriptiva es, como el nombre indica, describir la información contenida en los datos, para lo cual utiliza métodos que resumen la información: medidas de tendencia central/dispersión y gráficos. ¿Por qué es importante? Nos permite familiarizarnos con la información con la que contamos, ya sea para identificar patrones, seleccionar variables importantes, visualizar inconsistencias en los datos, detectar anomalías y mucho más. Notarás conforme avancemos en el curso que la visualización de nuestros datos juega un papel importantísimo durante la interpretación, pero empecemos desde abajo.\nEstoy seguro de que algunos de los conceptos que revisaremos en esta sesión los has revisado ya con anterioridad, por lo que no te haré el cuento largo, sino que me enfocaré más en las peculiaridades de cada una de las medidas y gráficos, en qué escenarios son útiles, cuándo no lo son tanto y lo enlazaremos con temas que veremos más adelante."
  },
  {
    "objectID": "c08_descriptiva.html#medidas-de-tendencia-central",
    "href": "c08_descriptiva.html#medidas-de-tendencia-central",
    "title": "8  Estadística descriptiva",
    "section": "8.2 Medidas de tendencia central",
    "text": "8.2 Medidas de tendencia central\n\nlibrary(ggplot2)\nlibrary(GGally)\nlibrary(palmerpenguins)\n\nComencemos hablando con lo que, por lo general, es nuestro mayor interés: hacia dónde tienden nuestros datos. Para ello tenemos algunas medidas de tendencia central; es decir, literalmente describimos nuestros datos a partir de dónde se acumulan más.\n\n8.2.1 Media\nLa primera de estas medidas es, posiblemente, la más común de todas: la media o el promedio. En su forma más simple; es decir, la media aritmética la obtenemos sumando todos nuestros datos y dividiéndolos entre su número:\n\\[\n\\overline{x} = \\frac{\\sum_{i = 1}^n x_i}{n}\n\\]\nEsto es conocimiento general, y es algo con lo que todos los estudiantes somos torturados eventualmente. Bueno, más que recordar cómo calcularlo e implementarlo en R (mean(x), donde x es un vector de observaciones), pensemos en qué representa. La media es, literalmente, un indicador de hacia qué valor se están acumulando nuestros datos, tal y como si colgáramos cosas en un tendedero (házme un favor y piensa en que el siguiente gráfico está invertido verticalmente):\n\nset.seed(0)\nx <- data.frame(x = rnorm(100, mean = 0))\nplot1 <- ggplot(data = x, aes(x = x)) +\n         geom_density(color = \"dodgerblue4\") +\n         theme_bw() +\n         geom_vline(xintercept = mean(x$x),\n                    color = \"dodgerblue4\")\nplot1\n\n\n\n\nEn este caso, la mayor parte de nuestros datos están acumulados alrededor de 0. ¿Qué crees que pase si añadimos otros 20 datos, esta vez acumulados en 5? Veámos el cambio:\n\nset.seed(0)\nx2 <- rbind(x, data.frame(x = rnorm(20, mean = 5)))\nplot2 <- plot1 +  geom_density(aes(x = x),\n                               data = x2,\n                               color = \"firebrick\")\nplot2 + geom_vline(xintercept = mean(x2$x),\n                   color = \"firebrick\")\n\n\n\n\nComo era de esperarse, la media se “jaló” a la derecha, y de manera bastante notable. Esos 5 valores extremos tuvieron un peso bastante importante. Literalmente fue como si hubiéramos colgado cinco cosas más en nuestro tendedero, pero alejadas de la ropa que colgamos en un inicio. Aunque la media es una manera muy efectiva de resumir nuestros datos, esto solo es cierto si estos se parecen a una distribución normal (en la sesión de técnicas paramétricas hablaremos duro y tendido con respecto a esto), pero si no, como en nuestro caso con los nuevos valores, es necesario buscar una alternativa.\n\n8.2.1.1 Media ponderada\nUna de ellas es una modificación a la media, en la cuál cada valor tiene su propia ponderación o su propia importancia. Esa es la media con la que más padecemos los estudiantes de secundaria hacia arriba, pues el examen tiene una ponderación distinta a las tareas, por ejemplo. En mi caso personal, era un tormento cuando se ponderaba más las tareas que el examen, pero eso es otra historia. ¿Por qué es importante? Porque podemos utilizarla para “regresar” nuestra media a su lugar; de hecho, esta es la base fundamental detrás de las regresiones robustas, en donde el peso de cada observación disminuye según incrementa su distancia de 0:\n\n# Pesos: fracción de la distancia máxima a 0\nw <- max(abs(0 - x2$x))/abs(0 - x2$x)\n# Media ponderada\nwmean <- weighted.mean(x2$x, w)\n\nplot2 + geom_vline(xintercept = wmean,\n                   color = \"firebrick\")\n\n\n\n\n\n\n8.2.1.2 Media geométrica\nEsta es menos conocida, y representa el promedio de porcentajes, razones o tasas de crecimiento. Se expresa como la raiz n-ésima del producto de los n valores:\n\\[MG = \\sqrt[n]{\\Pi_i^nx_i}\\]\nPensemos en que estimamos la tasa de crecimiento poblacional (\\(\\lambda\\)) anual de ballenas jorobadas en tres años seguidos, a partir de un modelo de marca-recaptura, y los valores que obtuvimos fueron 1.03, 0.98, 1.4, 0.94:\n\nlamb <- c(1.03, 0.98, 1.4, 0.94)\nprod(lamb)^(1/length(lamb))\n\n[1] 1.073569\n\n\nEs decir, el crecimiento poblacional promedio fue del 7%.\nOtra manera de calcularla es:\n\\[\nMG = e^\\overline{log(\\lambda)}\n\\]\n\nexp(mean(log(lamb)))\n\n[1] 1.073569\n\n\n¿Te animas a encontrar la igualdad matemática?\n\n\n\n8.2.2 Mediana\nUna alternativa más es la mediana. A diferencia de la media, que “busca” hacia donde se están acumulando los datos, la mediana nos indica exactamente que valor se encuentra en el centro de nuestra base de datos. Si partimos nuestra base de datos en 100 partes iguales (100%), cada parte representa un cuantil (1%). Cada diez cuantiles tenemos un decil, cada 25 cuantiles tenemos un cuartil (cuartiles 25%, 50% y 75%), y en el cuartil 50 tenemos la mediana. Esta es, entonces, mucho menos sensible a valores extremos:\n\nplot2 + geom_vline(xintercept = median(x2$x),\n                   color = \"firebrick\")\n\n\n\n\nLa estimación no es exactamente la misma que la media de los datos originales o de la media ponderada según su distancia a 0; sin embargo, el efecto es notablemente menor que con la media tradicional. Esta resistencia a valores extremos es lo que hace que las técnicas no paramétricas estén basadas en la mediana, en vez de la media.\n\n\n8.2.3 Moda\nLa moda corresponde al valor que más se repite en un conjunto de datos. Con datos continuos en el sentido estricto no existe; sin embargo, en muchos casos sí que podemos tener repetidos dependiendo de la escala y la precisión de nuestro instrumento. Otra manera de calcularla es discretizando nuestros datos y encontrar el intervalo más frecuente. Una propiedad interesante de la moda es que su valor corresponde con el valor que tiene la mayor probabilidad dentro de la distribución, por lo que puede ser útil en ciertos casos de Inferencia Bayesiana. Para calcularla podemos utilizar la función Mode(x) de la librería DescTools:\n\nletmode <- DescTools::Mode(c(\"a\", \"a\", \"b\", \"c\", \"d\"))\nletmode\n\n[1] \"a\"\nattr(,\"freq\")\n[1] 2\n\n\nA lo largo de este curso no aplicaremos la moda, solo la agregué para que la tengas presente."
  },
  {
    "objectID": "c08_descriptiva.html#medidas-de-dispersión",
    "href": "c08_descriptiva.html#medidas-de-dispersión",
    "title": "8  Estadística descriptiva",
    "section": "8.3 Medidas de dispersión",
    "text": "8.3 Medidas de dispersión\nAl igual que en el caso anterior, repasaremos rápidamente las medidas de dispersión, con el objetivo de explorar la intuición detrás de ellas y su relación con otros conceptos que revisaremos más adelante. Independientemente de cuál utilicemos, todas las medidas de dispersión indican justamente eso, qué tan grande es la variabilidad de una distribución, ya sea de nuestros datos o la distribución muestral del parámetro que estemos estimando.\n\n8.3.1 Desviación estándar y Varianza\nEn pocas palabras, la varianza es una medida de la dispersión promedio de los datos; es decir, cuál es el área promedio que abarca la dispersión de los datos. En la sección de Multivariado vamos a ver cuál es la relación entre la varianza y la covarianza, a entender a la varianza como un caso especial de la covarianza y ver de dónde sale esa suma de cuadrados. Esto último también me lleva a que cada que leas suma de cuadrados pienses en una medida de dispersión o en la varianza de los datos.\n\\[\n\\sigma^2 = \\frac{\\sum{(x_i - \\mu)^2}}{N}\n\\]\nLa desviación estándar, por otra parte, es simplemente la raíz cuadrada de la varianza. Si la varianza representa un área, la desviación estándar representa una distancia, la distancia promedio que existe entre cada uno de los datos y la media. Estas dos medidas (la desviación estándar y la varianza) son sumamente útiles y utilizadas en los procesos estadísticos; de hecho, junto con la media, son los principales parámetros poblacionales que usualmente queremos estimar a partir de nuestra muestra. Una aclaración es que la ecuación de arriba es para calcular la varianza poblacional, mientras que si queremos calcular la varianza muestral aplicaremos una corrección con los grados de libertad (que definiremos más adelante)\n\\[\ns^2 = \\frac{\\sum(x_i - \\overline{x})^2}{N-1}\n\\]\nEsta varianza muestral se considera un estimador insesgado de la varianza poblacional.\n\n8.3.1.1 Estimadores\nEste es un buen momento para hablar de los estimadores. ¿Qué es un estimador? Una medida que utilizaremos para estimar un parámetro poblacional. Evidentemente, no puede ser cualquier número ni cualquier medida, debe de tener ciertas características. Particularmente:\n\nInsesgado: Es decir, que la media de la distribución del estimador sea igual al parámetro. De nuevo, en la sesión de técnicas paramétricas vamos a hablar sobre distribuciones muestrales, el teorema del límite central y su implicación para el supuesto de normalidad que a veces puede ser un dolor de cabeza. Por lo pronto entiende que “la media de la distribución del estimador” hace referencia a que, si hicieramos una cierta cantidad de muestreos y calculamos algún parámetro para cada muestreo, el promedio de esas estimaciones debe ser igual al parámetro poblacional.\nConsistencia: Es la propiedad en la que un estimador se aproxima al valor del parámetro conforme incrementa el tamaño de muestra, lo cual también tiene que ver con el teorema del límite central que revisaremos más adelante.\nEficiencia: La estimación tiene el error estándar más pequeño cuando se compara con otro estimador. Por ejemplo, en una distribución Normal, la media y la mediana son prácticamente iguales; sin embargo, el error estándar de la media es \\(\\frac{\\sigma}{\\sqrt{n}}\\), mientras que el de la mediana es \\(\\approx 1.25\\) veces ese valor.\nSuficiencia: El estimador es, por sí mismo, capaz de transmitir toda la información disponible en la muestra sobre el valor del parámetro.\n\nPor estas 4 razones es que la mayor parte de nuestras inferencias están en relación a la media poblacional, estimada a partir de la media muestral. La mediana no se considera un buen estimador de la media poblacional si la distribución no es simétrica, pues, como vimos arriba, está sesgada en relación a la media poblacional. Por otra parte, su error estándar es mayor que el error estándar de la media (en términos de sus distribuciones muestrales), ni utiliza todos los datos (solo el cuantil 50).\n\n\n\n8.3.2 Coeficiente de variación\nEl coeficiente de variación, también conocido como la desviación estándar relativa, es la relación que existe entre la desviación estándar y la media de los datos, tal que:\n\\[\nCV = \\frac{\\sigma}{\\mu}*100\n\\]\nEste es especialmente útil cuando queremos comparar las dispersiones de dos cosas que están en distinta escala, o expresar en un porcentaje qué tan grande es nuestra variabilidad en relación a nuestra estimación. Si hacemos una estimación de tamaño poblacional de 1000 individuos, con un coeficiente de variación del 50% quiere decir que nuestra variabilidad es de la mitad de nuestra estimación.\n\n\n8.3.3 Error estándar\nEsta medida es especialmente útil en la estimación de los intervalos de confianza de cualquier parámetro, y representa la desviación estándar de su distribución muestral, el cuál podemos estimar como:\n\\[\n\\sigma_{\\overline{x}} \\approx \\frac{\\sigma_x}{\\sqrt{n}}\n\\]\nNormalmente nosotros no estimaremos o interpretaremos esta medida, sino que iremos directamente a los intervalos de confianza para expresar la incertidumbre alrededor de nuestras estimaciones.\n\n\n\n\n\n\nNota\n\n\n\n¿Qué son los intervalos de confianza? Son una medida de la incertidumbre en la estimación de nuestro parámetro de interés, pero hablaremos de ellos más a detalle en las siguientes sesiones."
  },
  {
    "objectID": "c08_descriptiva.html#gráficos",
    "href": "c08_descriptiva.html#gráficos",
    "title": "8  Estadística descriptiva",
    "section": "8.4 Gráficos",
    "text": "8.4 Gráficos\nEn la sesión de visualización vimos las consideraciones que debemos de tener en cuenta para hacer una visualización efectiva, pero no hablamos de los tipos de gráficos que podemos realizar. En esta sesión, entonces, no entraremos a ver los detalles de la visualización, simplemente hablaremos de los gráficos más comunes, en qué escenarios son útiles y cómo construirlos utilizando ggplot2. Para esta parte utilizaremos datos de biometrías de pinguinos, contenidos en la librería palmerpenguins\n\n8.4.1 Gráfico de frecuencias\nTambién nombrado a veces histograma. Es, posiblemente, el gráfico más sencillo de todos, pues lo único que hacemos es poner una barra a la altura del número de individuos que hay en una clase o intervalo. Para construirlo en ggplot2 utilizaremos la capa geom_histogram:\n\nggplot(data = penguins,\n       aes(x = bill_length_mm)) +\n  geom_histogram(fill = \"dodgerblue4\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite values (stat_bin).\n\n\n\n\n\nSi tenemos más de un grupo y queremos ver todas las distribuciones de frecuencia podemos pasar el argumento fill dentro de los argumentos de estética (aes())\n\nggplot(data = penguins,\n       aes(x = bill_length_mm, fill = species)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite values (stat_bin).\n\n\n\n\n\nHabrás notado que ggplot2 nos dio una notificación con respecto al número de intervalos (cajas) utilizados para estimar las frecuencias, en particular que utilizó 30. Podemos seleccionar una cantidad manualmente, por ejemplo 100, y especificarla con el argumento bins, o especificar la amplitud del intervalo con el argumento binwidth:\n\nggplot(data = penguins,\n       aes(x = bill_length_mm, fill = species)) +\n  geom_histogram(bins = 100)\n\nWarning: Removed 2 rows containing non-finite values (stat_bin).\n\n\n\n\n\n¿El problema? ¿Cómo sabemos cuántos intervalos utilizar? Si ya hay intervalos establecidos, por ejemplo longitudes para estados de madurez sexual, podemos establecer esos intervalos, pero si nuestra distribución es continua podemos utilizar una mejor alternativa.\n\n\n8.4.2 Gráfico de densidad\nLos gráficos de densidad nos permiten, justamente, representar gráficamente la distribución de una variable continua. Recordarás de la clase de probabilidad que la probabilidad de un valor continuo es teóricamente 0, pero podemos estimar la densidad de probabilidad de un punto determinado si hacemos el intervalo infinitesimalmente pequeño, y es esto lo que representa un gráfico de densidad. Piensa en él como una versión suavizada de un gráfico de frecuencias:\n\nggplot(data = penguins,\n       aes(x = bill_length_mm, fill = species)) +\n  geom_density(color = NA)\n\nWarning: Removed 2 rows containing non-finite values (stat_density).\n\n\n\n\n\nEso se ve bastante mejor, pues ya no tenemos la incertidumbre de cuántas “cajas” formar para encajonar nuestros datos, pero no es lo más adecuado para comparar distribuciones de manera gráfica. Afortunadamente, también tenemos alternativas para eso.\n\n\n8.4.3 Gráfico de cajas y bigotes\nUna es el clásico gráfico de cajas y bigotes, en el cuál hay (por grupo) un indicador (punto, línea horizontal o muesca) de la tendencia central (media, mediana), una caja que indica una medida de dispersión (desviación estándar, rango intercuantil, error estándar) y dos “bigotes” (líneas verticales) que indican los límites “aceptables” de la distribución. Adicionalmente tenemos puntos “libres” que indican valores extremos. En ggplot2 podemos construirlos con la capa geom_boxplot, siempre que especifiquemos en aes() una variable de agrupamiento para x, la variable de la que queremos ver el interés en y y, opcionalmente, un argumento de color que corresponda con los grupos en x.\n\nggplot(data = penguins,\n       aes(x = species, y = bill_length_mm,\n           color = species)) +\n  geom_boxplot(fill = NA)\n\nWarning: Removed 2 rows containing non-finite values (stat_boxplot).\n\n\n\n\n\nInfortunadamente perdemos mucha información si lo comparamos con el gráfico de densidad. Si tan solo hubiera una manera de mezclarnos… ¡Espera! Sí que la hay.\n\n\n8.4.4 Gráfico de violín\nLos gráficos de violín son, justamente, una alternativa más “fina” a los gráficos de cajas y bigotes, en los cuales podemos mostrar la distribución completa de cada grupo. En ggplot2 utilizaremos la capa geom_violin, que requiere exactamente la misma información que el gráfico de caja y bigotes:\n\nggplot(data = penguins,\n       aes(x = species, y = bill_length_mm,\n           color = species)) +\n  geom_violin(fill = NA)\n\nWarning: Removed 2 rows containing non-finite values (stat_ydensity).\n\n\n\n\n\nAfortunadamente, uno no está peleado con el otro, y podemos añadir la información resumida disponible en el gráfico de cajas y bigotes:\n\nggplot(data = penguins,\n       aes(x = species, y = bill_length_mm,\n           color = species)) +\n  geom_violin(fill = NA) +\n  geom_boxplot(fill = NA, width = 0.1)\n\nWarning: Removed 2 rows containing non-finite values (stat_ydensity).\n\n\nWarning: Removed 2 rows containing non-finite values (stat_boxplot).\n\n\n\n\n\nCon estos gráficos vemos la relación entre una variable categórica y una variable numérica, pero ¿qué pasa si tenemos dos variables numéricas?\n\n\n8.4.5 Gráfico de dispersión\nEl gráfico más básico es el gráfico de dispersión. Simplemente es un gráfico de las coordenadas dadas por una variable x con respecto a una variable y. En ggplot2 utilizaremos la capa geom_point:\n\nggplot(data = penguins,\n       aes(x = flipper_length_mm,\n           y = bill_length_mm)) +\n  geom_point(color = \"dodgerblue4\")\n\nWarning: Removed 2 rows containing missing values (geom_point).\n\n\n\n\n\nAl igual que en el gráfico de frecuencias, podemos colorear los puntos según una variable categórica:\n\nggplot(data = penguins,\n       aes(x = flipper_length_mm,\n           y = bill_length_mm)) +\n  geom_point(aes(color = species))\n\nWarning: Removed 2 rows containing missing values (geom_point).\n\n\n\n\n\nE, incluso, según una variable continua:\n\nggplot(data = penguins,\n       aes(x = flipper_length_mm,\n           y = bill_length_mm)) +\n  geom_point(aes(color = body_mass_g))\n\nWarning: Removed 2 rows containing missing values (geom_point).\n\n\n\n\n\n¿Y si no me interesa ver todos los puntos, sino las distribuciones bivariadas? También tenemos alternativas.\n\n\n8.4.6 Gráfico de densidad bivariado\nPodemos utilizar un gráfico de densidad bivariado, el cual muestra contornos correspondientes a la densidad de los puntos. Su interpretación es exactamente igual a un mapa topográfico. Los contornos más pequeños representan la mayor densidad, mientras que los más grandes una menor densidad.\n\nggplot(data = penguins,\n       aes(x = flipper_length_mm,\n           y = bill_length_mm)) +\n  geom_density2d(aes(color = species))\n\nWarning: Removed 2 rows containing non-finite values (stat_density2d).\n\n\n\n\n\n\n\n8.4.7 Heatmaps\n¿Y si mis dos variables son discretas? No temas, para eso tenemos heatmaps:\n\nggplot(data = penguins,\n       aes(x = species,\n           y = island)) +\n  geom_tile(fill = \"dodgerblue4\",\n            color = \"dodgerblue4\")\n\n\n\n\nSi tienes una variable adicional (pueden ser frecuencias o alguna variable continua) puedes agregarla con el argumento fill dentro de aes:\n\nggplot(data = penguins,\n       aes(x = species,\n           y = island)) +\n  geom_tile(aes(fill = bill_length_mm))\n\n\n\n\n¿Hay algún equivalente para tres variables numéricas? Por supuesto.\n\n\n8.4.8 Gráfico de contornos\nPodemos generar un gráfico de contornos, utilizando la capa geom_contour. OJO: para esta hay que hacer cierto procesamiento de los datos, en el sentido que hay que generar una malla uniforme de coordenadas x, y, z. Esto queda fuera de la discusión por este momento, pero lo revisitaremos cuando grafiquemos la zona de decisión de un análisis de funciones discriminantes lineales en la sesión de clasificación.\n\n\n8.4.9 Otros gráficos\nEstos gráficos están lejos de ser los únicos a nuestra disposición. Para comparar distribuciones también tenemos forest plots o ridgelines. Si queremos ver la relación entre más de dos variables, considerando cada individuo, podemos utilizar gráficos de coordenadas paralelas, para lo cual utilizaremos la función ggparcord de la librería GGally:\n\nGGally::ggparcoord(data = penguins,\n                   groupColumn = \"species\",\n                   columns = 3:6)\n\n\n\n\nY muchos otros más que no tendríamos tiempo de revisar en una sola sesión, cada uno con un uso particular. Algunos de ellos los veremos durante el curso, mientras que algunas otras alternativas las puedes encontrar en este enlace. También es importante mencionar que R base y ggplot2 no son las únicas librerías para generar gráficos en R. Podemos utilizar D3 o plotly, por ejemplo, para generar no solo otro tipo de gráficos, sino también hacerlos interactivos.\nEsto sería todo para esta sesión. Espero que haya sido un buen recordatorio y que te sea de utilidad en algún futuro."
  },
  {
    "objectID": "c09_ph0.html#hipótesis",
    "href": "c09_ph0.html#hipótesis",
    "title": "9  Pruebas de significancia estadística",
    "section": "9.1 Hipótesis",
    "text": "9.1 Hipótesis\nSi yo te preguntara ¿qué es una hipótesis? Yo esperaría una respuesta cercana a “una posible explicación a un fenómeno”. La definición “completa” que me gustaría que recordaras es “una serie de premisas concatenadas para dar una explicación a un fenómeno”, de la forma “Dado que A, entonces B” (¿recuerdas la probabilidad condicional?). Bueno, pues es importante reconocer que esa NO es la hipótesis que probamos con las pruebas de significancia, o al menos no directamente. Si a nosotros nos interesa saber si un nuevo alimento, llamémosle A, es capaz de darnos peces más grandes que nuestro alimento actual (control, C), entonces diseñaríamos un experimento con dos grupos: un grupo experimental al que le daríamos el alimento A, y un grupo control al que se le daría el alimento C. Nuestra hipótesis sería algo como “Dadas las diferentes composiciones de los alimentos A y C, la talla final será diferente entre ambos grupos”. Esa es nuestra hipótesis de trabajo; es decir, nuestra posible explicación; sin embargo, esta no es una hipótesis estadística.\n¿Qué es entonces una hipótesis estadística? Sería más correcto hablar de juegos de hipótesis estadísticas, el cual quedaría de la siguiente manera:\n\n\\(H_0: \\mu_1 = \\mu_2; \\overline{X_A} = \\overline{X_C}\\)\n\\(H_1: \\mu_2 ≠ \\mu_2; \\overline{X_A} ≠ \\overline{X_C}\\)\n\nEntonces, es un juego porque hay más de una, y representan un escenario de igualdad o de diferencia. Por cada juego de hipótesis hay al menos una hipótesis de nulidad (\\(H_0\\)) y una hipótesis alternativa (\\(H_1\\)). ¿Cómo llegamos a estas hipótesis y qué representan? Bien, es ahí donde debemos de hablar de las pruebas de significancia estadística.\n\n\n\n\n\n\nNota\n\n\n\nLos términos “pruebas de hipótesis de nulidad” y “pruebas de significancia estadística” hacen referencia a lo mismo; es decir, son sinónimos para referirnos al proceso mediante el cual utilizaremos a la estadística para tomar una decisión."
  },
  {
    "objectID": "c09_ph0.html#pruebas-de-hipótesis-de-nulidad",
    "href": "c09_ph0.html#pruebas-de-hipótesis-de-nulidad",
    "title": "9  Pruebas de significancia estadística",
    "section": "9.2 Pruebas de hipótesis de nulidad",
    "text": "9.2 Pruebas de hipótesis de nulidad\nEn el Capítulo 1 dimos algunas definiciones de estadística, pero hay una que, toma mucho sentido en el tema que vamos a abordar hoy. Esta fue propuesta por Savage (1954): “[La estadística] es la ciencia de tomar decisiones bajo situaciones de incertidumbre.\nPartiendo de esa definición, entendamos qué es la inferencia estadística y las pruebas de hipótesis, utilizando una analogía propuesta por Cassie Kozyrkov (científica jefa de decisión en Google). Pensemos que logramos contratarnos en una empresa de exploración espacial, y que nuestra actividad principal es salir al espacio en una nave espacial, visitar planetas y reportar si hay vida o no.\n\n\n\nFigura 9.1: A buscar planetas.\n\n\n¿Bonito? Tal vez demasiado para ser verdad y, como era de esperarse, lo es. Tenemos un jefe con una actitud bastante mejorable, pero dejando eso de lado, tenemos algunas limitaciones logísticas:la primera es que nuestro reporte es a través de un aparatito que cuenta con dos botones uno para decir que sí hay vida y otro para decir que no hay vida en un planeta dado, otra limitación es que tenemos recursos limitados y que únicamente podemos hacer caminatas de dos horas. Esto se traduce en que estamos en una situación de incertidumbre.\n\n\n\nFigura 9.2: Exploración espacial e incertidumbre.\n\n\nUn desafío más es que NO podemos ver un planeta y no dar un reporte. Debemos, sí o sí, decir si hay vida en el planeta o no. Esto lleva a que necesitemos establecer una decisión por defecto. ¿Por defecto para qué? Para aquellos casos en los que NO podamos aterrizar en el planeta y explorarlo (tal vez hay una tormenta eléctrica, o alguna otra situación). Estarás de acuerdo conmigo en que decir que NO hay vida en el planeta tiene más sentido que decir que sí, y no por la situación del planeta en si misma, sino porque de lo contrario no tiene caso hacer una exploración espacial. Me explico. Si la acción por defecto fuera decir que sí hay vida en el planeta no necesitariamos ni siquiera hacer exploraciones, simple y sencillamente presionaríamos el botón correspondiente cada que nos avisaran de un nuevo planeta.\n\n\n\nFigura 9.3: Decisiones por defecto y alternativa.\n\n\nUna vez planteadas nuestras decisiones por defecto, podemos pensar en probar si hay vida en el planeta o no. Esto es un escenario típico de una prueba de hipótesis de nulidad, lo que nos lleva a definir una hipótesis nula y una alternativa. Para nuestra hipótesis nula nos podemos preguntar:\n“Si supiera todo sobre este planeta, ¿qué me inclinaría a presionar el botón X?”.\nEspero que tu respuesta haya sido “Que no haya vida en el planeta”. Simple, ¿no? A final de cuentas lo sabemos TODO sobre el planeta, incluyendo si hay vida o no y en consecuencia presionaremos el botón X si y solo si no hay vida. ¿Y la hipótesis alternativa? Pues está dada por todas las situaciones en las que la hipótesis nula sea falsa, en este caso que sí haya vida en el planeta.\nEntonces, definimos nuestras hipótesis:\n\nNula (\\(H_0\\)): No hay vida en el planeta.\nAlternativa(\\(H_A\\)): Sí la hay.\n\n\n\n\nFigura 9.4: Hipótesis de nulidad e hipótesis alternativa.\n\n\nSalimos entonces a explorar el universo, encontramos un planeta, aterrizamos y damos nuestra caminata. El resultado: 0 organismos en las dos horas que caminamos. Te pregunto: ¿qué aprendimos que sea de interés? Y aquí espero que tu respuesta sea Nada. Me explico, tuvimos una muestra de 0 organismos, no sabemos de qué tamaño es la población (si es que la hay). ¿Explicaciones para el resultado? Bastantes, pero todo se reduce a que tuvimos que tomar nuestra decisión por defecto y, por lo tanto, no aprendimos nada del planeta. Literalmente obtuvimos el mismo resultado que si hubieramos pasado de largo y eso está bien. No entraré en la discusión de por qué el buscar siempre aprender algo es un sinsentido, solo diré que quien quiera hacerlo es porque tiene demasiada energía, pero sigamos con nuestro ejemplo.\n\n\n\nFigura 9.5: No sabes nada, Jon Snow.\n\n\nRecordemos la definición de Savage y tomemos una decisión. Para ello cambiemos nuestra pregunta a lo que debería de ser el mantra detrás de todas las pruebas de hipótesis: ¿Mi evidencia deja en ridículo a mi hipótesis de nulidad? La respuesta debería de ser no, pues no encontramos nada que la contradijera y, por lo tanto, presionaremos nuestro botón de no hay vida en el planeta.\n\n\n\nFigura 9.6: ¿Nuestra evidencia hace quedar en ridículo a nuestra \\(H_0\\)?\n\n\nSalimos del planeta y llegamos a otro. Repetimos el proceso, solo que aquí sí nos encontramos una forma de vida alienígena en forma de exactamente un individuo. Recordemos nuestro marco de decisión. Nuestra acción por defecto es presionar el botón X si no hay vida (hipótesis nula) y acabamos de encontrar un individuo (solo uno). ¿El tamaño de la población? No lo sabemos. ¿Nuestra evidencia deja en ridículo a nuestra hipótesis nula? Por supuesto que sí, entonces la rechazamos y tomamos nuestra acción alternativa: presionar el botón con la palomita verde. ¿Qué aprendimos que sea de interés? Que hay vida en el planeta.\n\n\n\nFigura 9.7: ¡Aprendimos algo!\n\n\nEste ejemplo es lo que hacemos o deberíamos de hacer al aplicar una prueba de hipótesis de nulidad. Establecer nuestra acción por defecto y la alternativa, definir nuestra hipótesis de nulidad, ir a tomar datos y luego preguntarnos si esa evidencia deja en ridículo a nuestra hipótesis nula. ¿Cómo definimos la acción por defecto? Eso es tarea del tomador de decisiones, y en la academia muy pocas veces las tenemos definidas. ¿Cómo concluimos? Si nuestra evidencia ridiculiza a nuestra hipótesis nula, tendremos una conclusión a favor de la hipótesis alternativa. ¿Si no? No aprendimos nada y tomamos la decisión por defecto. ¿Es la correcta? No lo sabemos, pero tampoco nos interesa… o al menos hasta cierto punto. Es aquí donde entran los tipos de errores y los valores de p, solo recuerda: buscar tus llaves antes de salir de casa y no encontrarlas después de 5 minutos NO indica que no estén, solo no sabes dónde están.\n\n\n\nFigura 9.8: Alimento experimental vs. control"
  },
  {
    "objectID": "c09_ph0.html#tipos-de-errores",
    "href": "c09_ph0.html#tipos-de-errores",
    "title": "9  Pruebas de significancia estadística",
    "section": "9.3 Tipos de errores",
    "text": "9.3 Tipos de errores\nAhora bien, el no poder responder inequívocamente a si nuestra decisión fue la correcta o no es lo que nos lleva a hablar de los tipos de errores estadísticos. En el Capítulo 7 hablamos de algunos errores de muestreo (sistemático y aleatorio), pero estos son diferentes a los errores que podemos cometer al momento de realizar nuestras pruebas de significancia. Estos errores los vamos a expresar en términos de probabilidades. ¿Probabilidades de qué? Por muy “obvio” que parezca, probabilidades de equivocarnos al tomar una decisión sobre nuestra hipótesis de nulidad. Te voy a presentar estos errores de dos maneras: una formal, otra que pudiera resultarte más intuitiva, y una analogía. Formalmente:\n\nError de tipo I (\\(\\alpha\\)): Probabilidad de rechazar la hipótesis nula cuando es verdadera.\nError de tipo II (\\(\\beta\\)): Probabilidad de NO rechazar la hipótesis nula cuando es falsa.\n\nSi recuerdas nuestro viaje espacial y el proceso mediante el cual decidíamos si había vida en un planeta o no, todo se reduce a si nos quedamos con nuestra decisión por defecto (no rechazar la hipótesis de nulidad) o si nuestra evidencia la dejó en ridículo (rechazar la hipótesis de nulidad). Si te es posible, procura recordar estas definiciones. ¿Por qué digo que si te es posible? Porque puede ser que no te acomode pensar en términos de “rechazo” y que prefieras pensar en términos de aceptación. Bajo ese esquema tendríamos:\n\nError de tipo I (\\(\\alpha\\)): Probabilidad de aceptar la hipótesis aternativa siendo falsa.\nError de tipo II (\\(\\beta\\)): Probabilidad de aceptar la hipótesis nula siendo falsa.\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nLa definición de los tipos de errores estadísticos está formalmente en términos de rechazar o no la hipótesis de nulidad. Este re-planteamiento en forma de aceptación es solo una herramienta didáctica. ¿Por qué? Porque nunca “aceptamos” o damos algo por cierto si es que estamos en un escenario de incertidumbre. Este re-planteamiento tiene solo un fin didáctico.\n\n\nAhora bien, hay una tercera forma de entender estos errores, la cual simplifica mucho las cosas. Imagina un escenario en el que van una mujer evidentemente embarazada y, por alguna extraña razón, un hombre a hacerse una prueba de embarazo. El doctor puede, entonces, dar dos veredictos por caso: la persona está embarazada o no está embarazada. Gráficamente esto podemos ponerlo de la siguiente manera:\n\n\n\nFigura 9.9: Errores y falsedades.\n\n\nQue un hombre esté embarazado es imposible, por lo que decirle a nuestro curioso paciente que lo está es un falso positivo o un error de tipo I. Por otra parte, decirle a la mujer que está embarazada que no lo está es un falso negativo, o un error de tipo II.\nAunque estos son los dos tipos de errores estadísticos formales, tenemos un tercero que, aún no siendo normal, es necesario que cuidemos:\n\nError Tipo III: Rechazar correctamente la hipótesis nula equivocada.\n\n¿A qué me refiero con esto? A utilizar la matemática correcta para responder una pregunta equivocada; es decir, utilizar pruebas que no responden o atacan directamente nuestro problema. Un ejemplo de esto puede ser utilizar un modelo lineal simple para describir una relación exponencial, pero hablaremos de estos problemas más adelante.\nAhora bien, hablamos de probabilidades, mientras que en el ejemplo de la Figura 9.9 vimos errores particulares. Pues es aquí donde entra el famosísimo (¿infame?) valor de p."
  },
  {
    "objectID": "c09_ph0.html#nivel-de-significancia",
    "href": "c09_ph0.html#nivel-de-significancia",
    "title": "9  Pruebas de significancia estadística",
    "section": "9.4 Nivel de significancia",
    "text": "9.4 Nivel de significancia\nSi te pregunto ¿qué es el valor de p y qué representa? Es posible que busques la definición funcional y que me digas que es un valor que si es menor a 0.05 indica que la prueba es “significativa” y que si es mayor no lo es. Eso es solo la regla de decisión, pero no define al valor de p. \n¿Qué es entonces? Retrocedamos un poco a nuestro ejemplo de la exploración espacial. Partíamos del supuesto (decisión por defecto) de que no hay vida en ningún planeta, y que nuestra evidencia debe de ridiculizar a nuestra hipótesis de nulidad para que presionemos el botón de que sí hay vida en el planeta. En nuestras pruebas de significancia es exactamente lo mismo. Partimos del supuesto de que no hay un efecto (o diferencia) significativa. Esto lo representamos con un modelo teórico de distribución de probabilidades, el cuál representa un universo donde la hipótesis nula siempre es verdadera. Es decir, esta distribución teórica de probabilidades son todos los casos en los cuales NO existe ningún efecto o diferencia entre nada. Un mundo gris y aburrido, vamos.\nEn este escenario entonces nosotros buscamos evidencia que nos haga cambiar de opinión, y lo evaluamos en términos probabilísticos lo evaluamos como la probabilidad de que el modelo teórico haya generado los datos que estamos viendo. ¿A que esto tiene más sentido que solo la regla de decisión?\nAhora puede que te preguntes: ¿de dónde sale esta probabilidad? Pues está definida por un estadístico de prueba. Tomando nuestros datos vamos a calcular el valor que les corresponde según la función del modelo teórico. Luego, vamos a obtener la probabildiad de encontrar, de forma aleatoria, ese valor, o uno con la misma probabilidad, o una menor. Ahora bien, si recuerdas algo de la sesión de probabilidad puede que esta última definición no te cuadre, y es que si tenemos una distribución continua no podemos buscar la probabilidad individual de un valor, por lo que la reformulamos como la probabilidad de encontrar otro valor al menos igual de grande, y esto es lo que conocemos como el nivel de significancia.\n\n\n\n\n\n\nImportante\n\n\n\nValor de p, p-value y nivel de significancia son exactamente lo mismo, solo que algunas personas utilizan “nivel de significancia” para referirse al valor de \\(\\alpha\\); e.g., “las pruebas se consideraron significativas a un nivel de significancia \\(\\alpha = 0.05\\). Esto es erróneo, pues \\(\\alpha\\) representa la probailidad de cometer un error de tipo 1 con la que estamos dispuestos a vivir, pero vamos a entrar a más detalles un poco más adelante.\n\n\nSé que esta última parte es sumamente abstracta, así que grafiquemos una distribución normal con un algunas áreas bajo la curva (probabilidades) de referencia:\n\nlibrary(ggplot2)\n\nn <- 100000\nv1 <- data.frame(var = rnorm(n))\nsds <- data.frame(xf = c(3, 1.96, 1))\n\nsds[\"AUC\"] <- NA\n\nfor (i in seq_along(sds$xf)) {\n  sds$AUC[i] <-\n    as.character(round(1 -length(v1$var[(v1$var < -sds$xf[i]) |\n                                        (v1$var > sds$xf[i])]) /n,\n                       2))\n}\n\nuni_norm <- ggplot() + \n            geom_rect(data = sds, aes(xmin = -xf, xmax = xf, \n                                      ymin = 0, ymax = Inf, \n                                      fill = AUC),  alpha = 0.3) +\n            geom_density(data = v1, aes(var),\n                         kernel = \"gaussian\", \n                         colour = \"deepskyblue4\",\n                         fill = \"deepskyblue4\", \n                         alpha = 0.6) +\n            labs(x = \"Z\",\n                 y = element_blank(),\n                 title =\n                   \"Gráfico de densidad de una distribución normal\",\n                 subtitle = expression(paste(\"n = 100000; \",\n                                             mu, \" = 0; \",\n                                             sigma, \" = 1\")),\n                 caption = \"Datos simulados\") +\n            theme(panel.grid.minor = element_blank(),\n                  panel.grid.major = element_blank(),\n                  panel.background = element_blank(),\n                  axis.line = element_blank(),\n                  aspect.ratio = 1/1.61,\n                  axis.ticks = element_blank(),\n                  text = element_text(family = \"Montserrat\",\n                                      colour = \"gray50\")\n                  ) +\n            scale_y_continuous(breaks = NULL) +\n            scale_x_continuous(breaks = c(-sds$xf, sds$xf))\n            \nuni_norm\n\n\n\n\nFigura 9.10: Áreas bajo la curva (AUC) para tres valores de Z de referencia. Estas representan las zonas de aceptación, con su amplitud (confianza) dada por el valor de AUC.\n\n\n\n\n¿Qué nos dice este gráfico? Imagina que hacemos el procedimiento estadístico correspondiente y obtenemos un valor de \\(Z = 1.0\\). El área comprendida entre \\(Z = 1\\) y \\(Z = -1\\) es del 0.68. ¿Esto quiere decir que nuestro valor de p es de 0.68? NO, todo lo contrario. Recuerda, buscamos un valor al menos igual de grande; por lo tanto, nos interesa lo que está fuera de la zona sombreada (buscamos valores \\(|Z| \\geq 1\\)) y entonces nuestro valor de \\(p = 1-0.68 = 0.32\\). ¿Es suficiente una probabilidad del 32% para decir que nuestra evidencia dejó en ridículo a nuestra hipótesis de nulidad? Creo que estarás de acuerdo conmigo en que no.\nPero volvamos a nuestro gráfico. Las zonas sombreadas representan las zonas de no rechazo a distintos niveles de “confianza”, de manera que si obtenemos un valor de Z (estadístico de prueba) \\(\\leq 1\\) estamos en el intervalo de confianza del 68%, \\(\\leq 1.96\\) del 95% y \\(\\leq 3\\) de \\(\\approx\\) 99%.\nImagina que ahora obtuvimos un valor de \\(Z = 1.97\\), recuerda lo que vimos en la sesión de probabilidad sobre el uso de la distribución normal en R. ¿Qué valor de \\(p\\) le corresponde? ¿Es un resultado “significativo? ¿Qué tal con \\(Z = 1.95\\)?\nEsto me llevaría al siguiente punto de discusión, sobre el obscurantismo alrededor del valor de p, pero tomemos un desvío para hablar sobre otro concepto muy relacionado con este último: los intervalos de confianza"
  },
  {
    "objectID": "c09_ph0.html#intervalos-de-confianza",
    "href": "c09_ph0.html#intervalos-de-confianza",
    "title": "9  Pruebas de significancia estadística",
    "section": "9.5 Intervalos de confianza",
    "text": "9.5 Intervalos de confianza\nLos intervalos de confianza, como recordarás que mencionamos someramente en el Capítulo 8, representan una medida de incertidumbre en alguna estimación, pero vayamos más a fondo. Estos intervalos, al igual que el valor de p, abordan el mismo problema: las estimaciones puntuales no son infalibles ni siempre son verídicas, por lo que necesitamos una referencia de “certeza” alrededor de ellas. Si lo piensas detenidamente eso justamente es lo que hacemos con el valor de p y, de hecho, podemos pensar en los intervalos de confianza como el valor de p visto desde otra perspectiva.\nEntonces, ¿qué son los intervalos de confianza? Son una referencia de la “certeza” que tenemos alrededor de una estimación. Es muy posible que te hayas encontrado con la notación \\(\\overline{x} ± SD\\), en la cual resumimos nuestros datos con el promedio y la desviación estándar de los datos. Esto está bien siempre y cuando solo nos interese describir todos nuestros datos. Si nos interesa dar información sobre nuestra estimación, entonces haríamos algo tal que \\(\\overline{x}; [IC_i, IC_s]\\), donde \\(IC_i\\) representa el límite inferior del intervalo de confianza e \\(IC_s\\) el límite superior.\nEstos límites indican entre dónde y dónde (o, mejor dicho, entre qué valores) se puede encontrar la estimación. Si son equivalentes al valor de p, entonces siguen también un modelo teórico de distribución de probabilidades, y su amplitud está dada por un porcentaje de esa distribución (igual que en la zona de no rechazo de una hipótesis de nulidad).\n\n\n\n\n\n\nAdvertencia\n\n\n\nDada esta descripción, es muy posible que quieras interpretarlos como “hay X% de probabilidad de que la estimación se encuentre en este intervalo”. NO SE INTERPRETAN DE ESA MANERA. La forma adecuada es, si realizo \\(N\\) muestreos independientes, cada uno con tamaño de muestra \\(n\\), y para cada uno estimo la media y sus intervalos de confianza, ≈ 95% de estas estimaciones incluirán la media “real” (poblacional; Figura 9.11).\n\n\n\n\n\nFigura 9.11: Representación gráfica de la interpretación de los intervalos de confianza: 100 muestreos independientes con tamaño de muestra 20. Para cada uno se estima la media (x) y sus intervalos de confianza (líneas verticales). Solo seis (rojo) no incluyeron la media real (línea azul):\n\n\n\n9.5.1 Cálculo de los intervalos de confianza\nTe mencioné anteriormente que los IC siguen un modelo de distribución de probabilidades. Los modelos más comunes son las distribuciones \\(t\\) de Student y normal. Por el momento no es necesario que te preocupes por el modelo subyacente; sin embargo, lo más común es que utilicemos la distribución \\(t\\). ¿La razón? La veremos más adelante cuando expliquemos propiamente la pruena \\(t\\) de Student. Las ecuaciones correspondientes las puedes encontrar en un libro de estadística básica (e.g., Zar, 2010), así que te ahorraré el que las leas aquí y mejor vayamos directamente a cómo calcularlos con R.\nPara ejemplificarlo utilicemos 15 muestras de nuestros 10,000 datos simulados bajo una distribución normal estándar (v1$var):\n\nset.seed(0)\nv2 <- sample(v1$var, size = 15)\n\nLa estimación de la media ya la conocemos. En este caso fue de -0.27, que podríamos decir se encuentra alejado de la media poblacional 0 que establecimos, pero ¿qué tanto es tantito? Es ahí donde entran los IC.\n\nmean(v2)\n\n[1] -0.1024773\n\n\nPero para calcularlos, posiblemente sin que sea sorpresa, tenemos distintas maneras de hacerlo. Comenzando con R base podemos utilizar la funciónt.test(x, conf.level), donde x es el vector de observaciones y conf.level el nivel de confianza deseado:\n\nt.test(v2, conf.level = 0.95)\n\n\n    One Sample t-test\n\ndata:  v2\nt = -0.3438, df = 14, p-value = 0.7361\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.7417717  0.5368171\nsample estimates:\n mean of x \n-0.1024773 \n\n\nEsta función aplica una prueba \\(t\\) de Student para una sola muestra, por lo que da un valor del estadístico de prueba \\(t\\), los grados de libertad df y un valor de p. Por el momento olvidemos eso y quedémonos con el intervalo de confianza al 95%: \\([-0.73, 0.18]\\). Este intervalo es grande en relación a nuestra media (\\(|IC_s - IC_i| = |0.18-0.73| = 0.55 > |\\overline{x}| = 0.27\\)), lo cual nos dice que no “confiamos” mucho en ese valor promedio, lo cual tiene sentido porque la distribución poblacional es una normal estandar (\\(\\mu = 0; \\sigma = 1\\)). Ahora bien, no podemos descartar que esa media sea diferente de 0, pues el IC incluye al 0 (ojo también al valor de p).\n\n\n\n\n\n\nTip\n\n\n\nTanto los IC como el valor de p pueden utilizarse para “probar hipótesis” (sensu pruebas de significancia). Si tus intervalos de confianza con amplitud \\(1-\\alpha\\) para un efecto dado incluyen al 0, entonces vas a tener un valor de \\(p > \\alpha\\).\n\n\nOtra forma de calcularlos es con la función Rmisc::CI(x, ci), donde x es el vector de observaciones y ci el nivel de confianza:\n\nRmisc::CI(x = v2, ci = 0.95)\n\n     upper       mean      lower \n 0.5368171 -0.1024773 -0.7417717 \n\n\nUna alternativa más es utilizar la función rcompanion::groupwiseMean(formula, data, conf, R), la cual tiene dos funcionalidades interesantes: i) permite calcular los IC para varios grupos, y ii) poder estimar los intervalos a partir de remuestreos Bootstrap. En esta función: formula es lo que vimos en la función aggregate, donde indicamos al mismo tiempo la variable numérica y la variable de agrupamiento, tal que: var~grupo; data indica el data.frame que contiene la información, conf la amplitud de los intervalos y R el número de réplicas Bootstrap a realizar. Sustituyendo:\n\nrcompanion::groupwiseMean(v2~1,\n                          data = as.data.frame(v2),\n                          conf = 0.95,\n                          R = NA)\n\n\n\n  \n\n\n\nAquí notarás algunas cosas interesantes:\n\nEn formula pasamos v2~1, porque no tenemos una columna de agrupamiento (solo tenemos 1 grupo).\nEn data pasamos nuestro vector como un data.frame utilizando as.data.frame(v2).\nEn R pasamos NA para indicar que NO queremos que los IC se estimen utilizando réplicas Bootstrap.\n\n\n\n\n\n\n\nNota\n\n\n\nLas réplicas Bootstrap son útiles cuando tenemos distribuciones muy sesgadas y no normales, a diferencia de nuestros datos de ejemplo. Cambia a R = 500. ¿Cambió algo?\n\n\nDespués de este pequeño rodeo en el cual vimos cómo los IC y los valores de p están íntimamente relacionados, volvamos a hablar de estos últimos y qué consideraciones debemos de tener al utilizarlos e interpretarlos."
  },
  {
    "objectID": "c09_ph0.html#valores-de-p-usos-y-abusos",
    "href": "c09_ph0.html#valores-de-p-usos-y-abusos",
    "title": "9  Pruebas de significancia estadística",
    "section": "9.6 Valores de p: usos y abusos",
    "text": "9.6 Valores de p: usos y abusos\nDependiendo de cuántas veces hayas llevado una materia de estadística estarás más o menos familiarizado con la frase “No se encontraron diferencias significativas (\\(p > \\alpha = 0.05\\)).” Pregunta: ¿de dónde salió ese \\(\\alpha = 0.05\\)? Ojo, no te estoy preguntando qué representa \\(\\alpha\\), sino de dónde salió el criterio de tomar 0.05 como el umbral para decidir si algo es significativo o no. Según qué tanto hayas leído (o qué tanto hayan hecho su tarea tus profesores), puedes dar una de tres respuestas:\n\n“Porque así me lo enseñaron” (sensu convención).\n“Porque es lo aceptado en el área de investigación” (idem).\n“Porque así lo planteó Ronald Fisher, pues consideró que 1/20 falsos positivos eran pocos”.\n\nY las tres respuestas tienen algo de razón, pero también ambas están equivocadas. Me explico. Si bien es cierto que en el área de ecología y biología en general un \\(\\alpha\\) de 0.05 se ha considerado como “suficiente”, justo porque estamos dispuestos a aceptar 1/20 falsos positivos, en otras áreas se requiere de mayor certeza para tomar una decisión. En el área de investigación médica y farmacéutica, por ejemplo, usualmente toman valores de 0.01 e, incluso, 0.001. La razón es que el nivel de \\(\\alpha\\) debe de decidirse a priori, según el problema que tengamos entre manos y cuántos falsos positivos estemos dispuestos a aceptar.\nVeamos el caso de la siguiente figura (Armhein, Greenland & McShane, 2019):\n\n\n\nFigura 9.12: Valores de p, intervalos de confianza y conclusiones erróneas.\n\n\nAquí tenemos dos estudios que, en realidad, dieron resultados equivalentes o, cuando menos, que no están en conflicto uno con el otro. El estudio azul tuvo el mismo efecto positivo promedio que el rojo, aunque con un IC mucho más angosto y, en consecuencia, un valor de p “signficativo” (pequeño). El estudio rojo, por el contrario tiene un intervalo de confianza mucho más amplios que incluyeron el 0 y, consecuentemente, un valor de p “no significativo”. ¿Cuál sería la forma correcta de interpretar estos resultados? Primero, no descartar al estudio rojo o tacharlo de “no significativo”. Mejor, pensemos en los IC como intervalos de compatibilidad, describamos los resultados en función de las consecuencias de estos intervalos, con especial énfasis en la estimación puntual, pues es la más compatible con los datos. La razón de esto es que, por puro efecto del azar, el estudio azul podría replicar exactamente el mismo estudio y esta vez obteer un IC similar al rojo o viceversa.\n\n\n\n\n\n\nAdvertencia\n\n\n\n¡Significancia estadística NO IMPLICA significancia biológica!\n\n\n¿A qué me refiero con esta advertencia? A que no porque la prueba arroje efectos o diferencias significativas quiere decir que esas diferencias sean importantes para el fenómeno que estemos analizando y, de hecho, en la siguiente sesión vamos a ver un ejemplo donde la diferencia es muy pequeña en magnitud, pero es estadísticamente significativa. Sin más preámbulo, vayamos a aplicar una prueba de significancia."
  },
  {
    "objectID": "c09_ph0.html#prueba-básica-t-de-student",
    "href": "c09_ph0.html#prueba-básica-t-de-student",
    "title": "9  Pruebas de significancia estadística",
    "section": "9.7 Prueba básica: \\(t\\) de Student",
    "text": "9.7 Prueba básica: \\(t\\) de Student\nLa prueba de hipótesis más conocida es la prueba \\(t\\) de Student, y con justa razón. Esta prueba se construye a partir de una distribución \\(t\\) de Student, la cual tiene tres parámetros: i) \\(\\mu\\): media o centro de la distribución (posición del punto de mayor densidad en el eje x), ii) \\(\\sigma\\): escala de la distribución (desviación estándar) y iii) \\(\\nu\\): grados de libertad. Recordarás que los parámetros de una distribución afectan su forma. En este caso, el parámetro “más importante” son los grados de libertad, pues conforme estos se acerquen a infinito más se acercará la distribución a una normal. De hecho, con \\(n \\geq 30\\) se considera que la distribución es prácticamente normal. ¿Qué es lo que cambia? La altura o el peso de las colas:\n\n\n\nFigura 9.13: Distribución \\(t\\) de Student con diferentes grados de libertad \\(k\\) (\\(\\nu\\)).\n\n\nEste cambio en la altura de las colas es sumamente importante, y es lo que hace que la distribución \\(t\\) sea uno de los “caballitos de batalla” de la estadística, pues literalmente estamos siendo conscientes que, conforme disminuye el tamaño de muestra (grados de libertad, pero ahorita los definimos), incrementa la probabilidad de tener valores extremos (el peso de las colas). Con esto también establecemos que no queremos que esos valores extremos desvíen nuestras estimaciones.\nEn este punto estarás pensando “todo eso está muy bien, pero ¿qué son los grados de libertad y con qué se comen?” Y este concepto es uno que se entiende mejor si lo deducimos con un ejercicio extremadamente simple. Si yo te digo que obtengas el promedio de los números 1, 2 y 3 harías esto:\n\nmean(c(1, 2, 3))\n\n[1] 2\n\n\nY si te digo ahora que hagas eso mismo para los números 1 y 3:\n\nmean(c(1, 3))\n\n[1] 2\n\n\nObtenemos exactamente el mismo resultado que antes, pues retiramos la media del conjunto de datos original. Eso son los grados de libertad: el número de observaciones independientes de la media, tal que \\(\\nu = n - 1\\). Es sumamente importante que tengas presente este concepto, pues un montón de pruebas involucran grados de libertad.\n\n\n\n\n\n\nNota\n\n\n\nEn algunas pruebas verás que los grados de libertad se estiman de distintas maneras, según qué involucre la prueba y qué grados de libertad sean los que se están calculando, pero la escencia siempre es la misma: retirar la media de los datos.\n\n\nVolviendo a la prueba \\(t\\), tenemos dos variantes y media: una prueba para una muestra, una para dos muestras independientes o una para dos muestras dependientes. ¿Por qué dos variantes y media? Ya lo verás con el caso de dos muestras dependientes (pareadas). En general, esta prueba nos permite comparar dos valores (un promedio contra una referencia o dos promedios). Sus supuestos son:\n\nNormalidad (si son dos grupos cada grupo debe de estar normalmente distribuido)\nMuestras independientes; es decir que la observación \\(x_i\\) sea independiente de la observación \\(x_j\\), que es diferente de tener muestras pareadas.\nHomogeneidad de varianzas.\n\n\n\n\n\n\n\nNota\n\n\n\nEn la siguiente sesión hablaremos largo y tendido de estos supuestos, pero por el momento entiéndelos como “requisitos” o “características” que deben de tener nuestros datos para que podamos confiar en los resultados de la prueba.\n\n\nAdicionalmente se “recomienda” que se utilice con tamaños de muestra menores a 30, por aquello de que a partir de 30 la distribución se vuelve prácticamente normal, pero también es recomendada si no conocemos la varianza poblacional.\n\n9.7.1 Prueba \\(t\\) para una muestra\nEsta prueba nos permite comparar la media de una muestra con un valor de referencia. La función de la prueba \\(t\\) para una muestra está dada por:\n\\[\nt = \\frac{\\overline{x}- \\mu_0}{\\frac{s}{\\sqrt{n}}}\n\\] Donde: \\(\\overline{x}\\) es el promedio de nuestra muestra, \\(\\mu_0\\) es un valor de referencia con el que queremos comparar \\(\\overline{x}\\), \\(s\\) es la desviación estándar de la muestra y \\(n\\) es el número de observaciones. Esta ya la aplicamos antes cuando estimamos los intervalos de confianza con la función t.test, pero volvamos a visitarla:\n\nt.test(v2, conf.level = 0.95)\n\n\n    One Sample t-test\n\ndata:  v2\nt = -0.3438, df = 14, p-value = 0.7361\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.7417717  0.5368171\nsample estimates:\n mean of x \n-0.1024773 \n\n\nAhora sí podemos revisar la salida completa. La primera línea nos dice el tipo de prueba \\(t\\) que estamos aplicando. En este caso pasamos un solo vector, por lo que la prueba a aplicar es para una sola muestra. La segunda línea nos da el nombre de los datos. La tercera nos da los resultados de la prueba: el valor del estadístico \\(t\\), los grados de libertad d.f. (degrees of freedoom) y el valor de p (p-value), mientras que la cuarta nos da la hipótesis alternativa que estamos evaluando (la nula no cambia, ¿o sí?) ¿Contra qué valor está contrastando? Por defecto contra 0 (podemos cambiarlo con el argumento mu).\n¿Cómo reportamos estos resultados? “La media no fue signficativamente diferente de 0 (\\(t = -1.29\\), \\(d.f. = 14\\), \\(p = 0.21 > \\alpha = 0.05\\)).\n\n\n\n\n\n\nImportante\n\n\n\nSiempre que hagas una prueba estadística es importante reportar el valor del estadístico de prueba y el/los parámetro(s) de la distribución, además del valor de p. ¿La razón? Para allá vamos.\n\n\nAhora bien, ¿por qué da una hipótesis alternativa? Porque podemos probar una de tres:\n\nLa media es diferente de \\(\\mu_0\\), en donde se aplica una prueba de dos colas.\nLa media es menor a \\(\\mu_0\\), en donde se aplica una prueba de una sola cola (considera la cola derecha).\nLa media es mayor a \\(\\mu_0\\), en donde se aplica también una prueba de una cola (la cola izquierda)\n\nLo cual me lleva a hablar de pruebas de dos o una cola. El qué hipótesis alternativa se prueba ya lo definimos, pero ¿a qué se refiere eso de una o dos colas? A qué cola(s) integramos. En la Figura 9.10 vimos que sumabamos lo que estaba fuera de nuestra área de aceptación, tanto a la derecha como a la izquierda; es decir, las colas de la distribución. Si solo sumamos lo que está a la derecha estamos preguntándonos si nuestra media es menor a \\(\\mu_0\\).\nEsto podemos modificarlo con el argumento alternative, que puede ser \"two.sided\" (por defecto), \"greater\" o \"less\". Veamos el resultado con la hipótesis alternativa de que la media es menor a 0:\n\nt.test(v2, conf.level = 0.95, alternative = \"less\")\n\n\n    One Sample t-test\n\ndata:  v2\nt = -0.3438, df = 14, p-value = 0.3681\nalternative hypothesis: true mean is less than 0\n95 percent confidence interval:\n      -Inf 0.4225146\nsample estimates:\n mean of x \n-0.1024773 \n\n\nAquí, además de la hipótesis alternativa, hay dos cambios que es importante notar: el valor de p y el intervalo de confianza. Empecemos con el segundo, donde tenemos [-Inf, 0.099]. Esto no es ningún error. Simple y sencillamente refleja nuestra hipótesis alternativa: Si la media es menor no tiene caso que consideremos la cola izquierda, por lo que el límite inferior se va a -Inf, mientras que el límite superior se reduce a 0.099. Compara ese valor de 0.099 con el resultado de la prueba de dos colas. ¿Qué notas? Es exactamente la mitad. Estos intervalos de confianza son simétricos, por lo que si solo estamos considerando un lado es lógico que la amplitud se reduzca a la mitad y eso me lleva al otro cambio: el valor de p. También es exactamente la mitad del que nos dio la prueba para dos colas, y es por la misma razón, estamos considerando la mitad del área bajo la curva. Pero aquí hay una advertencia muy, pero muy importante:\n\n\n\n\n\n\nAdvertencia\n\n\n\nLa decisión de si se aplica una prueba de una o dos colas es algo que se realiza a priori; es decir, desde un inicio seleccionamos nuestra hipótesis alternativa. No se vale aplicar la prueba de dos colas (hay diferencias significativas), ver que nuestro valor de p > 0.05, pero no es tan grande y luego aplicar una prueba de una cola para forzar un resultado significativo.\n\n\nEsta es la razón por la que hay que reportar la prueba, el valor del estadístico de prueba, los parámetros involucrados y el valor de p: podemos verificar que el resultado corresponda con la hipótesis que se dice se está probando. Me ha tocado saber de casos donde en la sección de métodos ponen la hipótesis alternativa en términos de “es diferente” (prueba de dos colas) y a la hora de presentar los resultados quieren “colar” una prueba de una cola. Pensemos que alguien nos dice en la sección de métodos: “Para evaluar si la media de la variable X fue significativamente de 0 se utilizó una prueba \\(t\\) de Student (\\(\\alpha = 0.05\\))”, y en los resultados: “La media fue significativamente diferente de 0 (\\(\\mu = 0.25; SD = 0.2\\); t = -1.7, df = 14, p = 0.027). Ese valor de t es”grande”, pero no lo suficiente para dar un valor de p tan “lejano” de 0.05, por lo que nos damos a la tarea de corroborarlo:\n\npt(q = -1.7, df = 14)\n\n[1] 0.05561478\n\n\n¡Sorpresa! El valor sí que era más pequeño (la mitad), por lo que esta persona aplicó una prueba de una cola y quiso darla como una prueba de dos colas. Otra forma de hacerlo sería comprobar el valor de t que le correspondería a ese valor de p:\n\nqt(p = 0.027, df = 14)\n\n[1] -2.103324\n\n\nMoraleja: dejemos las pruebas de una sola cola para cuando realmente sea nuestro interés probar si algo es mayor o menor que otra cosa que, te adelanto, son muy pocos casos en donde tenemos suficiente información para sospechar eso desde nuestro diseño experimental.\n\n\n9.7.2 Prueba para muestras independientes\nLa siguiente variación la tenemos cuando queremos comparar las medias de dos muestras independientes o, mejor dicho, dos grupos independientes. Es decir, los individuos que forman al grupo 1 son diferentes de los que conforman al grupo 2. La ecuación original sufre una ligera modificación, en donde ahora se considera la desviación estándar mancomunada (\\(sp\\)) de las muestras; es decir, la variación en el valor dada por ambos grupos. Esto es importante, pues no es lo mismo tener una diferencia promedio de 10g con una desviación mancomunada de 100 g a tener esos mismos 10g de diferencia con una desviación mancomunada de 10g. La función queda entonces:\n\\[\nt = \\frac{\\overline{x_1} - \\overline{x_2}}{sp\\cdot\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}}\n\\]\nDonde:\n\\[\nsp = \\sqrt{\\frac{(n_1 - 1)s_{x_1}^2 + (n_2 - 1)s_{x_2}^2}{n_1 + n_2 -2}}\n\\]\nAfortunadamente para implementarla en R solo vamos a utilizar la función t.test(formula, data) que ya conocíamos, solo que añadiremos dos argumentos adicionales: var.equal = T y paired = F. var.equal hace referencia al supuesto de homogeneidad de varianzas que mencionamos antes. Si no se cumple podemos pasar var.equal = F y entonces se aplicará la prueba \\(t\\) de Welch, la cual modifica el modo en el que se estima la varianza mancomunada y permite contender con varianzas desiguales. paired, por otra parte, define si es una prueba para muestras independientes (F) o muestras dependientes (pareadas, T). Esto último lo veremos más adelante.\n\n\n\n\n\n\nNota\n\n\n\n¿Qué prueba aplicar? ¿\\(t\\) de Welch o \\(t\\) de Student? Por defecto la función t.test aplica la prueba de Welch, la cual es más poderosa que la de Student cuando no se cumple el supuesto de homogeneidad de varianzas y al menos tan poderosa como la de Student cuando se cumplen todos sus supuestos. La decisión es tuya, pero la prueba \\(t\\) de Welch cubre la mayor parte de los casos.\n\n\nCreemos primero un conjunto de datos y luego apliquemos la prueba:\n\n# Datos del grupo A\nA <- rnorm(10, 10, 0.1)\n# Datos del grupo B\nB <- rnorm(10, 11, 0.1)\n\n# Formar un solo data.frame\ndf1 <- data.frame(grupo = \"A\", v1 = A)\ndf1 <- rbind(df1, data.frame(grupo = \"B\", v1 = B))\n\n# Aplicar la prueba\nt.test(v1~grupo, data = df1, var.equal = T, paired = F)\n\n\n    Two Sample t-test\n\ndata:  v1 by grupo\nt = -28.643, df = 18, p-value < 2.2e-16\nalternative hypothesis: true difference in means between group A and group B is not equal to 0\n95 percent confidence interval:\n -1.0535072 -0.9095223\nsample estimates:\nmean in group A mean in group B \n       10.00929        10.99081 \n\n\nLa salida es muy parecida al caso anterior: el tipo de prueba, el valor del estadístico de prueba, los grados de libertad, el valor de p, la hipótesis alternativa, un intervalo de confianza y los promedios de cada grupo. ¿Qué representa ese intervalo de confianza? Es el intervalo de confianza para la diferencia de medias, que es el cómo estamos comparando los grupos.\nPodemos también presentar los resultados de manera gráfica. Para ello necesitaremos guardar los resultados de nuestra prueba t.test en un objeto y extraer la información de ahí. ¿Cómo verificamos cuál es el tipo de objeto?\n\nttest <- t.test(v1~grupo, data = df1, var.equal = T, paired = F)\ntypeof(ttest)\n\n[1] \"list\"\n\n\nEl objeto es una lista NOMBRADA, por lo que podemos acceder a su contenido utilizando el operador de [[]] (posición numérica o \"nombre\") o el operador $. Guardemos el valor de p en un nuevo objeto para incluirlo en la gráfica:\n\np_val <- ttest$p.value\np_val\n\n[1] 1.814597e-16\n\n\nConstruyamos y grafiquemos los intervalos de confianza para la media de cada grupo (95%):\n\nICs <- rcompanion::groupwiseMean(v1~grupo, data = df1, conf = 0.95)\n\nerror.plot <- ggplot(data = ICs, aes(x = grupo, y = Mean)) +\n              geom_point(color = \"deepskyblue4\") + \n              geom_errorbar(aes(ymin = Trad.lower, ymax = Trad.upper),\n                            color = \"deepskyblue4\") +\n              theme(panel.grid.minor = element_blank(),\n                    panel.grid.major = element_blank(),\n                    panel.background = element_blank(),\n                    axis.line = element_blank(),\n                    aspect.ratio = 1/1.61,\n                    axis.ticks = element_blank(),\n                    text = element_text(colour = \"gray50\"),\n                    legend.position = \"none\") +\n              labs(x = \"Grupo\",\n                   y = \"x\",\n                   title = \"¿Diferencias significativas?\",\n                   subtitle = \n                     \"µ e IC para una variable aleatoria\",\n                   caption =  paste(\"t(v = \",\n                                    ttest$parameter, \", 0.05) = \",\n                                    round(ttest[[\"statistic\"]] ,2), \n                                    \"; \",\n                                    ifelse(p_val < 0.0001,\n                                           \"p < 0.0001\",\n                                           paste(\"p = \", p_val))))\n              \nerror.plot\n\n\n\n\n\n\n9.7.3 Prueba para muestras pareadas\nEsta prueba nos permite comparar los promedios de una variable de un mismo grupo en dos momentos diferentes en el tiempo, por ejemplo, comparar si la frecuencia cardiaca promedio de un grupo de participantes fue diferente antes y después de hacer ejercicio; es decir tendremos datos pareados cuando tengamos mediciones de los mismos individuos (identificados) en dos momentos diferentes. La prueba para muestras pareadas es la media variante que mencioné al inicio. ¿Por qué media? Porque en realidad aplica la prueba de una sola muestra, solo que con un paso previo: restamos los valores de cada individuo para quedarnos con un solo vector de diferencias y con ese aplicar la prueba para una muestra:\n\\[\nt = \\frac{\\overline{X}_D - \\mu_o}{\\frac{S_D}{\\sqrt n}}\n\\]\nDonde \\(\\overline{X_D}\\) y \\(S_D\\) son el promedio y la desviación estándar de las diferencias. Hagamos el ejercicio. Primero, carguemos los datos, que en este caso están contenidos en un archivo de excel:\n\ndependientes <- openxlsx::read.xlsx(\"datos/datos_t.xlsx\", sheet = 2)\ndependientes\n\n\n\n  \n\n\n\nNotar que está en formato compacto (no codificado), por lo tanto hay que transformarla:\n\ndependientes_m <- reshape2::melt(dependientes, value.name = \"FC\",\n                                 na.rm = T, variable.name = \"periodo\")\n\nNo id variables; using all as measure variables\n\ndependientes_m\n\n\n\n  \n\n\n\nAplicamos la prueba:\n\nt.test(FC~periodo, data = dependientes_m, paired = T)\n\n\n    Paired t-test\n\ndata:  FC by periodo\nt = -6.1115, df = 9, p-value = 0.0001768\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -24.11459 -11.08541\nsample estimates:\nmean difference \n          -17.6 \n\n\nComo era de esperarse, hubo diferencias en la frecuencia cardiaca. Ahora comprobemos que solo se hizo la resta de inicial vs final:\n\ndif_FC <- dependientes$Después - dependientes$Antes\nt.test(dif_FC)\n\n\n    One Sample t-test\n\ndata:  dif_FC\nt = 6.1115, df = 9, p-value = 0.0001768\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 11.08541 24.11459\nsample estimates:\nmean of x \n     17.6"
  },
  {
    "objectID": "c09_ph0.html#ejercicio",
    "href": "c09_ph0.html#ejercicio",
    "title": "9  Pruebas de significancia estadística",
    "section": "9.8 Ejercicio",
    "text": "9.8 Ejercicio\nEn esta sesión hay dos ejercicios:\n\nCarga el archivo Datos1.csv (ojo con los nombres de las columnas al cargarlo) y realiza la estimación de la media, la desviación estándar y los intervalos de confianza (calquiera de las formas) para al menos 3 tamaños de muestra diferentes (considera que tienes la población completa). El objetivo es que veas y describas cómo cambian tanto la estimación puntual como la amplitud de los intervalos de confianza al incrementar el tamaño de muestra. Puedes utilizar cualquiera de las variables LT o PT.\nRealiza la prueba t con los datos de la hoja 1 del archivo datos_t.xlsx, la cual contiene datos de dos muestras independientes. La tarea consiste en cargar los datos, realizar la prueba y presentar un gráfico en el que se reporten los resultados.\n\n\n\n\n\nArmhein V, Greenland S, McShane B. 2019. Scientists rise up against statistical significance. Nature 567:305-307. DOI: 10.1038/d41586-019-00857-9.\n\n\nSavage LJ. 1954. The Foundations of Statistics. John Wiley & Sons.\n\n\nZar JH. 2010. Biostatistical Analysis. Prentice Hall."
  },
  {
    "objectID": "c10_param.html#librerías",
    "href": "c10_param.html#librerías",
    "title": "10  Técnicas paramétricas",
    "section": "10.1 Librerías",
    "text": "10.1 Librerías\n\nlibrary(ggplot2)\nlibrary(corrplot)\nlibrary(gridExtra)\nlibrary(rstatix)\nlibrary(dplyr)\nlibrary(ggpubr)"
  },
  {
    "objectID": "c10_param.html#introducción",
    "href": "c10_param.html#introducción",
    "title": "10  Técnicas paramétricas",
    "section": "10.2 Introducción",
    "text": "10.2 Introducción\nPor fin dejaremos atrás los fundamentos de la estadística. Tal vez 10 sesiones con fundamentos se te hayan hecho largas, pero todos los conceptos que hemos visto tienen una razón de ser. No podía hablar de pruebas de hipótesis sin que antes supiéramos qué es la probabilidad y qué representa. No podíamos empezar a aplicar procedimientos estadísticos sin antes revisar las bases de R. No podíamos hacer visualizaciones efectivas sin hablar de las heurísticas correspondientes. En fin, creo que ya me entiendes. A partir de la siguiente sesión vamos a revisar solo la teoría correspondiente a cada prueba/técnica, asumiendo que ya cuentas con las bases que establecimos en las sesiones anteriores."
  },
  {
    "objectID": "c10_param.html#por-qué-paramétricas",
    "href": "c10_param.html#por-qué-paramétricas",
    "title": "10  Técnicas paramétricas",
    "section": "10.3 ¿Por qué paramétricas?",
    "text": "10.3 ¿Por qué paramétricas?\nLo primero que tenemos que abordar es ¿por qué se les conoce como pruebas paramétricas? Porque las inferencias que vamos a realizar son sobre los parámetros poblacionales, usualmente la media (\\(\\mu\\)) y la varianza (\\(\\sigma\\)). Ejemplos de pruebas paramétricas tenemos muchos, entre los más famosos tenemos la \\(t\\) de Student que revisamos en la sesión anterior, los análisis de la varianza (ANOVAs) que revisaremos en esta sesión, la correlación de Pearson y la regresión lineal que veremos en la siguiente, entre otros.\n\n\n\n\n\n\nNota\n\n\n\nNota para breviario cultural: todas las pruebas que mencioné con anterioridad son aplicaciones del modelo lineal general. ¿Eso qué significa? Que se asume que los grupos son separables linealmente, o que la relación entre dos o más variables es lineal.\n\n\nIndependientemente de la prueba o de la técnica, todas las técnicas paramétricas tienen al menos tres supuestos; i.e, requieren de al menos tres cosas:\n\nQue la distribución muestreal del valor de interés sea normal.\nQue las varianzas sean homogéneas.\nQue las muestras sean independientes.\n\nLa tercera sale sola si planteamos bien nuestro diseño experimental; es decir, que cada dato sea “independiente” de los demás. En otras palabras, que no sean mediciones de los mismos individuos, por ejemplo, o que las respuestas de cada individuo subsecuente no dependan del que estamos midiendo ahorita. Los primeros dos requieren de un poco más de explicación, así que vayamos paso a paso y hablemos de cada uno, empezando por la mayor tortura: el supuesto de normalidad."
  },
  {
    "objectID": "c10_param.html#supuesto-de-normalidad",
    "href": "c10_param.html#supuesto-de-normalidad",
    "title": "10  Técnicas paramétricas",
    "section": "10.4 Supuesto de Normalidad",
    "text": "10.4 Supuesto de Normalidad\nExiste por ahí un artículo titulado “Normalidad estadística y biología: una relación tortuosa”. Si bien es cierto que el cómo se desarrollaron los análisis, y por lo tanto sus conclusiones, no tiene ni pies ni cabeza, ni ningún tipo de fundamento, el título me parece bastante llamativo porque desde que nos empezamos a aproximar a las pruebas de hipótesis se nos habla del supuesto de normalidad o de que “nuestros datos deben de ser normales”. Bueno, hablemos a fondo de este supuesto, de qué es, qué no es, y por qué es importante.\n\n10.4.1 Versión corta\nSi no quieres entrar en demasiados detalles puedes quedarte con lo que voy a decir aquí y seguir con tu camino. Si te interesa conocer un poco más puedes leer después la versión larga.\nEn pocas palabras, el supuesto de normalidad se lee “La distribución muestral de la media es normal”. Si nosotros realizamos muestreos con tamaños de muestra \\(n\\) y para cada uno estimamos la media, conforme \\(n \\rightarrow \\infty\\) la distribución de esa media se volverá normal. Esa es la distribución muestreal de la media. Esta es una relación no controversial pues, aunque con \\(n \\geq 30\\) prácticamente lo garantizamos tenemos un problema: debemos de generalizar para cualquier \\(n\\) y simplificar, lo que nos lleva a que “las poblaciones muestreadas sigan una distribución normal”. Esto puede no ser práctico probarlo, pero sí que podemos utilizar nuestra muestra y ver si esta proviene de una distribución normal; ergo, aplicamos una prueba de normalidad para ver si nuestros datos se ajustan o no a una distribución normal.\n\n\n10.4.2 Versión larga\nPosiblemente la versión corta haya sido demasiado corta, y fue a propósito, pues me interesa que le des al menos una hojeada a la versión larga.\nVolvamos al asunto de que estamos tratando con la distribución muestral de la media. Esto es un problemón, porque una muestra NO contiene información sobre ella. A final de cuentas, una distribución muestral es el producto de distintas muestras, entonces debemos de llenar esos “vacíos” de información utilizando el centro, la dispersión y otros estadísticos descriptivos, y asumimos una distribución normal. ¿Por qué una distribución normal? Podría ser extremadamente breve y decirte que porque muchísimos atributos se distribuyen naturalmente de manera estadísticamente normal, pero en realidad es debido a otro concepto: el Teorema del Límite Central.\n\n10.4.2.1 Teorema del Límite Central\nEste teorema establece que: “Dadas muestras aleatorias e independientes con N observaciones cada una, la distribución de sus medias se aproxima a una distribución normal conforme N incrementa, INDEPENDIENTEMENTE de la distribución poblacional”; es decir, mientras N sea grande, \\(\\bar{x} \\sim Normal\\). Para probar esto podemos hacer un ejercicio en el cual simulemos una población con distribución Gamma, cuya zona de mayor densidad se encuentra desplazada a la izquierda:\n\nset.seed(0)\ndatos <- data.frame(x = 1:1000, y = rgamma(1000, 1))\ngamma <- ggplot(data = datos, aes(y)) + \n         geom_density(fill = rgb(118,78,144,\n                                 maxColorValue = 255),\n                      alpha = 0.5, colour = \"white\") +\n         theme_bw() +\n         labs(title = \"Distribución Gamma\",\n              x = element_blank(),\n              y = element_blank()) +\n         theme(text = element_text(colour = \"gray40\"))\n\n#cairo_pdf(\"gamma.pdf\", family = \"Montserrat\",\n#          height = 5, width = 5*1.6, pointsize = 20)\ngamma\n\n\n\n#dev.off()\n\nCon nuestra población definida, podemos seleccionar algunos tamaños de muestra, realizar 1000 muestreos aleatorios, obtener la media de cada muestreo y graficar su distribución. Primero para N = 3:\n\nN = 3\nmedias <- data.frame(x = 1:1000,\n                     y = replicate(1000,\n                                   mean(sample(datos$y, N))))\n\n\ndist_n3 <- ggplot(data = medias, aes(y)) +\n           geom_density(fill = \"dodgerblue4\",\n                        alpha = 0.5, colour = \"white\") +\n           theme_bw() +\n           labs(title = sprintf(\"Distribución muestreal con N = %d\", N),\n                x = element_blank(),\n                y = element_blank()) +\n           theme(text = element_text(colour = \"gray40\"))\n\n#cairo_pdf(\"n_3.pdf\", family = \"Montserrat\",\n#          height = 5, width = 5*1.6, pointsize = 20)\ndist_n3\n\n\n\n#dev.off()\n\nAhora para N = 10. La distribución se aproxima más a una distribución normal:\n\nN = 10\nmedias <- data.frame(x = 1:1000,\n                     y = replicate(1000,\n                                   mean(sample(datos$y, N))))\n\ndist_n10 <- ggplot(data = medias, aes(y)) +\n            geom_density(fill = \"dodgerblue4\",\n                         alpha = 0.5, colour = \"white\") +\n            theme_bw() +\n            labs(title = sprintf(\"Distribución muestreal con N = %d\",\n                                 N),\n                 x = element_blank(),\n                 y = element_blank()) +\n            theme(text = element_text(colour = \"gray40\"))\n\n#cairo_pdf(\"n_10.pdf\", family = \"Montserrat\",\n#          height = 5, width = 5*1.6, pointsize = 20)\ndist_n10\n\n\n\n#dev.off()\n\nCon N = 30 la distribución es más cercana a una normal que a la gamma, por lo que usualmente se acepta que: con N≥30 la distribución muestreal de la media DEBERÁ ser normal:\n\nN = 30\nmedias <- data.frame(x = 1:1000,\n                     y = replicate(1000, mean(sample(datos$y, N))))\n\n\ndist_n30 <- ggplot(data = medias, aes(y)) +\n            geom_density(fill = \"dodgerblue4\",\n                         alpha = 0.5, colour = \"white\") +\n            theme_bw() +\n            labs(title = sprintf(\"Distribución muestreal con N = %d\",\n                                 N),\n                 x = element_blank(),\n                 y = element_blank()) +\n            theme(text = element_text(colour = \"gray40\"))\n\n#cairo_pdf(\"n_30.pdf\", family = \"Montserrat\",\n#           height = 5, width = 5*1.6, pointsize = 20)\ndist_n30\n\n\n\n#dev.off()\n\nPara comprobar, hagámos el ejercicio con una distribución uniforme; es decir, en la cual todos los valores tienen la misma probabilidad de ser obtenidos (desviaciones debido al generador de números “aleatorios”):\n\nN = 30\ndatos <- data.frame(x = 1:1000, y = runif(1000))\nunif <- ggplot(data = datos, aes(y)) + \n        geom_density(fill = rgb(118,78,144,\n                                maxColorValue = 255),\n                     alpha = 0.5, colour = \"white\") +\n        theme_bw() +\n        labs(title = \"Distribución \\\"uniforme\\\"\",\n             x = element_blank(),\n             y = element_blank()) +\n        theme(text = element_text(colour = \"gray40\"))\n\n#cairo_pdf(\"unif.pdf\", family = \"Montserrat\",\n#           height = 5, width = 5*1.6, pointsize = 20)\nunif\n\n\n\n#dev.off()\n\n\nmedias <- data.frame(x = 1:1000,\n                     y = replicate(1000,\n                                   mean(sample(datos$y, N))))\n\ndist_n30 <- ggplot(data = medias, aes(y)) +\n            geom_density(fill = \"dodgerblue4\",\n                         alpha = 0.5,\n                         colour = \"white\") +\n            theme_bw() +\n            labs(title = sprintf(\"Distribución muestreal con N = %d\",\n                                 N),\n                 x = element_blank(),\n                 y = element_blank()) +\n            theme(text = element_text(colour = \"gray40\"))\n\n#cairo_pdf(\"n_30u.pdf\", family = \"Montserrat\",\n#          height = 5, width = 5*1.6, pointsize = 20)\ndist_n30\n\n\n\n#dev.off()\n\nUn aspecto importante a considerar es la “Primera Propiedad Conocida” de la distribución normal: dadas muestras aleatorias e independientes con N observaciones cada una (tomadas de una distribución normal), la distribución de medias muestreales es normal e insesgada (i.e., centrada en la media poblacional), independientemente del tamaño de N. Por lo tanto, aún con un tamaño de muestra de 1 debería dar una distribución parecida a la normal. Comprobemos:\n\nN = 1\ndatos <- data.frame(x = 1:1000, y = rnorm(1000))\nnorm <- ggplot(data = datos, aes(y)) + \n        geom_density(fill = rgb(118,78,144,\n                                maxColorValue = 255),\n                     alpha = 0.5,\n                     colour = \"white\") +\n        theme_bw() +\n        labs(title = \"Distribución Normal\",\n             x = element_blank(),\n             y = element_blank()) +\n        theme(text = element_text(colour = \"gray40\"))\n\n#cairo_pdf(\"norm.pdf\", family = \"Montserrat\",\n#          height = 5, width = 5*1.6, pointsize = 20)\nnorm\n\n\n\n#dev.off()\n\n\nmedias <- data.frame(x = 1:1000,\n                     y = replicate(1000,\n                                   mean(sample(datos$y, N))))\n\ndist_n1 <- ggplot(data = medias, aes(y)) +\n           geom_density(fill = \"dodgerblue4\",\n                        alpha = 0.5, colour = \"white\") +\n           theme_bw() +\n           labs(title = sprintf(\"Distribución muestreal con N = %d\",\n                                N),\n                x = element_blank(),\n                y = element_blank()) +\n           theme(text = element_text(colour = \"gray40\"))\n\n#cairo_pdf(\"n_1.pdf\", family = \"Montserrat\",\n#          height = 5, width = 5*1.6, pointsize = 20)\ndist_n1\n\n\n\n#dev.off()\n\nLa implicación de esta propiedad es que entre menos “normal” (en términos de su distribución estadística) sea nuestra población de estudio, necesitaremos un mayor tamaño de muestra para que nuestra distribución muestral de la media sea normal. El problema surge cuando nos debemos de enfrentar a tamaños de muestra pequeños (n < 30).\nAunque siempre podemos asumir (literalmente) que nuestra población se encuentra normalmente distribuida y “capitalizar en la robustez del modelo estadístico subyacente”, abusando del TLC, o reconocer que tamaños de muestra más pequeños nos pueden acercar lo suficiente (n > 30 es para casos extremos) podemos hacerlo mejor. La tercera opción es la evaluación formal, la cual consiste en hacer una prueba de bondad de ajuste para conocer si nuestros datos se desvían o no de una distribución normal teórica. Antes de entrar a esos métodos, analicemos la prueba de bondad de ajuste más conocida: la prueba \\(\\chi^2\\) de independencia."
  },
  {
    "objectID": "c10_param.html#pruebas-de-bondad-de-ajuste",
    "href": "c10_param.html#pruebas-de-bondad-de-ajuste",
    "title": "10  Técnicas paramétricas",
    "section": "10.5 Pruebas de bondad de ajuste",
    "text": "10.5 Pruebas de bondad de ajuste\n\n10.5.1 \\(\\chi^2\\) de independencia\nEsta prueba nos permite probar si la distribución de nuestros datos (frecuencias de variables nominales) son iguales a una distribución teórica. El ejemplo más sencillo lo tenemos al evaluar si la distribución de sexos en una población es diferente de 1:1. En este caso, la distribución de nuestros datos es binomial (dos categorías, verdadero/falso, éxito/fracaso, macho/hembra, etc.). En nuestro muestreo contamos 142 machos y 190 hembras. Coloquemos esos datos en un objeto y realicemos la prueba:\n\nsexos <- c(machos = 142, hembras = 190)\nsex_chi <- chisq.test(sexos)\nsex_chi\n\n\n    Chi-squared test for given probabilities\n\ndata:  sexos\nX-squared = 6.9398, df = 1, p-value = 0.00843\n\n\nVeamos la distribución teórica gráficamente y veamos la ubicación del estadístico de prueba:\n\nchi_data <- data.frame(x = rchisq(1000, 1))\n\nchisq_plot <- ggplot(data = chi_data, aes(x)) +\n              geom_density(fill = rgb(118,78,144,\n                                      maxColorValue = 255),\n                           alpha = 0.5, colour = \"white\") +\n              geom_vline(xintercept = sex_chi$statistic,\n                         color = \"firebrick\") +\n              annotate(geom = \"text\",\n                       x = sex_chi$statistic+1.1, y = 1,\n                       label = sprintf(\"X^2 = %.2f\",\n                                       round(sex_chi$statistic, 2))) +\n              theme_bw() +\n              labs(title = sprintf(\"Distribución X^2 teórica (g.l = %d)\",\n                                   sex_chi$parameter),\n                   x = element_blank(),\n                   y = element_blank()) +\n             theme(text = element_text(colour = \"gray40\"))\n#cairo_pdf(\"chi_plot.pdf\", family = \"Montserrat\",\n#           height = 5, width = 5*1.6, pointsize = 20)\nchisq_plot\n\n\n\n#dev.off()\n\nPartiendo del valor de p podemos concluir que la proporción fue diferente de nuestro modelo teórico 1:1, pero ¿qué pasa si nos interesara comprobar si es diferente a otra proporción, por ejemplo 40% machos y 60% hembras? En ese caso únicamente debemos de proporcionar un vector p en el cual establezcamos la probabilidad correspondiente a cada grupo:\n\nchisq.test(sexos, p = c(0.4, 0.6))\n\n\n    Chi-squared test for given probabilities\n\ndata:  sexos\nX-squared = 1.0622, df = 1, p-value = 0.3027\n\n\nAquí nuestros datos no ridiculizan a nuestra hipótesis de nulidad, por lo que no podemos rechazarla. Un ejemplo más complejo es el de la presentación, en donde tratamos de probar si el proceso de vacunación hizo alguna diferencia en el estado de salud de los empleados o, en otras palabras, ¿la incidencia de pneumonía fue la misma, INDEPENDIENTEMENTE de si los empleados se vacunaron o no? Al igual que en el caso anterior, coloquemos los datos en un objeto:\n\nvacunas <- data.frame(no_vacuna = c(23, 8, 61),\n                      vacuna = c(5, 10, 77),\n                      row.names = c(\"neumococo\", \"otra_neumonia\",\n                                    \"sin_neumonia\"))\nvacunas\n\n\n\n  \n\n\n\nAhora apliquemos la prueba:\n\nvacs <- chisq.test(vacunas)\nvacs\n\n\n    Pearson's Chi-squared test\n\ndata:  vacunas\nX-squared = 13.649, df = 2, p-value = 0.001087\n\n\nComo era de esperarse al ver las frecuencias, la incidencia de pneumonía aparentemente no fue la misma entre los empleados vacunados y los que no se vacunaron. En este caso, podemos extraer aún más información, tal y como la dependencia entre las variables. Para ello accederemos al atributo residuals de la salida de chisq.test, el cual representa los residuales de Pearson para cada celda:\n\nvacs$residuals\n\n               no_vacuna     vacuna\nneumococo      2.4053512 -2.4053512\notra_neumonia -0.3333333  0.3333333\nsin_neumonia  -0.9630868  0.9630868\n\n\nValores positivos muestran una asociación positiva entre las variables correspondientes; es decir, la incidencia de neumonía por neumococo aumentó (signo positivo) en aquellos empleados que no fueron vacunados y viceversa, valores negativos muestran una asociación negativa; es decir, la incidencia disminuyó en aquellos que sí fueron vacunados. Si nuestro interés fuera saber qué tanto contribuyó cada celda al valor de \\(\\chi^2\\) podemos elevar cada residual al cuadrado y dividirlo entre el valor de \\(\\chi^2\\) observado, tal que:\n\ncontrib <- 100*((vacs$residuals^2)/vacs$statistic)\ncontrib\n\n              no_vacuna    vacuna\nneumococo     42.390150 42.390150\notra_neumonia  0.814077  0.814077\nsin_neumonia   6.795773  6.795773\n\n\nEvidentemente, los residuales más grandes tuvieron la mayor contribución que, en este caso, estuvo dada por la incidencia de neumonía por neumococo en ambos grupos. Podemos ver estos resultados de manera gráfica utilizando la librería corrplot:\n\ncorrplot::corrplot(contrib, is.corr = F)\n\n\n\n\nAhora que tenemos una idea sobre cómo funcionan las pruebas de bondad de ajuste, podemos regresar a hablar sobre las pruebas de normalidad.\n\n\n10.5.2 Supuesto de Normalidad\nComo imaginarás, las pruebas de normalidad son pruebas de bondad de ajuste en donde la distribución teórica es una distribución normal, aunque el modo en el cual se evalúan las desviaciones de la normalidad (i.e., las diferencias) es diferente para cada prueba. Para aplicarlas, utilizaremos la base de datos de muestras independientes del archivo datos_t, particularmente la columna DC:\n\ndc <- openxlsx::read.xlsx(\"datos/datos_t.xlsx\", sheet = 1)\ndc\n\n\n\n  \n\n\n\nPodemos hacer una primera valoración utilizando un gráfico de densidad con un gráfico de densidad normal teórico superpuesto:\n\nset.seed(0)\nnorm_plot <- ggplot(data = dc, aes(DC)) +\n             geom_density(fill = rgb(118,78,144,\n                                     maxColorValue = 255),\n                          colour = \"white\", alpha = 0.5) +\n             stat_function(fun = dnorm, n = 100,\n                           args = list(mean = mean(dc$DC),\n                                       sd = sd(dc$DC))) +\n             # Límites expandidos para visualizar el\n             # kde normal \"completo\".\n             # El kde observado se encuentra extendido más allá\n             # de los límites de los datos:\n             xlim(c(40, 65)) + \n             theme_bw() +\n             labs(title = \"Gráfico de densidad de DC vs. normal teórica\",\n                  x = element_blank(),\n                  y = element_blank()) +\n             theme(text = element_text(colour = \"gray40\"))\n\n#cairo_pdf(\"norm_plot.pdf\", height = 5, width = 5*1.6, pointsize = 20)\nnorm_plot\n\n\n\n#dev.off()\n\n¿Qué opinas? Apliquemos ahora las pruebas de normalidad:\n\n10.5.2.1 Prueba de Shapiro-Wilk\nLa prueba más conocida para evaluar la normalidad de un conjunto de datos es la prueba de Shapiro-Wilk. Su estadístico de prueba (W) se calcula de una manera poco amigable, pero conceptualmente implica ordenar los valores de la muestra y evaluar las desviaciones (diferencias) con respecto a la media, la varianza y su covarianza (este concepto se retoma más adelante) esperadas. En pocas palabras, la covarianza indica cuánto cambia una variable (la media) con respecto a otra (la varianza).\n¿Qué tiene que ver la covarianza con el Supuesto de Normalidad? Tiene que ver con la Segunda Propiedad Conocida de la Distribución Normal, la cual establece que Dadas observaciones aleatorias e independientes (de una distribución normal), la media muestral y la varianza muestral son independientes. En otras palabras, cuando tomas una muestra y la usas para estimar tanto la media como la varianza de la población, qué tanto puedes equivocarte sobre la media es independiente de qué tanto puedes equivocarte sobre la varianza. Esta es una característica única de la distribución normal y es una de las razones por la que la prueba de S-W es de las más (por no decir la más) utilizada y recomendada, especialmente para muestras pequeñas. En algunos estudios de simulación como este ha demostrado ser más sensible a las desviaciones de la normalidad que la prueba de Kolmogorov-Smirnov, aunque antes de explicarla apliquemos la prueba de S-W:\n\nshapiro.test(dc$DC)\n\n\n    Shapiro-Wilk normality test\n\ndata:  dc$DC\nW = 0.95125, p-value = 0.6833\n\n\nEl valor de p no nos permite rechazar nuestra hipótesis de nulidad a un \\(\\alpha = 0.05\\), por lo que podemos concluir que los datos se ajustan a una distribución normal. Vuelve al gráfico de densidad normal, ¿qué opinas?\nComo añadido, visualicemos la segunda propiedad conocida de la distribución normal:\n\nmeans <- NA\nsds <- NA\n\nfor (i in 1:1000) {\n  norm_data <- rnorm(10)\n  means[i] <- mean(norm_data)\n  sds[i] <- sd(norm_data)\n}\n\nmean_sd <- data.frame(mean = means, sd = sds)\n\nprop_2 <- ggplot(data = mean_sd, aes(x = mean, y = sd)) +\n          geom_point(color = \"dodgerblue4\", size = 2, alpha = 0.5) +\n          theme_bw() +\n          labs(title = \n                 \"Segunda Propiedad Conocida de la Distribución Normal\",\n               subtitle = \"1000 muestreos de una población normal\",\n               x = \"Media\",\n               y = \"Desviación Estándar\") +\n          theme(text = element_text(colour = \"gray40\"))\n#cairo_pdf(\"prop_2.pdf\", family = \"Montserrat\",\n#          height = 5, width = 5*1.6, pointsize = 20)\nprop_2\n\n\n\n#dev.off()\n\nCon una distribución Gamma:\n\nmeans <- NA\nsds <- NA\n\nfor (i in 1:1000) {\n  gamma_data <- rgamma(10, shape = 1)\n  means[i] <- mean(gamma_data)\n  sds[i] <- sd(gamma_data)\n}\n\nmean_sd <- data.frame(mean = means, sd = sds)\n\nprop_g <- ggplot(data = mean_sd, aes(x = mean, y = sd)) +\n          geom_point(color = \"dodgerblue4\", size = 2, alpha = 0.5) +\n          theme_bw() +\n          labs(title =\n                 \"Segunda Propiedad Conocida de la Distribución Normal\",\n               subtitle = \"1000 muestreos de una población gamma\",\n               x = \"Media\",\n               y = \"Desviación Estándar\") +\n          theme(text = element_text(colour = \"gray40\"))\n\n#cairo_pdf(\"prop_g.pdf\", family = \"Montserrat\",\n#           height = 5, width = 5*1.6, pointsize = 20)\nprop_g\n\n\n\n#dev.off()\n\nEjericio: Realiza el mismo gráfico para la columna DC y para la columna CH.\n\n\n10.5.2.2 Prueba Kolmogorov-Smirnov\nA diferencia de la prueba S-W, la prueba K-S compara las función de densidad acumulada empírica (observada) vs. una función de densidad acumulada teórica (no necesariamente normal), lo cual causa que sea sensible a desviaciones en el centro de la distribución pero no en las colas; sin embargo, es importante mencionar, que la prueba K-S es convergente; es decir, que conforme \\(N \\rightarrow \\infty\\) la prueba converge a la “respuesta verdadera” en términos de probabilidad. Esta razón hace que esta prueba no se recomiende para tamaños de muestra pequeños. Para aplicarla:\n\nks.test(dc$DC, \"pnorm\")\n\n\n    Exact one-sample Kolmogorov-Smirnov test\n\ndata:  dc$DC\nD = 1, p-value < 2.2e-16\nalternative hypothesis: two-sided\n\n\nA diferencia del caso anterior, esta prueba si tuvo evidencia suficiente para ridiculizar nuestra hipótesis nula, por lo que podemos concluir que nuestros datos no se ajustan a una distribución normal. Vuelve nuevamente al gráfico KDE. ¿Qué opinas?\nVeamos las densidades acumuladas:\n\n# Generamos una cdf normal teórica:\ncdf <- data.frame(norm = rnorm(1000,\n                               mean = mean(dc$DC),\n                               sd = sd(dc$DC)))\n\n# Graficamos una vs. la otra:\ncdfplot <- ggplot(data = dc, aes(DC)) +\n           stat_ecdf(geom = \"step\",\n                     colour = rgb(118, 78, 144,\n                                  maxColorValue = 255),\n                     alpha = 1) +\n           stat_ecdf(data = cdf, aes(norm),\n                     geom = \"line\", colour = \"black\") +\n           theme_bw() +\n           labs(title = \"Densidades acum. empírica y teórica para DC\",\n                x = element_blank(),\n                y = element_blank())\n\n#cairo_pdf(\"cdf.pdf\", family = \"Montserrat\",\n#          height = 5, width = 5*1.6, pointsize = 20)\ncdfplot\n\n\n\n#dev.off()\n\nConjuntando con el gráfico kde original podemos ver por qué la prueba K-S arrojó un resultado significativo, ya que hubo desviaciones importantes en la zona central. Interpretar correctamente un gráfico CDF NO es sencillo y requiere de experiencia, por lo que únicamente lo incluí para acompañar a la prueba que se basa en la densidad acumulada.\nHabiendo explicado dos de las pruebas de normalidad más comunes, pasemos a los análisis paramétricos. El primero de ellos lo revisamos durante la clase de pruebas de hipótesis: la prueba t de Student, por lo que pasaremos directamente al Análisis de la Varianza."
  },
  {
    "objectID": "c10_param.html#análisis-de-la-varianza",
    "href": "c10_param.html#análisis-de-la-varianza",
    "title": "10  Técnicas paramétricas",
    "section": "10.6 Análisis de la Varianza",
    "text": "10.6 Análisis de la Varianza\nEn términos simples, podemos pensar en el ANOVA como una extensión de la prueba t-Student a más de dos grupos a comparar. Durante la clase de Comparaciones Multivariadas abordamos el riesgo que conlleva realizar múltiples pruebas de hipótesis (comparaciones) en nuestros datos; es decir, el problema de realizar dos o más comparaciones entre grupos como si se tratara de pruebas independientes. Por el momento, solo ten en mente que se incrementa la posibilidad de obtener un falso positivo únicamente por azar, por lo que hay que utilizar una técnica adecuada y es ahí donde entra el ANOVA o, mejor dicho, los ANOVAs. Como te imaginarás, estas pruebas nos permiten comparar medias entre más de dos grupos, aunque aquí la comparación se realiza de manera global y la hipótesis alternativa se expresa como “Al menos una de las medias es diferente”. Esto quiere decir que el ANOVA no nos dirá entre qué par(es) de grupos se encontraron las diferencias, sino que habrá que acompañarlo de una prueba post-hoc. Esta prueba es la prueba de diferencias honestas (HSD) de Tukey, la cual se encuentra basada en la distribución de los rangos estudentizados y fue diseñada para no incrementar la probabilidad de falsos positivos al realizar múltiples comparaciones. En esta sesión revisaremos tres modaliades de ANOVA: de una vía, de dos vías y factorial, de menor a mayor complejidad, aunque estos no son los únicos. Entre los demás diseños de ANOVA se encuentran el ANOVA de medidas repetidas (estudios de crecimiento en laboratorio con medidas intermedias entre el inicio y el final, por ejemplo) o el ANOVA anidado, en el cual el diseño es similar a una muñeca rusa.\nAntes de aplicar y explicar los modelos de ANOVA, es necesario desarrollar una intuición sobre el procedimiento. El nombre “Análisis de Varianza” viene de que, literalmente, se utilizan las varianzas para comparar las medias. Aunque el proceso matemático implica calcular promedios de promedios, varias sumas de cuadrados y cuadrados medios del error, podemos resumirlo para fines prácticos en que la comparación se realiza mediante una razón/cociente, tal que:\n\\[F = \\frac{\\sigma^2_{entre}}{\\sigma^2_{dentro}}\\]\nSé que esto puede sonar muy poco intuitivo, pero si nos detenemos un poco a analizar la ecuación podemos darle mucho sentido. La varianza dentro de los grupos podemos considerarla como la varianza “promedio” de cada grupo (razón por la que es importante que estas sean homogéneas entre todos nuestros grupos), mientras que la varianza entre los grupos representa la “separación” (dispersión) entre los grupos (sin considerar el error). Partiendo de esto, es evidente que si la varianza entre grupos es muy grande en relación a la varianza dentro de los grupos podemos inferir que existe un efecto del factor de agrupamiento pues “no hay” (ojo a las comillas y los supuestos) otra forma de que las distribuciones de los grupos se desplacen.\nGráficamente la varianza dentro de los grupos se representaría de la siguiente manera:\n\nanov_sim <- data.frame(grupo = as.factor(c(rep(\"A\", 1000),\n                                           rep(\"B\", 1000))),\n                       y = c(rnorm(1000, mean = 10, sd = 1),\n                             rnorm(1000, mean = 20, sd = 1)))\n\ndentro_plot <- ggplot(data = anov_sim,\n                      aes(y, fill = grupo, alpha = 0.5)) +\n               geom_density(trim = T, show.legend = F,\n                            colour = \"white\") +\n               theme_minimal() +\n               labs(title = \"Varianza dentro de los grupos\",\n                    x = element_blank(),\n                    y = element_blank()) +\n               scale_y_continuous(labels = NULL) +\n               xlim(c(5, 25))\ndentro_plot\n\n\n\n\nMientras que la varianza entre los grupos podemos, para fines de interpretación, visualizarla como la varianza dada por ambos grupos. En realidad esto representaría la varianza total y la varianza entre los grupos es el resultado de eliminar la varianza dada por el error, pero sigamos con el ejemplo:\n\nanov_sim$tot <- rnorm(200, mean = 15, sd = sd(anov_sim$y))\nentre_plot <- ggplot(data = anov_sim, aes(tot)) + \n              geom_density(fill = \"dodgerblue4\",\n                           alpha = 0.5, colour = \"white\") +\n              theme_minimal() +\n              labs(title = \"Varianza entre los grupos\",\n                   x = element_blank(),\n                   y = element_blank()) +\n              scale_y_continuous(labels = NULL) +\n              xlim(c(5, 25))\nentre_plot\n\nWarning: Removed 90 rows containing non-finite values (stat_density).\n\n\n\n\n\nVisualizándolas como si de un cociente se tratara es posible darse cuenta cómo la varianza “entre” los grupos es mucho mayor que la varianza dentro de los grupos, lo cual daría un valor de la razón de varianzas muy alto, sugiriendo un efecto del factor de agrupamiento.\n\n#cairo_pdf(\"anova_plot.pdf\", family = \"Montserrat\",\n#           height = 5, width = 5*1.6, pointsize = 20)\ngridExtra::grid.arrange(entre_plot, dentro_plot)\n\nWarning: Removed 90 rows containing non-finite values (stat_density).\n\n\n\n\n#dev.off()\n\nVeamos qué pasa cuando las medias son más cercanas entre sí:\n\nanov_sim2 <- data.frame(grupo = as.factor(c(rep(\"A\", 1000),\n                                            rep(\"B\", 1000))),\n                        y = c(rnorm(1000, mean = 10, sd = 1),\n                              rnorm(1000, mean = 11, sd = 1)))\nanov_sim2$tot <- rnorm(2000, mean(10.5), sd(anov_sim2$y))\ndentro_plot2 <- ggplot(data = anov_sim2,\n                       aes(y, fill = grupo, alpha = 0.5)) +\n               geom_density(trim = T, show.legend = F,\n                            colour = \"white\") +\n               theme_minimal() +\n               labs(title = \"Varianza dentro de los grupos\",\n                    x = element_blank(),\n                    y = element_blank()) +\n               scale_y_continuous(labels = NULL) +\n               xlim(c(5, 15))\nentre_plot2 <- ggplot(data = anov_sim2, aes(tot)) + \n              geom_density(fill = \"dodgerblue4\",\n                           alpha = 0.5, colour = \"white\") +\n              theme_minimal() +\n              labs(title = \"Varianza entre los grupos\",\n                   x = element_blank(),\n                   y = element_blank()) +\n              scale_y_continuous(labels = NULL) +\n              xlim(c(5, 15))\n#cairo_pdf(\"anova_plot2.pdf\", family = \"Montserrat\",\n#           height = 5, width = 5*1.6, pointsize = 20)\ngridExtra::grid.arrange(entre_plot2, dentro_plot2)\n\n\n\n#dev.off()\n\n\n10.6.1 Supuesto de homogeneidad de Varianzas\nComo podrás imaginar, el que las varianzas de los grupos no sean homogéneas generará un sesgo al momento de calcular el cociente y, en consecuencia, el nivel de significancia de la prueba. Esto es lo que da origen al Supuesto de Homogeneidad de Varianzas. Existe una gran diversidad de pruebas, cada una con sus consideraciones, fortalezas y desventajas, pero analizaremos únicamente las (posiblemente) más conocidas.\n\n10.6.1.1 Prueba de Bartlett\nLa prueba de Bartlett se considera como la prueba Uniformemente Más Poderosa; es decir, la que es menos propensa a cometer un falso negativo para cualquier valor de \\(\\alpha\\). Este poder, sin embargo, tiene sus bemoles o su bemol, mejor dicho. Esta prueba se apoya TOTALMENTE en que la variable de interés en cada factor se encuentra normalmente distribuída (¡Hola de nuevo, Supuesto de Normalidad!). De violarse este supuesto el valor de \\(\\alpha_v\\) (verdadero) para la prueba puede ser mayor o menor al definido por nosotros (\\(\\alpha_n\\), nominal). De manera particular, si la distribución de la variable analizada presenta una curtosis negativa el \\(\\alpha_v\\) será menor al nominal, mientras que con una curtosis positiva será el caso contrario. Esto lleva a que hagamos una prueba más o menos estricta de lo que habíamos planeado originalmente y que nuestros resultados no sean confiables. De cualquier manera, veamos cómo aplicarla:\n\nbartlett.test(y~grupo, data = anov_sim)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  y by grupo\nBartlett's K-squared = 0.4309, df = 1, p-value = 0.5115\n\n\nEn este caso, no podemos ridiculizar nuestra hipótesis de nulidad, por lo que podemos concluir que las varianzas entre nuestros grupos son homogéneas (y deben serlo, pues así las especificamos).\n\n\n10.6.1.2 Prueba de Levene\nEs la alternativa recomendada por muchos a la prueba de Bartlett. Aunque no es tan poderosa, sí es robusta a las violaciones al supuesto de normalidad, de modo que el \\(\\alpha\\) verdadero es muy similar al nominal para una gran cantidad de distribuciones, aunque es insensible a distribuciones simétricas con colas altas como la t de Student o doble exponencial (también conocida como distribución de Laplace). Aplicarla también es sumamente sencillo:\n\ncar::leveneTest(y~grupo, data = anov_sim)\n\n\n\n  \n\n\n\nComo era de esperarse, el resultado es consistente con la prueba de Bartlett para este caso.\n\n\n\n10.6.2 ANOVA de una sola vía\nHabiendo revisado los conceptos básicos detrás del ANOVA, podemos pasar a aplicar algunos modelos. El más sencillo es el ANOVA de una sola vía, el cual es el caso más sencillo; es decir, comparamos una sola variable numérica entre los niveles de un solo factor (pesos finales para tres alimentos distintos, por ejemplo). Para ejemplificarlo utilizaremos la base datos1 que se trabajó para la tarea de Intervalos de confianza, con una columna extra: id, el cual es un identificador para cada individuo. Esta columna fue añadida únicamente para ejemplificar un caso de ANOVA posterior. En este ejemplo, compararemos los pesos totales entre los tres periodos (OJO: este es un diseño para un ANOVA factorial, únicamente lo utilizaremos como ejemplo).\nEl primer paso es, evidentemente, cargar la base de datos:\n\ndf <- read.table(\"datos/Datos1 2.csv\", header = F, skip = 1, sep = \",\")\ncolnames(df) <- c(\"Dieta\", \"Periodo\", \"Rep\", \"LT\", \"PT\", \"id\")\ndf$Periodo <- factor(df$Periodo, levels = c(\"I\", \"M\", \"F\"))\nhead(df)\n\n\n\n  \n\n\n\n\n10.6.2.1 Comprobación de supuestos\nEl segundo paso es la comprobación de supuestos. Primero el de Normalidad. Tenemos varios grupos, por lo que las pruebas de normalidad son para cada grupo.\n\n#Normalidad\n## Data.frame a llenar\nnorm <- data.frame(grupo = NA, W = NA, p = NA)\n\n## Niveles a probar:\nlvls <- levels(df$Periodo)\n\nfor (i in seq_along(lvls)) {\n  temp <- shapiro.test(df$PT[df$Periodo == lvls[i]])\n  norm[i,] <- c(lvls[i], temp$statistic, temp$p.value)\n}\nnorm\n\n\n\n  \n\n\n\nLa prueba de S-W sugiere que no hay desviaciones significativas de la normalidad. Corroboremos con un gráfico de violín. Al parecer, los resultados son coherentes con la distribución de los datos.\n\nggplot(data = df, aes(x = Periodo, y = PT, fill = Periodo)) +\n  geom_violin(alpha = 0.5, show.legend = F) +\n  labs(title = \"Distribución de PT en los tres momentos de medición\",\n       x = element_blank(),\n       y = element_blank()) +\n  theme_bw()\n\nWarning: Removed 22 rows containing non-finite values (stat_ydensity).\n\n\n\n\n\nAhora el supuesto de igualdad de varianzas, utilizando la prueba de Levene. Podemos concluir que las varianzas no son homogéneas, por lo que la recomendación sería recurrir a una prueba no paramétrica; sin embargo, sigamos con el ejercicio y escalando la complejidad del análisis antes de saltar apresuradamente a conclusiones.\n\ncar::leveneTest(PT~Periodo, data = df)\n\n\n\n  \n\n\n\n\n\n10.6.2.2 Aplicación del ANOVA\nEl siguiente paso es aplicar el ANOVA. El valor de p es bastante bajo, lo cual ridiculiza nuestra hipótesis de nulidad y concluimos que al menos un par de medias son significativamente diferentes entre sí (F(2, 155) = 574.3; p < 0.0001).\n\nuna_via <- aov(PT~Periodo, data = df)\nsummary(una_via)\n\n             Df Sum Sq Mean Sq F value Pr(>F)    \nPeriodo       2 10.600   5.300   574.3 <2e-16 ***\nResiduals   155  1.431   0.009                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n22 observations deleted due to missingness\n\n\nTe has de preguntar, ¿las medias de qué grupos son diferentes? Eso no lo podemos saber con un ANOVA, sino que hay que aplicar una prueba post-hoc: una prueba que evalúe las diferencias entre cada par de grupos.\n\n\n10.6.2.3 Prueba post-hoc\nEl último paso es aplicar la prueba post-hoc. Esta prueba se construye a partir de la distribución de rangos estudentizados, y fue diseñada para evitar el conflicto entre el \\(\\alpha\\) y el número de comparaciones, por lo que la interpretación del valor de p es directa (sin correcciones).\n\n\n\n\n\n\nImportante\n\n\n\n¿Por qué no aplicar una prueba \\(t\\) para cada par de grupos? Porque en ese caso inflaríamos nuestro \\(\\alpha\\). Hablaremos más de esta relación y algunas correcciones al valor de p en el Capítulo 16. Por el momento quédate con que cada prueba para comparar múltiples grupos (tipo ANOVA) tiene su prueba post-hoc que permite realizar comparaciones pareadas.\n\n\nEn este caso, el valor de p fue muy pequeño para las tres comparaciones, por lo que rechazamos nuestra hipótesis de nulidad en los tres casos. El resto de la tabla es también informativo, pues nos indica la magnitud de las diferencias y sus intervalos de confianza (tal y como en la prueba t de Student):\n\nTukeyHSD(una_via)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = PT ~ Periodo, data = df)\n\n$Periodo\n         diff       lwr       upr p adj\nM-I 0.2949615 0.2518882 0.3380349     0\nF-I 0.6376957 0.5931429 0.6822484     0\nF-M 0.3427341 0.2967181 0.3887501     0\n\n\nCon esos 4 pasos terminamos nuestro ANOVA de una vía. Pasemos entonces al ANOVA de dos vías.\n\n\n\n10.6.3 ANOVA de dos vías\nSi una vía es a un factor, dos vías es a dos factores. En este análisis compararemos el efecto de ambos factores simultáneamente, pero de manera independiente; es decir, aunque se hará la comparación para ambos, no se considerará la interacción entre ellos.\n\n\n\n\n\n\nNota\n\n\n\n¿Qué es la interacción? Eso lo veremos más adelante, pero se resume a que el efecto de un factor dependa del efecto de otro.\n\n\nNuestro segundo factor será la Dieta. Los pasos son exactamente los mismos que en el anterior:\n\n10.6.3.1 Comprobación de Supuestos\nDado que ya comprobamos los supuestos para el factor Periodo, solo habrá que hacerlo para el factor Dieta:\n\n#Normalidad\ndf$Dieta <- factor(df$Dieta, levels = c(\"A\", \"B\", \"C\"))\n## Data.frame a llenar\nnorm <- data.frame(grupo = NA, W = NA, p = NA)\n\n## Niveles a probar:\nlvls <- levels(df$Dieta)\n\nfor (i in seq_along(lvls)) {\n  temp <- shapiro.test(df$PT[df$Dieta == lvls[i]])\n  norm[i,] <- c(lvls[i], temp$statistic, temp$p.value)\n}\nnorm\n\n\n\n  \n\n\n\nDebido a que el factor dieta incluye el efecto del periodo de medición y detectamos diferencias entre ellos, es esperable que no se cumpla el supuesto de normalidad. En este caso, el diseño sería mejor analizado utilizando un ANOVA factorial que uno de dos vías pero, al igual que en el caso anterior, seguiremos únicamente para fines ilustrativos.\nPara la homogeneidad de varianzas la interpretación es la misma, aunque la consecuencia es la contraria. No violamos el supuesto de homogeneidad de varianzas debido a que tampoco se violó entre los periodos. Esto da un poco más de respaldo a seguir con el análisis, pues es más robusto a la violación del supuesto de normalidad que al de homogeneidad de varianzas.\n\ncar::leveneTest(PT~Dieta, data = df)\n\n\n\n  \n\n\n\n\n\n10.6.3.2 Aplicación del ANOVA.\nEl ANOVA de dos vías es un caso especial del ANOVA factorial, en el cuál únicamente hay dos factores y NO se considera su interacción, por lo que el modo de declararlo es una fórmula en la cuál los factores se consideran de manera aditiva: Respuesta~Factor1+Factor2. La forma tradicional de reportar los resultados de este ANOVA sería: hubo un efecto significativo de las dietas (F(2, 153) = 11.45; p < 0.0001) y de los periodos (F(2, 153) = 560.42; p < 0.0001).\n\ndos_vias <- aov(PT~Dieta+Periodo, data = df)\nsummary(dos_vias)\n\n             Df Sum Sq Mean Sq F value   Pr(>F)    \nDieta         2  0.212   0.106   11.45 2.33e-05 ***\nPeriodo       2 10.399   5.199  560.42  < 2e-16 ***\nResiduals   153  1.419   0.009                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n22 observations deleted due to missingness\n\n\n\n\n10.6.3.3 Prueba post-hoc.\nEn este caso tuvimos valores de p muy pequeños para ambos factores, realicemos la prueba HSD de Tukey. Al ver la salida puedes interpretar que esta es una lista, y que podríamos acceder a los resultados de cualquier factor utilizando el operador $ (TukeyHSD(aov_obj)$factor). Aquí, las diferencias se encontraron entre la dieta C y las otras dos, pero no entre A y B.\n\nTukeyHSD(dos_vias)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = PT ~ Dieta + Periodo, data = df)\n\n$Dieta\n           diff         lwr         upr     p adj\nB-A -0.02402094 -0.06672932  0.01868745 0.3801380\nC-A -0.09036834 -0.13594353 -0.04479315 0.0000176\nC-B -0.06634740 -0.11227232 -0.02042249 0.0023101\n\n$Periodo\n         diff       lwr       upr p adj\nM-I 0.2912145 0.2480228 0.3344062     0\nF-I 0.6266462 0.5819709 0.6713214     0\nF-M 0.3354316 0.2892891 0.3815741     0\n\n\nConsiderando el diseño factorial de la base de datos, ¿cómo interpretarías estos resultados? ¿podemos confiar en ellos? La respuesta que yo esperaría es que no, pues si el experimento fue bien diseñado, al inicio todos los animales debían tener aproximadamente las mismas características y vimos tanto gráficamente como en ambos ANOVAs que hubo un crecimiento. Veamos qué pasa con las distribuciones utilizando un gráfico de interacción.\n\nggplot(data = df, aes(x = Dieta, y = PT, fill = Periodo)) +\n  geom_violin(alpha = 0.5, show.legend = T) +\n  labs(title = \"Distribución de PT en los tres momentos de medición\",\n       x = element_blank(),\n       y = element_blank()) +\n  theme_bw()\n\nWarning: Removed 22 rows containing non-finite values (stat_ydensity).\n\n\n\n\n\nEs evidente que en los tres tratamientos hubo un crecimiento, el cual además parece haber sido bastante similar. Este es un ejemplo del error de tipo III que mencionaba en la clase de pruebas de hipótesis: utilizar la matemática correcta para responder la pregunta equivocada. Veamos qué pasa si realizamos un ANOVA factorial.\n\n\n\n10.6.4 ANOVA factorial\nComo te podrás imaginar a partir de lo mencionado sobre el ANOVA de dos vías, este ANOVA es la versión más generalizada en la cual podemos utilzar más de dos factores y analizar su interacción. Sigamos con la base anterior, en este caso considerando también el factor réplica:\n\ndf$Rep <- factor(df$Rep, levels = c(\"A\", \"B\"))\n\n\n10.6.4.1 Comprobación de supuestos\nNo hay sorpresas en ninguno de los dos casos, las interpretaciones de los resultados son las mismas que en el caso anterior; es decir, este NO es el modo correcto de comprobar la normalidad. Cuando hablemos del ANOVA de medidas repetidas veremos un ejemplo de cómo hacerlo de manera correcta (normalidad de un factor dados los niveles del otro factor).\n\n## Data.frame a llenar\nnorm <- data.frame(grupo = NA, W = NA, p = NA)\n\n## Niveles a probar:\nlvls <- levels(df$Rep)\n\nfor (i in seq_along(lvls)) {\n  temp <- shapiro.test(df$PT[df$Rep == lvls[i]])\n  norm[i,] <- c(lvls[i], temp$statistic, temp$p.value)\n}\nnorm\n\n\n\n  \n\n\n\n\ncar::leveneTest(PT~Rep, data = df)\n\n\n\n  \n\n\n\n\n\n10.6.4.2 Aplicación del ANOVA\nLa única diferencia con el caso anterior es que esta vez utilizaremos el operador * para añadir los nuevos términos, en vez de hacerlo de forma aditiva. Haciendo esto la tabla del ANOVA cambia, en donde primero aparece el efecto de cada factor analizado de manera independiete (como si hubieramos hecho un ANOVA de “tres vías”) y después los términos de interacción. La interacción entre dos factores representa un efecto combinado de los factores involucrados en la variable analizada; es decir, cuando hay interacción entre dos factores el efecto de uno “depende” del el nivel del otro.\n\nfact <- aov(PT~Dieta*Periodo*Rep, data = df)\nsummary(fact)\n\n                   Df Sum Sq Mean Sq F value   Pr(>F)    \nDieta               2  0.212   0.106  12.185 1.32e-05 ***\nPeriodo             2 10.399   5.199 596.509  < 2e-16 ***\nRep                 1  0.060   0.060   6.874  0.00971 ** \nDieta:Periodo       4  0.053   0.013   1.522  0.19912    \nDieta:Rep           2  0.001   0.000   0.038  0.96240    \nPeriodo:Rep         2  0.048   0.024   2.755  0.06703 .  \nDieta:Periodo:Rep   4  0.038   0.009   1.076  0.37066    \nResiduals         140  1.220   0.009                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n22 observations deleted due to missingness\n\n\n\n\n10.6.4.3 Prueba post-hoc\nEn este caso el único término de interacción con resultados significativos es la interacción entre Periodo y Réplica (Periodo:Rep), lo cual indica que el comportamiento de los periodos fue diferente entre réplicas. Realicemos las pruebas post-hoc correspondientes. Aunque encontramos un efecto significativo de las réplicas, este factor únicamente tiene dos niveles, por lo que realizar la prueba post-hoc es ocioso y, por tanto, la realizaremos únicamente para Periodo:Rep. Nota que debido a la presencia del operador : en el nombre del término es necesario utilizar comillas para poder acceder a ese atributo:\n\nTukeyHSD(fact)$\"Periodo:Rep\"\n\n               diff         lwr         upr        p adj\nM:A-I:A  0.28301798  0.20959477  0.35644118 4.152234e-14\nF:A-I:A  0.66044327  0.58556599  0.73532054 1.265654e-14\nI:B-I:A -0.02480111 -0.09480740  0.04520517 9.092990e-01\nM:B-I:A  0.27169429  0.19681702  0.34657157 6.084022e-14\nF:B-I:A  0.55653205  0.47803934  0.63502476 1.265654e-14\nF:A-M:A  0.37742529  0.30254802  0.45230257 1.265654e-14\nI:B-M:A -0.30781909 -0.37782537 -0.23781281 1.376677e-14\nM:B-M:A -0.01132368 -0.08620096  0.06355359 9.979453e-01\nF:B-M:A  0.27351408  0.19502137  0.35200679 6.727952e-14\nI:B-F:A -0.68524438 -0.75677422 -0.61371454 1.265654e-14\nM:B-F:A -0.38874898 -0.46505261 -0.31244534 1.265654e-14\nF:B-F:A -0.10391121 -0.18376573 -0.02405669 3.335386e-03\nM:B-I:B  0.29649541  0.22496556  0.36802525 1.909584e-14\nF:B-I:B  0.58133317  0.50602701  0.65663933 1.265654e-14\nF:B-M:B  0.28483776  0.20498324  0.36469228 6.306067e-14\n\n\nVemos que prácticamente todos los contrastes fueron significativos, con excepción del periodo inicial (p = 0.9). Esto sugeriría que el comportamiento de las réplicas no fue homogéneo a través del tiempo. Si regresamos brevemente a la tabla del ANOVA veremos que hubo 22 observaciones faltantes, las cuales corresponden a la mortalidad durante el experimento y podrían también explicar estos cambios. Debido a la impraciticidad/imposibilidad de marcar o identificar cada guppy no es posible aplicar un anova de medidas repetidas con estos datos; sin embargo, podemos ejemplificarlo con otros datos.\n\n\n\n10.6.5 ANOVA de medidas repetidas\nEl ANOVA de medidas repetidas es otro de los modelos de ANOVA, el cual podemos considerar como una extensión de la prueba t para muestras dependientes; es decir, en la cual los mismos individuos fueron medidos en más de dos ocasiones, denominado ANOVA de medidas repetidas de una vía. Si tenemos no solo los distintos tiempos de medición sino también factores adicionales entonces tendremos ANOVAs de medidas repetidas de dos vías (tiempo y un factor adicional) o de tres vías (tiempo y dos factores adicionales). Al igual que en el ANOVA “normal” comencemos desde abajo con el de una vía.\n\n10.6.5.1 ANOVA de medidas repetidas de una vía\nCarguemos los datos de ejemplo (selfesteem de la librería datarium), los cuales son una medida de autoestima medida en tres ocasiones distintas:\n\nselfesteem <- datarium::selfesteem\nhead(selfesteem)\n\n\n\n  \n\n\n\nLa base se encuentra en formato corto, por lo que habrá que pasarla a formato largo:\n\nestima <- reshape2::melt(selfesteem, # Datos a modificar\n                         # Identificador para cada individuo\n                         id.vars = \"id\",\n                         # Variables en columnas\n                         measure.vars = c(\"t1\", \"t2\", \"t3\"),\n                         # Nombre de la nueva variable de agrupamiento\n                         variable.name = \"tiempo\",\n                         # Nombre de la nueva variable medida\n                         value.name = \"estima\")\nhead(estima)\n\n\n\n  \n\n\n\n\n10.6.5.1.1 Comprobación de supuestos\nApliquemos entonces la prueba de Shapiro-Wilk a los datos agrupados, esta vez simplificando con tidy:\n\n# Toma el data.frame, agrúpalo por tiempo y para cada nivel\n# aplica la función shapiro_test a la columna estima:\nestima |>  group_by(tiempo) |>  shapiro_test(estima)\n\n\n\n  \n\n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nNota que la función es shapiro_test, la cual está contenida en rstatix, y no shapiro.test de R base. Si tratas de pasar shapiro.test te encontrarás con un error pues no está pensada para ser utilizada con pipes.\n\n\nAhora la homocedasticidad o, mejor dicho, el supuesto de esfericidad. Este supuesto es una “extensión” del supuesto de homogeneidad de varianzas. Definimos esfericidad como la condición en la que las varianzas de las diferencias entre todas las combinaciones de los niveles de interés son iguales. La violación de este supuesto conlleva un incremento en la probabilidad de un falso positivo; es decir, vuelve a la prueba demasiado “liberal” o “crédula”. Aunque este supuesto es sumamente importante, no necesitamos probarlo directamente, pues la función con la que implementaremos el análisis hace la prueba correspondiente (prueba de Mauchly para esfericidad) y, además, aplica una corrección (corrección de Greenhouse-Geisser) a los grados de libertad de aquellos factores que violen el supuesto.\n\n\n10.6.5.1.2 Aplicación del ANOVA\nEsta vez no utilizaremos la notación de fórmula ni tan siquiera la función aov, sino que recurriremos a la función anova_test() de la librería rstatix para hacer más intuitiva la declaración, donde data es el data.frame con los datos (dah!), dv es la variable dependiente; es decir, nuestra variable a comparar, wid es un identificador único para cada individuo y within el factor dentro del cual queremos hacer las comparaciones:\n\nanova_rep1 <- rstatix::anova_test(data = estima,\n                                  dv = estima, wid = id,\n                                  within = tiempo)\nget_anova_table(anova_rep1)\n\n\n\n  \n\n\n\nLo primero que llama la atención es que la tabla de ANOVA reporta resultados para una prueba de tipo III. OJO! esto no tiene nada que ver con el error tipo III que mencioné en la clase de pruebas de hipótesis (ese error no es formal). El tipo de prueba hace referencia al tipo de ANOVA que se está realizando o, mejor dicho, al modo en el que se calculan las sumas de cuadrados. Si te interesa leer más al respecto, visita este o este enlace.\nOtra cosa que debe llamar tu atención es el término ges. Este es el factor de corrección de Greenhouse-Geisser a los grados de libertad. El modo de reportar estos resultados sería algo como “las medidas de autoestima a través del tiempo fueron significativamente diferentes (\\(F_{2,18}\\) = 55.5, p < 0.0001; \\(\\eta^2\\) generalizado = 0.82)”. El término \\(\\eta^2\\) generalizado lo puedes encontrar también como \\(\\hat{\\epsilon}\\).\n\n\n10.6.5.1.3 Prueba post-hoc\nDebido a que las medidas son repetidas no podemos aplicar la prueba de diferencias honestas de Tukey, pero sí podemos aplicar pruebas t de Student pareadas y corregir el valor de p con una corrección de Bonferroni. En la sección de multivariado se abordará esta corrección, pero entiéndela en este momento como el modo de evitar que incrementemos la probabilidad de un falso positivo y, en consecuencia, deberemos de interpretar los valores de la columna p.adj. Viendo la tabla, es posible concluir que hubo diferencias entre las medidas de autoestima en los tres periodos.\n\npwt <- pairwise_t_test(data = estima,\n                       estima~tiempo, paired = T,\n                       p.adjust.method = \"bonferroni\")\npwt\n\n\n\n  \n\n\n\n\n\n\n10.6.5.2 ANOVA de medidas repetidas de dos vías\nAl igual que en el ANOVA “normal”, hablamos de dos vías cuando tenemos dos factores, en este caso son el tiempo y alguno adicional. Para ejemplificarlo utilizaremos la base de datos selfesteem2 de datarium.\n\nselfesteem2 <- datarium::selfesteem2\nestima2 <- reshape2::melt(selfesteem2, # Datos a modificar\n                         # Identificadores para cada individuo\n                         id.vars = c(\"id\", \"treatment\"),\n                         # Variables en columnas\n                         measure.vars = c(\"t1\", \"t2\", \"t3\"),\n                         # Nombre de la nueva variable de agrupamiento\n                         variable.name = \"tiempo\",\n                         # Nombre de la nueva variable medida\n                         value.name = \"estima\")\nhead(estima2)\n\n\n\n  \n\n\n\n\n10.6.5.2.1 Comprobación de supuestos\nAl igual que en el ANOVA de medidas repetidas, únicamente comprobaremos el Supuesto de Normalidad, solo que aquí lo haremos considerando la “anidación” de los factores; es decir, que las medidas repetidas fueron para cada tratamiento:\n\nestima2 |> group_by(treatment, tiempo) |> shapiro_test(estima)\n\n\n\n  \n\n\n\nAparentemente hubo desviaciones de la normalidad en el t1 para el grupo control. Podemos corroborarlo con un gráfico QQ. Aparentemente es culpa de de algunos puntos ligeramente fuera del resto de la tendencia ubicados en el centro. Recordemos que el ANOVA es robusto a ciertas violaciones de la normalidad, y en este caso no parecen ser especialmente serias. Sigamos con el análisis.\n\nggpubr::ggqqplot(estima2, \"estima\", ggtheme = theme_bw()) +\n  facet_grid(tiempo~treatment, labeller = \"label_both\")\n\n\n\n\n\n\n10.6.5.2.2 Aplicación del ANOVA\nUtilizaremos la misma estructura que en el caso anterior; la unica diferencia es que al argumento within le pasaremos un vector con dos factores:\n\nanova_rep2 <- anova_test(data = estima2,\n                         dv = estima, wid = id,\n                         within = c(treatment, tiempo))\nget_anova_table(anova_rep2)\n\n\n\n  \n\n\n\nDe la tabla podemos concluir que todos los contrastes fueron significativos, lo cual indica que hubo diferencias entre los tratamientos (F(1, 11), = 15.5; p = 0.02), entre los tiempos (F(1.31, 14.37), = 27.4; p < 0.0001) y tambien un efecto combinado (F(2,22) = 30.4; p < 0.0001). Debido a que los efectos principales (“solos”) no son suficientes para describir los datos, el proceso post-hoc es un poco más complicado que en el caso anterior.\n\n\n10.6.5.2.3 Pruebas post-hoc\nDebido a la significancia del término de interacción es necesario descomponerlo en:\n\nEfecto principal simple; es decir, un modelo de una vía de la primera variable para cada nivel de la segunda. Debido a que hacerlo a mano es un poco tedioso, encadenemos el proceso:\n\n\nanova_rep2_post1 <- estima2 |> \n                    # Agrupa la base por cada nivel de tiempo\n                    group_by(tiempo) |>\n                    # ANOVA de medidas repetidas para cada nivel\n                    anova_test(dv = estima, wid = id, \n                               within = treatment) |>\n                    # Extrae los resultados\n                    get_anova_table() |> \n                    # Ajusta los valores de p\n                    adjust_pvalue(method = \"bonferroni\")\n\nanova_rep2_post1\n\n\n\n  \n\n\n\n\nAplicar una prueba t de Student para datos dependientes en los términos significativos. Debido a que tratamiento tiene solo dos niveles, realizar este proceso es redundante; de hecho, los valores de p serán iguales a los mostrados atrás; sin embargo, hagámoslo con fines demostrativos:\n\n\npwt_2 <- estima2 |>\n         group_by(tiempo) |>\n         pairwise_t_test(estima~treatment, paired = T,\n                         p.adjust.method = \"bonferroni\")\npwt_2\n\n\n\n  \n\n\n\nEsto es todo para la clase de hoy. Es una clase bastante extensa y aún con ello se quedaron fuera algunas variantes de ANOVA; sin embargo, creo que estos cubren los casos más generales. ¡Nos vemos en la siguiente!"
  },
  {
    "objectID": "c11_rls.html#librerías",
    "href": "c11_rls.html#librerías",
    "title": "11  Modelo lineal",
    "section": "11.1 Librerías",
    "text": "11.1 Librerías\n\nlibrary(ggplot2)\nlibrary(performance)\nlibrary(stats4)\nlibrary(Metrics)"
  },
  {
    "objectID": "c11_rls.html#introducción",
    "href": "c11_rls.html#introducción",
    "title": "11  Modelo lineal",
    "section": "11.2 Introducción",
    "text": "11.2 Introducción\nEn muchísimos lugares encontramos, de forma completamente natural, patrones que se repiten una y otra vez, tal y como en la música. El mundo de la estadística y del aprendizaje automatizado se construye de la misma manera, partiendo de pequeños motivos que aparecen una y otra vez. En esta sesión vamos a hablar de el, posiblemente, más popular de todos: el modelo lineal. Hablaremos entonces del caso más básico y escalaremos paso a paso en la complejidad."
  },
  {
    "objectID": "c11_rls.html#regla-de-tres-y-el-modelo-lineal",
    "href": "c11_rls.html#regla-de-tres-y-el-modelo-lineal",
    "title": "11  Modelo lineal",
    "section": "11.3 Regla de tres y el modelo lineal",
    "text": "11.3 Regla de tres y el modelo lineal\nAntes de empezar a hablar propiamente de la regresión lineal, sus diferencias con la correlación, en qué consiste, cómo aplicarlas y demás detalles, demos un paso hacia atrás y expliquemos el fundamento con manzanitas (literalmente).\nImagina que te digo que gasté 50 pesos para comprar 10 manzanas y luego te pregunto ¿cuánto cuesta una? Para responder a la pregunta aplicarás, aunque no seas consciente, el modelo lineal, pero primero resolvamos el problema con una regla de tres\n\\[\\begin{align*}\n10 🍎 = \\$50 \\\\\n1🍎 = ?\n\\end{align*}\\]\nAquí multiplicaríamos \\(1🍎 \\times 50\\$\\) y dividimos entre \\(10🍎\\), lo cual nos lleva a decir que una manzana me costó 5\\(\\$\\):\n\\[\n\\frac{1🍎 \\times 50 \\$}{10🍎} = 5\\frac{\\$}{🍎}\n\\]\nHasta aquí nada nuevo, así que hagamos ese resultado a un lado por el momento y volvamos al problema del modelo lineal. ¿En qué consiste un modelo lineal? En utilizar la ecuación de la recta (\\(y = a + bx\\)) para establecer una relación (lineal, dah) entre dos variables. Con nuestras manzanas podemos representarlo de la siguiente manera, donde \\(y\\) es el dinero (\\(\\$\\)) gastado para alguna cantidad de manzanas (🍎):\n\\[\n\\$ = a + b*🍎\n\\]\n¿Qué representan \\(a\\) y \\(b\\) Empecemos, por simplicidad didáctica, definiendo \\(b\\):\n\\[\\begin{align*}\nSi \\\\\n\\$ = a + b*🍎 \\\\\n\\Rightarrow \\$ - a = b*🍎 \\\\\n\\therefore \\frac{\\$ - a}{🍎} = b\n\\end{align*}\\]\nEste pequeño ejercicio algebráico nos dice que \\(b\\) es el resultado de dividir nuestro precio (menos \\(a\\)) entre el número de manzanas. ¿Te suena? ¡Es el precio por una manzana! Te preguntarás: ¿entonces **qué es \\(a\\) y por qué no lo consideramos antes? Para darle sentido pensemos en qué haría que ambas aproximaciones nos lleven al mismo resultado: que \\(a\\) fuera 0, ¿no? Pues eso tiene todo el sentido del mundo, pues es el precio de 0 manzanas. Tomando esto en cuenta podemos asignarle nombre a nuestros distintos elementos:\n\n\\(y\\) es nuestra variable dependiente (lo que queremos predecir); es decir, el número de pesos gastados.\n\\(x\\) es nuestra variable independiente (con lo que vamos a predecir); es decir, el número de manzanas compradas.\n\\(a\\) es la ordenada al origen o intercepto, representada como \\(\\beta_0\\) o \\(\\alpha\\), indica el precio de 0 manzanas. Matemáticamente esto lo definimos como el punto donde la recta corta a la ordenada (eje y) o, en palabras más sencillas, el punto donde x = 0.\n\\(b\\) es la pendiente, representada como \\(\\beta_1\\) o \\(\\beta\\), e indica el precio de una manzana. Formalmente es la tasa de cambio que existe del eje \\(x\\) al eje \\(y\\); es decir, “cuántas unidades nos vamos a mover en el eje y por una unidad en el eje x”.\n\n\n\n\nFigura 11.1: De la regla de tres al modelo lineal\n\n\n¿Por qué se le denomina lineal? Porque si lo graficamos tendremos una línea recta:\n\nmanzanas <- seq(1,10)\nb <- 5\na <- 0\nprecio <- data.frame(manzanas, precio = a+b*manzanas)\n\nggplot(data = precio, aes(x = manzanas, y = precio)) +\n  geom_line(color = \"dodgerblue4\") +\n  geom_point(color = \"dodgerblue4\") +\n  scale_x_continuous(breaks = scales::pretty_breaks()) +\n  see::theme_lucid() +\n  labs(title = \"Precio por cantidad de manzanas (n)\",\n       subtitle = \"Modelo: $ = 0 + 5*n\",\n       x = element_blank(),\n       y = element_blank())\n\n\n\n\nFigura 11.2: Modelo lineal con el precio por cantidad de manzanas.\n\n\n\n\nAunque esto nos lleva a un pequeño inconveniente o una consideración. Si utilizamos un modelo lineal estamos asumiendo explícitamente (aunque a veces inconscientemente) que el cambio entre nuestras variables es constante, determinado por \\(\\beta\\). Reflexiona: ¿en la naturaleza cuántos procesos crees que sean realmente lineales? Con esto no quiero decir que debamos de olvidarnos del modelo lineal y que, entonces, esta sesión es una pérdida de tiempo, no. Quiero decir que debemos de ser consciente del supuesto bajo el cual estamos trabajando, muchas veces para simplificarnos la existencia.\n\n\n\n\n\n\nNota\n\n\n\n¿Recuerdas la definición formal de un modelo? Es una representación SIMPLIFICADA de la realidad; es decir, siempre que involucremos un modelo, sea cual sea, estamos dejando cosas en el tintero. Recuerda el aforismo propuesto por Box: “Todos los modelos están equivocados, pero algunos son útiles”."
  },
  {
    "objectID": "c11_rls.html#regresión-lineal-simple",
    "href": "c11_rls.html#regresión-lineal-simple",
    "title": "11  Modelo lineal",
    "section": "11.4 Regresión lineal simple",
    "text": "11.4 Regresión lineal simple\nAntes mencioné que la regla de tres es una aplicación del modelo lineal, lo que no dije es que es, en realidad, un ejercicio de regresión. ¿Qué es una regresión? Es una parte del aprendizaje automatizado supervisado, del cual hablaremos con más lujo de detalle en la sección correspondiente, pero podemos definirla como el proceso de estimar los parámetros de un modelo matemático a partir de ciertos datos. En una regresión la variable dependiente siempre es continua. ¿Qué pasa si tenemos una variable dependiente categórica? Entonces estamos en un escenario de clasificación, pero no nos adelantemos.\n\n\n\n\n\n\nNota\n\n\n\n¿Qué es el aprendizaje automatizado? Por el momento entiendelo como la filosofía de “enseñar con ejemplos” llevada a modelos matemáticos y predicciones. En cualquier modelo de aprendizaje automatizado supervisado tenemos una serie de ejemplos (datos), en los cuales una o más variables sirven como predictoras de otra(s); es decir, el objetivo es generar buenas predicciones.\n\n\n\n\n\n\n\n\nNota\n\n\n\n¿Qué es un parámetro en este contexto? Una manera fácil de entender los parámetros de un modelo es verlos como perillas que regulan cómo se transforma lo que está a la derecha del símbolo de igualdad para llegar a lo que está a la izquierda. Si te das cuenta es básicamente lo que sucede con las distribuciones de probabilidad, y con justa razón: las distribuciones de probabilidad son, en sí mismas, modelos. No confundas esta definición con la definición de parámetro poblacional.\n\n\nAhora bien, hay una gran cantidad de métodos de auste de una regresión, pero todos se reducen a una cosa: minimizar una función de pérdida. ¿Qué es una función de pérdida y con qué se come? Es una forma rebuscada de llamarle a la distancia que existe entre nuestros valores observados y los valores predichos por el modelo o, en palabras más sencillas, qué tan lejos quedó la flecha del blanco; es decir, al ajustar un modelo estamos minimizando su error. ¿Cuáles valores predichos? Aaaah, que bueno que preguntaste. Si vuelves a la Figura 11.2 te darás cuenta que, utilizando los parámetros estimados con nuestra regla de tres (\\(a\\) = 0 y \\(b\\) = 5) calculamos cuánto gastaríamos si compraramos desde 1 hasta 10 manzanas. De esos costos solo teníamos el dato de que \\(10 🍎 = \\$50\\), todo lo demás es una predicción.\n\n\n\n\n\n\nNota\n\n\n\nLo que hicimos fue, de hecho, una extrapolación, pues predijimos valores fuera del alcance de nuestros datos observados. Si tuvieramos “huecos” en nuestros datos y quisiéramos rellenarlos con el modelo tendríamos una interpolación; es decir, predeciríamos valores dentro del alcance de nuestros datos observados.\n\n\nPero volvamos a nuestra regresión lineal. El modelo más simple es el que veremos en esta sesión: la regresión lineal simple. En esta, tal y como en nuestro ejemplo con las manzanas, describimos la relación entre dos variables continuas utilizando la ecuación de la recta. Formalmente este lo expresamos como:\n\\[\nY = \\beta_0 + \\beta_1*x + \\epsilon\n\\]\nMencioné que existen distintas formas de ajustar sus parámetros, entre las que tenemos (ordenadas de mayor a menor complejidad):\n\nMínimos cuadrados, que veremos a continuación.\nMáxima verosimilitud, que también veremos a continuación.\nInferencia Bayesiana, que en realidad incluye como caso especial a la máxima verosimilitud.\nDescenso estocástico de gradiente, en ciertos escenarios de redes neuronales.\n\nEmpecemos entonces con el ajuste por mínimos cuadrados.\n\n11.4.1 Mínimos cuadrados\nEn este método de ajuste la función de pérdida es la función cuadrática:\n\\[\nD(y_i - \\hat{y_i}) = \\sum_{i=1}^n(y_i - \\hat{y_i})^2 = \\sum_{i=1}^n \\epsilon^2\n\\]\nEs decir, minimizamos la distancia (diferencia) cuadrática entre los valores observados y los valores predichos. Analíticamente (cálculo) el proceso consiste en obtener la derivada parcial de \\(D(y_i - \\hat{y_i})\\), igualarla a 0, y encontrar una expresión para \\(\\beta_0\\) y \\(\\beta_1\\). Te voy a ahorrar toda la matemática correspondiente y te daré, solo como referencia, estas últimas expresiones. Para \\(\\beta_1\\):\n\\[\n  \\beta_1 = \\frac{\\Sigma_{i = 1}^n(x_i - \\overline{x})(y_i - \\overline{y})}{\\Sigma_{i = 1}^n(x_i - \\overline{x})^2}\n\\]\nY para \\(\\beta_0\\):\n\\[\n\\beta_0 = \\overline{y}- \\beta_1*\\overline{x}\n\\]\n¿Por qué solo como referencia? Porque (afortunadamente para nosotros) R ya tiene codificado todo el proceso en la función lm(formula, data) y no tenemos que preocuparnos por nada de eso.\n\n\n\n\n\n\nNota\n\n\n\nRecordarás que en el Capítulo 4 hablé (y ejemplifiqué) sobre cómo ajustar una regresión lineal simple, tanto con R como con tidymodels. En esta sesión tidymodels aún nos queda “un poco grande”, en el sentido de que aún no podemos aprovechar todo lo que ofrece, por lo que me voy a limitar a utilizar la forma de R base.\n\n\nPara ejemplificarlo carguemos los datos contenidos en example_data.csv:\n\ndf_reg1 <- read.csv(\"datos/example_data.csv\")\n\nLuego grafiquémoslos:\n\nplot_data_reg1 <- ggplot(data = df_reg1,\n                         aes(x = v1, y = v2)) +\n                  geom_point(color = \"dodgerblue4\",\n                             alpha = 0.7,\n                             size = 2) +\n                  labs(title = \"Relación entre v1 y v2\") +\n                  see::theme_lucid()\nplot_data_reg1\n\n\n\n\n\n11.4.1.1 Ajuste y Bondad de ajuste\nAhora ajustemos el modelo de mínimos cuadrados (lm()) a los datos y veamos los resultados de la regresión (summary()):\n\nreg1 <- lm(v2~v1, data = df_reg1)\nsummary(reg1)\n\n\nCall:\nlm(formula = v2 ~ v1, data = df_reg1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.8956 -1.9924 -0.5525  1.5351 15.3006 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -3.72654    0.73213   -5.09 1.81e-06 ***\nv1           1.17765    0.08141   14.47  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.095 on 95 degrees of freedom\nMultiple R-squared:  0.6878,    Adjusted R-squared:  0.6845 \nF-statistic: 209.3 on 1 and 95 DF,  p-value: < 2.2e-16\n\n\nDescribamos la salida elemento por elemento:\n\nCall: es el cómo llamamos a la función. ¿La razón? En caso de que estemos llamando a la función lm dentro de otra función, cosa que no hicimos. De cualquier manera, sirve como una forma de verificar que pusimos las cosas en orden. -Residuals: Nos da información sobre la distribución de los residuales (la diferencia entre observado y predicho). Esta es útil con fines diagnósticos, pero hablaremos más a fondo de ellos más adelante.\nCoefficients: Nos da una tabla con los valores de los parámetros del modelo, donde la pendiente tiene el nombre de la variable predictora, su error estándar y una prueba \\(t\\) para cada uno. ¿Contra qué está comparando? Contra un modelo nulo; es decir, contra un modelo donde ese parámetro tenga un valor = 0.\nResidual standard error: Más información sobre los residuales, aunque en este caso es el error estándar. Este es sumamente útil para darnos una idea de qué tan preciso es el modelo, pues indica en cuántas unidades, en promedio, se desvía la predicción de los datos observados. Este valor, dividido entre el promedio de la variable predicha nos da la tasa de error del modelo.\nMultiple R-squared: Es el valor del famosísimo (¿infame?) coeficiente de determinación (\\(R^2\\)). Si ya has llevado clases de estadística y de regresión lineal es muy posible que lo entiendas como “la varianza de los datos explicada por el modelo”. ¿Por qué digo infame? Porque, al igual que el valor de p, es un valor del cuál se abusa. Nuevamente, los seres humanos somos flojos por naturaleza, por lo que nos gusta resumir las cosas en un solo número. En este sentido, el \\(R^2\\) es una medida de bondad de ajuste; es decir, de qué tan bien ajustado está el modelo. Es muy práctico, pues está contenido en el intervalo \\([0,1]\\) y representa un porcentaje; sin embargo, para que podamos confiar en él debemos de haber cubierto con el resto de supuestos de la RLS. Personalmtente te sugiero mejor tomar el RSE como medida de ajuste, aunque la interpretación no sea tan directa.\nAdjusted R-squared: Es un ajuste al \\(R^2\\) que lo hace menos optimista, especialmente diseñado para escenarios de regresión múltiple (de ahí el “Multiple” del punto anterior). Por el momento lo vamos a ignorar.\nF-statistic: Resultados de un ANOVA que, al igual que el punto anterior vamos a ignorar porque es informativo solo en regresiones múltiples. Es un ANOVA para comparar todo el modelo contra un modelo nulo, por lo que aquí solo es redundante con las pruebas \\(t\\) para cada parámetro.\n\n\n\n\n\n\n\nNota\n\n\n\n¿Qué es la bondad de ajuste? Como dice el nombre, qué tan bien ajustado está el modelo, en el sentido de qué tan buenas son las predicciones.\n\n\nLos resultados del modelo los podemos reportar como:\nEn el modelo de regresión lineal simple tanto el intercepto (\\(\\beta_0 = -3.72\\)) como la pendiente (\\(\\beta_1 = 1.17\\)) son significativamente diferentes de 0 (\\(\\beta_0: t_{\\nu = 95} = -5.09; p < 0.0001\\); \\(\\beta_1: t_{\\nu = 95} = 14.47; p < 0.0001\\)). El valor de \\(R^2\\) indica que el modelo explica alrededor del 70% de la varianza de los datos, lo cual sugiere un ajuste aceptable, con un error estándar de los residuales de 3.095 unidades.\nUsualmente acompañaríamos este reporte de un gráfico, el cual podemos construir con la capa geom_smooth(method = \"lm\", se = FALSE), tal que:\n\nplot_reg1 <- plot_data_reg1 +\n             geom_smooth(method = \"lm\",\n                         se = FALSE,\n                         colour = rgb(118,78,144,\n                                      maxColorValue = 255)) +\n             labs(caption = paste(\"Modelo ajustado: v2 = \", \n                                   round(reg1$coefficients[1],2), \n                                   \" + \",\n                                   round(reg1$coefficients[2],2),\n                                   \"*v1 + e\"))\n\nplot_reg1\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nAhora bien, recordarás que ninguna estimación es infalible, por lo que tanto el reporte como el gráfico están incompletos. Hablemos entonces de los intervalos de confianza.\n\n11.4.1.1.1 Intervalos de confianza para los parámetros\nEn un modelo de regresión lineal tenemos “dos” intervalos de confianza: los intervalos de confianza para la estimación de los parámetros y el intervalo de confianza para la regresión. Los primeros, como te imaginarás, representan la incertidumbre alrededor de la estimación de nuestros parámetros de regresión. Como recordarás de lo que mencioné en el capítulo Capítulo 8, estos se construyen a partir de su error estándar (\\(IC_{95\\%} = \\beta ± 1.96*EE\\)) y, a diferencia de lo que vimos en el Capítulo 9, ahora los obtenemos con la función confint():\n\nconfint_reg1 <- confint(reg1, level = 0.95)\nconfint_reg1\n\n                2.5 %    97.5 %\n(Intercept) -5.179989 -2.273085\nv1           1.016035  1.339263\n\n\nCon esta información podemos realizar un gráfico donde representemos esta incertidumbre, en donde obtengamos dos límites para la línea de regresión utilizando esos valores, tal que:\n\n# Construimos el límite inferior con el límite del 2.5% de los intervalos\ndf_reg1[\"inf_int\"] <- confint_reg1[1,1] + confint_reg1[2,1]*df_reg1$v1\n# Construimos el límite superior con el límite del 97.5% de los intervalos\ndf_reg1[\"sup_int\"] <- confint_reg1[1,2] + confint_reg1[2,2]*df_reg1$v1\n\nAñadiéndolos al gráfico de los datos:\n\nplot_reg1 +\n  geom_ribbon(data = df_reg1,\n              aes(ymin = inf_int,\n                  ymax = sup_int),\n              fill = \"gray70\",\n              alpha = 0.3) +\n  labs(subtitle = \"Intervalos de confianza para los parámetros\")\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nPero este no es el gráfico que usualmente veremos. Si deseáramos inlcuir esta información solo añadiríamos los IC al reporte, tal que:\nEn el modelo de regresión lineal simple tanto el intercepto (\\(\\beta_0 = -3.72\\); \\(IC_{95\\%}: [-5.18, -2.27]\\)) como la pendiente (\\(\\beta_1 = 1.17\\); \\(IC_{95\\%}: [1.02, 1.34]\\)) son significativamente diferentes de 0 (\\(\\beta_0: t_{\\nu = 95} = -5.09; p < 0.0001\\); \\(\\beta_1: t_{\\nu = 95} = 14.47; p < 0.0001\\)). El valor de \\(R^2\\) indica que el modelo explica alrededor del 70% de la varianza de los datos, lo cual sugiere un ajuste aceptable, con un error estándar de los residuales de 3.095 unidades.\n\n\n\n\n\n\nAdvertencia\n\n\n\nEn este caso nuestros datos son “adimensionales”; es decir, no tenemos unidades de ninguna variable. Si nuestras variables no son adimensionales debemos de incluir también las unidades correspondientes (pesos, manzanas y pesos/manzana, si volvemos a nuestro ejemplo).\n\n\n\n\n11.4.1.1.2 Intervalo de confianza para la regresión\n¿Si el gráfico anterior no es el que presentamos, cuál es? Uno que incluya el intervalo de confianza para la regresión per-se (también llamado para la recta). ¿Recuerdas el RSE? Pues se construye con ese valor. Para incluirlo en el gráfico solo tenemos que modificar ligeramente geom_smooth() y hacer se = TRUE:\n\nplot_reg1 + \n  geom_smooth(method = \"lm\",\n              se = TRUE,\n              colour = rgb(118,78,144, maxColorValue = 255))\n\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nFigura 11.3: Modelo de regresión lineal simple de v2 y v1. La línea morada representa el ajuste lineal, y el área sombreada el intervalo de confianza para la regresión\n\n\n\n\n\n\n\n\n\n\nNota\n\n\n\nEstamos obteniendo dos mensajes de geom_smooth() porque en plot_reg1 ya teníamos esa capa (con se = FALSE). Diré lo obvio, pero cuando hagas tus gráficos NO es necesario que repitas capas.\n\n\nNuestro reporte completo quedaría entonces como:\nEn el modelo de regresión lineal simple (Figura 11.3) tanto el intercepto (\\(\\beta_0 = -3.72\\); \\(IC_{95\\%}: [-5.18, -2.27]\\)) como la pendiente (\\(\\beta_1 = 1.17\\); \\(IC_{95\\%}: [1.02, 1.34]\\)) son significativamente diferentes de 0 (\\(\\beta_0: t_{\\nu = 95} = -5.09; p < 0.0001\\); \\(\\beta_1: t_{\\nu = 95} = 14.47; p < 0.0001\\)). El valor de \\(R^2\\) indica que el modelo explica alrededor del 70% de la varianza de los datos, lo cual sugiere un ajuste aceptable, con un error estándar de los residuales de 3.095 unidades.\nY ahora toca abordar el “elefante en el cuarto” y comprobar que nuestra regresión sea confiable.\n\n\n\n\n11.4.2 Supuestos de la Regresión Lineal\n¿Qué no ya habíamos evaluado la bondad del ajuste? Sí y no. En realidad hice una pequeña trampa para que veas por qué el \\(R^2\\) no cuenta toda la historia, y por qué no debemos de confiar ciegamente en él. La regresión lineal simple, como buena técnica paramétrica, tiene sus supuestos:\n\nLinealidad: Existe una relación lineal entre las variables involucradas.\nIndependencia: El error es independiente; i.e., no hay correlación entre el error de puntos consecutivos (aplica para series de tiempo).\nNormalidad: El error sigue una distribución normal.\nHomocedasticidad: El error tiene una varianza constante para cada valor de \\(X\\).\n\n\n\n\n\n\n\nNota\n\n\n\nTe darás cuenta de que todo está en términos del “error”, tal y como hablábamos de distribuciones muestrales de la media en el capítulo Capítulo 10. Siguiendo la misma lógica, nuestras inferencias para comprobar los supuestos las haremos sobre los residuales.\n\n\nSi no cumplimos con uno, varios, o ninguno, la confiabilidad de nuestro modelo de regresión para fines de interpretación va disminuyendo. Los primeros dos son bastante lógicos. El primero es auto-explicativo: si la relación no es lineal, el modelo lineal no es suficiente para describirla. El segundo tiene que ver con datos de series de tiempo y algo que se conoce como autocorrelación, pero esto se reduce a que el error del punto \\(t_i\\) no dependa del punto \\(t_{i-1}\\).\nEl supuesto de normalidad, para variar, requiere de un poco más de explicación: la distribución que debe ser normal es la del error. En ningún lugar se habla de que \\(Y\\) o \\(X\\) deban de estar normalmente distribuidos, solamente el error. Esto puede sonarte extraño, pero tiene todo el sentido del mundo: si estamos optimizando el modelo a partir de los residuales, ¿por qué habría de importarnos la distribución cruda de las variables?\nEl supuesto de homocedasticidad, por otra parte, es análogo al supuesto de homogeneidad de varianzas pero, nuevamente, nos interesa qué pasa con el error, no con las variables originales (por favor, no hagas una prueba de Levene con tus variables como grupos). En este caso lo importante es que el error sea parejo, independientemente de si tenemos valores pequeños o grandes del predictor.\n¿Cómo evaluamos estos supuestos? Mayoritariamente con gráficos de dispersión, pero vamos uno a uno:\n\nLinealidad: Con un gráfico de residuales. En este gráfico tenemos los residuales estandarizados (cada residual menos el promedio de los residuales, dividido entre la desviación estándar) en el eje \\(y\\), y los valores ajustados por el modelo (predicciones) en el eje \\(x\\). Se pueden añadir dos referencias: una línea de referencia horizontal en \\(y = 0\\) y una curva LOESS (hablaremos un poco de este modelo en el Capítulo 13). Si la relación entre nuestras variables predicha y predictora es perfectamente lineal, entonces todos los puntos caerán sobre la línea de referencia y la curva LOESS será completamente horizontal en 0. Entre menos lineal sea la relación más se alejarán los puntos de la línea y mayores curvaturas tendrá el modelo LOESS. Adicionalmente, puede servir para identificar valores extremos (puntos que caigan fuera de \\([-1.96, 1.96]\\) si se trabaja a un 95% de confianza).\nIndependencia: Con un gráfico de autocorrelación. Dado que el análisis de series de tiempo está fuera del alcance de este grupo lo vamos a obivar.\nNormalidad: Una prueba de normalidad de los residuales y con un gráfico cuantil-cuantil (QQ plot). En este gráfico se grafican (valga la redundancia) los residuales en el eje \\(y\\), y cuantiles teóricos según una distribución normal en el eje \\(x\\). Además se traza una línea de referencia con una pendiente de 1, que representa una distribución normal perfecta. El objetivo es que la prueba de normalidad sea no significativa y que los residuales caigan lo más cercanamente posible a la línea de referencia.\nHomocedasticidad: Una prueba Breusch & Pagan (1979) y un gráfico de escala-locación (scale-location). La prueba, como te imaginarás, tiene la hipótesis de nulidad de que el error (residuales) está homogéneamente distribuido. En el gráfico tenemos en el eje \\(y\\) la raíz cuadrada del absoluto de los residuales estandarizados (\\(\\sqrt{|re|}\\)) y en el eje \\(x\\) los valores ajustados. ¿Por qué la raíz del absoluto? Ese detalle ya es clavarse demasiado, y prefiero que nos adentremos bien a otro tema un poco más adelante, así que conformemonos por saber que queremos que los puntos estén distribuidos de manera aleatoria (homogénea) en todo el eje \\(x\\). ¿Qué quiere decir esto? que no tengamos una mayor dispersión de los puntos (varianza de los residuales) en valores ajustados pequeños (a la izquierda del gráfico) que en los valores más grandes (derecha del gráfico), lo que se vería como un > imaginario, o viceversa, o algún otro patrón.\n\n¿Muchos gráficos? Tal vez. Podríamos hacerlo a mano, o podemos aprovechar la librería performance y obtenerlos todos de una vez con la función check_model(object), donde object es el objeto con los resultados del ajuste. Esta función nos da todos los gráficos en un solo paso:\n\nreg1_diags <- performance::check_model(reg1)\nreg1_diags\n\n\n\n\nFigura 11.4: Gráficos diagnósticos de la regresión.\n\n\n\n\nY se acabó el encanto. Resulta que no cumplimos con ninguno de los supuestos, y entonces nuestro \\(R^2 \\approx 0.7\\) nos mintió vilmente. Cada quien reacciona de forma diferente a cuándo alguien le miente, pero lo que es seguro que pase es que desconfiará, al menos un poco, de todo lo que esa persona le diga en un futuro. En defensa del \\(R^2\\), no es su culpa y, de hecho, estoy siendo demasiado duro con él. Me explico: el \\(R^2\\) es confiable como medida de ajuste sí y solo si se cumplen los mismos supuestos de la regresión lineal simple, lo cual no es el caso; ergo, no podíamos confiar en él desde un principio porque nuestros datos no lo permiten. El problema es que se ha malversado su uso, y muchas personas lo utilizan como si fuera el único sello de garantía, cuando en realidad es el último.\nPero volvamos a nuestros gráficos. En la Figura 11.4 se ven muy pequeños, y hay un par que no hemos explicado:\n\nPosterior predictive check: Este tipo de gráficos es uno de los más socorridos en inferencia Bayesiana, para comprobar que la distribución posterior obtenida por el modelo sea consistente con los datos observados. Aquí no tenemos un modelo Bayesiano, pero sí que podemos utilizar la información de la distribución de los parámetros (estimación y error estándar) para simular datos aleatorios. Si el modelo tiene una buena capacidad predictiva, entonces los datos observados \\(Y\\) deben de caer dentro del área comprendida por las líneas de los datos predichos \\(\\hat{Y}\\). En este caso, la capacidad predictiva del modelo no es del todo mala, solamente tenemos una sobre-estimación notable en valores cercanos a 10. Esto nos habla de la robustez del modelo lineal, pero no por ello hay que abusar de él.\n\n\nplot(performance::check_predictions(reg1)) +\n  see::theme_lucid()\n\n\n\n\n\nLinearity: Es el gráfico de residuales. Desafortunadamente no hay una forma de obtenerlo en una sola línea, así que tocará construirlo a mano y, de paso, modificar un par de cosas. La primera es graficar los residuales estandarizados (menos su media y divididos entre su desviación estándar) para poder darnos una idea de si tenemos valores extremos o no. La segunda es asignar una escala de colores para facilitarlo. Para poder hacer esto vamos a pasarle a ggplot() directamente el objeto de regresión, y en aes() vamos a utilizar dos atributos ocultos del objeto reg1: .fitted con los valores predichos y .stdresid con los residuales estandarizados. En el gráfico resultante podemos ver que hay varios valores con residuales altos (tonos rojizos), y algunos extremos (> 1.96). La curva LOESS no se ve exageradamente desviada de la línea de referencia, lo cual indica que un modelo lineal puede no ser tan mala elección.\n\n\n# Inicializar el espacio gráfico\nggplot(data = reg1, # Objeto de regresión\n       # Datos ajustados y residuales estandarizados\n       aes(x = .fitted, y = .stdresid,\n           colour = .stdresid)) +\n  # Gráfico de dispersión\n  geom_point(size = 3,\n             alpha = 0.7,\n             show.legend = F) +\n  # Referencia en 0\n  geom_hline(yintercept = 0,\n             colour = \"black\",\n             linetype = \"dashed\") +\n  # Referencia loess\n  geom_smooth(method = \"loess\",\n              colour = \"#3aaf85\") +\n  # Gradiente de colores\n  # \"#cd201f\": color para los extremos (rojo)\n  # \"#1b6ca8\": color para el punto intermedio (0)\n  # Escala de -2 a 2 (debería ser 1.96)\n  # oob: ¿qué hacer con datos fuera de los límites?\n  # scales::squish : marcarlos como si estuvieran en el límite\n  scale_color_gradient2(low = \"#cd201f\",\n                        midpoint = 0, \n                        mid = \"#1b6ca8\", \n                        high = \"#cd201f\",\n                        breaks = c(-2, 0, 2),\n                        limits = c(-2, 2),\n                        oob = scales::squish) +\n  labs(title = \"Linearity\",\n       subtitle = \"Reference line should be flat and horizontal\",\n       y = \"Standardized residuals\",\n       x = \"Fitted values\") +\n  see::theme_lucid()\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\nHomogeneity of variance: Homocedasticidad/Heterocedasticidad. Con un poco de imaginación puedes trazar un >, indicando que hay una mayor varianza en los valores pequeños que en los valores grandes. La prueba Breusch & Pagan (1979), por otra parte, sugiere que la varianza es homogénea. Este es un típico caso donde conviene errar en el lado de la precaución y profundizar en el análisis antes de sacar una conclusión. ¿Qué pasa con nuestro > si quitamos el punto extremo con un residual estandarizado > 4? Se vuelve mucho menos marcado, ¿no? Posiblemente sea eso lo que está viendo la prueba y que no esté siendo engañada por ese punto. Dada la estructura de los datos (muchos acumulados en valores pequeños y pocos en valores grandes), yo decidiría que no se cumple el supuesto de homogeneidad de varianzas.\n\n\nreg1_homoced <- performance::check_heteroscedasticity(reg1)\nplot(reg1_homoced) +\n  see::theme_lucid()\n\n\n\nreg1_homoced\n\nOK: Error variance appears to be homoscedastic (p = 0.206).\n\n\n\nInfluential Observations: Este es otro gráfico diagnóstico que no había mencionado porque no está directamente relacionado con los supuestos. Este gráfico se conoce como gráfico de apalancamiento, y señala observaciones que pudieran estar “engañando” o afectando de manera importante la estimación de la recta. En otras palabras, que “jalen” la recta hacia ellos, por estar extremadamente lejos de la tendencia central de \\(y\\) para ese punto \\(x\\). En menos palabras: nos permite identificar “outliers”. Hay una gran cantidad de métodos, y la función performance::check_outliers() califica cada valor con una nota compuesta por el promedio de los resultados binarios (“outlier” o no, 1 o 0) de cada método. Representa la probabilidad de que cada observación sea clasificada como “outlier” por al menos un método. Se considera un “outlier” si su calificación es superior o igual a 0.5 (líneas verdes punteadas); es decir, vamos a buscar puntos que estén fuera del “cono” formado por los contornos. En este caso ninguno está fuera del contorno, pero el punto 1 se ve sospechoso.\n\n\nreg1_outliers <- performance::check_outliers(reg1)\nplot(reg1_outliers)\n\n\n\n\n\nNormality: Por último, el gráfico de normalidad. No es de sorprender que tengamos “desviaciones de la normalidad” bastante marcadas en algunos puntos, especialmente cerca de la cola derecha de la distribución. La prueba de normalidad de los residuales también rechaza que se cumpla el supuesto.\n\n\nreg1_norm <- performance::check_normality(reg1)\nplot(reg1_norm, type = \"qq\")\n\nFor confidence bands, please install `qqplotr`.\n\n\n\n\nreg1_norm\n\nWarning: Non-normality of residuals detected (p < .001).\n\n\nIndependientemente de mi “bullying” al \\(R^2\\), ahora ya sabes todo lo que implica hacer una regresión lineal simple, y que es mucho más que simplemente picar botones en alguna suite estadística o utilizar la función lm en R o alguna otra función en otro lenguaje de programación.\n\n\n11.4.3 Predicción vs. Interpretación\nAhora bien, mencioné en varias ocasiones cosas relacionadas con la “predicción” y la “interpretación”. Pues resulta que, como vimos arriba, para fines predictivos no importan demasiado los supuestos, y antes de que agarres un trinche y una antorcha escucha lo que tengo que decir. Antes te dije que la regresión forma parte del aprendizaje automatizado supervisado y, como tal, su principal (por no decir único) objetivo es la predicción. Un modelo de regresión exitoso es un modelo que pueda predecir adecuadamente, punto. ¿Y la interpretación? Esa es otra cara de la moneda. De hecho, están inversamente correlacionadas, en el sentido de que entre más poderosa es una técnica, menos interpretable es. De todos estos detalles vamos a hablar más adelante en el Capítulo 17, pero por el momento quiero que te quedes con lo siguiente:\n\n\n\n\n\n\nImportante\n\n\n\nLa validación de supuestos es solo necesaria si nos interesa explicar los parámetros del modelo, no si solo nos interesan sus predicciones.\n\n\nSin ir demasiado lejos, el modelo que construimos arriba no cumple con el supuesto de normalidad (los demás están en la cuerda floja) y aún así las predicciones posteriores (en el posterior predictive check) se ven bastante aceptables. Desafortunadamente para nostros, usualmente nos interesa la interpretación, así que hay que hacer la tarea completa.\n¿Y si solo me interesan las predicciones? Bueno, igual hay que verificar algunas cosas, pero eso lo veremos en el Capítulo 17.\n\n\n11.4.4 Máxima verosimilitud\nBueno, ya sabemos cómo aplicar, interpretar, y validar los supuestos de una regresión lineal en R utilizando el método de mínimos cuadrados, pero antes te mencioné que también había otros métodos entre los que se encuentra el ajuste por máxima verosimilitud. Entonces es necesario explicar qué es la verosimilitud para luego ver cómo maximizarla, ¿no crees?\nRecordemos por un momento lo que revisamos en el Capítulo 6 sobre la probabilidad. Dijimos que, cuando tenemos resultados mutuamente excluyentes y exhaustivos (todos los resultados), la suma de todas sus probabilidades es exactamente 1, tal que:\n\\[\n\\sum_i^n p_i == 1\n\\]\nHasta aquí todo bien, pero ¿dónde entra la verosimilitud? Si buscas en el diccionario de la Real Academia Española te vas a encontrar con una de sus siempre útiles definiciones: “Cualidad de verosímil”, por lo que hay que definir verosímil: “que tiene apariencia de verdadero”. Eso ya tiene más sentido. En un escenario de investigación nosotros podemos plantear múltiples hipótesis, que no son necesariamente excluyentes entre sí, entonces no podemos simplemente utilizar la probabilidad. Entendamos la verosimilitud con un ejemplo:\nImagina que alguien a quien conoces te dice que uno de sus amigos tiene poderes psíquicos. Tú, como persona de ciencia, decides ponerlo a prueba. Acuerdan una reunión y le pones un “desafío” simple: vas a lanzar diez volados, y el debe de adivinar el resultado. Al final, él adivina correctamente 7/10 volados. Sin dejarte llevar por tu escepticismo planteas algunas hipótesis: i) simple coincidencia, el tamaño de muestra no es lo suficientemente grande; ii) la moneda no es del todo “justa”, sino que tiende a caer más hacia cierto lado; iii) esta persona tiene una visión cinética sobre-humana y puede ver qué es lo que está arriba antes de que atrapes la moneda, iv) esta persona realmente tiene algo de clarividente. ¿Cuál crees que sea más verosímil? Espero que me digas que la primera hipótesis, especialmente después de lo que vimos en el Capítulo 6. Si por el contrario hubieran sido 1000 lanzamientos y hubiera adivinado 700, la historia sería otra, pero con 7/10 puede ser un capricho del mundo. Eso que hicimos fue, justamente, un ajuste por máxima verosimilitud: seleccionar la hipótesis más verosímil de entre un conjunto dado, solo que vamos a cambiar hipótesis por valores de parámetros.\n¿Formalmente? Un ajuste por máxima verosimilitud consiste en estimar parámetros de un modelo, dado un conjunto de observaciones, en donde se encuentra valores que maximicen la verosimilitud de las observaciones dados los valores de los parámetros. Puesto de otra manera, buscamos el conjunto de valores que maximicen la probabilidad de que nos hayamos encontrado nuestros datos, según el modelo que escogimos. Esto se parece mucho a lo que vimos en el Capítulo 9 sobre el nivel de significancia, solo que nuestro modelo “deja de ser una distribución de probabilidades” para ser un modelo de regresión. ¿Por qué las comillas? Porque nuestro error no puede quedar “suelto”, pero en máxima verosimilitud podemos trabajar con cualquier distribución de probabilidad que se ajuste a nuestro problema; de hecho, esto es lo que da lugar a los modelos lineales generalizados, pero eso lo veremos en el Capítulo 19.\n¿Cómo lo llevamos a la práctica? Primero quiero que te des la oportunidad de ver una relación interesante que puede ahorrarte mucho trabajo, o que puede abrirte la puerta a otro tipo de análisis.\n\n11.4.4.1 Mínimos cuadrados, máxima verosimilitud e inferencia Bayesiana\nEsta parte es completamente teórica y asumo que el ver procedimientos algebraicos no te supone un problema. De no ser así, puedes saltar al final para obtener la idea clave. Si decides que te interesa, vamos allá.\nRecordemos que un problema de regresión consiste en estimar los parámetros de un modelo matemático dado un conjunto de observaciones, y que tenemos una gran diversidad de formas de hacerlo. Comencemos hablando del método de ajuste más común para una RLS: mínimos cuadrados. Como mencioné antes, con este método minimizamos la función de pérdida cuadrática; es decir:\n\\[\\begin{align*}\n\\epsilon = y - \\hat{y}\\\\\nL = \\epsilon^2\n\\end{align*}\\]\nCuando utilizamos este método asumimos algunas cosas, entre ellas que nuestros residuales (no nuestra variable) se encuentran normalmente distribuidos. Aunque este método funciona, se prefiere utilizar métodos probabilísticos (Gerrodette (2011)), tal como la aproximación por máxima verosimilitud. Antes utilizamos un ejemplo práctico para definirla, pero ahora aproximémosla desde el teorema de Bayes:\n\\[\np(\\theta|x) = \\frac{p(x|\\theta)p(\\theta)}{p(x)}\n\\]\nQue podemos simplificar como:\n\\[\nPosterior = \\frac{verosimilitud \\times previa}{evidencia}\n\\]\nEl teorema de Bayes es lo que da lugar al paradígma de la inferencia Bayesiana, el cual no vemos en este curso; sin embargo explicar el teorema es bastante sencillo: ¿qué tan probable es una hipótesis (\\(\\theta\\)), dada cierta evidencia (datos, \\(x\\))? (probabilidad posterior, \\(p(\\theta|x)\\)). Para responderlo vamos a obtener la relación que hay entre qué tan probable es la evidencia, dada la hipótesis (\\(p(x|\\theta)\\), nuestra verosimilitud), qué tan probable creamos nosotros que es nuestra hipótesis (probabilidad previa, \\(p(\\theta)\\)) y la probabilidad de la evidencia en sí misma (\\(p(x)\\)). Obtener la probabilidad de la evidencia es un tema en sí mismo (en realidad solo la aproximamos), así que lo vamos a obviarla de la ecuación. Recordarás que en la inferencia estadística frecuentista partimos del hecho de que no sabemos nada sobre nuestro problema, y podemos entonces, al menos de manera teórica, establecer eso en el teorema de Bayes, lo cual nos lleva a cancelar nuestros términos de previa y evidencia y terminar con la siguiente equivalencia:\n\\[\nPosterior \\equiv verosimilitud\n\\]\nEste caso especial de la inferencia Bayesiana tiene un nombre: Estimación Máxima A posteriori (Maximum A posteriori Estimate, MAP) y es equivalente a la estimación puntual de un ajuste por máxima verosimilitud. ¿Por qué? Porque en esa aproximación tratamos de encontrar valores de nuestros parámetros que maximicen la verosimilitud de las observaciones, dados los parámetros. De manera matemática definimos la equivalencia como:\n\\[\np(x|\\theta) \\equiv L(\\theta|x) \\implies p(x_1, x_2, ..., x_n|\\theta)\n\\]\nOtro de nuestros supuestos en este paradigma es que las muestras son independientes entre sí, por lo que podemos expandir nuestra probabilidad conjunta con \\(P(A,B) = P(A)P(B)\\):\n\\[\nL(\\theta|x_1, x_2, ..., x_n) \\equiv p(x_1|\\theta)p(x_2|\\theta),...,p(x_n|\\theta)= \\prod p(x_i|\\theta)\n\\]\nY este término es lo que queremos maximizar, por lo cual lo podemos escribir tal que:\n\\[\n\\begin{matrix} max \\\\ \\theta \\end{matrix}\n\\left\\{ \\prod p(x_i|\\theta) \\right\\}\n\\]\nEl problema es que ni a nosotros ni a las computadoras nos gusta hacer multiplicaciones, por lo que podemos aplicar un logaritmo para convertir el productorio en una sumatoria. Recuerda: el logaritmo de un producto es igual a la suma del logaritmo de cada uno de sus componentes, por lo tanto:\n\\[\n\\begin{matrix} max \\\\ \\theta \\end{matrix}\n\\left\\{ log \\left( \\prod p(x_i|\\theta) \\right) \\right\\} \\implies\n\\begin{matrix} max \\\\ \\theta \\end{matrix}\n\\left\\{ \\sum_i^n log(p(x_i|\\theta)) \\right\\}\n\\]\nEsta última parte era un poco innecesaria, nada más que un breviario cultural para que conocieras por qué utilizamos logaritmos de verosimilitud, cosa que haremos más adelante, pero ahora vayamos al meollo del asunto:\n\n\n\n\n\n\nImportante\n\n\n\nUno de los supuestos del ajuste por mínimos cuadrados es un error normalmente distribuido. En máxima verosimilitud podemos ajustar nuestro error a cualquier distribución de probabilidad. Si utilizamos a la distribución normal como la distribución del error, entonces mínimos cuadrados, máxima verosimilitud (distribución normal) e inferencia Bayesiana (verosimilitud normal y previas muy planas) dan estimaciones equivalentes.\n\n\n\n\n11.4.4.2 Ajuste por máxima verosimilitud\nEn este punto puedes estar en uno de estos escenarios: a) lograste seguir toda la explicación y se te hizo lógica (si fue así, ¡felicidades! Eres un tan ñoño o ñoña como yo); b) seguiste la explicación y se te hizo lógica, pero no terminaste de entender el teorema de Bayes (igualmente, ¡felicidades! Vas para ñoño/ñoña que chutas); c) lo leíste pero te perdiste solo con las ecuaciones (también ¡felicidades!, tienes la intención de convertirte en ñoño/ñoña); o d) saltaste directamente a lo importante (¡felicidades a ti también! Tienes una vida 🥲). Si estás en los casos c y d, y puede que b seguramente no estés del todo convencido de que mínimos cuadrados y máxima verosimlitud con un error normal sean equivalentes. Si estás en el caso a, te gustaría una demostración. ¿Y si no te interesa? Igual la vamos a hacer.\nA diferencia de la implementación de una regresión por mínimos cuadrados, ajustar el modelo mediante máxima verosimilitud no es tan intuitivo. El primer paso es establecer manualmente nuestra función de verosimilitud, ajustando una distribución normal a los residuales:\n\ndata <- df_reg1[c(\"v1\", \"v2\")]\nLL <- function(b0, b1, mu, sigma){\n  # Encontrar los residuales. Modelo a ajustar\n  R = data$v2 - data$v1 * b1 - b0\n  \n  # Calcular la verosimilitud. Residuales con distribución normal.\n  \n  R = suppressWarnings(dnorm(R, mu, sigma))\n  \n  # Sumar el logaritmo de las verosimilitudes para\n  # todos los puntos de datos.\n  -sum(log(R))\n}\n\nAhora ajustemos el modelo que acabamos de crear, utilizando la función stats4::mle(fun, start = list()) (*maximum likelihood estimation), donde fun es la función de verosimilitud a ajustar y start son los valores iniciales de los parámetros. En este paso lo que estamos haciendo es estimar los dos parámetros (media y desviación estándar) que mejor describen los datos:\n\nmle_fit <- mle(LL, start = list(b0 = 1, b1 = 1, sigma = 1), \n               fixed = list(mu = 0), \n               nobs = length(data$v2))\n\nsummary(mle_fit)\n\nMaximum likelihood estimation\n\nCall:\nmle(minuslogl = LL, start = list(b0 = 1, b1 = 1, sigma = 1), \n    fixed = list(mu = 0), nobs = length(data$v2))\n\nCoefficients:\n       Estimate Std. Error\nb0    -3.726537 0.72453828\nb1     1.177649 0.08056368\nsigma  3.063056 0.21991454\n\n-2 log L: 492.4402 \n\n\n\n\n\n\n\n\nNota\n\n\n\nHay que definir valores iniciales para los parámetros porque, a diferencia de por mínimos cuadrados, el proceso de minimización del negativo de la suma del logaritmo de la verosimilitud es iterativo mediante un algoritmo de búsqueda. El más común es el algoritmo de búsqueda de Newton-Raphson (o Newton-Fourier). ¿A qué me refiero con iterativo? A que la computadora variará los parámetros hasta llegar a la solución “optima”:\n\n\n\nFigura 11.5: Ajuste iterativo de parámetros\n\n\n\n\nEsta salida fue un poco más simple que la salida de la función lm() pero, ¿fue de diferente la estimación? Si vemos los coeficientes de nuestro ajuste por mínimos cuadrados veremos que la estimación puntual es la misma, y los errores estándares son prácticamente iguales:\n\nsummary(reg1)$coefficients\n\n             Estimate Std. Error   t value     Pr(>|t|)\n(Intercept) -3.726537 0.73212521 -5.090027 1.805160e-06\nv1           1.177649 0.08140729 14.466134 9.513254e-26\n\n\nAdemás, el error estándar de la estimación por mínimos cuadrados es prácticamente igual al sigma de la estimación por máxima verosimilitud:\n\nsummary(reg1)$sigma\n\n[1] 3.095131\n\n\nMoraleja: no utilices estimación por máxima verosimilitud si vas a utilizar una distribución normal. Lo único que ganas es hacer pasos adicionales. ¿Cómo utilizar otras distribuciones? Eso lo veremos a detalle en el Capítulo 19."
  },
  {
    "objectID": "c11_rls.html#correlación-y-covarianza",
    "href": "c11_rls.html#correlación-y-covarianza",
    "title": "11  Modelo lineal",
    "section": "11.5 Correlación y covarianza",
    "text": "11.5 Correlación y covarianza\nSé que la sesión hasta este momento ha sido larga y tediosa, pero el tema de regresión merece entrar a la teoría para no obtener conclusiones equivocadas. Dejemos de lado esa parte y cerremos hablando de dos conceptos relacionados: la correlación y la covarianza.\nLa covarianza nos indica cómo varía una variable en relación a otra. La correlación describe la relación entre dos variables. Ya me imagino la expresión de confusión que tienes mientras te preguntas ¿entonces son lo mismo? Pues no, la diferencia es que la correlación es un índice; es decir, está contenida en el intervalo \\([-1, 1]\\), por lo que esta mide no solo la dirección, sino la “fuerza” o el grado de linealidad de la relación, donde 0 es una relación lineal nula. Matemáticamente la diferencia es que la correlación entre dos variables es su covarianza dividida entre el producto de sus desviaciones estándar:\n\\[\\begin{align*}\ncov(X,Y) = \\frac{\\Sigma_{i=1}^n(X_i-\\bar{X})(Y_i-\\bar{Y})}{n-1} \\\\\ncor(X,Y) = \\frac{cov(X,Y)}{\\sigma_x * \\sigma_y}\n\\end{align*}\\]\nEs decir, son conceptos que están muy relacionados entre sí. En ambos el signo del valor indica la dirección de la relación, mientras que la magnitud indica la fuerza de la relación. El problema con la covarianza es que no tiene límites, entonces no puedes saber si una covarianza de 500 es particularmente grande salvo que tengas otra covarianza con la cual comparar, mientras que una correlación de 0.7 es una correlación moderadamente fuerte.\nMuy seguramente por tu cabeza haya pasado la pregunta: “si ambas nos dicen cómo es la relación entre dos variables, ¿cuál es la diferencia con la regresión?”. Pues que la regresión es un modelo predictivo, mientras que la correlación/covarianza es un estadístico descriptivo. Aquí no estamos comprometiendo que haya una tasa de cambio de \\(X\\) hacia \\(Y\\), ni estamos interesados en qué valor de \\(X\\) le corresponde a \\(y = 0\\). Aquí no nos interesa predecir, sino describir. Si no quieres comprometerte con todo lo que implica un modelo predictivo (aún nos faltó ver el tema del sobre-ajuste), solo calcula el coeficiente de correlación correspondiente.\n¿Por qué correspondiente? Porque tenemos más de una forma de calcular la correlación, y cada una tiene sus bemoles. La ecuación que vimos arriba es para el coeficiente de correlación de Pearson, el cual elevado al cuadrado nos da el coeficiente de determinación que vimos antes. Como tal, es un coeficiente paramétrico y tiene algunos supuestos:\n\nVariables en escala de intervalo o razón\nRelación lineal entre ambas variables. Sí, asume que el cambio de una variable a otra es constante.\nNormalidad Ambas variables deben de tener una distribución aproximadamente normal, por lo que aquí sí nos interesa la distribución de nuestras variables.\nCada observación debe de tener el par de datos \\((v1, v2)\\).\n\nEstos supuestos, a estas alturas, son autoexplicativos. ¿Qué pasa si no cumplimos con alguno de los primeros tres? Podemos utilizar la alternativa no paramétrica: el coeficiente de correlación \\(\\rho\\) de Spearman. En este los supuestos son:\n\nVariables al menos en escala ordinal\nRelación monótona entre ambas variables; es decir, que la relación vaya en un solo sentido (conforme aumenta una aumenta la otra, o conforme aumenta la otra disminuye), independientemente de que la tasa de cambio de una a otra no sea constante.\nCada observación debe de tener el par de datos \\((v1, v2)\\)\n\nBastante más relajado, ¿no? Pero esto no quiere decir que solo debas de utilizar este coeficiente, utiliza el que más se ajuste a tus datos particulares.\n\n\n\n\n\n\nNota\n\n\n\nEn el Capítulo 12 vamos a hablar de las técnicas no paramétricas y sus ventajas y desventajas con respecto a las técnicas paramétricas.\n\n\nAhora bien, ¿cómo obtenemos estos coeficientes en R? Muy sencillo, con las funciones cor(X, Y) y cov(X, Y):\n\n\n\n\n\n\nNota\n\n\n\nEstos coeficientes son simétricos, por lo que asignar variables \\(x\\) y \\(y\\) como dependientes e independientes es un error. ¿Notaste que arriba todo lo puse en términos de \\(v1\\) y \\(v2\\)?\n\n\nPrimero, generemos un par de variables donde la segunda sea una función lineal de la primera:\n\ndf1 <- data.frame(v1 = -20:20)\ndf1[\"v2\"] <- (10+2*df1$v1)\ndf1\n\n\n\n  \n\n\n\nAhora obtengamos la covarianza:\n\ncovar <- cov(df1$v1, df1$v2)\ncovar\n\n[1] 287\n\n\nY el índice de correlación de Pearson:\n\ncorre <- cor(df1$v1, df1$v2)\ncorre\n\n[1] 1\n\n\nAquí queda también demostrado por qué la covarianza es tan difícil de interpretar por sí sola, pues en este caso con una relación lineal perfecta fue de 287, pero si cambias los valores de v1 de alguna manera vas a obtener otro valor, mientras que la correlación seguirá siendo 1. Gráficamente:\n\nggplot(data = df1, aes(x = v1, y = v2)) +\n  geom_point(colour = \"dodgerblue4\", alpha = 0.8) +\n  labs(title = \"Relación entre v2 y v1\") +\n  annotate(\"text\", x = 15, y = 0,\n           label = paste(\"R = \", round(corre, 2))) +\n  annotate(\"text\", x = 15, y = -5,\n           label = paste(\"Cov. = \", round(covar, 2))) +\n  see::theme_lucid()\n\n\n\n\n¿Qué pasa cuando nos alejamos de esta relación lineal?\n\ndf1[\"v3\"] <- (-df1$v1^2)\ncorre <- cor(df1$v1, df1$v3)\ncovar <- cor(df1$v1, df1$v3)\n\nggplot(data = df1, aes(x = v1, y = v3)) +\n  geom_point(colour = \"dodgerblue4\", alpha = 0.8) +\n  labs(title = \"Relación entre v2 y v1\") +\n  annotate(\"text\", x = 15, y = 0,\n           label = paste(\"R = \", round(corre, 2))) +\n  annotate(\"text\", x = 15, y = -50,\n           label = paste(\"Cov. = \", round(covar, 2))) +\n  see::theme_lucid()\n\n\n\n\nOtro ejemplo:\n\ndf1[\"v4\"] <- sin(df1$v1)\ncorre <- cor(df1$v1, df1$v4)\ncovar <- cor(df1$v1, df1$v4)\n\nggplot(data = df1, aes(x = v1, y = v4)) +\n  geom_point(colour = \"dodgerblue4\", alpha = 0.8) +\n  labs(title = \"Relación entre v2 y v1\") +\n  annotate(\"text\", x = 15, y = 3,\n           label = paste(\"R = \", round(corre, 2))) +\n  annotate(\"text\", x = 15, y = 2,\n           label = paste(\"Cov. = \", round(covar, 2))) +\n  expand_limits(y = c(5, -3)) +\n  see::theme_lucid()\n\n\n\n\nAunque todas estas relaciones pueden ser predichas, un coeficiente de correlación lineal no es capaz de capturarlas y, por definición, tampoco un modelo de regresión lineal. ¿Qué hacer en estos casos? Veremos algunas alternativas en el Capítulo 13. Ahora ejemplifiquemos el coeficiente de correlación de Spearman. Primero, y para dejar más claro el concepto, veamos la diferencia entre una relación monótona y una no monótona:\n\ndf2 <- data.frame(v1 = df1$v1[df1$v1 > 0], \n                  v2 = df1$v1[df1$v1 > 0]^2,\n                  mono = \"monótona\")\n\ndf2 <- rbind(df2, data.frame(v1= df1$v1, \n                             v2 = df1$v3, \n                             mono = \"no monótona\"))\n\nggplot(data = df2, aes(x = v1, y = v2)) +\n  geom_point(colour = \"dodgerblue4\", alpha = 0.8) +\n  facet_wrap(~mono, nrow = 2, scales = \"free_y\") +\n  see::theme_lucid() +\n  theme(aspect.ratio = 1/1.61)\n\n\n\n\nY ahora calculemos los coeficientes de correlación de Pearson y Spearman para la relación monótona. La diferencia fue de 0.3, lo cual no es muy grande, pero conforme nos empezamos a alejar de escenarios ideales el coeficiente de Pearson empieza a perder sensibilidad y confiabilidad.\n\npaste(\"Pearson = \",\n      round((cor(df2$v1[df2$mono == \"monótona\"],\n                 df2$v2[df2$mono == \"monótona\"], \n                 method = \"pearson\")), 2)\n      )\n\n[1] \"Pearson =  0.97\"\n\npaste(\"Spearman = \",\n      round((cor(df2$v1[df2$mono == \"monótona\"],\n                 df2$v2[df2$mono == \"monótona\"], \n                 method = \"spearman\")), 2)\n      )\n\n[1] \"Spearman =  1\"\n\n\nYa para cerrar, ¿son estos los únicos coeficientes de correlación? Para nada, tenemos también: \\(\\tau\\) de Kendall, \\(\\phi k\\) (Baak et al. (2019)), V de Cramer, Predictive Power Score y el coeficiente de Máxima Información (Reshef et al. (2011)). Te invito a que leas más sobre ellos, sus ventajas y sus desventajas. Predictive Power Score no es direccional, por ejemplo (Figura 11.6).\n\n\n\nFigura 11.6: Predictive Power Score en una relación parabólica\n\n\nPor fin llegamos al final de esta extensa (pero espero no aburrida) sesión. Espero que la hayas encontrado útil, y que te motive a hacer toda la chamba detrás de un modelo de predicción como lo es la regresión lineal simple."
  },
  {
    "objectID": "c11_rls.html#ejercicio",
    "href": "c11_rls.html#ejercicio",
    "title": "11  Modelo lineal",
    "section": "11.6 Ejercicio",
    "text": "11.6 Ejercicio\nAunque me encantaría que para esta sesión no hubiera ejercicio, es importante que practiques lo que discutimos aquí. El ejercicio que vas a realizar es realizar una regresión entre las variables LT (longitud total) y AM (altura máxima) de los datos de peces haemúlidos de los datos Haem.csv. Responde:\n\nLa regresión asume un cambio direccional entre las variables \\(X\\) y \\(Y\\); es decir, el cambio en una modifica a la otra. ¿Cuál es la variable dependiente? ¿LT o AM? ¿Por qué?\nCalcula la correlación entre ambas variables. ¿Qué coeficiente utilizas y por qué?\nRealiza la regresión lineal simple. ¿Qué método de ajuste utilizas y por qué?\nReporta los resultados de la regresión (incluyendo el gráfico correspondiente).\nRealiza la comprobación de los supuestos. ¿Es confiable el modelo para fines de predicción? ¿Y para interpretación?\n\n\n\n\n\nBaak M, Koopman R, Snoek H, Klous S. 2019. A new correlation coefficient between categorical, ordinal and interval variables with Pearson characteristics. ArXiV. DOI: 10.48550/arxiv.1811.11440.\n\n\nBreusch TS, Pagan AR. 1979. A simple test for heteroscedasticity and random coefficient variation. Econometrica 47:1287-1294.\n\n\nGerrodette T. 2011. Inference without significance: measuring support for hypotheses rather than rejecting them. Marine Ecology 32:404-418. DOI: 10.1111/j.1439-0485.2011.00466.x.\n\n\nReshef DN, Reshef YA, Finucane HK, Grossman SR, McVean G, Turnbaugh PJ, Lander ES, Mitzenmacher M, Sabeti PC. 2011. Detecting Novel Associations in Large Datasets. Science 334:1518-1524. DOI: 10.1126/science.1205438."
  },
  {
    "objectID": "s03_noparnolin.html#objetivo-de-aprendizaje",
    "href": "s03_noparnolin.html#objetivo-de-aprendizaje",
    "title": "Técnicas no paramétricas y modelación no lineal",
    "section": "Objetivo de aprendizaje",
    "text": "Objetivo de aprendizaje\nEn esta sección del curso te adentrarás en las caras “opuestas” a las técnicas que has revisado hasta ahora; es decir, explorarás casos en los cuales no puedes o debes de aplicar una prueba paramétrica, así como casos en los cuales no asumirás que tus variables tienen una relación lineal, ni tampoco necesitas cumplir con el supuesto de normalidad."
  },
  {
    "objectID": "s04_mv.html#objetivo-de-aprendizaje",
    "href": "s04_mv.html#objetivo-de-aprendizaje",
    "title": "Técnicas Multivariadas",
    "section": "Objetivo de aprendizaje",
    "text": "Objetivo de aprendizaje\nEn esta sección del curso llegarás al punto de máxima abstracción y te adentrarás en diversas técnicas que te permitirán obtener conclusiones a partir de datos multivariados. Comenzarás analizando las relaciones entre tus variables mediante matrices de varianzas/covarianzas, formarás agrupaciones, compararás las mediciones multivariadas entre grupos, realizarás clasificaciones y, por último, realizarás regresiones múltiples y verás cómo controlar la complejidad de tus modelos."
  },
  {
    "objectID": "c17_clasif.html#bosques-aleatorios",
    "href": "c17_clasif.html#bosques-aleatorios",
    "title": "17  Aprendizaje supervisado: Clasificación",
    "section": "17.1 Bosques aleatorios",
    "text": "17.1 Bosques aleatorios\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n\niris.plot <- ggplot(iris, aes(x = Species, y = Petal.Length)) +\n             geom_point(color = \"dodgerblue4\", alpha = 0.8) +\n             theme_bw()\niris.plot\n\n\n\n\nEl primer paso es separar nuestros datos en datos de entreenamiento y prueba, lo cual podemos hacer con las funciones initial_split, training y testing de la librería rsample:\n\nset.seed(123)\niris_split <- initial_split(iris, strata = Species)\niris_train <- training(iris_split)\niris_test <- testing(iris_split)\n\nLuego, construimos una receta para el preprocesamiento de los datos:\n\nLe damos a la receta (recipe()) la fórmula y los datos de entrenamiento\nAñadimos un paso para centrar los datos numéricos. Recuerda, solo estamos poniendo todos los valores numéricos en la misma escala\n\nEl objeto iris_prep sigue los pasos a seguir para el preprocesamiento de los datos (por ello receta) y obtiene los parámetros con los que se van a preprocesar los datos, mientras que juiced obtiene los datos procesados.\n\niris_rec <- recipe(Species~., data = iris_train) |> \n            step_center(all_numeric())\niris_prep <- iris_rec |> prep()\njuiced <- juice(iris_prep)\n\nAhora podemos especificar el modelo de bosques aleatorios, donde ajustaremos sus hiperparámetros:mtry (el número máximo de predictores por árbol), min_n (el número de observaciones necesarias para seguir dividiendo los datos) y trees (el número de árboles en el ensemble). Después especificamos que es un bosque para clasificación, y por último le indicamos que utilice la librería ranger para construir el bosque:\n\ntune_spec <- rand_forest(mtry = tune(),\n                         trees = tune(),\n                         min_n = tune()) |> \n             set_mode(\"classification\") |>\n             set_engine(\"ranger\")\n\nFinalmente, formamos un flujo de trabajo que contenga ambos pasos: la receta de preprocesamiento y el modelo\n\ntune_wf <- workflow() |> \n           add_recipe(iris_rec) |> \n           add_model(tune_spec)\n\nAhora sí, podemos ajustar nuestros hiper-parámetros. Primero, asignemos un confjunto de remuestreos para la validación cruzada, construidos a partir de los datos de entrenamiento:\n\nset.seed(0)\niris_fold <- vfold_cv(iris_train)\n\nLuego establezcamos el procesamiento en paralelo para hacer el procedimiento más rápido. En este primer proceso vamos a escoger 20 puntos aleatorios para guiar nuestra búsqueda y no abusar de la fuerza bruta para resolver el problema:\n\ndoParallel::registerDoParallel(cores = 6)\ntune_res <- tune_grid(tune_wf,\n                      resamples = iris_fold,\n                      grid = 20)\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\nVeamos nuestros AUCs:\n\ntune_res |> collect_metrics() |> \n            filter(.metric == \"roc_auc\") |> \n            select(mean, min_n, mtry, trees) |>\n            pivot_longer(min_n:trees,\n                         values_to = \"value\",\n                         names_to = \"parameter\") |> \n            ggplot(aes(value, mean, color = parameter)) +\n            geom_point(show.legend = F) +\n            facet_wrap(~parameter, scales = \"free_x\")"
  },
  {
    "objectID": "c19_glm.html#librerías",
    "href": "c19_glm.html#librerías",
    "title": "19  Modelos Lineales Generalizados",
    "section": "19.1 Librerías",
    "text": "19.1 Librerías\n\nlibrary(ggplot2)\nlibrary(stats4)\nlibrary(brms)\nlibrary(visreg)\nlibrary(pscl)\nlibrary(MASS)\nlibrary(MuMIn)"
  },
  {
    "objectID": "c19_glm.html#funciones-personalizadas",
    "href": "c19_glm.html#funciones-personalizadas",
    "title": "19  Modelos Lineales Generalizados",
    "section": "19.2 Funciones personalizadas",
    "text": "19.2 Funciones personalizadas\n\n# Tema personalizado\nblank_theme <- function(){\n  theme(panel.grid.minor = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.background = element_blank(),\n        axis.line = element_blank(),\n        aspect.ratio = 1/1.61,\n        axis.ticks = element_blank(),\n        text = element_text(colour = \"gray50\"),\n        legend.position = \"none\"\n        )\n}"
  },
  {
    "objectID": "c19_glm.html#problemas-familiares",
    "href": "c19_glm.html#problemas-familiares",
    "title": "19  Modelos Lineales Generalizados",
    "section": "19.3 Problemas familiares",
    "text": "19.3 Problemas familiares\nEn la sesión anterior mencionamos cómo utilizar combinaciones lineales de variables para predecir una variable continua, pero también recordarás que en la sesión de RLS hablamos de modificar el supuesto de la distribución de nuestros errores para obtener una mejor estimación, lo cual conforma una parte fundamental de los modelos lineales generalizados (GLMs).\nLa modificación puede ser tan “simple” (al menos en términos de intuición) como relajar el supuesto de normalidad, pero podemos también utilizar otras distribuciones que nos permitan modelar otro tipo de información. Bueno, hoy aterrizaremos esa última idea: La estructura principal de un GLM sigue siendo un modelo lineal, aunque nuestro supuesto de la distribución de errores no será exclusivamente normal, sino que puede tomar alguna otra familia de distribuciones, enlazada a nuestros datos con alguna función. En la sesión de hoy revisaremos:\n\nRegresiones robustas, expandiendo un poco la distribución t como distribución de errores y revisando una alternativa.\nFunciones de enlace y enlace inverso.\nRegresiones para conteos: Poisson, Binomial Negativa y sus variantes infladas en zeros.\nSelección de modelos; i.e., cómo seleccionar el mejor de un conjunto de modelos candidatos.\nRegresiones para clases: Regresión logística binomial y multinomial.\n\nOJO: Por practicidad obviaré la división datos de entrenamiento-prueba; sin embargo, es algo que SIEMPRE se debe de tener en cuenta"
  },
  {
    "objectID": "c19_glm.html#regresiones-robustas",
    "href": "c19_glm.html#regresiones-robustas",
    "title": "19  Modelos Lineales Generalizados",
    "section": "19.4 Regresiones robustas",
    "text": "19.4 Regresiones robustas\nLa idea de una regresión robusta la revisamos en la sesión de RLS; es decir, utilizar una distribución con colas más altas que una distribución normal para poder contender con el efecto de puntos extremos, pero expandamos esa idea. ¿Qué significa el “peso” de las colas de una distribución? Qué tanta densidad (o masa, para distribuciones discretas) de probabilidad está acumulada lejos de la tendencia central. En palabras más sencillas, una distribución con colas ligeras como la normal piensa que la probabilidad de tener valores lejos de la tendencia central es muy baja; por lo tanto, consiidera “todos” los datos como igual de importantes y reacciona moviendo la estimación. ¿No me crees? Veamos un caso extremo, utilizando el tercer conjunto de datos de el cuarteto de Anscombe:\n\n# Almacenados en R como anscombe\nansc <- read.csv(\"datos/anscombe.csv\")\nansc$x <- scale(ansc$x, center = TRUE, scale = FALSE)\nggplot(data = ansc, aes(x = x, y = y, color = conjunto)) +\n  geom_point() +\n  facet_wrap(~conjunto) +\n  blank_theme() +\n  labs(title = \"Cuarteto de Anscombe\") +\n  scale_color_manual(values = c(\"gray70\", \"gray70\", \"#1f77b4\", \"gray70\"))\n\n\n\n\nSi vemos su distribución de y notaremos que no es exactamente normal, debido a ese punto extremo en 12.5:\n\nansc_iii = ansc[ansc$conjunto == \"III\",]\nggplot(data = ansc_iii, aes(x = y)) +\n  geom_density(color = \"#1f77b4\", fill = NA) +\n  blank_theme() +\n  labs(title = \"Densidad de y del conjunto III de Anscombe\")\n\n\n\n\nAjustemos entonces nuestra regresión lineal simple:\n\nansciii_lm <- lm(y~x, data = ansc_iii)\nsummary(ansciii_lm)\n\n\nCall:\nlm(formula = y ~ x, data = ansc_iii)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   7.5000     0.3728  20.120 8.61e-09 ***\nx             0.4997     0.1179   4.239  0.00218 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\n\nGráficamente:\n\nggplot(data = ansc_iii,\n       aes(x = x, y = y)) +\n  geom_point(color = \"#1f77b4\") +\n  geom_smooth(method = \"lm\",\n              color = \"gray50\",\n              fill = NA) +\n  blank_theme() +\n  labs(title = \"RLS con supuesto de normalidad\")\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nNo se ve mal; sin embargo, es claro que el punto extremo está influenciando la estimación.Podemos aplicar algún criterio de detección de valores extremos (o deformar nuestros datos) para cumplir con el supuesto de normalidad; sin embargo, más que parchar nuestros datos, es preferible modificar nuestro modelo. Cambiemos entonces a una regresión con una verosimilitud t de Student:\n\nLLt <- function(b0, b1, df, sigma){\n  # Encontrar los residuales. Modelo a ajustar (lineal)\n  R = ansc_iii$y - ansc_iii$x*b1 - b0\n  \n  # Calcular la verosimilitud. Residuales con distribución t de student\n  \n  R = suppressWarnings(brms::dstudent_t(R, df = df,\n                                        mu = 0, sigma = sigma))\n  \n  # Sumar el logaritmo de las verosimilitudes\n  # para todos los puntos de datos.\n  -sum(R, log = TRUE)\n}\n\nmlet_fit <- mle(LLt, \n                start = list(b0 = 0, b1 = 0, df = 2, sigma = 1),\n                nobs = length(ansc_iii$y),\n                lower = list(b0 = -20, b1 = -12, df = 1, sigma = 0.1),\n                upper = list(b0 = 20, b1 = 12, df = 30, sigma = 10))\nsummary(mlet_fit)\n\nWarning in sqrt(diag(object@vcov)): NaNs produced\n\n\nMaximum likelihood estimation\n\nCall:\nmle(minuslogl = LLt, start = list(b0 = 0, b1 = 0, df = 2, sigma = 1), \n    nobs = length(ansc_iii$y), lower = list(b0 = -20, b1 = -12, \n        df = 1, sigma = 0.1), upper = list(b0 = 20, b1 = 12, \n        df = 30, sigma = 10))\n\nCoefficients:\n        Estimate   Std. Error\nb0     7.1141555  0.015785311\nb1     0.3453896  0.005152567\ndf    30.0000000 36.983300378\nsigma  0.1000000          NaN\n\n-2 log L: -81.09539 \n\n\nAhora empatemos ambas regresiones en un mismo gráfico:\n\ncoefs_t <- coef(mlet_fit)\nfitted <- coefs_t[1] + coefs_t[2]*ansc_iii$x\nggplot(data = ansc_iii,\n       aes(x = x, y = y)) +\n  geom_line(aes(x = x, y = fitted),\n            color = \"#ff7f0e\",\n            size = 1, alpha = 0.7) +\n  geom_smooth(method = \"lm\",\n              color = \"gray50\",\n              fill = NA) +\n  geom_point(color = \"#1f77b4\") +\n  labs(title = \"Regresión robusta\",\n       subtitle = \"OLS (gris) vs. t de Student (naranja)\") +\n  theme_bw()\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nEn este caso el punto extremo ya no influenció la estimación de la regresión, lo cual en la mayoría de los casos es algo deseable. Aunque esta es una forma de realizar una regresión robusta, existen otras. Una de ellas es modificar la función de pérdida, como por ejemplo con la Regresión con pérdida Huber. Los detalles matemáticos los dejaré para tu investigación, lo realmente importante es entender que los errores (residuales) son ponderados diferencialmente en función de su magnitud; es decir, se resta importancia a aquellos residuales que sean grandes y, de hecho, si están por encima de cierto límite, son descartados por completo. Su implementación es sumamente sencilla, pues lo único que tenemos que hacer es modificar lm por la función rlm de la librería MASS:\n\nrr.huber <- MASS::rlm(y~x, data = ansc_iii)\nsummary(rr.huber)\n\n\nCall: rlm(formula = y ~ x, data = ansc_iii)\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0049962 -0.0028591 -0.0007219  0.0028667  4.2421008 \n\nCoefficients:\n            Value     Std. Error t value  \n(Intercept)    7.1150    0.0013  5309.3547\nx              0.3457    0.0004   815.8284\n\nResidual standard error: 0.005248 on 9 degrees of freedom\n\n\n¿Notas algo interesante? Los resultados son los mismos que en la regresión t de Student, aunque aquí podemos ver la ponderación dada a cada punto:\n\nhweights <- data.frame(x = ansc_iii$x,\n                       resid = rr.huber$residuals,\n                       weight = rr.huber$w)\nhweights\n\n\n\n  \n\n\n\nGráficamente:\n\nggplot(data = ansc_iii,\n       aes(x = x, y = y)) +\n  geom_smooth(method = MASS::rlm,\n             color = \"#ff7f0e\",\n             size = 1, alpha = 0.7,\n             fill = \"blue\") +\n  geom_smooth(method = \"lm\",\n              color = \"gray50\",\n              fill = NA) +\n  geom_point(aes(color = hweights$weight)) +\n  labs(title = \"Regresión robusta\",\n       subtitle = \"OLS (gris) vs. Huber (naranja)\") +\n  scale_color_gradient(name = \"Peso\",\n                       low = \"firebrick\",\n                       high = \"#1f77b4\") +\n  theme_bw()\n\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nComo era de esperarse, los resultados son los mismos que los de la regresión t de Student. En la definición de un GLM tenemos tres elementos importantes:\n\nEl modelo lineal, que es el mismo en ambos casos.\nUna familia para el error, que no modificamos en la regresión Huber; sin embargo la familia sería Gaussiana (Normal).\nUna función de enlace (o enlace inverso), que en ambos casos sería una función de identidad; es decir, ninguna modificación para pasar de nuestros datos a la distribución del error.\n\nEs decir, que en un sentido amplio, ambas aproximaciones son GLMs, aunque usualmente nos referimos a GLMs cuando la distribución del error es diferente a una distribución normal. ¿Cuál aplicar? Ya que el resultado es el mismo, puedes escoger una u otra, solo ten en cuenta que la implementación por máxima verosimilitud de un modelo t de Student puede tener muchos bemoles al momento de optimizarse, además de que su salida es incompatible con algunas otras funciones, incluyendo el cálculo de los intervalos de confianza (para los coeficientes de la regresión Huber puedes utilizar la función confint.default(rr.huber))."
  },
  {
    "objectID": "c19_glm.html#funciones-de-enlace",
    "href": "c19_glm.html#funciones-de-enlace",
    "title": "19  Modelos Lineales Generalizados",
    "section": "19.5 Funciones de enlace",
    "text": "19.5 Funciones de enlace\nEste es un buen momento para hablar de un tema que a veces causa bastante confusión: las funciones de enlace o las funciones de enlace inverso. Estas son funciones “arbitrarias” (ojo a las comillas) que tienen una sola función (valga la redundancia): poner la salida de nuestro modelo lineal en los “requerimientos” de la familia de nuestro error. En el caso anterior, la distribución t es una distribución continua de probabilidad que está centrada en 0, como esperaríamos de nuestros residuales, por lo que la función de enlace es una función de identidad; es decir, no hacemos nada a la salida del modelo para poder obtener residuales continuos centrados en 0. Pero este no siempre es el caso; de hecho, las aplicaciones más comunes de GLM siempre requieren de algún enlace. ¿Y los enlaces inversos? Son simple y sencillamente el inverso de la función de enlace aplicados al lado contrario de la igualdad. Matemáticamente es más claro:\nEn un GLM con una función de enlace tendríamos la siguiente estructura para nuestro modelo lineal:\n\\[\nf(y) = \\beta_0 + \\beta_1*x\n\\] Es decir, modificamos la salida (\\(y\\)) de nuestro modelo lineal utilizando una función \\(y\\). Si es una función logarítmica, por ejemplo, se vería de la siguiente manera:\n\\[\nlog(y) = \\beta_0 + \\beta_1*x\n\\]\nPero obtendríamos exactamente lo mismo si utilizamos un poco de álgebra y resolvemos para \\(y\\), aplicando un exponencial a ambos lados de la igualdad:\n\\[\ne^{log(y))} = e^{(\\beta_0 + \\beta_1*x)} \\\\\n\\therefore \\\\\ny = e^{(\\beta_0 + \\beta_1 *x)}\n\\]\nEl apelativo “inversa” es simplemente para indicar el lado dónde se está aplicando el enlace. Habiendo dicho esto, vayamos a una de las aplicaciones más comunes para GLM: la regresión para conteos."
  },
  {
    "objectID": "c19_glm.html#regresiones-para-conteos",
    "href": "c19_glm.html#regresiones-para-conteos",
    "title": "19  Modelos Lineales Generalizados",
    "section": "19.6 Regresiones para conteos",
    "text": "19.6 Regresiones para conteos\nTe preguntarás qué tienen de especial los conteos, y la respuesta es muy simple: son valores enteros mayores o iguales a 0. Esto quiere decir que una distribución normal (t, o cualquier otra distribución continua) NO es adecuada para modelar los datos. ¿Qué hacemos? Utilizamos alguna distribución discreta que nos permita tratar con el número de veces en que algo sucede.\n\n19.6.1 Regresión Poisson\nMuy posiblemente esto te suene a ensayos de Bernoulli o ejercicios con distribuciones Poisson (¿cuántos autos rojos pasan en una hora por un punto determinado?, por ejemplo). Pues justamente podemos utilizar esa misma distribución (Poisson). Esta distribución tiene un par de peculiaridades. La primera es que asume que los eventos ocurren de manera independiente entre sí, a un intervalo fijo de espacio o tiempo. La segunda es que su único parámetro (\\(\\lambda\\)) representa tanto la media como la varianza de la distribución (más adelante hablaremos de las implicaciones de esto), por lo que DEBE ser positivo. ¿El problema? Nuestros residuales pueden ser negativos. ¿Qué podemos hacer? Aplicar una función de enlace inverso que nos permita restringir nuestro predictor a valores positivos, justo como la función exponencial, por lo que nuestro modelo se expresaría de la siguiente forma:\n\\[\n\\lambda = e^{(\\beta_0 + \\beta_1*x)} \\\\\ny \\sim Poisson(\\lambda)\n\\]\nPara aplicarlo resolvamos un problema en el cuál trataremos de predecir el número de peces capturados en un lago por un pescador, considerando el número de hijos y si llevan o no un camper:\n\ncsvurl <- \"https://stats.idre.ucla.edu/stat/data/fish.csv\"\nfish <- read.csv(csvurl)[,c(\"child\", \"camper\", \"count\")]\nhead(fish)\n\n\n\n  \n\n\n\nExploremos nuestros datos. Al tratarse de una variable discreta podemos, sin ningún problema, utilizar un gráfico de frecuencias:\n\nggplot(aes(x = count), data = fish) +\n  geom_bar(stat = \"count\", color = NA, fill = \"dodgerblue4\") +\n  blank_theme() +\n  labs(title = \"Frecuencia de peces capturados en un lago\",\n       x = element_blank(),\n       y = element_blank())\n\n\n\n\n¿Qué otro gráfico utilizarías? Por otra parte, habrás notado un par de cosas: a) hay una gran cantidad de ceros y b) tenemos algunos puntos “extremos”; i.e., algunos pescadores que tuvieron demasiada suerte y que capturaron demasiados peces en comparación con el resto. Este tipo de distribuciones no son extrañas en la naturaleza, y tienen un par de bemoles de los cuales hablaremos después. Por lo pronto, construyamos nuestro GLM. Para ello utilizaremos la función glm de R base, cuyo uso es sumamente similar al de la función lm, salvo que indicaremos la familia como un argumento adicional:\n\npoiss <- glm(count~., data = fish, family = \"poisson\")\nsummary(poiss)\n\n\nCall:\nglm(formula = count ~ ., family = \"poisson\", data = fish)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.7736  -2.2293  -1.2024  -0.3498  24.9492  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  0.91026    0.08119   11.21   <2e-16 ***\nchild       -1.23476    0.08029  -15.38   <2e-16 ***\ncamper       1.05267    0.08871   11.87   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2958.4  on 249  degrees of freedom\nResidual deviance: 2380.1  on 247  degrees of freedom\nAIC: 2723.2\n\nNumber of Fisher Scoring iterations: 6\n\n\nLa salida es muy similar a otras que hemos visto. Cómo llamamos a la función glm, un descriptor de los residuales, los coeficientes con sus respectivas pruebas de nulidad (después hablaremos de su interpretación), seguidas de algunos elementos propios de la función GLM. Primero tenemos una nota sobre un parámetro de dispersión, que se asumió como 1. Esto quiere decir que estamos asumiendo que la media es igual a la varianza, lo cual podemos tomar solo como un recordatorio para que revisemos dicho supuesto. Después tenemos información sobre la devianza del modelo. Podemos utilizar la devianza residual para realizar una prueba de bondad de ajuste para el modelo global. Esta es la diferencia entre la devianza del modelo y la máxima devianza de un modelo ideal, donde los valores predichos son idénticos a los observados (devianza nula). Por lo tanto, buscamos valores pequeños de la devianza residual. Dicha prueba podemos realizarla de la siguiente manera:\n\nwith(poiss, cbind(res.deviance = deviance,\n                  df = df.residual,\n                  p = pchisq(deviance, df.residual,\n                             lower.tail = F)))\n\n     res.deviance  df p\n[1,]      2380.12 247 0\n\n\nTenemos un valor de p sumamente pequeño, lo cual sugiere que el modelo no se encuentra bien ajustado. ¿Alguna idea de por qué? Como te imaginarás, tiene que ver con la distribución de nuestros datos, eso que mencionamos sobre muchos ceros y algunos pescadores con mucha suerte. De hecho, cada una de estas características es un problema en sí mismo, así que abordemoslos uno por uno. Una pregunta que puedes estarte haciendo es ¿y la función de enlace? Va implícita en la familia. En este caso, es una función de enlace logarítmica, que es el equivalente a la función de enlace inverso que revisamos antes.\n\nstr(poisson()[1:5])\n\nList of 5\n $ family  : chr \"poisson\"\n $ link    : chr \"log\"\n $ linkfun :function (mu)  \n $ linkinv :function (eta)  \n $ variance:function (mu)  \n\n\n\n\n19.6.2 Exceso de ceros: Regresión Poisson Inflada en Cero\nLa primera peculiaridad de nuestros datos es que hay una cantidad enorme de ceros. Aunque esto puede suceder de manera natural, la distribución Poisson no es capaz de contender adecuadamente con estos casos. Afortunadamente, hay una manera de extender el modelo Poisson para permitirnos arreglar esto. En su forma más fundamental, asumiremos que tenemos dos procesos:\n\nUno modelado con una distribución Poisson.\nUno generando ceros adicionales.\n\nEs decir, cuando hablemos de modelos “inflados en cero” estamos hablando de una situación en la que tenemos ceros “falsos” o, mejor dicho, extras a los ceros verdaderos que nos podemos encontrar. Veamos qué pasa al ajustar este modelo a nuestros datos. Para este modelo necesitaremos de la función zeroinfl() de la librería pscl:\n\nzi_poiss <- pscl::zeroinfl(count~child+camper, data = fish)\nsummary(zi_poiss)\n\n\nCall:\npscl::zeroinfl(formula = count ~ child + camper, data = fish)\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-1.2395 -0.8340 -0.4694 -0.1764 24.1051 \n\nCount model coefficients (poisson with log link):\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  1.64535    0.08278  19.877   <2e-16 ***\nchild       -0.77272    0.09103  -8.489   <2e-16 ***\ncamper       0.75526    0.09112   8.289   <2e-16 ***\n\nZero-inflation model coefficients (binomial with logit link):\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   0.0424     0.2426   0.175   0.8613    \nchild         1.0244     0.2200   4.656 3.22e-06 ***\ncamper       -0.7085     0.2926  -2.422   0.0155 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 10 \nLog-likelihood: -1025 on 6 Df\n\n\nLa salida es similar al caso anterior, solo tenemos coeficientes para la regresión logística para clasificar ceros verdaderos de falsos y los coeficientes del modelo Poisson sin el exceso de ceros; sin embargo, notarás que no hay ningún indicativo sobre si este modelo es mejor a nuestro modelo Poisson, por lo que podemos compararlos. Para ello podemos utilizar distintas alternativas: una prueba de Vuong (función vuong(mod_1, mod_2) de pscl) o utilizar una aproximación multi-modelo para la selección de modelos. Optaremos por esa última vía, la cual exploraremos a detalle más adelante. Por lo pronto, es suficiente que sepas que utilizaremos una medida llamada Criterio de Información de Akaike (AIC), y el mejor modelo será aquel que tenga el menor valor de AIC:\n\nAIC(poiss, zi_poiss)\n\n\n\n  \n\n\n\nComo era de esperarse, el modelo inflado en cero es un mejor candidato; sin embargo, tenemos un problema pendiente: nuestros pescadores muy suertudos.\n\n\n19.6.3 Sobre-dispersión: Regresión Binomial Negativa\nEsos pescadores muy suertudos pueden hacer lo mismo que nuestro punto extremo en el ejemplo de regresión robusta; es decir, jalar nuestras estimaciones hacia ellas y alejarlas de la estimación “real”, solo que aquí es un tanto diferente y tiene que ver con el supuesto de nuestra distribución Poisson: la media y la varianza son iguales. Este supuesto, evidentemente, no se sostiene cuando tenemos una dispersión muy grande de nuestros datos (varianza > media), lo cual genera el problema de la sobre dispersión de nuestro modelo (no de los datos). Una estrategia es cambiar nuestra verosimilitud a una distribución Binomial Negativa, la cual tiene un parámetro adicional a la distribución Poisson. Este parámetro modela, justamente, la dispersión de nuestros datos. Apliquemos entonces nuestra regresión binomial negativa:\n\nbineg <- MASS::glm.nb(count~., data = fish, trace = F)\nsummary(bineg)\n\n\nCall:\nMASS::glm.nb(formula = count ~ ., data = fish, trace = F, init.theta = 0.2552931119, \n    link = log)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.3141  -1.0361  -0.7266  -0.1720   4.0163  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   1.0727     0.2425   4.424 9.69e-06 ***\nchild        -1.3753     0.1958  -7.025 2.14e-12 ***\ncamper        0.9094     0.2836   3.206  0.00135 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(0.2553) family taken to be 1)\n\n    Null deviance: 258.93  on 249  degrees of freedom\nResidual deviance: 201.89  on 247  degrees of freedom\nAIC: 887.42\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  0.2553 \n          Std. Err.:  0.0329 \n\n 2 x log-likelihood:  -879.4210 \n\n\nNotarás que esta salida es prácticamente la misma que la que tuvimos en nuestro GLM Poisson, salvo que ahora nos da el valor del parámetro de sobredispersión. Te estarás preguntando: ¿Cómo sé si, en efecto, mis modelos están sobre-dispersos? Para eso podemos utilizar una prueba de razón de verosimilitud, en la cual compararemos la verosimilitud de ambos modelos (binomial negativa y Poisson) y veremos si se ajustan a la misma distribución; es decir, la prueba de razón de verosimilitud es una prueba de bondad de ajuste, con distribución \\(\\chi^2\\). ¿Qué es lo que estamos comparando? Si el parámetro adicional ayuda a que el ajuste del modelo mejore significativamente. Para aplicarla podemos utilizar la función odTest(mod_bn) de la librería pscl:\n\npscl::odTest(bineg)\n\nLikelihood ratio test of H0: Poisson, as restricted NB model:\nn.b., the distribution of the test-statistic under H0 is non-standard\ne.g., see help(odTest) for details/references\n\nCritical value of test statistic at the alpha= 0.05 level: 2.7055 \nChi-Square Test Statistic =  1837.7652 p-value = < 2.2e-16 \n\n\nA ojo de buen cubero era más que evidente que nuestro modelo estaba sobre disperso, por lo que estos resultados no son sorprendentes. Algo que puedes pensar es “si estoy comparando qué modelo está mejor ajustado, ¿puedo entonces utilizar el AIC?” Y la respuesta es, por supuesto:\n\nAIC(poiss, zi_poiss, bineg)\n\n\n\n  \n\n\n\nY los resultados son, como debe de ser, consistentes. Llegados a este punto podrías preguntarme: “Ok, Arturo, ya corregimos para el exceso de ceros y para la sobre dispersión, pero lo hicimos de manera independiente. ¿Hay alguna manera de hacer ambas cosas al mismo tiempo?” En efecto, y es justo lo siguiente que vamos a revisar.\n\n\n19.6.4 Exceso de ceros y sobre dispersión: Regresión Binomial Negativa Inflada en Cero\nY, justamente, es una combinación de ambas; es decir, utilizaremos una distribución de error binomial negativa inflada en cero. La lógica es, entonces, una combinación de ambas aproximaciones; es decir, modelaremos a los ceros verdaderos y luego construiremos el modelo de regresión binomial negativa. Para hacerlo utilizaremos la función zeroinfl() que vimos antes, solo que cambiaremos la familia a negbin:\n\nzi_bineg <- pscl::zeroinfl(count~., data = fish, dist = \"negbin\")\nsummary(zi_bineg)\n\n\nCall:\npscl::zeroinfl(formula = count ~ ., data = fish, dist = \"negbin\")\n\nPearson residuals:\n      Min        1Q    Median        3Q       Max \n-0.512182 -0.497136 -0.325130 -0.003367 13.978082 \n\nCount model coefficients (negbin with log link):\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   1.0515     0.2700   3.895 9.84e-05 ***\nchild        -0.9113     0.2851  -3.196  0.00139 ** \ncamper        0.7976     0.3054   2.611  0.00902 ** \nLog(theta)   -1.2960     0.1316  -9.849  < 2e-16 ***\n\nZero-inflation model coefficients (binomial with logit link):\n            Estimate Std. Error z value Pr(>|z|)\n(Intercept)  -11.499     55.699  -0.206    0.836\nchild         10.483     55.659   0.188    0.851\ncamper        -9.501     55.663  -0.171    0.864\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nTheta = 0.2736 \nNumber of iterations in BFGS optimization: 84 \nLog-likelihood: -434.9 on 7 Df\n\n\nFinalmente, podemos comparar nuestros cuatro modelos candidatos para encontrar el más adecuado:\n\nAIC(poiss, zi_poiss, bineg, zi_bineg)\n\n\n\n  \n\n\n\nVemos que los AIC de los modelos con distribución de error binomial negativa tienen los menores valores; por lo tanto seleccionaremos a alguno de los dos. ¿Cuál? En la siguiente sección hablaremos de las peculiaridades. Por lo pronto, sigamos con nuestro criterio de seleccionar el que tenga el menor AIC, que corresponde a la distribución binomial negativa inflada en cero. Ahora sí, podemos interpretar nuestros coeficientes.\n\n\n19.6.5 Interpretación\nDesafortunadamente, la interpretación no es tan simple como en la RLM o RLS, debido a la función de enlace que utilizamos.\nPara facilitarnos la existencia, planteemos un modelo Poisson con un solo predictor, y la función de enlace logarítmica:\n\\[\nY \\sim Poisson(\\theta) \\\\\nlog(\\theta) = \\alpha + \\beta x \\\\\n\\therefore \\\\\n\\theta = e^{\\alpha + \\beta x}\n\\]\nPero, por las leyes de los exponentes, podemos reescribir la última ecuación como:\n\\[\n\\theta = e^{a}e^{\\beta x}\n\\] Esto quiere decir que los coeficientes no son aditivos, sino multiplicativos:\n\nIntercepto: \\(e^\\alpha\\), valor de \\(\\theta\\) cuando \\(x = 0\\). Si este parámetro es o no de interés depende totalmente del problema.\nPendiente(s): \\(e^\\beta\\).\n\n\nSi \\(\\beta = 0\\), entonces \\(e^\\beta = 1\\); es decir, no hay un efecto del predictor.\nSi \\(\\beta > 0\\), entonces \\(e^\\beta > 1\\); es decir, el predictor incrementa el valor de \\(\\theta\\) a una tasa de \\(e^\\beta\\) por cada incremento unitario en \\(x\\)\nSi \\(\\beta < 0\\), entonces \\(e^\\beta < 1\\); es decir, el predictor disminuye el valor de \\(\\theta\\) a una tasa de \\(e^\\beta\\) por cada incremento unitario en \\(x\\).\n\nRecuperemos los coeficientes de nuestra regresión binomial negativa inflada en cero y exponenciémoslos:\n\nexp(coef(zi_bineg)[1:3])\n\ncount_(Intercept)       count_child      count_camper \n        2.8620079         0.4019951         2.2202680 \n\n\nPongamos atención solo a aquellos coeficientes con count_, pues son los que realmente nos interesan. La interpretación entonces sería:\n\nIntercepto: Cuando el pescador no tiene hijos y no lleva un camper, el promedio de peces capturados es de 2.86\nPendientes:\n\n\nChild: El promedio de peces capturados disminuye 1-0.4 veces por cada hijo adicional.\nCamper: Si el pescador tiene un camper, el promedio de peces capturados incrementa 2.2 veces.\n\nCorolario: La interpretación depende totalmente de la función de enlace que utilicemos, y siempre es necesario aplicar el enlace para poder interpretarlos.\nPara construir un gráfico podemos generar una línea con valores predichos o, mejor dicho, dos líneas: una para cada nivel de camper (en la sección de Extras encontrarás cómo funcionan los predictores categóricos en modelos de regresión).\n\nfish$pred <- predict(zi_bineg, type = \"response\")\n\nggplot(data = fish, aes(x = child, y = count,\n                        color = factor(camper))) +\n  geom_point() +\n  geom_line(aes(y = pred)) +\n  labs(title = \"GLM Binomial negativo\",\n       x = \"Número de hijos\",\n       y = \"Peces capturados\") +\n  scale_color_discrete(name = \"Camper\",\n                       labels = c(\"NO\", \"SI\")) +\n  scale_y_continuous(limits = c(0, 12),\n                     label = scales::label_comma(accuracy = 1))\n\nWarning: Removed 16 rows containing missing values (geom_point).\n\n\n\n\n\nO utilizando visreg:\n\npartial_plots <- visreg::visreg(bineg,\n                                scale = \"response\",\n                                ylab = \"Capturas\",\n                                gg = TRUE)\n\nfor (plt in partial_plots) {\n  plot(plt + blank_theme() +\n         scale_y_continuous(limits = c(0, 12),\n                            label = scales::label_comma(accuracy = 1)) +\n         scale_x_discrete(limits = c(0, max(plt$data$x))) +\n         labs(x = plt$labels$x))\n}\n\nWarning: Continuous limits supplied to discrete scale.\nDid you mean `limits = factor(...)` or `scale_*_continuous()`?\nContinuous limits supplied to discrete scale.\nDid you mean `limits = factor(...)` or `scale_*_continuous()`?\n\n\n\n\n\n\n\n\nAntes de cambiar la hoja, es necesario aclarar que aquí NO hay supuesto de normalidad. ¿Cuál es el punto de cambiar la distribución del error si vamos a seguir casados buscando normalidad estadística?\nY ahora podemos hablar de la selección de modelos."
  },
  {
    "objectID": "c19_glm.html#selección-de-modelos",
    "href": "c19_glm.html#selección-de-modelos",
    "title": "19  Modelos Lineales Generalizados",
    "section": "19.7 Selección de modelos",
    "text": "19.7 Selección de modelos\nEl tener varios varios modelos candidatos no es algo extraño, entonces es necesario tener algún tipo de criterio que nos permita comparar entre ellos. Una aproximación es la que hemos utilizado hasta el momento; es decir, utilizar el Criterio de Información de Akaike (AIC) y seleccionar el menor. ¿Qué es el AIC y con qué se come?\n\n19.7.1 Criterio de Información de Akaike\nEl AIC es un criterio basado en teoría de la información, particularmente en la divergencia Kullback-Leibler. Ese detalle matemático va más allá del alcance de este curso; sin embargo, podemos entender que está basado en la verosimilitud de un modelo dado; es decir, qué tan verosímil es que ese modelo haya generado los datos. Un detalle es que, como hemos visto en sesiones anteriores, el “ajuste” de un modelo es directamente proporcional a su complejidad (al menos vs. los datos de entrenamiento). Es, entonces, necesario penalizar de alguna manera el número de parámetros en el modelo, para no comernos un “gol” con un modelo excesivamente complejo. Puesto en una ecuación, el AIC queda de la siguiente manera:\n\\[\nAIC = -2ln(L) + 2k\n\\]\nDonde \\(L\\) es la verosimilitud del modelo y \\(k\\) el número de parámetros. Si nuestro modelo tiene un gran número de parámetros, el valor de AIC se hará más grande, mientras que, si tiene un menor número, se hará más pequeño. ¿Qué nos dice un AIC en sí mismo? NADA, absolutamente nada. Si yo te digo que un modelo tiene un AIC de 800 no puedes saber si es bueno o malo, pues no hay una referencia. Esto nos lleva a hablar sobre algunas consideraciones que debemos de tener al utilizar el AIC:\n\nValores más bajos indican modelos más parsimoniosos.\nEs una medida relativa de la parsimonia de un modelo, por lo que solo tiene sentido cuando comparamos AIC para hipótesis (modelos) alternativas.\nPodemos comparar modelos no anidados. De hecho, podríamos comparar un modelo lineal con uno no lineal.\nLas comparaciones son válidas SOLO para modelos ajustados con los mismos valores de respuesta; i.e., mismos valores de \\(y\\).\nComparar muchos modelos con AIC es una mala idea, pues caemos en el mismo problema de las comparaciones múltiples, donde podemos encontrar por azar un modelo con el valor más bajo de AIC, cuando en realidad no es el modelo más apropiado.\nPara variar, cuando tratamos con tamaños de muestra pequeños (n/k < 40) el AIC pierde confiabilidad, por lo que hay que aplicar una corrección:\n\n\\(AIC_c = AIC + \\frac{2k(k+1)}{n-k-1}\\)\nDado que conforme incrementa n, el \\(AIC_c\\) se aproxima al \\(AIC\\), es una buena idea utilizar \\(AIC_c\\).\n\nPodemos encontrar múltiples modelos que tengan AICs similares, esto solo sugiere que estas hipótesis alternativas tienen soportes similares. ¿Qué tanto es tantito? Esa respuesta es un poco más compleja, y requiere que presentemos el \\(\\Delta{AIC}\\) (también lo puedes encontrar como \\(\\Delta_i\\)):\n\n\\(\\Delta AIC = AIC_i - AIC_{min}\\); es decir, la diferencia de cada AIC respecto al valor mínimo de AIC entre los modelos candidatos. Esta transformación forza al “mejor” modelo a tener un \\(\\Delta AIC = 0\\), y representa la pérdida de información si utilizamos un modelo candidato \\(m_i\\) en vez de \\(m_{min}\\).\nModelos con un \\(\\Delta_i \\leq 2\\) tienen soporte substancial (evidencia),\nModelos con un \\(4 \\leq \\Delta_i \\leq 7\\) tienen considerablemente menos soporte y\nModelos con \\(\\Delta_i > 10\\) carecen, escencialmente, de soporte.\n\n\nRecuperemos nuestra comparación anterior, calculemos los \\(AIC_c\\) (con la función AICc de la librería MuMIn) y calculemos los \\(\\Delta_i\\). Como mencionábamos antes, los modelos con distribuciones binomial negativas son los que tienen el mayor soporte, mientras que los Poisson carecen de cualquier soporte (dados estos datos y modelos candidatos). También podemos ver que pesa más la sobredispersión que el exceso de ceros, pues el modelo inflado en cero es marginalmente mejor que aquel no inflado.\n\nAICs <- MuMIn::AICc(poiss, zi_poiss, bineg, zi_bineg)\nAICs$Delta <- AICs$AIC - min(AICs$AIC)\nAICs"
  },
  {
    "objectID": "c19_glm.html#regresiones-para-clases",
    "href": "c19_glm.html#regresiones-para-clases",
    "title": "19  Modelos Lineales Generalizados",
    "section": "19.8 Regresiones para clases",
    "text": "19.8 Regresiones para clases\nEn la sesión de clasificación hablamos sobre los problemas de clasificación, y aplicamos una regresión logística; sin embargo, no entramos en detalles. Aprovechemos, entonces, para revisar un poco más a profundidad la intuición detrás de ella.\n\n19.8.1 Regresión logística binaria\nCuando la revisamos anteriormente, definimos a la regresión logística como un modelo de RL al cual aplicaríamos una función logística, lo que nos permite restringir nuestra salida al intervalo \\([0,1]\\) y, por lo tanto, predecir la probabilidad de pertenencia a una clase dada. Esa definición es correcta para resumir lo más posible la técnica; sin embargo, los detalles son un poco más complejos.\nLa regresión logística forma parte de los GLMs; por lo tanto, consta de un predictor lineal, una familia de distribución del error y una función de enlace. La familia de distribución del error es binomial; es decir, está en términos de la probabilidad de éxitos vs. la probabilidad de fracasos. Es por esto que la regresión logística tradicional solo nos permite clasificar entre dos clases. Esto ya lo sabíamos, simplemente lo estamos formalizando, lo que tiene un poco más de detalles es la función de enlace: la función logística:\n\\[\nlogit(z) = \\frac{1}{1 + e^{-z}}\n\\]\nEsta función tiene la peculiaridad de que, independientemente de los valores de \\(z\\) (el predictor lineal), el resultado siempre estará contenido entre 0 y 1, el cual es, convenientemente, el mismo que el dominio del parámetro \\(p\\) de la distribución binomial (la probabilidad de éxito). Expresado matemáticamente:\n\\[\n\\theta = logistic(\\alpha + \\beta x)\n\\]\n\\[\ny \\sim Binom(\\theta)\n\\]\nApliquemos entonces una regresión logística para clasificar entre versicolor y virginica de la base iris, solo para ilustrar cómo interpretar los coeficientes. Primero, filtremos los datos:\n\niris_dat <- iris\niris_dat <- subset(iris,\n                   Species == \"versicolor\" | Species == \"virginica\")\n\nAhora ajustemos el modelo. No te olvides de dividir en entrenamiento-prueba (o, mejor aún, realizar validación cruzada), considerar si es necesario escalar los datos, y que puedes también puedes entrenar los glm utilizando la función caret::train(). Por practicidad, hagámoslo directamente.\n\nlogit_reg <- glm(Species~., data = iris_dat, family = \"binomial\")\nsummary(logit_reg)\n\n\nCall:\nglm(formula = Species ~ ., family = \"binomial\", data = iris_dat)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-2.01105  -0.00541  -0.00001   0.00677   1.78065  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)  \n(Intercept)   -42.638     25.707  -1.659   0.0972 .\nSepal.Length   -2.465      2.394  -1.030   0.3032  \nSepal.Width    -6.681      4.480  -1.491   0.1359  \nPetal.Length    9.429      4.737   1.991   0.0465 *\nPetal.Width    18.286      9.743   1.877   0.0605 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 138.629  on 99  degrees of freedom\nResidual deviance:  11.899  on 95  degrees of freedom\nAIC: 21.899\n\nNumber of Fisher Scoring iterations: 10\n\n\nY obtengámos los gráficos parciales, utilizando la función visreg(mod, scale = \"response\"), donde mod es el modelo aujustado:\n\npartial_plots <- visreg::visreg(logit_reg, scale = \"response\",\n                                ylab = \"P(Especie)\",\n                                line.par = c(col = \"dodgerblue4\"),\n                                fill.par = c(fill = \"gray90\"),\n                                gg = TRUE)\n\nfor (plt in partial_plots) {\n  plot(plt +\n         scale_y_continuous(n.breaks = 3,\n                            labels = c(\"versicolor\", 0.5, \"virginica\")) +\n         theme_bw())\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\nEsto ya lo conocíamos, pero ¿cómo interpretamos los coeficientes? Antes de pasar a eso, es necesario que pongamos atención al argumento scale de visreg, el cual indicamos como response. Esto lo que hizo fue poner nuestra salida en lo que nos interesa: la probabilidad de pertenencia a una especie, dadas las medidas de cada variable. Veamos qué pasa si retiramos ese argumento:\n\npartial_plots <- visreg::visreg(logit_reg, ylab = \"log-odds(Especie)\", gg = TRUE)\n\nfor (plt in partial_plots) {\n  plot(plt + blank_theme())\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\nAhora nuestros gráficos están en la escala de nuestro modelo lineal; es decir, con response estamos introduciendo la función de enlace y graficando el GLM completo; sin embargo, esto no está presente en los coeficientes “crudos” arrojados por la función GLM, por lo que toca aplicar el álgebra correspondiente:\nEl modelo básico es:\n\\[\n\\theta = logistic(\\alpha + \\beta x)\n\\]\nEl inverso de la función logística es la función logit, dada por:\n\\[\nlogit(z) = log \\left( \\frac{z}{1-z} \\right)\n\\]\nPor lo que si tomamos la primera ecuación y aplicamos la función logit a ambos términos, obtenemos esta ecuación:\n\\[\nlogit(\\theta) = \\alpha + \\beta x\n\\]\nO, de manera equivalente:\n\\[\nlog \\left( \\frac{\\theta}{1-\\theta} \\right) = \\alpha + \\beta x\n\\]\nAhora, recordemos que \\(\\theta\\) en nuestro modelo es \\(p(y = 1)\\) (la probabilidad de “éxito”, o de ser virginica):\n\\[\nlog \\left(\\frac{p(y = 1)}{1 - p(y = 1)} \\right) = \\alpha + \\beta x\n\\]\nLa cantidad \\(\\frac{p(y = 1)}{1 - p(y = 1)}\\) se conoce como los odds, que representan la probabilidad de éxito sobre la probabilidad de fracaso. Mientras que la probabilidad de obtener 2 al lanzar un dado es de 1/6, los odds para el mismo evento son \\(\\frac{1/6}{5/6} \\approx 0.2\\), o un éxito a cinco fracasos. En una regresión logística, \\(\\beta\\) representa el incremento en log-odds por incremento unitario en \\(x\\), no en la probabilidad de pertenencia a una clase, aunque la relación entre odds y probabilidad es monótona; es decir, conforme incrementa una, la otra también.\n\n\n19.8.2 Regresión logística multinomial\nAl igual que en el caso anterior, simplemente extenderemos aquellos detalles que no se aterrizaron por completo, particularmente el utilizar una red neuronal como análogo a una regresión logística multinomial. Como acabamos de ver, una regresión logística binaria nos permite predecir la probabilidad de éxito; i.e., de pertenecer a una sola clase. ¿Cómo lo extendemos a más de dos clases? Podemos construir modelos una clase vs. las demás, podemos utilizar una regresión softmax, o podemos utilizar una red neuronal. Una red neuronal está formada por capas, las cuales están conectadas entre sí tal cual neuronas:\n\n\n\nRed neuronal\n\n\nTenemos una capa de entrada, correspondiente a nuestros valores, seguida de una o más capas ocultas, compuestas por neuronas (perceptrones) que tienen funciones de activación, las cuales están conectadas por constantes multiplicadoras (pesos o weights) a las cuales se les añade una constante (sesgo o bias), cuyo resultado, finalmente, se envía a la capa de salida \\(y\\) (nuestras clases objetivo), resultando en la siguiente forma:\n\\[\ny = f(bias + \\sum(weight*input))\n\\]\nGráficamente:\n\n\n\nNeurona\n\n\n¿Suena familiar? Con solo una capa oculta y una función (\\(f\\)) de identidad tendríamos un modelo lineal cualquiera, solo que se ajusta mediante descenso estocástico de gradiente (fuera de esta discusión) en vez de mínimos cuadrados o máxima verosimilitud. Si esa función \\(f\\) la hacemos una función sigmoide (logística), tenemos entonces una regresión logística para más de dos clases. En un sentido estricto, esta aproximación no es un GLM (no tenemos una familia de distribución del error), pero se puede considerar una generalización a más de dos clases. Algo importante a tener en cuenta es que al ajustar este modelo, R toma una clase como referencia, para la cual no otorga los coeficientes. En este caso, es setosa.\n\nlogit_mult <- caret::train(form = Species~.,\n                          data = iris,\n                          method = \"multinom\",\n                          trace = F)\n\nLoading required package: lattice\n\nsummary(logit_mult)\n\nCall:\nnnet::multinom(formula = .outcome ~ ., data = dat, decay = param$decay, \n    trace = ..1)\n\nCoefficients:\n           (Intercept) Sepal.Length Sepal.Width Petal.Length Petal.Width\nversicolor    1.444874   -0.4096645   -2.179216     2.772008  -0.3107031\nvirginica    -2.860780   -2.5296923   -4.150196     5.878699   4.5107445\n\nStd. Errors:\n           (Intercept) Sepal.Length Sepal.Width Petal.Length Petal.Width\nversicolor    9.430403     2.847250    2.409686     2.832649    5.829681\nvirginica    10.247224     3.044536    2.741261     3.078484    6.076775\n\nResidual Deviance: 52.0787 \nAIC: 72.0787 \n\n\n¿Es un problema? Sí y no. Usualmente solo nos interesa saber qué variables son más importantes para la clasificación; es decir, qué variables son más “diferentes” entre nuestras clases, para lo cual podemos utilizar la función VarImp:\n\ncaret::varImp(logit_mult, scale = F)\n\nmultinom variable importance\n\n             Overall\nPetal.Length   8.651\nSepal.Width    6.329\nPetal.Width    4.821\nSepal.Length   2.939\n\n\nEsta importancia de variables está dada por la suma de absolutos de los coeficientes para una variable (ver lecturas recomendadas para más detalles). Una manera más fácil de interpretarlos es utilizando los valores escalados con respecto al valor máximo:\n\ncaret::varImp(logit_mult)\n\nmultinom variable importance\n\n             Overall\nPetal.Length  100.00\nSepal.Width    59.36\nPetal.Width    32.95\nSepal.Length    0.00\n\n\nEsto sería todo para esta clase de GLM, aunque no quiere decir que sean los únicos. Si te interesa modelar el tiempo entre eventos puedes utilizar un modelo Gamma, puedes cambiar la relación entre la media y la varianza de la regresión para conteos utilizando un modelo Quasi-Poisson en vez de un modelo con distribución binomial negativa (ver lecturas recomendadas), entre otros."
  },
  {
    "objectID": "c19_glm.html#despedida",
    "href": "c19_glm.html#despedida",
    "title": "19  Modelos Lineales Generalizados",
    "section": "19.9 Despedida",
    "text": "19.9 Despedida\nCon esto llegamos al final del curso. Espero que el contenido haya sido de tu agrado, que te hayas llevado algo y, sobre todo, que lo aprendido te sea útil. En este momento cuentas con bases sólidas para adentrarte más en cualquiera de los temas aquí vistos, e incluso incursionar en otros temas. Una recomendación personal es revisar un paradigma de inferencia diferente: la Inferencia Bayesiana. Si te agradó mi manera de explicar, te recomiendo ampliamente que esperes mi curso “Introducción a la Inferencia Bayesiana”, que estará disponible también aquí en Dr. Plancton en los próximos meses. Te recomiendo también adentrarte más en el área del aprendizaje automatizado, pues es un mundo con un potencial enorme de aplicación a problemas biológicos. Si te interesaría un curso más enfocado a eso, házmelo saber y es posible organizarlo con algunos miembros del equipo de Dr. Plancton. Recuerda que siempre puedes contactarme en el servidor de Discord, y que siempre tendrás acceso a las actualizaciones que se hagan al curso.\nTe deseo lo mejor, hoy y siempre."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Referencias",
    "section": "",
    "text": "Armhein V, Greenland S, McShane B. 2019. Scientists rise up against\nstatistical significance. Nature 567:305–307. DOI: 10.1038/d41586-019-00857-9.\n\n\nBaak M, Koopman R, Snoek H, Klous S. 2019. A new correlation coefficient\nbetween categorical, ordinal and interval variables with\nPearson characteristics. ArXiV. DOI: 10.48550/arxiv.1811.11440.\n\n\nBreusch TS, Pagan AR. 1979. A simple test for\nheteroscedasticity and random coefficient variation.\nEconometrica 47:1287–1294.\n\n\nCairo A. 2012. The functional art. Berkeley, USA: New Riders,\nPearson Education.\n\n\nGerrodette T. 2011. Inference without significance: Measuring support\nfor hypotheses rather than rejecting them. Marine Ecology\n32:404–418. DOI: 10.1111/j.1439-0485.2011.00466.x.\n\n\nHaig BD. 2010. What is a spurious corelation? Understanding\nStatistics 2:125–132. DOI: 10.1207/S15328031US0202_03.\n\n\nHöfer T, Przyrembel H, Verleger S. 2004. New evidence for the\ntheory of the stork. Paediatric and Perinatal Epidemiology\n18:88–92.\n\n\nKnuth DE. 1984. Literate programming. The Computer Journal\n27:97–111. DOI: 10.1093/comjnl/27.2.97.\n\n\nMatloff N. 2020. Teaching\nR in a kinder, gentler, more effective manner: Teach\nbase-R, not just the tidyverse.\n\n\nR Core Team. 2022. R: A\nlanguage and environment for statistical computing. Vienna,\nAustria: R Foundation for Statistical Computing.\n\n\nReshef DN, Reshef YA, Finucane HK, Grossman SR, McVean G, Turnbaugh PJ,\nLander ES, Mitzenmacher M, Sabeti PC. 2011. Detecting novel associations\nin large datasets. Science 334:1518–1524. DOI: 10.1126/science.1205438.\n\n\nRougier NP, Droettboom M, Bourne PE. 2014. Ten simple rules for better\nfigures. PLoS computational biology 10:e1003833–7. DOI: 10.1371/journal.pcbi.1003833.\n\n\nSavage LJ. 1954. The foundations of statistics. John Wiley\n& Sons.\n\n\nSawyer SF. 2013. Analysis of Variance: The\nFundamental Concepts. Journal of Manual & Manipulative\nTherapy 17:27E–38E. DOI: 10.1179/jmt.2009.17.2.27e.\n\n\nSies H. 1988. A new parameter for sex education.\nNature 332:495.\n\n\nTufte E. 1983. The visual display of quantitative information.\nCheshire, Connecticut: Graphics Press.\n\n\nWilkinson L. 2005. The grammar of graphics. USA: Springer.\n\n\nZar JH. 2010. Biostatistical Analysis. Prentice\nHall.\n\n\nZuur AF, Ieno EN, Walker N, Saveliev AA, Smith GM. 2009. Mixed effects models and extensions in ecology with\nR. Springer. DOI: 10.1007/978-0-387-87458-6."
  }
]