[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bioestadística Aplicada con R y RStudio",
    "section": "",
    "text": "Prefacio\n¡Hola! Te doy la bienvenida al curso Bioestadística Aplicada con R y RStudio.\nEste es el libro de acompañamiento del curso. Aquí encontrarás tanto la teoría como el código que se aborda en el curso, dispuestos en una manera que facilita su lectura, y cuyo objetivo es simplemente proveer una versión lista para ser revisada en cualquier momento, sin necesidad de iniciar RStudio.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "index.html#objetivos-de-aprendizaje",
    "href": "index.html#objetivos-de-aprendizaje",
    "title": "Bioestadística Aplicada con R y RStudio",
    "section": "Objetivos de aprendizaje",
    "text": "Objetivos de aprendizaje\nEl objetivo de este curso es que seas capaz no solo de implementar distintas técnicas de análisis de datos utilizando R, sino que también puedas ser crítico con tus resultados y que minimices, en la medida de lo posible, el sesgo algorítmico. En este curso aprenderás los fundamentos detrás de las pruebas vistas, en donde abordaremos la teoría desde un punto de vista práctico, buscando que puedas formarte una intuición propia. También trataremos de desmitificar el valor de p y, sobre todo, cómo no interpretarlo. En las partes más abstractas te adentrarás en el aprendizaje automatizado, y verás que hay vida más allá del \\(R^2\\) (regresiones) y la exactitud (clasificaciones). Adicionalmente, y no por ello menos importante, te adentrarás en la visualización de datos y aprenderás a realizar reportes que faciliten compartir y leer tu trabajo.\nUno de los errores más comunes al enseñar estadística con R es tratar de enseñar ambas cosas al mismo tiempo. En este curso, R es solo un medio y no un fin; es decir, no es un curso de programación en R tanto como es un curso de ciencia de datos aplicada; sin embargo, hay una amplia introducción al lenguaje al inicio, y espero que el explicar línea por línea el código te permita familiarizarte con el lenguaje. Dicho esto, el curso fue diseñado para que personas con nulo o muy poco conocimiento de programación en general puedan seguirlo, y siempre podrás contactarnos en caso de que no hayamos explicado algo adecuadamente.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "index.html#discord-y-acceso-al-material",
    "href": "index.html#discord-y-acceso-al-material",
    "title": "Bioestadística Aplicada con R y RStudio",
    "section": "Discord y acceso al material",
    "text": "Discord y acceso al material\nSi bien es cierto que puedes utilizar y acceder a todo el material del curso desde esta página, te recomiendo encarecidamente unirte al servidor de Discord utilizando tu enlace único (enviado a tu correo al registrarte), pues ahí podrás interactuar no solamente con tus compañeros y compañeras, sino también con tu profesor. Un último comentario, NO es necesario que instales el cliente de Discord en tu computadora o dispositivo móvil, aunque si lo haces podrás recibir las notificaciones sobre modificaciones que se hagan al material.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "index.html#estructura-del-libro",
    "href": "index.html#estructura-del-libro",
    "title": "Bioestadística Aplicada con R y RStudio",
    "section": "Estructura del libro",
    "text": "Estructura del libro\nEste libro es, como ya mencioné, el material utilizado en el curso y, por lo tanto, está dividido en distintas secciones, cada una formada por distintos capítulos (sesiones). Esta división trata de seguir un órden lógico, desde los conceptos y temas más fundamentales hasta los procedimientos más abstractos y, por lo tanto, todos los temas están conectados: cada capítulo asume que ya no tienes ningún problema con los anteriores.\nEn la esquina superior izquierda hay una barra de búsqueda, por si deseas realizar alguna consulta rápida. Debajo tienes todas las secciones y capítulos, en la porción central el contenido y a la derecha la tabla de contenidos del tema que estás leyendo. Referente al contenido, al inicio de cada capítulo tienes el video al tema correspondiete (ojo al desfase), mientras que en el texto encontrarás algunas anotaciones con información relevante al tema:\n\n\n\n\n\n\nNota\n\n\n\nAquí habrá información adicional al tema que se está tratando.\n\n\n\n\n\n\n\n\nTip\n\n\n\nAquí habrá consejos relacionados con la implementación de una técnica, o sobre su interpretación, o sobre la intuición detrás del tema que se esté abordando.\n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nAquí habrá puntos/conceptos/procedimientos con los cuales hay que tener mucho cuidado y la razón.\n\n\n\n\n\n\n\n\nImportante\n\n\n\nAquí habrá información que es sumamente importante que tengas presente.\n\n\nTambién encontrarás resaltados especiales para diferenciar claramente si estoy haciendo referencia a código en texto, funciones, librerías o software o a algunos conceptos clave. Todo el texto que veas con formato de hipervínculo te llevará a la referencia correspondiente: una figura, un capítulo, un enlace externo o una referencia bibliográfica.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "index.html#compartir-o-no-compartir-he-ahí-el-dilema",
    "href": "index.html#compartir-o-no-compartir-he-ahí-el-dilema",
    "title": "Bioestadística Aplicada con R y RStudio",
    "section": "¿Compartir o no compartir? He ahí el dilema",
    "text": "¿Compartir o no compartir? He ahí el dilema\nNo hay nada que te impida compartir este libro y con ello todo el curso. De hecho, nada me daría más gusto que saber que el curso te fue lo suficientemente agradable y útil como para dirigirlo a alguien más, pero te pido que lo hagas lo menos posible. Tu acceso al curso es vitalicio, pero ni el contenido ni el material son estáticos, por lo que dependemos de las nuevas ventas para poder seguir mejorándolo y mantenerlo en línea. Si quieres compartirlo puedes utilizar tu código de referido, el cuál da un descuento adicional a cualquier otro descuento o promoción.\nDicho esto, te doy nuevamente la bienvenida y espero que el curso cubra con tus expectativas (y un poco más).",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "s0_preparacion.html",
    "href": "s0_preparacion.html",
    "title": "Preparación",
    "section": "",
    "text": "R\nEvidentemente, lo primero que deberás instalar es R. Puedes encontrar el instalador de la versión más reciente para tu sistema operativo en CRAN (The Comprehensive R Archive Network).\nSimplemente da click en el enlace al servidor más cercano a tu ubicación, por ejemplo https://cran.itam.mx/.\nDespués simplemente descarga la versión que corresponde a tu sistema operativo.\nIndependientemente del sistema operativo que utilices, la instalación consiste en ejecutar el instalador y seguir los pasos indicados en su ventana, dando click en aceptar y continuar según sea necesario.",
    "crumbs": [
      "Preparación"
    ]
  },
  {
    "objectID": "s0_preparacion.html#r",
    "href": "s0_preparacion.html#r",
    "title": "Preparación",
    "section": "",
    "text": "Importante\n\n\n\nEl material del curso fue desarrollado con la versión 4.2.1 “Funny-Looking Kid” de R. No puedo garantizar que el código funcione sin problemas con versiones previas.",
    "crumbs": [
      "Preparación"
    ]
  },
  {
    "objectID": "s0_preparacion.html#rstudio",
    "href": "s0_preparacion.html#rstudio",
    "title": "Preparación",
    "section": "RStudio",
    "text": "RStudio\nUna vez instalado R podemos instalar RStudio. Más adelante veremos cuál es la diferencia entre ambos, pero por el momento piensa en RStudio como una ventana para R. Primero, dirígete a https://posit.co/download/rstudio-desktop/.\n\n\n\n\n\n\nNota\n\n\n\n¿Por qué el enlace para descargar RStudio es de posit? En octubre del 2022 la empresa se volvió posit. Aquí puedes leer más al respecto, pero el resumen es que el nombre de la empresa cambió, pero el nombre de su ambiente integrado de desarrollo (IDE, RStudio) se mantendrá igual.\n\n\nEn la sección All Installers ubica tu sistema operativo y descarga el instalador correspondiente\nUna vez descargado, ejecuta el instalador y sigue los pasos que aparecen en pantalla.",
    "crumbs": [
      "Preparación"
    ]
  },
  {
    "objectID": "s0_preparacion.html#quarto-y-tinytex",
    "href": "s0_preparacion.html#quarto-y-tinytex",
    "title": "Preparación",
    "section": "Quarto y tinytex",
    "text": "Quarto y tinytex\nCon estas dos instalaciones sería más que suficiente para empezar a trabajar; sin embargo, podemos ir más allá y utilizar RStudio para crear reportes, artículos científicos, libros (incluyendo tesis), páginas web, presentaciones, y más. Esto solía (y puede) hacerse con librerías como rmarkdown, distilled o bookdown; sin embargo, tenemos la siguiente generación: Quarto. En el curso veremos una introducción para hacer reportes y, de hecho, este material fue escrito en documentos Quarto. Al igual que en los casos anteriores, dirígete a la página de descargas y descarga e instala la versión correspondiente a tu sistema operativo.\n\n\n\n\n\n\nImportante\n\n\n\nNO verás un ejecutable (acceso directo) después de que se haya realizado la instalación. Para utilizar Quarto necesitaremos de a) la línea de comandos o b) una interfaz, que en nuestro caso es RStudio. Debajo de los enlaces de descarga hay más información sobre cómo utilizarlo, incluyendo detalles sobre su uso en VS Code, Jupyter y en un editor de textos (línea de comandos). Puedes explorarlos, aunque aquí verás los detalles correspondientes para RStudio.\n\n\nAdicional a Quarto, y si deseas exportar tus documentos a archivos PDF, es necesario instalar una distribución de LaTeX. Si ya cuentas con alguna, puedes saltarte este paso, de lo contrario puedes o realizar la instalación completa o simplemente instalar TinyTeX desde aquí. Si optas por instalar TinyTeX y no tienes nada de experiencia con R, te recomiendo seguir los pasos de su página una vez que hayas pasado por el 3  Bases de R. No te preocupes, hay un recordatorio al final de la sesión.",
    "crumbs": [
      "Preparación"
    ]
  },
  {
    "objectID": "s0_preparacion.html#instalaciones-opcionales",
    "href": "s0_preparacion.html#instalaciones-opcionales",
    "title": "Preparación",
    "section": "Instalaciones “opcionales”",
    "text": "Instalaciones “opcionales”\nAdicionalmente, te recomiendo encarecidamente (por no decir te solicito) que instales, dependiendo de tu sistema operativo, algunas herramientas. Tienes la descripción de cada una, así como desde donde realizar su instalación:\n\nWindows: Rtools. Esta es una serie de herramientas que nunca vas a ver cuando se utilicen, pero que son un dolor de cabeza si no cuentas con ellas y tienes la pésima fortuna de necesitar compilar un paquete desde su código fuente, o la dicha de querer publicar un paquete. A final de cuentas permiten justamente eso, administrar esas ejecuciones. Solía ser parte de la instalación de R en Windows, pero ya no es así, por lo que recomiendo la instales siguiendo las instrucciones oficiales (selecciona la versión más nueva de Rtools disponible).\nmacOS: XQuartz. R es un lenguaje con un enfoque muy fuerte hacia la generación de gráficos, lo cual permite hacer visualizaciones de muy alta calidad. Desafortunadamente, algunas cosas se apoyan del sistema de ventanas X.Org X Window System y que, de no estar disponible, pueden dar algunos dolores de cabeza. Apple solía incluir una implementación en la aplicación X11 en las versiones de OS X 10.5 a 10.7, pero ya no en sistemas más modernos. Es ahí donde entra el proyecto XQuartz. El instalador está disponible en la página del proyecto.\nmacOS: Xcode Command Line Tools. Si bien es cierto que macOS juega un papel importante en algunos círculos de desarrollo de software, no incluye algunas herramientas necesarias para el mismo objetivo que Rtools en Windows: compilar desde fuente. En este caso, hay dos formas de instalar lo que necesitamos, ambas provistas por Apple: a) instalar el entorno de desarrollo Xcode (40 GB) o, mi recomendación si no desarrollas aplicaciones para SOs de Apple, b) instalar las herramientas de línea de comando. Xcode lo puedes instalar directamente desde tu App Store, mientras que las herramientas de línea de comando desde el Terminal con el comando xcode-select --install. Puedes abrir el terminal utilizando Spotlight (CMD + espacio), o ejecutando su aplicación, ubicada en Aplicaciones -&gt; Utilidades.\n\nUna vez instalado todo esto estás más que listo para avanzar con el programa del curso. Recuerda que si tuviste algún problema siempre puedes contactarme en el servidor de Discord del curso.",
    "crumbs": [
      "Preparación"
    ]
  },
  {
    "objectID": "s01_biolcdatos.html",
    "href": "s01_biolcdatos.html",
    "title": "Biología, Ciencia de Datos y R",
    "section": "",
    "text": "Objetivo de aprendizaje\nEn esta sección del curso comenzarás reflexionando sobre el cómo se han aplicado tradicionalmente las técnicas estadísticas a los problemas biológicos. Después, te introducirás a RStudio, la elaboración de reportes con Quarto y posteriormente a las bases del lenguaje R y su dialecto tidy.",
    "crumbs": [
      "Biología, Ciencia de Datos y `R`"
    ]
  },
  {
    "objectID": "c01_biolcdatos.html",
    "href": "c01_biolcdatos.html",
    "title": "1  Ciencia de datos y biología",
    "section": "",
    "text": "¡Hola! En esta primera clase del curso Bioestadística Aplicada con R y RStudio abriremos con la relación emergente entre la biología y la ciencia de datos.\nPara comenzar, hablaremos un poco sobre lo que conocemos como estadística. El origen de esta disciplina se encuentra en la necesidad de los gobiernos de conocer cuál es el estado de su población, de ahí su raíz etimológica “relativo al Estado”. Sin embargo, en la actualidad, existen diversas definiciones y opiniones sobre lo que representa. Hay quienes mencionan que es la primera de las ciencias inexactas, mientras otras personas la consideran como la ciencia que nos permite cambiar nuestras ideas ante la incertidumbre. Si bien es cierto que estas visiones son en apariencia muy diferentes, ambas son perspectivas bastante válidas: la primera hace referencia a la relativa facilidad con la que se pueden manipular los datos o las pruebas (intencionalmente o no) para llegar al resultado que nosotros deseemos y la segunda a que nos permite tomar decisiones aún sin conocer en su totalidad un fenómeno. Si deseas leer sobre algunos mitos y realidades de la estadística, sigue este enlace.\nUna definición más formal es la de la Real Academia de la Lengua Española: “Rama de la matemática que utiliza grandes conjuntos de datos numéricos para obtener inferencias basadas en el cálculo de probabilidades”. De esta definición podemos retomar algunas ideas claves: la primera es que es una rama de la matemática, por lo cual nuestra resolución de problemas debe basarse en un razonamiento lógico bajo la cobertura de las matemáticas, por lo que los procedimientos y resultados deben de ser expresados sin ambigüedades. La siguiente es que requiere de grandes conjuntos de datos, lo cual hace referencia a un tema que será abordado más adelante en el curso: la representatividad. Por último, podemos también reconocer su objetivo, el cual es permitirnos obtener conclusiones a partir de este conjunto de datos. A partir de esta definición podemos considerar a la bioestadística como la aplicación de la estadística a problemas biológicos, desde la estimación del tamaño poblacional de una especie, comparaciones de tallas entre sitios de muestreo, modelar el crecimiento corporal, entre muchas otras.\n\n\n\n\n\n\nNota\n\n\n\nDebido a que la bioestadística está formada por una gran cantidad de procedimientos, muchos de ellos específicos a áreas particulares del conocimiento, en este curso abordaremos los fundamentos básicos de la estadística y técnicas de uso general.\n\n\nLlegados a este punto, quiero introducir otro concepto: la ciencia de datos. En pocas palabras, esta estudia los métodos para extraer información sobre los datos y, en última instancia, facilitar la toma de decisiones. Sus objetivos son i) describir los datos, comparar entre grupos/poblaciones/clases, ordenar o clasificar observaciones y, en última instancia, predecir un resultado futuro a partir de los datos con las que se cuenta.\nAl igual que con la definición de estadística, existen distintas definiciones y visiones, pero por el momento analicemos este diagrama que nos habla sobre las habilidades de un individuo realizando un procedimiento a sus datos y cuál sería el resultado:\n\n\n\n\n\n\nFigura 1.1: Ciencia de datos\n\n\n\n\nConsideremos las habilidades de hackeo como habilidades de programación y el conocimiento técnico necesario para la aplicación de distintas técnicas de análisis de datos, a la experiencia como el conocimiento que el individuo tenga sobre el fenómeno que está analizando y al conocimiento sobre matemáticas y estadística como el conocimiento teórico sobre las técnicas que está aplicando.\nEn la intersección de las habilidades de hackeo y la experiencia se encuentra una “zona de peligro”, la cual también podemos definir como el área de la “caja negra”, es decir, el área en la que se aplican pruebas y métodos sin conocer sus fundamentos y las conclusiones o inferencias están en función de la experiencia y los prejuicios de quien las realice, sin considerar su pertinencia.\nPor otra parte, en la intersección de la experiencia y el conocimiento de estadística se encuentra la investigación tradicional; es decir, existe un conocimiento teórico sobre las pruebas que se están aplicando y la experiencia para poder realizar inferencias sobre los datos considerando las limitaciones, fortalezas y debilidades de las técnicas; sin embargo, la visión sobre el problema se encuentra normalmente limitada a las pruebas tradicionales, lo cual a su vez limita el tipo de análisis y preguntas que se puedan resolver.\nEn la intersección de las habilidades de hackeo y el conocimiento sobre estadística se encuentra la disciplina del “aprendizaje automatizado”, algo que discutiremos más profundamente en la sección de Técnicas Multivariadas, pero que hace referencia al extraer relaciones entre los datos de manera eficiente, independientemente de si estas relaciones son causales o no.\nFinalmente, en el centro, recibiendo entradas de las tres áres encontramos a la ciencia de datos.\n\nTomando esto en consideración, te propongo hacer un ejercicio de reflexión sobre la pertinencia de aproximarnos a los problemas biológicos de una manera más flexible, eficiente, informada y que considere también la experiencia del investigador, más allá del modo tradicional e inamovible que nos ha llevado a pensar de manera incorrecta que la falta de significancia estadística es falta de significancia biológica o viceversa. ¿Es esto un problema muy grave? Esa pregunta la dejaremos para un tema donde es más adecuado abordarla: pruebas de hipótesis, en especial al hablar sobre los usos y abusos del famosísimo (¿infame?) valor de p.\nCon esta idea terminamos la primera clase del curso. Nos vemos en la siguiente para familiarizarnos con RStudio y cómo elaborar reportes utilizando Quarto.",
    "crumbs": [
      "Biología, Ciencia de Datos y `R`",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Ciencia de datos y biología</span>"
    ]
  },
  {
    "objectID": "c02_intro_rs.html",
    "href": "c02_intro_rs.html",
    "title": "2  Introducción a RStudio y Quarto",
    "section": "",
    "text": "2.1 R vs. RStudio\nAntes de volar y pretender formar un sitio web o una tesis, comencemos hablando de RStudio. En la sección de preparación instalamos tres cosas diferentes: R, RStudio, y Quarto. Olvidemos a este último por un momento y centrémonos en los dos primeros. R es un lenguaje de programación. Como tal, se ejecuta en consola, ya sea el terminal (macOS/Linux) o el interpretador de comandos cmd en Windows. Lo único que veremos si abrimos/ejecutamos R per-se es una ventana como la siguiente:\nEs decir, solamente veremos nuestra consola, compuesta por una descripción de la versión de R que estamos utilizando y un prompt (el símbolo &gt;) que nos presiona a darle a la computadora una instrucción. Más allá de ser una interfaz extremadamente simple, no está pensada para el desarrollo de reportes como los que nosotros realizamos. No podemos escribir texto libre, ni tampoco podemos guardar nuestro progreso. Para eso habría que abrir un script, pero hay una mejor alternativa que nos permite hacer eso y mucho más: RStudio. Por el momento hasta aquí vamos a llegar con R, pero no te preocupes, le vamos a dedicar mucho más tiempo posteriormente.",
    "crumbs": [
      "Biología, Ciencia de Datos y `R`",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducción a `RStudio` y `Quarto`</span>"
    ]
  },
  {
    "objectID": "c02_intro_rs.html#r-vs.-rstudio",
    "href": "c02_intro_rs.html#r-vs.-rstudio",
    "title": "2  Introducción a RStudio y Quarto",
    "section": "",
    "text": "Figura 2.1: Consola de R (R GUI)",
    "crumbs": [
      "Biología, Ciencia de Datos y `R`",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducción a `RStudio` y `Quarto`</span>"
    ]
  },
  {
    "objectID": "c02_intro_rs.html#el-ide-rstudio",
    "href": "c02_intro_rs.html#el-ide-rstudio",
    "title": "2  Introducción a RStudio y Quarto",
    "section": "2.2 El IDE RStudio",
    "text": "2.2 El IDE RStudio\nMientras que R es un lenguaje de programación ejecutable en consola, RStudio es un ambiente gráfico de desarrollo (IDE). ¿Qué significa? Que es una interfaz gráfica que nos permite no solo ejecutar nuestro código línea a línea, sino que también incluye otros páneles que nos facilitan enormemente la existencia y, además, abre otras puertas para la creación de documentos como este libro. Vayamos por partes.\n\n2.2.1 La ventana de RStudio\nAl abrir RStudio por primera vez te vas a topar con una ventana como la siguiente:\n\n\n\n\n\n\nFigura 2.2: Ventana de RStudio\n\n\n\nYa sé, ya sé, no se ve mucho más amable que la ventana de R. Es más, se ve mucho más intimidante porque ahora tenemos la consola y otros espacios. Al ser un IDE, RStudio incluye elementos gráficos para todo lo que pudiéramos llegar a necesitar mientras desarrollamos nuestros análisis, entonces vamos a descomponer esta ventana panel por panel, de arriba a abajo y de izquierda a derecha.\n\n\n\n\n\n\nImportante\n\n\n\nSi no te aparecen los cuatro páneles no te preocupes, simplemente ve a File -&gt; New File -&gt; R script. Con eso se abrirá el panel faltante.\n\n\n\n2.2.1.1 El editor\nEl primer panel es el editor:\n\n\n\n\n\n\nFigura 2.3: Editor en RStudio\n\n\n\nEste es, como el nombre sugiere, un editor de textos, que no debemos de confundir con procesador de palabras (i.e., Word o similares). En él vamos a poder escribir sripts o libretas que contengan la serie de pasos que realizamos durante nuestro análisis. Cada una de las pestañas en este panel es siempre un documento de texto simple, independientemente de si es un script o una libreta. Esto tiene varias ventajas, pero la más importante es que nos podemos llevar esos archivos a cualquier computadora y estar bastante seguros de que podremos, cuando menos, ver su contenido y editarlo sin preocuparnos por problemas de compatibilidad entre versiones del software o, peor aún, sistemas operativos (*ejem* Word *ejem*). Estos archivos de texto simple pueden, dependiendo del tipo de archivo, enviar instrucciones a R.\n\n\n2.2.1.2 La consola\nEl siguiente panel es la consola:\n\n\n\n\n\n\nFigura 2.4: Consola en RStudio\n\n\n\nEste panel es, literalmente, lo que veíamos al abrir R por sí solo; es decir, un espacio donde tenemos nuestro prompt y donde se ejecutarán nuestras instrucciones o líneas de comandos. Notarás que hay otras tres pestañas: una llamada Terminal, y otra llamada Background Jobs. Estas son interfaces a la terminal del sistema y a los trabajos que estemos ejecutando en segundo plano. Nuestra interacción con estas dos pestañas tiende a ser limitada (o incluso nula), salvo que realicemos algo muy especializado.\n\n\n2.2.1.3 Scripts: qué, cómo, cuándo y por qué.\nAntes de pasar al siguiente panel es importante hablar de los scripts, las libretas, e intentar hacer un poco de labor de convencimiento. Si has tenido un acercamiento previo a R/RStudio, es bastante probable que trabajar con scripts te sea familiar. Si no es así, un script de R es un archivo de texto con extensión .R en el que ponemos nuestro código línea a línea. Pensemos en un ejercicio en el que queremos primero sumar 1 y 1, y luego 2 y 2. Nuestro script en RStudio se vería así:\n\n\n\n\n\n\nFigura 2.5: Script en el editor\n\n\n\nSolo tenemos el código, no tenemos los resultados. ¿La razón? Aún no le hemos dicho a la computadora que queremos que las ejecute. ¿Cómo le decimos? Tenemos dos formas:\n\nEjecutar el script línea a línea, para lo que debemos posicionar nuestro cursor (dar click) sobre la línea a ejecutar, utilizar el atajo de teclado CMD + R en macOS/Linux o CTRL + R en Windows, o dar click sobre el botón Run que está cerca de la esquina superior derecha del panel.\nEjecutar el script completo, para lo que seleccionaríamos todo su contenido y utilizaríamos el mismo atajo de teclado o botón que antes.\n\nSea cual sea la opción que hayas escogido, la salida (el resultado) aparecerá en la consola:\n\n\n\n\n\n\nFigura 2.6: Script ejecutado en consola\n\n\n\n¿Cuál es el problema? El primero es que en el momento en el que cerremos RStudio esos resultados se van a perder, salvo que los hayamos guardado manualmente en algún lugar. El segundo tiene que ver con una falta de unificación: el código (las sumas) están por un lado, mientras que los resultados están en otro que, además, es volátil. El tercero se deriva de los dos anteriores: falta de legibilidad y reproducibilidad. No podemos hacer el reporte al mismo tiempo en que analizamos los datos y, si en algún momento volvemos al script, debemos de ejecutarlo todo nuevamente para ver los resultados. Suena engorroso, ¿no? Un cuarto problema es que no tenemos descripciones de nuestros resultados. Si ya has trabajado con estos archivos me vas a decir “para eso existen los comentarios”, a lo que yo te respondería que no, los comentarios no son para eso. Si no has trabajado con R ni ningún otro lenguaje de programación te preguntarás qué es un comentario. Bien, un comentario es un fragmento de texto no ejecutable; es decir, es algo que podemos escribir y pasarle a la consola pero que no se va a ejecutar. En R estos están dados por el operador #. Agreguemos un comentario a nuestro script con la palabra “Sumas” y ejecutémoslo todo nuevamente:\n\n\n\n\n\n\nFigura 2.7: Script con comentarios ejecutado en consola\n\n\n\nComo esperábamos, en la consola no hay una salida asociada a la instrucción # Sumas, por lo tanto puedo usar esos comentarios para describir mis resultados, ¿no? La respuesta es, como en muchas otras cosas, depende. O, mejor dicho, de que se puede, se puede, que debamos hacerlo, es otra historia. Los comentarios tienen la función de describir muy brevemente qué intención tiene el código, no escribir párrafos completos con el reporte de los resultados. Comentarios válidos son agregar al inicio del script quién lo escribió, qué hace el código contenido en él, un medio de contacto, y breves descripciones de qué se hace en cada línea, sin repetir el código en texto simple (no decir # Suma 1 y 1 si el código es 1+1, por ejemplo). Existe otro gran problema el cuál no es obvio en este ejercicio, pero que tiene que ver con la carga de datos en archivos dentro de nuestra computadora (archivos .csv o .xlsx, por ejemplo), pero eso lo veremos en un tema posterior. Por el momento veamos una alternativa que resuelve todos estos problemas.\n\n\n2.2.1.4 Libretas y reportes: Quarto\nAquí es donde entran Quarto y las libretas. Al instalar Quarto no instalamos un programa per-se, sino que instalamos una extensión a RStudio que es, y cito textualmente, “un sistema de publicación científica y técnica de código abierto construido sobre Pandoc”, que permite, citando nuevamente: i) crear contenido dinámico no solo con R sino con otros lenguajes de programación; ii) escribir documentos como texto plano; iii) publicar artículos, reportes, presentaciones, sitios web, blogs y libros de alta calidad en formatos HTML, PDF, MS Word, ePUB; y iv) escribir con markdown científico, incluyendo ecuaciones, citas, referencias cruzadas, páneles de figuras, anotaciones, diseños avanzados y más. ¿A que ya suena mejor que los scripts? Sin ir más lejos, todo el material que utilizaremos en este curso fue escrito en RStudio utilizando Quarto, y puedes ver la versión final en el sitio web de acompañamiento. Debido a que explicar Quarto es un tema que merece le dediquemos tiempo y estar más arriba que un subtema de IDE RStudio, vamos a dejarlo de lado por el momento, solo revisemos cómo crear un nuevo documento y las diferencias fundamentales con los scritps. Para crear un documento podemos ir a la barra de herramientas -&gt; File -&gt; New file -&gt; Quarto document, o utilizar el botón correspondiente en la ventana de RStudio:\n\n\n\n\n\n\nFigura 2.8: Nuevo documento Quarto\n\n\n\nAl darle click nos va a aparecer la siguiente ventana para personalizar el documento:\n\n\n\n\n\n\nFigura 2.9: Opciones documento Quarto\n\n\n\nAquí añadirás el título de tu documento y opcionalmente el autor. Por el momento dejaremos todo lo demás tal y como está y daremos click en Create, lo que abrirá una pestaña nueva en el editor:\n\n\n\n\n\n\nFigura 2.10: Opciones documento Quarto\n\n\n\nLa pestaña se parece al contenido de un script, la única diferencia es un texto contenido entre ---. A esta parte la podemos identificar como el preámbulo del documento, y es un fragmento de nuestro documento escrito en formato YAML ¿Qué es eso y con qué se come? No te preocupes ahorita por eso, solo necesitas saber ahorita que vamos a desarrollar nuestro trabajo debajo del preámbulo. En la Figura 2.11 puedes ver un ejemplo básico de un documento Quarto con el código de nuestras sumas.\n\n\n\n\n\n\nFigura 2.11: Un documento Quarto básico\n\n\n\nUn ejemplo más claro del potencial de Quarto es, nuevamente, el material de este curso. Pero sigamos explorando la ventana de RStudio\n\n\n\n2.2.2 El ambiente de trabajo\nEl siguiente panel es lo que se conoce como el ambiente de trabajo (workspace), que nos da un listado de las cosas que le hemos dado a R para que recuerde (objetos). En la siguiente sesión hablaremos largo y tendido de esto, pero veamos un ejemplo donde, utilizando la consola, le digamos a R que recuerde el resultado de la suma 1 + 1, asignándolo a una referencia que arbitráriamente llamaré suma:\n\n\n\n\n\n\nFigura 2.12: Guardar un resultado en la consola\n\n\n\nAquí hay dos cosas a tener en cuenta: 1. La asignación se hizo con el operador &lt;-. Esto es sumamente importante: en R guardamos cosas utilizando el operador &lt;- y no =. 2. Al ejecutar la línea no obtuvimos el resultado. Esto es porque solo le dijimos que lo recordara, que lo anotara en un post-it, si quieres, no que nos lo mostrara. Si queremos que nos lo muestre solo tenemos que llamarlo por su nombre (ejecutar en la consola):\n\n\n\n\n\n\nFigura 2.13: Imprimir el resultado\n\n\n\n¿Qué tiene que ver esto con el ambiente de trabajo? Pues ahora ya no está vacío, ya tenemos una entrada en la lista, en donde se muestra el nombre del objeto y su valor. Conforme vayamos creando más objetos, más entradas tendrá esa lista. Prueba a crear un objeto que contenga el texto \"Hola mundo\" (ojo a las comillas) y dar click en su nombre en el ambiente de trabajo. ¿Qué ocurre?\n\n\n2.2.3 El ambiente gráfico\nEl último panel corresponde al ambiente gráfico. Este panel junta varios elementos a los que es más fácil acceder visualmente. La primera pestaña es un explorador de archivos. puedes dar click a cada carpeta para ver sus elementos, crear nuevas carpetas, y mucho más. En este curso no lo utilizaremos más que como una referencia visual de dónde están ubicados nuestros archivos.\n\n\n\n\n\n\nFigura 2.14: Explorador de archivos\n\n\n\nLa siguiente pestaña nos muestra la serie de gráficos que hemos ido generando. Podemos ejecutar en la consola el comando plot(cars) y verás que el gráfico se muestra en esta pestaña.\n\n\n\n\n\n\nFigura 2.15: Gráfico en el ambiente gráfico\n\n\n\nEn la siguiente pestaña encontraremos un listado de las librerías/paqueterías que tenemos instaladas, sus versiones, una breve descripción de para qué son, así como botones para instalarlas o actualizarlas.\n\n\n\n\n\n\nFigura 2.16: Paquetes\n\n\n\nDespués tenemos la pestaña de ayuda. Aquí podemos ver, valga la redundancia, ayuda sobre R, pero también sobre funciones en las que estemos interesados. Si queremos ver la ayuda de una función FUN podemos o utilizar el comando ?FUN Figura 2.17 o utilizar la barra de búsqueda de la pestaña\n\n\n\n\n\n\nFigura 2.17: ¡Ayuda!\n\n\n\nEn la siguiente pestaña tenemos un visor (Viewer), donde tendremos vistas previas de los documentos RMarkdown o Quarto con los que estemos trabajando. Tomando como ejemplo el documento Quarto que generamos antes, da click al botón Render con la flecha azul. ¿Qué obtienes?\nLa última pestaña es un visor de presentaciones, el cual no utilizaremos en este curso.\n\n\n2.2.4 Personalizando RStudio\nComo todo IDE, podemos personalizar la apariencia de RStudio. Para hacerlo simplemente ve a la barra de herramientas, luego en Tools -&gt; Global Options. Te aparecerá la siguiente ventana\n\n\n\n\n\n\nFigura 2.18: Ventana de ajustes\n\n\n\nVamos a revisar algunas de las opciones que te recomiendo tener presentes. Puedes hacer los cambios y aplicarlos todos juntos al final con el botón Apply:\n\nGeneral\n\nBasic\n\nWorkspace: Restore .RData into workspace at startup & Save workspace to .RData on exit. ¿Quieres que cada que cierres RStudio todos los objetos de tu espacio de trabajo se guarden en un archivo, y que esos mismos se carguen la siguiente vez que abras RStudio? Personalmente no es algo con lo que esté de acuerdo, porque más frecuentemente que no vas a querer un ambiente limpio, por lo que estas opciones están desmarcada y en Never, respectivamente.\nHistory: La misma historia (je) que en el caso anterior. ¿Quieres que tu historial de comandos ejecutados se guarde, aún si no guardas el .RData? ¿Quieres que se remuevan los duplicados? Por las mismas razones que antes también las tengo desmarcadas.\n\nGraphics\n\nBackend a Cairo. Simplemente es con qué se están graficando las cosas. ¡Es INDISPENSABLE que hayas instalado XQuartz si estás en macOS!\n\n\nAppearance\n\nZoom: ¿Qué tan grandes quieres todos los elementos de la ventana? Para mi trabajo personal, y dependiendo de la resolución del monitor donde se encuentre la ventana, esta oscila entre 100% y 125%, para este curso está en 175-200%\nEditor font y Editor font size: Tipo y tamaño de letra. Personalmente recomiendo no cambiar el tipo de letra.\nEditor theme: Cambia el color de fondo y los colores de realce de la sintaxis. Usualmente trabajo por las noches, por lo que prefiero un tema con fondo obscuro como Tomorrow Night Bright, pero puedes buscar el que tú quieras.\n\nPane Layout\n\nAquí se muestran los cuatro paneles de la ventana de RStudio. Personalmente prefiero tener los dos elementos que más utilizo lado a lado y no uno encima del otro. ¿La razón? El código crece hacia abajo, entonces el espacio vertical tiende a ser más importante que el espacio horizontal. Es decir, que en el panel superior derecho pongo la consola, y en el panel inferior izquierdo el ambiente de trabajo.\n\n\nEl resto de opciones son más específicas, por lo que recomiendo no tocarlas salvo que sepas qué estás moviendo y con qué objetivo. Todo se puede revertir, pero no hay necesidad de buscar dolores de cabeza.",
    "crumbs": [
      "Biología, Ciencia de Datos y `R`",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducción a `RStudio` y `Quarto`</span>"
    ]
  },
  {
    "objectID": "c02_intro_rs.html#preámbulo",
    "href": "c02_intro_rs.html#preámbulo",
    "title": "2  Introducción a RStudio y Quarto",
    "section": "3.1 Preámbulo",
    "text": "3.1 Preámbulo\nEl primer elemento es el preámbulo, que mencionamos está en formato YAML y que está contenido entre ---. YAML es un formato de serialización de datos legible por humanos. ¿En castellano? Una lista con niveles de pares de claves:valores que definen los metadatos de nuestro documento. En nuestro ejemplo tenemos:\n---\ntitle: \"Untitled\"\n---\nEs decir, el título que aparecerá en el reporte es \"Untitled\", tal y como vimos en la vista previa. Esto no tiene mucho sentido, así que cambiémoslo por \"Mi primerQuarto\" y añadamos una nueva entrada para el autor, tal que:\n---\ntitle: \"Mi primer `Quarto`\"\nauthor: \"Tu nombre\"\n---\nOtro elemento que usualmente se agrega en el preámbulo es el formato de salida del documento renderizado; es decir, ya presentado para compartir/imprimir. Mi recomendación es exportar a archivos HTML, salvo que vayas a imprimir el documento (PDF), necesites paginación (PDF de nuevo) o que por alguna desafortunada razón necesites un archivo MS Word. Un HTML lo declaramos tal que:\n---\ntitle: \"Mi primer `Quarto`\"\nauthor: \"Tu nombre\"\nformat:\n  html:\n    code-fold: true\n---\nNotarás algunas cosas. La primera es que html está indentado; es decir, no comienza en la misma posición que format. Esto es para indicar que html pertenece a format, al igual que code-fold pertenece a html. La siguiente es, justamente, que agregamos a la lista la entrada code-fold. Esta es una opción que indica si queremos que el código sea colapsable mediante un botón en el archivo final. En este caso, la indicamos como true, por lo que así será. Si no lo quisiéramos así indicaríamos false. Si renderizamos nuestro documento ahora tendremos:\n\n\n\n\n\n\nFigura 3.1: Primer Quarto renderizado\n\n\n\nSi tienes curiosidad por saber qué características YAML dieron lugar al libro de acompañamiento, puedes revisar el archivo _Quarto.yml.",
    "crumbs": [
      "Biología, Ciencia de Datos y `R`",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducción a `RStudio` y `Quarto`</span>"
    ]
  },
  {
    "objectID": "c02_intro_rs.html#markdown",
    "href": "c02_intro_rs.html#markdown",
    "title": "2  Introducción a RStudio y Quarto",
    "section": "3.2 Markdown",
    "text": "3.2 Markdown\nPasando el preámbulo tenemos una sección de texto libre, con algunas anotaciones para el formato del texto. Estas anotaciones están hechas en lenguaje markdown. Markdown es un “lenguaje de programación para textos” y permite hacer cosas bastante interesantes. Las anotaciones más básicas son:\n\nEncabezados y secciones: #, ##, ###, ####, etc.\n*Itálicas* : Itálicas\n**Itálicas**: Negritas\n`Código`: Código\nHipervínculos: [Google](https://www.google.com): Google\n$y_i = \\alpha + \\beta*x_i + \\epsilon$: \\(y_i = \\alpha + \\beta*x_i + \\epsilon\\)\n\nPuedes hacer listas numeradas, como la anterior, o listas sin numerar:\n\nElemento\nOtro elemento\n\nE, incluso, puedes hacer listas anidadas añadiendo una indentación de doble tabulación a los elementos anidados:\n\nIntroducción a RStudio y Quarto\n\nR vs. RStudio\nIDE RStudio\n\n\nEn el archivo .qmd este capítulo puedes ver cómo añadí las capturas de la ventana de RStudio. Añadir imágenes es básicamente el mismo procedimiento que con un enlace, solo añadiendo el operador ! antes. Por ejemplo, la siguiente línea añade el logo de R desde su dirección oficial, le asigna el pie de foto “Logo R” y una etiqueta interna que se puede utilizar para referencias cruzadas (Figura 3.2) con @fig-logoR :\n![Logo `R`](https://www.r-project.org/logo/Rlogo.png){#fig-logoR}\n\n\n\n\n\n\nFigura 3.2: Logo R",
    "crumbs": [
      "Biología, Ciencia de Datos y `R`",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducción a `RStudio` y `Quarto`</span>"
    ]
  },
  {
    "objectID": "c02_intro_rs.html#referencias-y-citas",
    "href": "c02_intro_rs.html#referencias-y-citas",
    "title": "2  Introducción a RStudio y Quarto",
    "section": "3.3 Referencias y citas",
    "text": "3.3 Referencias y citas\nPodemos agregar referencias, siempre y cuando estas estén contenidas en un archivo .bib como el archivo references.bib que está en este directorio y, por supuesto, referenciarlas en el texto (e.g. Knuth, 1984). Si estás viendo la página web con este material, pasa tu cursor sobre la cita y notarás como aparece la información bibliográfica completa. Este archivo .bib está compuesto por entradas en formato bibTeX, heredado del hermano mayor de Markdown: LaTeX. La sintaxis básica es la siguiente:\n@TIPO{CLAVE,\n      author = {},\n      year = {},\n      title = {}}\nLos campos adicionales dependerán del TIPO de referencia que se esté añadiendo. Si quieres ver algunas de las opciones más comunes, te recomiendo revisar esta página. Para incluir una referencia cruzada lo único que tienes que hacer es: @CLAVE. Si tomamos como ejemplo el artículo de Knuth de 1984 sobre la programación literal sería @Knuth_1984 para una referencia en el texto, Knuth (1984), o [@Knuth_1984] para una referencia dentro de paréntesis (Knuth, 1984). En cualquiera de los dos casos, si estás viendo el material renderizado, asegúrate de pasar el cursor sobre las citas, y verás que aparece la referencia bibliográfica completa. Para agregar la lista de referencias al final del texto, debes de agregar el div #refs y tener un encabezado de referencias al final del documento, tal que:\n# Referencias{.unnumbered}\n::: {#ref}\n:::\n\n\n\n\n\n\nImportante\n\n\n\nLa clave del divisor en el bloque mostrado es #ref, pero debería de ser #refs. Esto es para evitar que Quarto se confunda y ponga aquí las referencias en vez de en la sección correspondiente. También puedes cambiar el nombre del encabezado, si así lo deseas, o retirar la anotación {.unnumbered} si quieres que la sección esté numerada.\n\n\n\n\n\n\n\n\nNota\n\n\n\nYo construyo mis archivos .bib a mano, pero no es necesario. Si quieres pasar de un listado de referencias que ya tienes en Word puedes considerar text2bib o Edifix (1. OJO con las opciones; 2. Edifix es de pago). Si quieres una interfaz gráfica para manejar tus archivos .bib, puedes considerar JabRef. Finalmente, gestores de referencias como Mendeley o ReadCube Papers permiten exportar las referencias en formato bib.",
    "crumbs": [
      "Biología, Ciencia de Datos y `R`",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducción a `RStudio` y `Quarto`</span>"
    ]
  },
  {
    "objectID": "c02_intro_rs.html#consideraciones-sobre-quarto",
    "href": "c02_intro_rs.html#consideraciones-sobre-quarto",
    "title": "2  Introducción a RStudio y Quarto",
    "section": "3.4 Consideraciones sobre Quarto",
    "text": "3.4 Consideraciones sobre Quarto\nAunque Quarto es extremadamente potente y flexible, es importante tener presente algunas cosas. La primera es que NO hay manera de que en una sola sesión yo pueda explicarte con lujo de detalle todas sus funciones y posibilidades, para eso prefiero dirigirte a la (bastante completa) guía de Quarto. Otra consideración es que, aunque puedes exportar tus documentos como PDF, Word o ePUB u otros, mi recomendación es que siempre que tengas la libertad exportes a un HTML, que es un poco más permisivo con líneas de código muy largas, o al mostrar tablas con muchas columnas. Si NECESITAS de un PDF, asegúrate de tener instalada alguna distribución de LaTeX o, si no quieres la instalación completa, cuando menos asegurarte de haber instalado TinyTeX como sugerimos en la sesión de preparación, de lo contrario NO podrás exportar tus reportes a PDF.",
    "crumbs": [
      "Biología, Ciencia de Datos y `R`",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducción a `RStudio` y `Quarto`</span>"
    ]
  },
  {
    "objectID": "c02_intro_rs.html#ejercicio",
    "href": "c02_intro_rs.html#ejercicio",
    "title": "2  Introducción a RStudio y Quarto",
    "section": "3.5 Ejercicio",
    "text": "3.5 Ejercicio\nUtilizando Quarto genera un documento HTML con tabla de contenidos en el que te presentes. Las características son:\n\nTítulo: tu nombre\nIncluye tu correo electrónico en el preámbulo\nIncluye las secciones:\n\nGrado académico e institución de procedencia\nMotivación para tomar el curso\nExpectativas sobre el curso (¿hay alguna técnica particular que quieras aprender/revisar?)\nLibro(s) favorito(s) (como cita(s) en el texto, incluyendo la(s) referencia(s) completa(s) al final del documento)\nUna captura de pantalla con tu ventana de RStudio. No importa si es con los ajustes por defecto, si la pusiste igual a la mía, o si pusiste un tema con colores estridentes.",
    "crumbs": [
      "Biología, Ciencia de Datos y `R`",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducción a `RStudio` y `Quarto`</span>"
    ]
  },
  {
    "objectID": "c02_intro_rs.html#qué-sigue",
    "href": "c02_intro_rs.html#qué-sigue",
    "title": "2  Introducción a RStudio y Quarto",
    "section": "3.6 ¿Qué sigue?",
    "text": "3.6 ¿Qué sigue?\nEn esta sesión revisamos muy someramente el poder de RStudio y Quarto, limitándonos a las funciones más indispensables y que se utilizan de manera cotidiana, pero hay mucho más por ver. Proyectos dentro de RStudio, elaborar libros/páginas web/tesis o incluso artículos científicos con formato editorial según los requerimientos de algunas revistas, solo por mencionar algunos.\nPor otra parte, una de las ventajas poco conocidas de trabajar con archivos de texto simple (.R, .Rmd o .qmd), al menos en el área de ciencias biológicas, es el poder aprovechar al máximo sistemas de gestión de versiones, incluyendo por supuesto al más famoso: git. Muy posiblemente te hayas encontrado en algún momento con GitHub, que no era otra cosa mas que un almacén público de repositorios git. Ahora es un servidor capaz de mantener servicios simples en ejecución o, incluso, alojar páginas web (la página web de acompañamiento de este curso es un ejemplo), por lo que te recomiendo que le eches un ojo al funcionamiento de ambos (git y GitHub), y que revises la integración de git en RStudio. Personalmente utilizo solo la línea de comandos, pero es una herramienta más dentro de nuestro IDE y que puede resultarte más intuitiva.\n\n\n\n\nKnuth DE. 1984. Literate Programming. The Computer Journal 27:97-111. DOI: 10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Biología, Ciencia de Datos y `R`",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introducción a `RStudio` y `Quarto`</span>"
    ]
  },
  {
    "objectID": "c03_bases_r.html",
    "href": "c03_bases_r.html",
    "title": "3  Bases de R",
    "section": "",
    "text": "3.1 El lenguaje de programación R\nEn la sesión anterior hablamos de RStudio como una interfaz gráfica a R, pero no fuimos más allá de decir que R es un lenguaje de programación que se ejecuta en consola. Pues bien, R es, y cito textualmente, “un lenguaje y ambiente para la computación estadística y la creación de gráficos” (R Core Team, 2022). ¿En castellano? Es un lenguaje creado especialmente para procedimientos estadísticos y el graficado de datos. Es un software libre, por lo que además de ser gratuito es auditable (i.e., cualquiera puede revisar el código fuente), “cualquiera” puede contribuir (ojo a las comillas). En su código fuente base (R base) incluye diversos algoritmos, modelos y distribuciones de probabilidad, aunque también es fácilmente extensible por medio de paquetes o librerías.\nEn este punto me dirás “Ok, Arturo, de acuerdo con lo que dices, pero ¿por qué se ha vuelto tan popular en análisis bioestadísticos?” Pues tiene que ver con varios factores. El primero es que es un lenguaje de alto nivel; es decir, está hecho para ser leído por humanos y no por computadoras. El segundo es que es interactivo, por lo que podemos ir modificando el código y ver su salida en tiempo real sin necesidad de compilar primero el código. El tercero tiene que ver su extensibilidad. Hay una gran variedad de librerías que agregan funcionalidades particulares, que van desde funcionalidades muy generales hasta la implementaciones muy particulares dependientes del área de investigación. ¿Algunos ejemplos? En tidyverse tenemos un dialecto enfocado a la ciencia de datos y la programación funcional. En ggplot2 tenemos un potente graficador. En vegan tenemos una gran cantidad de funciones para problemas de ecología de comunidades. En SIBER tenemos una forma de estimar tamaños de nichos isotópicos. En STAN (rstan, para ser más preciso) tenemos un ambiente general para inferencia Bayesiana. En fin, hay tantas librerías como problemas a resolver, y continuamente se añaden nuevas, pero antes de saltar a utilizarlas necesitamos entender las bases de R base (je).",
    "crumbs": [
      "Biología, Ciencia de Datos y `R`",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bases de R</span>"
    ]
  },
  {
    "objectID": "c03_bases_r.html#objetos",
    "href": "c03_bases_r.html#objetos",
    "title": "3  Bases de R",
    "section": "3.2 Objetos",
    "text": "3.2 Objetos\nR es un lenguaje en el cual domina el paradigma de la programación orientada a objetos; i.e., en R todo es un objeto. El método de creación es el mismo para todos los casos: utilizar el símbolo de asignación &lt;- (atajo de teclado alt -). Como ejemplo, creemos un objeto que contenga el texto “¡Hola Mundo!”:\n\ntexto &lt;- \"¡Hola mundo!\"\n\nNotarás que no hubo salida ni en la libreta ni en la consola. Esto quiere decir que el objeto fue creado satisfactoriamente, y ahora podemos acceder o utilizar ese texto llamando al objeto texto:\n\ntexto\n\n[1] \"¡Hola mundo!\"\n\n\n\n3.2.1 Consideraciones\nAunque podemos poner virtualmente cualquier nombre a nuestros objetos, es necesario que tengamos algunas cosas en cuenta:\n\nNo empezar nombres de objetos con números.\nNo utilizar nombres de funciones u otros objetos creados anteriormente: enmascaramiento (funciones) o sobre-escritura (variables)\nEvitar empezar con punto (.), pues el objeto queda oculto del ambiente de trabajo\nUtilizar nombres cortos, pero lo más descriptivos posibles (amundsen_plot &gt;&gt; plot)\nSe pueden utilizar _ o . dentro del nombre (como separadores, por ejemplo). La guía de estilo de R sugiere el uso de _, aunque la guía de estilo de Google para R sugiere el uso de CamelCase (AmundsenPlot). Lo más importante para uso personal/interno es ser consistente y evitar mezclar estilos.",
    "crumbs": [
      "Biología, Ciencia de Datos y `R`",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bases de R</span>"
    ]
  },
  {
    "objectID": "c03_bases_r.html#librerías-y-funciones",
    "href": "c03_bases_r.html#librerías-y-funciones",
    "title": "3  Bases de R",
    "section": "3.3 Librerías y funciones",
    "text": "3.3 Librerías y funciones\nAunque en R predomina el paradigma de la programación orientada a objetos, también podemos hacer uso del paradigma funcional de la programación; es decir, podemos construir nuestros programas mediante la aplicación y construcción de funciones. Aunque en este momento te suene poco intuitivo o como si estuviéramos comenzando por el final, vamos a comenzar con las funciones, para después hablar de los tipos de objetos que tenemos, pues resulta que para crear cierto tipo de objetos necesitamos utilizar funciones.\n\n3.3.1 Funciones\nLas funciones representan una serie de métodos para obtener un resultado, para utilizarlas emplearemos la estructura fun(arg1, arg2, ..., argn), donde arg*representa un argumento; es decir, un elemento “pasado” a la función para regular sus procesos. El ejemplo más simple, y con el cuál ya hemos estado en contacto, es la función print():\n\nprint(\"¡Hola Mundo!\")\n\n[1] \"¡Hola Mundo!\"\n\n\nEsta función “imprime” un resultado en pantalla, pero puede ser utilizada para mucho más que para imprimir texto. A final de cuentas, este mismo resultado lo podemos obtener simplemente poniendo el texto en la consola. ¿Qué puede hacer la función? Eso lo podemos ver consultando la ayuda de la función, utilizando el operador ? antes del nombre, tal que:\n\n?print\n\nTe darás cuenta que no tenemos una salida en la libreta, lo cuál es normal, pues la ayuda tiene su propio espacio en el ambiente gráfico. Si estuviéramos trabajando solo con la línea de comandos en el terminal, ahí sí veríamos la salida en la consola. Una de las partes más importantes de la documentación de ayuda es que tenemos los argumentos de la función; es decir, qué necesita la función para realizar su trabajo, así como el papel de cada uno de estos elementos.\nPor otra parte, cuando declaramos una función los llamaremos parámetros. Para declarar una función generaremos una variable cuyo nombre será el nombre “llamable” de la función, a la cual asignaremos el cuerpo de la función utilizando function(parámetros){cuerpo}. Para ejemplificar, creemos una función para calcular la media aritmética de un conjunto de números x:\n\nmedia.arit &lt;- function(x){\n  # Mejora: Trabajo con NAs\n  suma &lt;- sum(x)\n  n &lt;- length(x)\n  return(suma/n)\n}\n\nmedia.arit(1:5)\n\n[1] 3\n\n\n¿Cuándo declarar una función? Cuando tengamos un flujo de trabajo que consista de exactamente los mismos pasos con posibles pequeñas variaciones, el cuál aplicaremos de manera continua. De hecho, uno de los productos que obtendremos de estas reuniones será un script con las funciones relacionadas con los análisis tróficos IIR, PSIRI, gráficos de Amundsen, etc. para que puedan ser utilizados de manera sencilla, repetitiva, y consistente.\nPor último, debido a que un gran número de funciones son altamente regulables (cuentan con un gran número de parámetros), te recomiendo hacer uso extensivo (excesivo) de la ayuda (?fun) para que obtengas de primera mano el conocimiento sobre su objetivo, sus entradas, su salida y, en consecuencia, ayudarte a prevenir o solucionar errores.\n\n\n3.3.2 Librerías\nAfortunadamente para nostros, muchas de las técnicas o procedimientos que realizamos ya fueron programados por alguien con más experiencia, y usualmente compilados en una librería de R. Hay una gran cantidad de librerías disponibles, algunas instalables directamente desde CRAN (R), mientras que otras son instalables desde repositorios públicos como GitHub. Por lo general, la gran mayoría las instalaremos desde R, utilizando la función install.packages(\"package\", dependencies = T). Esta función buscará la versión más reciente del paquete solicitado y la descargará e instalará para que pueda ser utilizada por nosotros. Un ejemplo:\ninstall.packages(\"tidyverse\", dependencies = T)\nEsta línea descargará el paquete tidyverse, que es un “súper” paquete formado por muchos otros paquetes que forman un “dialecto” dentro de R. Por el momento no te preocupes por eso, solo lo instalamos para evitarnos muchas descargas independientes de paquetes que pueden llegar a serte extremadamente útiles, tal como ggplot2. Es importante mencionar que una cosa es instalar la librería y otra cosa es utilizar la librería, para lo cual necesitamos de la función library(package), tal que:\n\nlibrary(ggplot2)\n\nUna vez que hicimos esto ya podemos utilizar TODAS las funciones que forman parte de la librería. ¿Y si solo quiero utilizar una función particular? En ese caso puedes utilizar el operador ::. Probemos viendo la ayuda de la función str_extract de la librería stringr que forma parte del tidyverse:\n\n?stringr::str_extract\n\nAhora que sabemos qué es una función, qué es una librería y cómo utilizarlos, vayamos a explorar el otro tipo de objetos: las variables.\n\n\n\n\n\n\nNota\n\n\n\nSi te interesa ver la referencia de una librería puedes hacerlo con la función citation(\"librería\").",
    "crumbs": [
      "Biología, Ciencia de Datos y `R`",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bases de R</span>"
    ]
  },
  {
    "objectID": "c03_bases_r.html#variables",
    "href": "c03_bases_r.html#variables",
    "title": "3  Bases de R",
    "section": "3.4 Variables",
    "text": "3.4 Variables\nA diferencia de una función en la cual almacenamos series de pasos para obtener un resultado, una variable nos permite almacenar todo lo demás: resultados, números, texto, tablas, e incluso otras variables. Su declaración la vimos arriba: con el símbolo de asignación (&lt;-). Para imprimir el resultado en pantalla podemos llamar a la variable o utilizar la función print(var):\n\nvar &lt;- 1:5\nprint(var)\n\n[1] 1 2 3 4 5",
    "crumbs": [
      "Biología, Ciencia de Datos y `R`",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bases de R</span>"
    ]
  },
  {
    "objectID": "c03_bases_r.html#tipos-de-variables",
    "href": "c03_bases_r.html#tipos-de-variables",
    "title": "3  Bases de R",
    "section": "3.5 Tipos de variables",
    "text": "3.5 Tipos de variables\nExisten dos tipos de variables, los cuales a su vez se subdividen en otros tipos. Para conocer el tipo de una variable utilizamos la función typeof(var), mientras que las funciones is.*() nos permiten probar si una variable es de un tipo en específico (e.g. is.character(var)).\n\n3.5.1 Datos\nLas variables que contienen un solo elemento se conocen como datos:\n\nCharacter: Cadena de caracteres, indicadas por comillas dobles o sencillas:\n\n\nchar &lt;- \"a\"\ntypeof(char)\n\n[1] \"character\"\n\n\n\nInteger: Números enteros, indicados por la letra “L” después del número:\n\n\ninteger &lt;- 5L\ntypeof(integer)\n\n[1] \"integer\"\n\n\n\nDouble: Fracciones, también conocidos como floating points:\n\n\ndbl &lt;- 7/5\ntypeof(dbl)\n\n[1] \"double\"\n\n\n\nLogical: Valor lógico o booleano. Solo puede tomar dos valores: TRUE o FALSE o sus abreviaturas T o F\n\n\nbool &lt;- is.double(dbl)\nprint(paste('valor: ', bool))\n\n[1] \"valor:  TRUE\"\n\nprint(paste('tipo: ', typeof(bool)))\n\n[1] \"tipo:  logical\"\n\n\n\nComplex: Números complejos, con una parte real y una imaginaria:\n\n\ncomp.n &lt;- 8+3i\ntypeof(comp.n)\n\n[1] \"complex\"\n\n\n\n\n3.5.2 Estructuras/arreglos\nLas estructuras son colecciones de valores, cada una con sus propiedades y sus métodos de acceso a los valores que las conforman (indexación/indización):\n\n3.5.2.1 Vector\nLa estructura más básica. Una colección unidimensional de elementos. Las funciones para crearlos son: c(), la cual combina una serie de elementos en un vector (mismo tipo) o una lista (diferentes tipos); vector(mode, length): genera un vector “vacío” con longitud (número de elementos) length y tipo de datos 'mode'.\n\nvect_1 &lt;- c(1:5)\nvect_2 &lt;- vector(mode = 'double', length = 5)\nprint(vect_1)\n\n[1] 1 2 3 4 5\n\nprint(vect_2)\n\n[1] 0 0 0 0 0\n\n\nPara indexar un vector utilizamos: var[i], donde i representa la posición del (los) elemento(s) de interés:\n\nprint(vect_1[4])\n\n[1] 4\n\nprint(vect_1[2:3])\n\n[1] 2 3\n\n\nEjercicio: Genera un vector donde cada elemento sea una letra de tu primer nombre, luego extrae los elementos último, primero y segundo.\n\n\n3.5.2.2 Factor\nRepresentan variables categoricas. Contienen los valores de la variable así como los valores posibles que puede tomar (niveles). Se crean con la función factor(x, levels, labels), donde x representa los valores de la variable, levels representa los posibles niveles y labels (opcional) representan etiquetas de cada nivel:\n\nfact_1 &lt;- factor(x = c('a', 'b', 'c'), \n                 levels = c('a', 'b', 'c', 'd', 'e'))\nfact_1\n\n[1] a b c\nLevels: a b c d e\n\n\nEjercicio: 1. Genera un factor con 5 categorías de acuerdo, con 4 observaciones que representen 2 de esas 5 categorías. Las categorías tienen orden. 2. Extrae los elementos 2 y 4 de ese factor.\n\n\n3.5.2.3 Matrix\nUna estructura bidimensional (columnas/renglones). Se generan utilizando la función matrix(data, nrow, ncol, byrow), donde data representa la colección de objetos que formarán la matriz, nrow y ncol el número de renglones y columnas, respectivamente, y byrow si se llenará por renglones (FALSE) o por columnas (TRUE, por defecto)\n\nmat_1 &lt;- matrix(c(T, F, F, T), nrow = 2, ncol = 2)\nmat_1\n\n      [,1]  [,2]\n[1,]  TRUE FALSE\n[2,] FALSE  TRUE\n\n\nPara indexar una matriz utilizaremos también corchetes; sin embargo, indicaremos el par renglón,columna donde se ubica el elemento:\n\nprint(mat_1[1,1])\n\n[1] TRUE\n\nprint(mat_1[2,1])\n\n[1] FALSE\n\n\nSi quisieramos indexar toda una dimensión (renglón o columna), utilizaríamos el mismo método, dejando en blanco la dimensión contraria; es decir, si nos interesa una columna, dejaremos en blanco el número de renglón y si nos interesa un renglón dejaremos en blanco el número de columna:\n\nprint(mat_1[,2])\n\n[1] FALSE  TRUE\n\nprint(mat_1[1,])\n\n[1]  TRUE FALSE\n\n\nOJO: print(mat_1[c(1,1)]) NO da la diagonal de la matriz, esa la obtenemos con diag(mat1), sino que repite 2 veces el primer elemento de la matriz\nEjercicio:\n\nExtrae los elementos F de mat_1:\n\n\nExtrae los elementos de la diagonal de mat_1:\n\n\n\n3.5.2.4 DataFrame\nEl DataFrame es la estructura con la que más comúnmente estaremos en contacto. Es una tabla completa que, a diferencia de la matriz, contiene nombres de columnas. Tiene dos particularidades que hay que considerar: 1) todos los elementos que forman a cada columna deberán ser del mismo tipo y 2) El número de renglones de todas las columnas debe de ser el mismo. Se crean utilizando la función data.frame(col_name = data, ...):\n\ndf_1 &lt;- data.frame(col_a = c(0:5), \n                   col_b = c(20:25), \n                   col_c = c(15:20))\n# Nota: Si no se indica el nombre de las columnas este será asignado automáticamente\ndf_1\n\n\n  \n\n\n\nExisten distintos modos de indexar un DataFrame. El primero de ellos var$col_name:\n\ndf_1$col_a\n\n[1] 0 1 2 3 4 5\n\n\nComo vemos, este modo de indexación extrae la columna completa en forma de un vector, por lo que si queremos accesar un valor en particular solo habrá que utilizar ese método de indexación:\n\ndf_1$col_b[4]\n\n[1] 23\n\n\nFinalmente, también podemos utilizar el método de indexación de matrices, recordando que se especifica el par renglón, columna:\n\ndf_1[4,2]\n\n[1] 23\n\n\nEjercicio:\n\nExtrae los elementos 1, 3 y 5 de la columna c:\n\n\nExtrae los elementos 1, 3 y 5 de las columnas a y c:\n\nEsta es la estructura con la que más debemos de familiarizarnos, pues la mayor parte de nuestros datos los representamos en ella. ¿Siempre debemos de ingresar los datos manualmente? Para nada, tenemos todo un abanico de funciones que nos permiten cargar datos directamente de archivos, pero eso lo veremos más adelante.\n\n\n3.5.2.5 List\nLas listas son una colección de cualquier combinación de datos o estructuras, incluyendo otras listas:\n\nl_1 &lt;- list(df_1, mat_1, vect_1)\nprint(l_1)\n\n[[1]]\n  col_a col_b col_c\n1     0    20    15\n2     1    21    16\n3     2    22    17\n4     3    23    18\n5     4    24    19\n6     5    25    20\n\n[[2]]\n      [,1]  [,2]\n[1,]  TRUE FALSE\n[2,] FALSE  TRUE\n\n[[3]]\n[1] 1 2 3 4 5\n\n\nEn la salida de arriba vemos el método de indexación: var[[i]][j,k], donde i representa el número de objeto en la lista y j,k el par renglón,columna (de aplicar). En el caso de DataFrames podemos seguir utilizando el operador $ para utilizar los noombres de columnas:\n\nl_1[[1]]$col_a[6]\n\n[1] 5\n\n\nEjercicios: Desde la lista:\n\nExtrae el segundo elemento de la segunda columna de mat_1:\n\n\nExtrae el quinto elemento de vect_1:\n\n\nExtrae los elementos 3 y 4 de la columna b de df_1:\n\n\nExtrae los elementos 6 y 1 de las columnas a y c de df_1:\n\nAhora que hemos hablado de todos los tipos de estructuras, y antes de encaminarnos hacia los procesos de automatización, hablemos de cómo cargar nuestros datos en R.",
    "crumbs": [
      "Biología, Ciencia de Datos y `R`",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bases de R</span>"
    ]
  },
  {
    "objectID": "c03_bases_r.html#carga-de-datos",
    "href": "c03_bases_r.html#carga-de-datos",
    "title": "3  Bases de R",
    "section": "3.6 Carga de datos",
    "text": "3.6 Carga de datos\nEl cómo carguemos nuestros datos depende de varios factores: a) el formato del archivo en el que estén archivados, b) el cómo esté acomodada la información, c) qué necesitemos para hacer los análisis posteriores. El ejemplo más simple es cargar un archivo de texto separado por comas, en el cuál las comas separan las columnas y los saltos de línea los renglones. Tomemos como ejemplo el archivo \"datos1.csv\":\n\ndatos1 &lt;- read.table(\"datos/datos1.csv\", sep = \",\", header = T)\n\nPodemos verificar la información obteniendo el encabezado del data.frame:\n\nhead(datos1)\n\n\n  \n\n\n\nLos archivos separados por comas son uno de los formatos más comunes, por lo que R cuenta con una función dedicada (la función read.table() con valores predefinidos):\n\ndatos1 &lt;- read.csv(\"datos/datos1.csv\")\nhead(datos1)\n\n\n  \n\n\n\nAquí todo se cargó sin ningún problema porque el archivo estaba listo para ser leído, pero esto no siempre es el caso. Por ejemplo, los datos pueden estar en la segunda hoja de un archivo Excel, la cuál tiene 5 renglones de encabezado dando una descripción de los datos y en el renglón 6 están dispuestos los nombres de las variables. Además, sabemos que vamos a realizar un análisis de agrupamientos jerárquicos (clúster), el cuál requiere que los nombres de los individuos estén marcados en los nombres de los renglones (Ver archivo datos2.xlsx):\n\ndatos2 &lt;- read.table(\"datos/datos2.xlsx\")\n\nEsto, evidentemente, da un error, pues le dimos a la función read.table() un archivo que no es de texto simple, sino un Excel.\nVamos entonces por partes:\n\nFormato: es un archivo Excel, por lo que hay que utilizar una función que permita leer ese tipo de archivo. En nuestro caso utilizaremos la función readxl::read_xlsx(). Aquí no obtendremos ningún error, pues el tipo de archivo es el correcto. Lo único que obtenemos es un mensaje (New names:) que nos diría a qué columnas se les asignó nombres nuevos (y cuáles).\n\n\ndatos2 &lt;- readxl::read_xlsx(\"datos/datos2.xlsx\")\n\nNew names:\n• `Lp14-C` -&gt; `Lp14-C...94`\n• `Lp14-C` -&gt; `Lp14-C...109`\n\n\nPero, ¿qué pasa si leemos el encabezado? Resulta que la función cargó la primera hoja del excel, cuando en realidad nosotros queríamos la segunda\n\nhead(datos2)\n\n\n  \n\n\n\nNecesitamos entonces indicar explícitamente que queremos se cargue la segunda hoja:\n\ndatos2 &lt;- readxl::read_xlsx(\"datos/datos2.xlsx\", sheet = 2)\n\nNew names:\n• `` -&gt; `...2`\n• `` -&gt; `...3`\n• `` -&gt; `...4`\n• `` -&gt; `...5`\n• `` -&gt; `...6`\n• `` -&gt; `...7`\n• `` -&gt; `...8`\n• `` -&gt; `...9`\n• `` -&gt; `...10`\n• `` -&gt; `...11`\n• `` -&gt; `...12`\n• `` -&gt; `...13`\n• `` -&gt; `...14`\n• `` -&gt; `...15`\n• `` -&gt; `...16`\n• `` -&gt; `...17`\n• `` -&gt; `...18`\n• `` -&gt; `...19`\n• `` -&gt; `...20`\n• `` -&gt; `...21`\n• `` -&gt; `...22`\n• `` -&gt; `...23`\n• `` -&gt; `...24`\n• `` -&gt; `...25`\n• `` -&gt; `...26`\n• `` -&gt; `...27`\n• `` -&gt; `...28`\n• `` -&gt; `...29`\n• `` -&gt; `...30`\n• `` -&gt; `...31`\n• `` -&gt; `...32`\n• `` -&gt; `...33`\n• `` -&gt; `...34`\n• `` -&gt; `...35`\n• `` -&gt; `...36`\n• `` -&gt; `...37`\n• `` -&gt; `...38`\n• `` -&gt; `...39`\n• `` -&gt; `...40`\n• `` -&gt; `...41`\n• `` -&gt; `...42`\n• `` -&gt; `...43`\n• `` -&gt; `...44`\n• `` -&gt; `...45`\n• `` -&gt; `...46`\n• `` -&gt; `...47`\n• `` -&gt; `...48`\n• `` -&gt; `...49`\n• `` -&gt; `...50`\n\nhead(datos2)\n\n\n  \n\n\n\n\nSaltar renglones: Ya tenemos la hoja que nos interesa, el problema es que cargó el encabezado como renglones con observaciones, por lo que hay que saltarlos:\n\n\ndatos2 &lt;- readxl::read_xlsx(\"datos/datos2.xlsx\",\n                            sheet = 2,\n                            skip = 5)\nhead(datos2)",
    "crumbs": [
      "Biología, Ciencia de Datos y `R`",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bases de R</span>"
    ]
  },
  {
    "objectID": "c03_bases_r.html#operaciones-comunes",
    "href": "c03_bases_r.html#operaciones-comunes",
    "title": "3  Bases de R",
    "section": "3.7 Operaciones comunes",
    "text": "3.7 Operaciones comunes\nComo ya vimos, no siempre vamos a obtener la información en el formato que necesitamos. Aunque podemos solventar algunas de estas carencias durante la carga de los archivos, a veces necesitamos “masajear” los datos o “manipularlos” para llevarlos a lo que las funciones que nos interesan nos piden. Tomemos como ejemplo los datos de la hoja número 1 del archivo datos2.xlsx:\n\ndatos3 &lt;- readxl::read_xlsx(\"datos/datos2.xlsx\", sheet = 1)\nhead(datos3)\n\n\n  \n\n\n\n\n3.7.1 Transposición\nEn estos datos las presas están en los renglones, y los individuos de los depredadores en las columnas. Aunque esta disposición no tiene fundamentalmente nada de malo, normalmente las instancias (observaciones individuales/réplicas) están en los renglones, y las variables (presas) en las columnas. Es necesario entonces transponer los datos. Esto lo podemos hacer de manera sencilla con la función t():\n\nhead(t(datos3))[,3]\n\n    Prey    Cu1-C    Cu2-C    Cu3-C    Cz1-C    Ar1-C \n\"Prey_3\"      \"0\"      \"0\"      \"0\"     \" 0\"      \"0\" \n\n\nEsto logró nuestro objetivo, aunque con un pequeño gran problema: toda la información es texto. ¿Por qué? Resulta que las columnas solo pueden contener datos de un solo tipo, por lo que al tener el texto de las especies presa todas las columnas son transformadas a cadenas de caracter. ¿Qué podemos hacer? Transponer los datos en tres pasos.\n\n\n3.7.2 “Rebanadas” (slices)\nEl primer paso es separar los nombres de las presas de los datos de los depredadores:\n\nprey &lt;- datos3$Prey\nhead(prey)\n\n[1] \"Prey_1\" \"Prey_2\" \"Prey_3\" \"Prey_4\" \"Prey_5\" \"Prey_6\"\n\ncounts &lt;- datos3[,2:ncol(datos3)]\nhead(counts)\n\n\n  \n\n\n\nTransponer la matriz de conteos:\n\ncountst &lt;- as.data.frame(t(counts))\nhead(countst)\n\n\n  \n\n\n\n\n\n3.7.3 Cambiar nombres de columnas\nPara acceder o asignar los nombres de las columnas o renglones de arreglos bidimensionales podemos utilizar los atributos colnames(data) y rownames(data):\n\ncolnames(countst) &lt;- prey\nhead(countst)\n\n\n  \n\n\n\n\n\n3.7.4 Transformaciones\nEl resultado de las operaciones anteriores es una matriz; sin embargo, podemos pasarlo a un data.frame:\n\ncountst &lt;- as.data.frame(countst)\nhead(countst)\n\n\n  \n\n\n\n\n\n3.7.5 Añadir vectores como columnas\nAhora tenemos un data.frame; sin embargo, tenemos las claves como nombres de los renglones y, según qué querramos realizar, podemos necesitar que estas formen su propia columna. Una forma de hacerlo es: 1) extraer los nombres de los renglones y 2) añadirlos como una columna adicional:\n\nkeys &lt;- rownames(countst)\ncountst &lt;- cbind(keys, countst)\nhead(countst)",
    "crumbs": [
      "Biología, Ciencia de Datos y `R`",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bases de R</span>"
    ]
  },
  {
    "objectID": "c03_bases_r.html#operadores-lógicos",
    "href": "c03_bases_r.html#operadores-lógicos",
    "title": "3  Bases de R",
    "section": "3.8 Operadores lógicos",
    "text": "3.8 Operadores lógicos\nLos operadores lógicos nos sirven para hacer comparaciones y obtener un resultado booleano (T o F). Los más comunes son:\n\ncond1|cond2: Condicional “O”. T si se cumple alguna de las dos condiciones\n\n\nc &lt;- 5L\nis.integer(c)|is.double(c)\n\n[1] TRUE\n\n\n\ncond1&cond2: Condicional “Y”. T si se cumplen ambas condiciones\n\n\nis.integer(c)&(c&gt;3)\n\n[1] TRUE\n\n\n\n&lt;, &gt;: Comparaciones, menor qué o mayor qué\n\n\nprint(c&lt;10)\n\n[1] TRUE\n\nprint(c&gt;5)\n\n[1] FALSE\n\n\n\n&lt;=, &gt;=: Comparaciones, menor o igual qué; mayor o igual qué.\na!=b: Desigualdad, T si a es diferente de b\n\n\nc!=5\n\n[1] FALSE",
    "crumbs": [
      "Biología, Ciencia de Datos y `R`",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bases de R</span>"
    ]
  },
  {
    "objectID": "c03_bases_r.html#automatización",
    "href": "c03_bases_r.html#automatización",
    "title": "3  Bases de R",
    "section": "3.9 Automatización",
    "text": "3.9 Automatización\nEl primer paso de la automatización es generar funciones que te permitan realizar la misma acción múltiples veces y no cometer algún error en dichas repeticiones. Esta práctica se deriva de una de las máximas más importantes en programación: “Don’t repeat yourself” (DRY, no te repitas a ti mismo); es decir, dejar que la computadora haga las repeticiones por sí mismas. En la práctica, esto implica no estar copiando y pegando el mismo bloque de código una y otra vez y luego modificarlo manualmente, sino que escribirlo solo una vez y luego decirle a la computadora que repita esa acción n veces, modificando algún(os) argumento(s), o que cambie el comportamiento en función de si se cumple o no una condición en nuestros datos. En ese caso, las estructuras de control son nuestras mejores aliadas.\n\n3.9.1 Ciclos for\nHay distintas formas de realizar la repetición de acciones, pero hoy introduciremos únicamente los ciclos for por ser los más probables a ser requeridos. Un ciclo for consta de cuatro elementos:\nfor (variable in vector) {action}\n\nLa estructura de control for, evidentemente\nUna variable que hace las veces de un marcador de posición; es decir, la utilizaremos para indicar en dónde se van a sustituir los valores que queremos ciclar\nLos valores que ciclaremos, contenidos en un vector\nLa acción a realizar\n\nComo muchas otras cosas, es más fácil entenderlo utilizando algunos ejemplos. El primero de ellos es simplemente imprimir la secuencia de números del 1 al 10. Aunque podemos escribir 10 veces la función print e ir cambiando el número, es mucho más sencillo:\n\nfor (i in 1:10) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\n¿Qué fue lo que hizo la computadora? Utilizó la función print() para mostrarnos el contenido de i, el cuál es el iésimo elemento de la secuencia 1:10. En otras palabras, si es la segunda vuelta que da, imprimirá el número 2, si es la séptima imprimirá 7, y así hasta que termine con todos los elementos en la secuencia. Ahora, sumemos 2 a cada número de la secuencia:\n\nfor (i in 1:10) {\n  print(i+2)\n}\n\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n[1] 11\n[1] 12\n\n\nAl igual que en el caso anterior, tomó uno por uno los valores (de forma secuencial), le añadió 2 y luego mostró el resultado en pantalla. Este tipo de estructuras son sumamente útiles, pues no solo nos ahorran errores, sino también tiempo de ejecución. Sobra decir que no es la única manera de hacer este tipo de ciclos, ni tampoco es la más rápida. Tenemos algo que se conocen como funciones vectorizadas.\n\n\n3.9.2 Familia de funciones apply()\nPodemos entender la vectorización como la aplicación de una función a cada elemento de un vector (igual que el ciclo for), aunque procesando todo el vector “al mismo tiempo”. Esta última parte no es estrictamente verdad, aunque lo cierto es que son más rápidas que los ciclos for tradicionales. En R tenemos toda una gama de funciones que hacen justo eso, la familia apply y sus relacionadas, compuesta por las funciones:\n\napply\nlapply\nsapply\ntapply\nmapply\nby\naggregate\n\nTodas estas funciones manipulan porciones de datos como matrices, arreglos, listas o data frames de forma repetitiva. Básicamente nos permiten evitar el uso explícito de un ciclo for. Toman como argumento una lista, matriz o data frame y le aplican una función con uno o más argumentos adicionales. Esta función puede ser:\n\nUna función de agregación, por ejemplo la media o la suma\nFunciones de transformaciones o para extraer sub-conjuntos\nOtras funciones vectorizadas, que dan como resultado estructuras más complejas como listas, vectores, matrices o arreglos. Pero basta de cháchara, ¿cómo y cuándo debemos de utilizarlas? La respuesta depende totalmente de la estructura de los datos y el formato de salida que se necesite. Veamos algunos casos de uso:\n\n\n3.9.2.1 apply()\nEsta es la función “madre” de las demás, la cual opera sobre arreglos. Para simplicidad, vamos a limitarnos a arreglos bi-dimensionales como las matrices o data.frames. La sintaxis para su uso es apply(X, MARGIN, FUN, ...), donde X es el arreglo, MARGIN es el márgen sobre el cuál va a actuar la función; es decir, si queremos que la aplique a cada renglón (MARGIN = 1) o a cada columna (MARGIN = 2), y FUN es el nombre de la función a aplicar (puede ser cualquiera, incluso una función definida por nosotros).\nPodemos pensar en obtener la suma o el promedio de cada columna de nuestro data.frame con las presas utilizando un ciclo, o podemos utilizar apply, tal que:\n\nsums &lt;- apply(X = datos2[,2:ncol(datos2)], MARGIN = 2, FUN = sum)\nhead(sums)\n\nPrey_1 Prey_2 Prey_3 Prey_4 Prey_5 Prey_6 \n     4     12      9     44      5      5 \n\n\n\n\n3.9.2.2 aggregate()\nOtra función extremadamente útil es la función aggregate(). Esta función nos permite aplicar una función a distintos grupos. Un escenario clásico es obtener el promedio de una variable para cada grupo, por ejemplo el promedio de conteos de quetognatos. La forma “tradicional” es: aggregate(x, by, FUN), donde x es el objeto R a agrupar, by es una lista con los grupos y FUN es la función a aplicar; sin embargo, podemos utilizar una notación más compacta utilizando una formula: aggregate(forumla, data, FUN). Las fórmulas en R son objetos sumamente útiles y que se utilizan para una gran diversidad de cosas. Su estructura es: Y ~ X, y se lee “Y con respecto a X”. En nuestro caso particular para obtener los conteos promedio de la presa 28:\n\naggregate(Prey_28~sp, data = datos1, FUN = sum)\n\n\n  \n\n\n\n\n\n\n3.9.3 Condicionales\nOk, ahora conocemos una manera de aplicar una función a una serie de elementos, pero que pasa si queremos aplicarla de manera condicionada; es decir, si queremos solo imprimir los números mayores a 5, por ejemplo. Eso es justo de lo que se tratan los condicionales, particularmente if, else, e ifelse. La lógica detrás de ellos es sumanmente simple: si se cumple una condición, realiza una acción, si no se cumple, realiza otra (o no realices nada). Comencemos con if. Su estructura es: if(condition){action T}, que notarás es básicamente la descripción que dimos, solo que sin una acción en caso de que no se cumpla la condición. Un ejemplo sería proporcionar un número y que nos diga si es mayor a 5:\n\nx &lt;- 6\nif (x&gt;5) {print(\"x es mayor a 5\")}\n\n[1] \"x es mayor a 5\"\n\n\n¿Y si no se cumple la condición? Veamos qué pasa:\n\nx &lt;- 1\nif (x&gt;5) {print(\"x es mayor a 5\")}\n\nR no nos da ninguna salida, pues no sabe qué hacer. Una forma de decirle es utilizando el complemento de if, else, que nos permite establecer una acción secundaria:\n\nif (x&gt;5) {print(\"x es mayor a 5\")\n  }else{print(\"x no es mayor a 5\")}\n\n[1] \"x no es mayor a 5\"\n\n\nUna notación mucho más compacta para este tipo de casos es utilizar la función ifelse(condition, action T, action F):\n\nx &lt;- 4\nifelse(x &gt; 5,\n       \"x es mayor a 5\",\n       \"x no es mayor a 5\")\n\n[1] \"x no es mayor a 5\"\n\n\nEl resultado fue el mismo que el anterior, pero qué pasa si tenemos más de dos escenarios, que, por ejemplo, nos interesara decir si el número es mayor, menor o igual a 5. Veamos primero lo que sucede si establecemos que x sea 5:\n\nx &lt;- 5\nifelse(x &gt; 5,\n       \"x es mayor a 5\",\n       \"x no es mayor a 5\")\n\n[1] \"x no es mayor a 5\"\n\n\nLa computadora no hizo nada mal, 5 no es mayor a 5. En otros lenguajes de programación añadiríamos una estructura llamada elif, pero en R solo hay que añadir otro(s) if. Mientras que else aplica para todos los casos donde la condición no se cumpla, estos if secundarios nos permiten establecer condiciones adicionales para cuando la condición principal no se cumpla. Volviendo a nuestro problema con x = 5:\n\nx &lt;- 4\nif (x &gt; 5) {\n  print(\"x es mayor a 5\")\n}\nif (x == 5) {\n  print(\"x es 5\")\n}\nif (x &lt; 5){\n  print(\"x es menor a 5\")\n  }\n\n[1] \"x es menor a 5\"\n\n\nAhora sí cubrimos todas nuestras bases para este problema de comparación. Como te imaginarás, el siguiente paso lógico es mezclar for e if o, mejor dicho, anidarlos:\n\nx &lt;- 1:10\n\nfor (i in x) {\n  if (i &gt; 5) {\n    print(\"x es mayor a 5\")\n  }\n  if (i == 5) {\n    print(\"x es 5\")\n  }else if (i &lt; 5) {print(\"x es menor a 5\")}\n}\n\n[1] \"x es menor a 5\"\n[1] \"x es menor a 5\"\n[1] \"x es menor a 5\"\n[1] \"x es menor a 5\"\n[1] \"x es 5\"\n[1] \"x es mayor a 5\"\n[1] \"x es mayor a 5\"\n[1] \"x es mayor a 5\"\n[1] \"x es mayor a 5\"\n[1] \"x es mayor a 5\"\n\n\nCon esto también quiero decir que podemos anidar ciclos for, tal que:\n\nx &lt;- 1:5\ny &lt;- 100:105\n\nfor (i in x) {\n  for (j in y) {\n    print(c(i,j))\n  }\n}\n\n[1]   1 100\n[1]   1 101\n[1]   1 102\n[1]   1 103\n[1]   1 104\n[1]   1 105\n[1]   2 100\n[1]   2 101\n[1]   2 102\n[1]   2 103\n[1]   2 104\n[1]   2 105\n[1]   3 100\n[1]   3 101\n[1]   3 102\n[1]   3 103\n[1]   3 104\n[1]   3 105\n[1]   4 100\n[1]   4 101\n[1]   4 102\n[1]   4 103\n[1]   4 104\n[1]   4 105\n[1]   5 100\n[1]   5 101\n[1]   5 102\n[1]   5 103\n[1]   5 104\n[1]   5 105",
    "crumbs": [
      "Biología, Ciencia de Datos y `R`",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bases de R</span>"
    ]
  },
  {
    "objectID": "c03_bases_r.html#ejercicios",
    "href": "c03_bases_r.html#ejercicios",
    "title": "3  Bases de R",
    "section": "3.10 Ejercicios",
    "text": "3.10 Ejercicios\n\n¿Qué es un objeto?\n¿Cuál es la diferencia entre una variable y una función?\n¿Qué es una librería? ¿Qué pasa si utilizas la función ggplot() sin haber cargado la librería ggplot2?\n¿Cuál es la diferencia entre un dato y una estructura?\nCarga los datos1.csv y añade un renglón adicional con los totales para cada columna.\nCarga los datos1.csv y obten una tabla con los conteos promedio de cada presa para cada especie (aggregate/apply o ciclos).\nCombina el ciclo for anidado del último ejemplo con un condicional, en el cuál se imprima si la suma de ambos números (i, j) es mayor, menor o igual a 105.\nEjecuta el siguiente código y responde ¿qué hace cada línea? Recuerda que ante la duda siempre puedes revisar la ayuda de las funciones.\n\n```{r}\npkgs &lt;- c(\"tidyverse\", \"ggtext\", \"Rmisc\", \"rcompanion\",\n          \"gap\", \"brms\", \"stats4\", \"Metrics\",\n          \"performance\", \"ggdendro\", \"dendextend\",\n          \"factoextra\", \"cluster\", \"NbClust\",\n          \"FactoMineR\", \"MASS\", \"klaR\", \"vegan\",\n          \"tidymodels\", \"MVN\", \"Hotelling\",\n          \"gridExtra\", \"GGally\", \"car\", \"FSA\",\n          \"randomcoloR\", \"gganimate\", \"reshape2\",\n          \"DescTools\", \"PerformanceAnalytics\",\n          \"corrplot\", \"ggrepel\", \"pROC\", \"RCurl\",\n          \"glmnet\", \"pcsl\", \"MuMIn\", \"nlraa\")\n\ninstall.packages(pkgs, dependencies = T)\n```\n\n\n\n\n\n\nImportante\n\n\n\nSi tienes algún problema con la ejecución por favor házmelo saber. Esto cubre la instalación de las librerías que utilizaremos durante el curso, por lo que es sumamente importante que se ejecuten correctamente.\nSi en algún momento de la instalación R te pregunta si quieres instalar desde la fuente dile que NO. Aunque las versiones fuente están más actualizadas pueden llegar a romper la compatibilidad con otras librerías.\n\n\n\n\n\n\nR Core Team. 2022. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing.",
    "crumbs": [
      "Biología, Ciencia de Datos y `R`",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Bases de R</span>"
    ]
  },
  {
    "objectID": "c04_tidyverse.html",
    "href": "c04_tidyverse.html",
    "title": "4  Introducción a tidyverse",
    "section": "",
    "text": "4.1 Un dialecto dentro de R\nEn la sesión anterior mencionamos que hay un “paquete de paquetes” que da lugar a un “dialecto” dentro de R: tidyverse, pero no especifiqué a qué me refería con ello. Pues bien, mientras que en R base los procedimientos los realizamos línea a línea, generando a veces una gran cantidad de objetos intermedios o sobreescribiendo los existentes, tidyverse está basado en el paradigma funcional de la programación, con una “gramática” (sintaxis) distinta, muy similar a lo que veremos en ggplot2. De hecho, ggplot2 es un paquete del tidyverse, por lo que el “dialecto” tidy comparte la filosofía declarativa y fomenta la “encadenación” de comandos. A muchas personas les gusta más la forma tidy, a otras les gusta más trabajar con R base. En lo personal soy partidario de que utilices lo que más te acomode, siempre y cuando lo que hagas tenga sentido, pero más de esto cerca del final de la sesión.\nEste nuevo dialecto fue creado con un objetivo en particular: la ciencia de datos. Como tal, cuenta con una gran cantidad de librerías (y por lo tanto funciones) especializadas para realizar operaciones rutinarias. ¿Quieres realizar gráficos? En la siguiente sesión hablaremos de ggplot2. ¿Quieres hacer “manipulación” (ojo, no cuchareo) de datos? Aquí veremos algnas funciones de dplyr. ¿Quieres trabajar con procesamiento de cadenas de caractér? Para esto está stringr. ¿Quieres herramientas para programación funcional? Ve hacia purrr (sí, triple r). readxl es otra librería con la que ya estás familiarizado y que forma parte del tidyverse. ¿Tienes un problema en el que que necesitas manejar fechas? lubridate puede ser una opción. Puedes conocer todos los paquetes que forman el tidyverse con:\ntidyverse::tidyverse_packages(include_self = T)\n\n [1] \"broom\"         \"conflicted\"    \"cli\"           \"dbplyr\"       \n [5] \"dplyr\"         \"dtplyr\"        \"forcats\"       \"ggplot2\"      \n [9] \"googledrive\"   \"googlesheets4\" \"haven\"         \"hms\"          \n[13] \"httr\"          \"jsonlite\"      \"lubridate\"     \"magrittr\"     \n[17] \"modelr\"        \"pillar\"        \"purrr\"         \"ragg\"         \n[21] \"readr\"         \"readxl\"        \"reprex\"        \"rlang\"        \n[25] \"rstudioapi\"    \"rvest\"         \"stringr\"       \"tibble\"       \n[29] \"tidyr\"         \"xml2\"          \"tidyverse\"\nA partir de aquí, el cómo aprovecharlos depende mucho de el problema que tengas entre manos, pero la idea general es la misma: utilizar una gramática declarativa para llegar a la solución. Veamos en qué consiste con un ejemplo cotidiano: obtener el promedio de una variable para varios grupos.",
    "crumbs": [
      "Biología, Ciencia de Datos y `R`",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducción a `tidyverse`</span>"
    ]
  },
  {
    "objectID": "c04_tidyverse.html#un-dialecto-dentro-de-r",
    "href": "c04_tidyverse.html#un-dialecto-dentro-de-r",
    "title": "4  Introducción a tidyverse",
    "section": "",
    "text": "4.1.1 Promedios de grupos: aggregate vs group_by() |&gt; summarise()\nVeamos un ejemplo en el cual calcularemos la longitud de pico promedio para cada especie de pingüino, según los datos de palmerpenguins. Primero, carguemos los datos:\n\ndatos1 &lt;- palmerpenguins::penguins\nhead(datos1)\n\n\n  \n\n\n\nAhora, obtengamos los promedios con la función aggregate, tal y como vimos en la sesión anterior:\n\naggregate(bill_length_mm~species, data = datos1, FUN = mean)\n\n\n  \n\n\n\nAhora repliquémoslo con tidyverse, particularmente con las funciones group_by y summarise (o summarize) de la librería dplyr:\n\nlibrary(dplyr)\n\ndatos1 |&gt;\n  group_by(species) |&gt;\n  summarise(mbill_l = mean(bill_length_mm, na.rm = T),\n            mbill_d = mean(bill_depth_mm, na.rm = T))\n\n\n  \n\n\n\nNotarás que el resultado es exactamente el mismo, aunque la forma de hacerlo es diferente. Descompongámosla paso a paso para ver qué es lo que está pasando:\n\nlibrary(dplyr): cargamos la librería dplyr, la cual contiene las funciones que nos interesa aplicar: group_by y summarise.\nLlamamos directamente a nuestros datos (datos1) y utilizamos un operador que no habíamos visto: |&gt;. Este es el operador pipe, el cual pasa lo que está a la izquierda de él como el primer argumento de lo que está a la derecha de él. Esto puede sonar confuso, pero la instrucción datos1 |&gt; group_by(sp) es equivalente a group_by(datos1, sp). Podemos ver al operador pipe como su nombre sugiere: una tubería que manda la información de un lado hacia otro.\ngroup_by(sp): Como te mencionaba, tidy es un poco más explícito que R base. Mientras que el argumento con la fórmula en aggregate indica cómo se van a agrupar los datos, aquí primero los agrupamos y después aplicamos la función que nos interesa. group_by hace justamente eso, agrupar nuestros datos, nada más, nada menos. El argumento principal de esta función es la(s) columnas bajo las cuales queremos agrupar nuestros datos. En este caso solo es una (sp), por lo que la pasamos directamente.\nNuevamente utilizamos el operador |&gt;. Hasta este punto hemos pasado los datos1 a la función group_by(sp), por lo que ya están agrupados por especie, pero falta aplicar la función mean() para obtener el promedio, entonces volvemos a encadenar hacia la función summarise(). Esta función recibe una serie de pares nombres de columnas y funciones a aplicar. En este caso, estamos generando una columna llamada mbill_l que contiene los promedios de la columna bill_length_mm de los datos1.\n\n\n\n\n\n\n\nImportante\n\n\n\nSi revisas documentación “antigua” sobre tidyverse (previa a R 4.1, de hecho), notarás que el operador pipe es %&gt;% en vez de |&gt;. El resultado es el mismo y, de hecho, a partir de R 4.1 puedes seleccionar cuál utilizar en las preferencias de RStudio. En el curso utilizaremos |&gt; por ser el operador pipe nativo de R, el cual fue introducido (como te imaginarás) en R 4.1.\n\n\n¿Abstracto? Sin duda. ¿Útil? También. ¿Más explícito que R base con aggregate? Debatible. Lo que no es debatible es que esta notación brilla especialmente en cierto tipo de problemas. Pensemos que nos interesa conocer el promedio de las longitudes de picos por especie para cada isla. Intenta hacerlo con R base y te darás cuenta de que no es tan intuitivo, salvo que estés familiarizado con el uso de fórmulas o ciclos y condcionales. ¿En tidy? Solamente hay que agregar la nueva variable de agrupamiento a group_by:\n\ndatos1 |&gt;\n  group_by(species, island) |&gt;\n  summarise(mbill_l = mean(bill_length_mm,\n                           na.rm = T))\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n\n  \n\n\n\nAhora tenemos un tibble (funcionalmente equivalente a un data.frame) con tres columnas, en donde se da el promedio para cada combinación de las variables de agrupamiento. ¿A que es más sencillo que intentar hacerlo con R base?. Esto último no es del todo cierto, pues hay una forma muy sencilla de hacerlo con aggregate(), pero encontrar una manera es parte de tu tarea, así que no arruinaré la diversión.\n\n\n\n\n\n\nNota\n\n\n\nCon esto no quiero deciro que tidy sea mejor que R base o viceversa, solamente que son dos aproximaciones, cada una con sus propias ventajas y desventajas. Por un lado, R base puede llegar a ser más compacto, pero tidy tiende a ser más explícito. Por supuesto, también hay casos en los que lo contrario es verdad.\n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nHay ocasiones en las cuales querrás evitar el uso de tidyverse, y una de ellas es al crear nuevas librerías. ¿La razón? Puedes crear un conflicto de dependencias si no lo manejas con cuidado. Otra es que es sumamente complicado depurar errores.\n\n\nTe habrás dado cuenta de que hasta este momento no hemos asignado nuestros resultados a ningún objeto. Esto se debe a dos razones. La primera, y tal vez la que pasa por tu cabeza, es que de esta manera podemos mostrar los resultados más rápidamente, y sí, pero el trasfondo está en la segunda razón: La asignación sigue un sentido opuesto al encadenamiento. Mientras que con |&gt; la información fluye de izquierda a derecha, con &lt;- la información fluye de derecha a izquierda. Quise, entonces, que primero te acostumbraras al flujo de información con pipe, pues asignar el resultado a un objeto es lo mismo que hemos hecho hasta ahora:\n\ngmeans &lt;- datos1 |&gt;\n          group_by(species, island) |&gt;\n          summarise(mbill_l = mean(bill_length_mm,\n                                   na.rm = T))\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\ngmeans\n\n\n  \n\n\n\n\n\n4.1.2 Subconjuntos de datos\nEn la sesión anterior nos familiarizamos con máscaras booleanas y la función subset. tidy tiene su propia aproximación. Pensemos que queremos quedarnos solo con los pingüinos provenientes de Biscoe. Con subset:\n\nsubset(datos1, island == \"Biscoe\")\n\n\n  \n\n\n\nMientras que con tidy:\n\ndatos1 |&gt; filter(island == \"Biscoe\")\n\n\n  \n\n\n\nLa notación no es tan diferente como en el caso anterior, y el resultado es el mismo. ¿Cuál utilizar? Depende totalmente de la preferencia de cada quien. A diferencia del caso anterior, subset no se vuelve tan compleja conforme vamos escalando en complejidad, y la equivalencia entre aproximaciones con filter se mantiene. Veamos qué pasa si obtenemos SOLO los pingüinos Adelie de Biscoe. Nuevamente, con subset:\n\nsubset(datos1, species == \"Adelie\" & island == \"Biscoe\")\n\n\n  \n\n\n\nCon tidy:\n\ndatos1 |&gt; filter(species == \"Adelie\" & island == \"Biscoe\")\n\n\n  \n\n\n\nEjercicio: Obtén todos los individuos de las especies Adelie y Gentoo:\n¿Por qué empezar con un ejemplo tan “complejo” como el caso anterior? Para dejar “lo peor” al inicio, y a partir de ahí las cosas puedan fluir un poco mejor.\n\n\n4.1.3 Añadir o modificar columnas\nOtra tarea cotidiana que vimos en la sesión anterior fue el añadir nuevas columnas a nuestro data.frame. Pensemos que tiene sentido obtener el “área” que utiliza el pico, la cual obtendríamos multiplicando bill_length_mm y bill_depth_mm. Este producto lo almacenaríamos en una nueva columna llamada bill_area. En R base:\n\ndatos1[\"bill_area\"] &lt;- datos1$bill_length_mm * datos1$bill_depth_mm\ndatos1$bill_area\n\n  [1]  731.17  687.30  725.40      NA  708.31  809.58  692.42  768.32  617.21\n [10]  848.40  646.38  653.94  723.36  818.32  730.06  651.48  735.30  879.75\n [19]  632.96  989.00  691.74  704.99  689.28  691.42  667.36  667.17  755.16\n [28]  724.95  704.94  765.45  659.65  673.32  703.10  773.01  618.80  827.12\n [37]  776.00  780.70  725.68  760.18  657.00  750.72  666.00  868.77  625.30\n [46]  744.48  780.90  708.75  644.40  896.76  700.92  757.89  626.50  819.00\n [55]  624.45  770.04  682.50  763.28  605.90  718.16  603.33  871.43  639.20\n [64]  748.02  622.44  748.80  575.10  785.01  595.94  810.92  636.50  730.48\n [73]  681.12  865.62  621.25  791.80  687.12  721.68  582.82  804.11  595.12\n [82]  755.04  689.96  680.94  663.94  838.39  707.85  686.34  735.36  731.32\n [91]  642.60  743.91  581.40  716.76  626.26  771.12  708.66  745.55  532.91\n[100]  799.20  626.50  820.00  603.20  756.00  704.94  750.33  663.92  764.00\n[109]  647.70  820.80  628.65  925.68  702.69  822.90  819.72  781.41  656.20\n[118]  764.65  606.90  764.46  622.64  746.46  683.40  765.90  559.68  771.40\n[127]  682.88  759.45  666.90  793.80  689.15  827.52  680.80  693.75  670.56\n[136]  719.25  623.00  808.02  610.50  710.63  687.42  698.32  497.55  691.90\n[145]  626.64  729.30  729.12  673.44  640.80  684.18  615.60  767.75  608.52\n[154]  815.00  686.67  760.00  690.20  627.75  662.84  714.51  580.22  720.72\n[163]  560.33  788.90  623.35  706.64  668.68  774.01  567.00  747.84  669.90\n[172]  735.37  717.86  653.95  674.25  731.54  561.99  696.11  636.35  717.00\n[181]  689.26  765.00  723.69  607.76  653.95 1013.20  726.68  788.92  583.62\n[190]  768.12  598.40  764.59  584.99  793.60  620.61  744.00  802.95  606.04\n[199]  632.45  802.95  597.17  714.16  661.72  683.85  649.44  751.50  669.60\n[208]  693.00  608.82  682.50  626.40  771.12  625.14  688.38  635.23  852.51\n[217]  650.36  836.64  665.28  801.90  617.70  760.50  715.50  723.84  751.92\n[226]  688.20  696.00  777.60  674.50  832.93  623.76  741.28  711.95  819.00\n[235]  692.04  795.00  619.62  878.84  624.96  728.46  665.00  885.70  712.50\n[244]  892.62  659.75  796.95  654.15  797.56  780.52  684.74  696.96  843.15\n[253]  727.50  950.30  731.60  736.50  652.74  753.48  612.99  843.72  606.20\n[262]  726.31  767.60  791.82  661.20  839.45  651.42  881.60  698.65  790.56\n[271]  646.64      NA  669.24  791.28  668.96  803.39  832.35  975.00  984.96\n[280]  848.98 1043.46  804.56  839.02  933.66  869.40 1020.87  829.48 1049.51\n[289]  813.10  941.20  784.89  989.80 1006.00 1032.40  863.04  895.44  733.52\n[298]  848.75  717.12  981.64  835.93  988.00  929.20  940.50  825.92 1056.00\n[307]  678.94 1127.36  709.75  958.80  924.42  798.00  871.08 1076.40  778.54\n[316] 1064.65  955.50  808.50  972.19  773.50  911.11  939.80  896.79  960.40\n[325]  963.05  861.54  788.84  976.60  790.61  998.79  735.25  981.36  750.32\n[334]  981.07  943.76  884.64 1012.05  772.20  776.90 1104.84  787.35  902.72\n[343]  965.20  938.74\n\n\nAhora hagámoslo con tidy, pero primero reestablezcamos el objeto datos1:\n\ndatos1 &lt;- palmerpenguins::penguins\n\nY ahora hagamos la operación con tidy:\n\ndatos1 &lt;- datos1 |&gt; \n          mutate(bill_area = bill_length_mm * bill_depth_mm)\ndatos1 |&gt; select(bill_area)\n\n\n  \n\n\n\nEvidentemente, los resultados son los mismos, lo cual me lleva directamente a la siguiente sección.",
    "crumbs": [
      "Biología, Ciencia de Datos y `R`",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducción a `tidyverse`</span>"
    ]
  },
  {
    "objectID": "c04_tidyverse.html#por-qué-no-enseñar-tidy-desde-el-inicio",
    "href": "c04_tidyverse.html#por-qué-no-enseñar-tidy-desde-el-inicio",
    "title": "4  Introducción a tidyverse",
    "section": "4.2 ¿Por qué no enseñar tidy desde el inicio?",
    "text": "4.2 ¿Por qué no enseñar tidy desde el inicio?\nSi tidy se acomodó a tu forma de ver las cosas, o si se te hizo más fácil de leer, es probable que te preguntes por qué no me salté R base para entrar directamente a tidy. Además de incrementar las horas del curso (broma), fue porque (pedagógicamente) tidy puede introducir ciertas barreras para quienes se van introduciendo a R. Matloff (2020) hace una excelente y extensiva recopilación de las razones por las cuales enseñar solo tidy (o solo R base) es una mala idea, así que te recomiendo leas su opinión; sin embargo, me gustaría darte mi perspectiva. Te adelanto: si no tienes experiencia en programación, el paradigma de tidy supone una curva de aprendizaje más alta que solo R base (solo R a partir de aquí) y, tal vez más importante, no es necesario casarse con uno u otro.\n\n4.2.1 tidy es más abstracto\nComo habrás notado en esta sesión, tidy es, escencialmente, más complejo que R. Esto no es una falla en el diseño, sino que tiene que ver con la filosofía de la aproximación: generar código que sea más fácilmente leíble por seres humanos. Sin duda alguna, el utilizar data |&gt; group_by() |&gt; summarise() puede parecer menos críptico o más “entendible” que solo aggregate(formula, data, FUN); sin embargo, esto se debe a que ya conocías qué es una librería y cómo cargarla, qué es una función y qué es un argumento, pero aún así hubo que explicar qué es encadenamiento de operaciones/funciones y el operador pipe y también tuvimos que entender que la información fluye de izquierda a derecha al encadenar y de derecha a izquierda al asignar.\nSi esto no fuera suficiente, el utilizar pipes puede complicar demasiado las cosas al querer depurar errores. ¿La razón? Es una capa más de abstracción. Mientras que cuando aprendemos R es común generar objetos intermedios con los resultados y ver sus salidas (o detectar errores en cada paso), en tidy esto tiende a no ser el caso. ¿Qué obtienes si solo aplicas group_by() en el ejemplo anterior? (i.e., no aplicas summarise()). El ejemplo que vimos es relativamente sencillo pero, en la medida que los problemas se van haciendo más complejos, es fácil perder la pista de qué sale de una función y entra a otra.\nEn mi opinión esto no es un problema TAN grande como pudiera parecer. Tomemos de ejemplo a Python, el lenguaje de programación reconocido como el más intuitivo. Al utilizarlo, el encadenamiento y uso de pipes es cotidiano y, aún más, preferido. En este sentido, tidyverse me recuerda mucho a la funcionalidad que de pandas en Python pero, al igual que aquí, lo correcto es aproximarse primero a Python base y luego pensar en aprender pandas. En mi opinión, el problema real (quitando la capa de abstracción) es en realidad dos problemas: uno relacionado con la filosofía de tidy y otro con quienes enseñamos R.\n\n\n4.2.2 tidy limita tus opciones\nUn ejemplo de esto está en esta misma clase. ¿Cómo obtendríamos los promedios de datos agrupados a dos niveles? Dejé el ejercicio “en el tintero” (es parte de tu tarea para esta sesión) pero, si solo te hubiera enseñado tidy, no podrías pensar en ciclos, el operador $ o cualquier otra forma de indización. ¿La razón? tidy no está enfocado a tratar con vectores individuales, aborrece el uso de ciclos y tampoco sigue las notaciones básicas de indización. Recuerda: en programación siempre es mejor tener más herramientas a tu disposición. Esta limitación la podemos probar rápidamente si queremos extraer los elementos 10:25 de la columna sex de los datos1. Con R base:\n\ndatos1$sex[10:25]\n\n [1] &lt;NA&gt;   &lt;NA&gt;   &lt;NA&gt;   female male   male   female female male   female\n[11] male   female male   female male   male  \nLevels: female male\n\n\n¿Con tidy? Realmente no hay una función que permita hacerlo. Podemos extraer la columna sex utilizando la función select:\n\ndatos1 |&gt; select(sex)\n\n\n  \n\n\n\nPero si queremos indizar el resultante obtenemos un error:\n\ndatos1 |&gt; select(sex)[10:25]\n\nError: function '[' not supported in RHS call of a pipe (&lt;text&gt;:1:11)\n\n\n\n\n\n\n\n\nNota\n\n\n\nRHS es el acrónimo de “Right Hand Side”; es decir, el operador [ no está soportado a la derecha de |&gt;.\n\n\n¿La alternativa? Primero indizar datos1 y luego utilizar select. Esto, como ves aquí abajo, funciona, pero hubiera sido mucho más fácil solo utilizar R base, sin mencionar que en tidy “puro” esta solución no es aceptable. ¿La razón? En tidy predomina el paradigma funcional de la programación; es decir, la salida depende únicamente de los argumentos pasados a la función.\n\ndatos1[10:25,] |&gt; select(sex)\n\n\n  \n\n\n\nPor otra parte, hay una falta de consistencia interna derivada de una estrategia publicitaria (tal vez) un poco mal llevada. ggplot2 no surgió dentro del tidyverse, sino que fue incluído después. Si bien es cierto que la filosofía es similar (i.e., una estructura declarativa), el cómo funcionan es completamente diferente. Una de las máximas de tidyverse (sin ggplot2) es que todo lo que entra o lo que sale es un data.frame (o tibble), mientras que en ggplot2 entra un data.frame y sale una lista. De hecho, más adelante veremos cómo podemos utilizar ciclos para automatizar la generación de gráficos, pero esto va en contra de la filosofía de tidy y no sería posible si nos hubiéramos enfocado únicamente en ella.\n\n\n4.2.3 Los ponentes somos necios\nEl mayor problema al que nos enfrentamos al aprender algún lenguaje de programación (y en muchas otras cosas) es que estamos sujetos a los prejuicios y preferencias de la persona que nos está enseñando. Al aprender a manejar nuestro tío amante de los autos nos va a decir que la transmisión manual (estándar) es mejor que la automática, pero nuestro papá, quien ve los carros solo como un medio de transporte, nos va a decir que con la automática es suficiente. ¿Cuál es mejor? Para variar, la respuesta es: “depende”. ¿De qué? De la situación en la que nos encontremos. En ciudad tener una transmisión manual puede ser muy cansado, pero puede darnos un mayor control en una carretera con un descenso empinado y muchas curvas.\nEn el problema R base vs. tidy es lo mismo. Hay ponentes “puristas” en ambos sentidos: personas que creen que tidy debería considerarse sacrilegio, y personas que creen que R base es obsoleto, arcaico, y que debería de caer en desuso. No te conviertas en ninguno de ellos y mejor toma lo mejor de ambos.\n\n\n4.2.4 tidy homogeneiza procesos\nEn mi opinión, lo que te acabo de exponer son los problemas principales de enseñar solo tidy y, de acuerdo con Matloff (2020), las razones por las cuales enseñar una mezcla de ambos es la mejor opción. Aprender R base nos permite resolver problemas que en tidy sería muy largo, mientras que tidy nos permite simplificar procedimientos que serían más complicados en R base. Otra ventaja de tidy es que permite unificar procesos bajo una misma sintaxis.\nUn ejemplo de esto lo tenemos en el aspecto de aprendizaje automatizado. Si te pones a revisar tutoriales/referencias sobre aprendizaje automatizado es muy probable que te encuentres con un montón de librerías (una por problema), funciones dedicadas y sintáxis que son específicas a la técnica que quieras aplicar. Teníamos/tenemos un excelente intento de solventar este problema: caret. Funcionaba bien en el sentido de que permitía conjuntar una gran diversidad de técnicas de aprendizaje automatizado en un mismo entorno, unificadas en un mismo estilo. Utilizando solo caret podríamos ir desde el preprocesamiento de los datos hasta el entrenamiento y validación de nuestros modelos. ¿El problema? Las pocas funciones que forman su esqueleto se volvieron sumamente complejas, algunas de ellas con 30 o más argumentos. El equipo de posit se ha puesto el mismo desafío, y su solución es tidymodels. A diferencia de caret es altamente modular, pero mantiene la intención de unificar el flujo de trabajo en una misma estructura. Tal vez yendo en contra de nuestro consejo, la mayor parte de nuestros procedimientos de aprendizaje automatizado los realizaremos bajo tidymodels, pero haremos referencia a las librerías y funciones involucradas en cada paso. Para ejemplificar, realicemos una regresión lineal simple entre la longitud y la profundidad del pico de los pingüinos.\nEn R base lo podemos hacer en una sola línea, llamando a la función lm con una fórmula y unos datos:\n\nlm(bill_length_mm~bill_depth_mm, data = datos1)\n\n\nCall:\nlm(formula = bill_length_mm ~ bill_depth_mm, data = datos1)\n\nCoefficients:\n  (Intercept)  bill_depth_mm  \n      55.0674        -0.6498  \n\n\nEn tidymodels es un poco más complejo:\n\nlibrary(tidymodels)\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt; \n  set_mode(\"regression\") |&gt; \n  fit(bill_length_mm~bill_depth_mm, data = datos1)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = bill_length_mm ~ bill_depth_mm, data = data)\n\nCoefficients:\n  (Intercept)  bill_depth_mm  \n      55.0674        -0.6498  \n\n\n¿Por qué utilizar tidymodels entonces? Eso lo dejaremos para las sesiones en las que hablemos de aprendizaje automatizado, pero verás que toma mucho sentido en el momento en el que empiezas a hacer particiones entrenamiento/prueba, optimización mediante validación cruzada, preprocesamiento de datos, evaluación, y demás tareas necesarias. Lo único que te diré en este punto es que el poder homogeneizar todos estos procedimientos en un solo estilo de trabajo simplifica las cosas.",
    "crumbs": [
      "Biología, Ciencia de Datos y `R`",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducción a `tidyverse`</span>"
    ]
  },
  {
    "objectID": "c04_tidyverse.html#conclusión",
    "href": "c04_tidyverse.html#conclusión",
    "title": "4  Introducción a tidyverse",
    "section": "4.3 Conclusión",
    "text": "4.3 Conclusión\nAunque tidy puede llegar a verse más elegante o moderno que R base, no es un substituto total. Aunque R base te permite resolver los mismos problemas que tidy, a veces no es tan intuitivo. ¿Solución? No casarse con ninguno de los dos y exprimirlos lo mejor posible. ¿Tu problema se resuelve más rápidamente con tidy? Úsalo. ¿R base se presta mejor? Aprovéchalo. Recuerda, R (como cualquier otro lenguaje de programación) es una caja de herramientas en la cual debes de buscar la que mejor se adapte al problema o pregunta que quieras responder.\nEsto es todo para esta sesión, nos vemos en la siguiente para hablar sobre teoría y buenas prácticas para la visualización de datos.",
    "crumbs": [
      "Biología, Ciencia de Datos y `R`",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducción a `tidyverse`</span>"
    ]
  },
  {
    "objectID": "c04_tidyverse.html#ejercicio",
    "href": "c04_tidyverse.html#ejercicio",
    "title": "4  Introducción a tidyverse",
    "section": "4.4 Ejercicio",
    "text": "4.4 Ejercicio\n\nUtilizando R base obtén los promedios de las longitudes de picos de los pingüinos de palmerpenguins para cada especie en cada isla. OJO: Hay al menos dos formas de hacerlo, una muy simple y una más rebuscada. No importa cuál realices, el objetivo es que te rompas la cabeza un rato ;).\nUtilizando tidy (dplyr), y en una sola cadena, filtra los datos para la isla Biscoe, crea una columna que tenga cada valor de la masa corporal menos la media global de esa columna, y luego obtén el promedio de esta columna para cada especie.\nRealiza la misma operación del punto 2 con R base. Algunas funciones que puedes tomar en cuenta para estos dos puntos son subset, filter, mutate, aggregate, summarise y/o for.\nOpcionalmente puedes intentar mezclar ambos procedimientos para llegar al mismo resultado.\n¿Qué opción se te hizo más sencilla? ¿tidy, R base o una combinación de las dos?\n\n\n\n\n\nMatloff N. 2020. Teaching R in a Kinder, Gentler, More Effective Manner: Teach Base-R, Not Just the Tidyverse.",
    "crumbs": [
      "Biología, Ciencia de Datos y `R`",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducción a `tidyverse`</span>"
    ]
  },
  {
    "objectID": "s02_fundan.html",
    "href": "s02_fundan.html",
    "title": "Fundamentos del análisis de datos",
    "section": "",
    "text": "Objetivo de aprendizaje\nEn esta sección del curso comenzarás a adentrarte al análisis de datos. Comenzarás con una introducción a la teoría de la visualización de datos y después aborarás el concepto más importante: la probabilidad. Posteriormente escalarás a la teoría del muestreo, revisarás cómo describir tus datos (tanto numérica como gráficamente) y por último cómo probar si tu evidencia puede dejar en ridículo a una (a veces ridícula) hipótesis de nulidad.",
    "crumbs": [
      "Fundamentos del análisis de datos"
    ]
  },
  {
    "objectID": "c05_ggplot2.html",
    "href": "c05_ggplot2.html",
    "title": "5  Principios de visualización de datos y ggplot2",
    "section": "",
    "text": "5.1 Representaciones de la realidad\nComo seres humanos, con una tendencia a encontrar el camino que ofrezca una menor resistencia (otra forma de decir que somos flojos), usualmente resumimos una realidad altamente compleja utilizando distintas estrategias. Si yo te pido que me digas “qué es una orca” puedes darme una descripción textual del tipo “las orcas son mamíferos del orden Cetartiodactyla”, una descripción numérica en forma de mediciones (Longitud: 6-8 m) o medidas de tendencia central/dispersión (Peso máximo: 5.5 toneladas), pero usualmente preferimos medios audiovisuales como un dibujo, un video o, en el caso de la investigación y el tema principal de hoy, gráficos.",
    "crumbs": [
      "Fundamentos del análisis de datos",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Principios de visualización de datos y `ggplot2`</span>"
    ]
  },
  {
    "objectID": "c05_ggplot2.html#visualización-de-datos",
    "href": "c05_ggplot2.html#visualización-de-datos",
    "title": "5  Principios de visualización de datos y ggplot2",
    "section": "5.2 Visualización de datos",
    "text": "5.2 Visualización de datos\n\n\n\n\n\n\nFigura 5.1: Calidad de gráficos en artículos científicos (Fuente: xkcd)\n\n\n\nTenemos una gran variedad de razones y objetivos para los cuales necesitamos o recurrimos a visualizaciones de datos (solo visualizaciones a partir de aquí), algunas de ellas son:\n\nExplorar nuestros datos\nElaborar reportes con nuestros análisis\nComunicar hallazgos gráficamente\n\nSoportar resultados en una publicación\nPresentar a un público no especializado\n\n\n\n\n\n\n\n\nImportante\n\n\n\nIndependientemente de para qué hagamos la visualización, lo cierto es que es algo que merece mucha dedicación y que, en realidad, va más allá de hacer un simple gráfico: la visualización de datos nos permite contar una historia.\n\n\nSi obviamos no tenemos cuidado al hacer nuestras visualizaciones procedimiento podemos terminar con un gráfico como el siguiente:\n\n\n\n\n\n\nFigura 5.2: Gráfico “por defecto” en Statistica\n\n\n\n¿Qué tiene de malo y cómo podemos mejorarlo? Esa es la pregunta que vamos a responder en esta sesión, replanteándola como ¿qué debo tener en cuenta para hacer una buena visualización?, y para lo cual seguiremos algunas heurísticas que nos ayudarán a ser conscientes de qué elementos incluiremos en el gráfico y cuáles no.\n\n\n\n\n\n\nNota\n\n\n\n¿Qué es una heurística? Es una guía que vamos a seguir hasta que encontremos algo mejor; es decir, las recomendaciones que vamos a revisar son, al final del día, eso: recomendaciones. No tienes ninguna obligación de seguirlas, ni mi objetivo es imponer estas ideas, sino hacer que seas consciente de todo lo que implica una visualización de datos.\n\n\n\n5.2.1 Cairo y su rueda\nLa primera heurística que vamos a revisar es la rueda de Cairo (2012). Alberto es una de las figuras más relevantes en el área de la visualización de datos, y una de las muchas heurísticas que propone es considerar la siguiente rueda:\n\n\n\n\n\n\nFigura 5.3: Rueda de Cairo\n\n\n\nEsta rueda no es una guía tal cual, sino más bien una herramienta que nos permite evaluar las compensaciones que debemos hacer al realizar nuestras visualizaciones. La rueda nos muestra seis ejes que hacen referencia a distintas características de una visualización:\n\nAbstracción-Figuración: Este primer eje refiere a qué tipo de gráfico estamos utilizando. ¿Tenemos un gráfico abstracto como un gráfico de barras, dispersión, etc.? O, por el contrario, tenemos un dibujo que describe exactamente el proceso (gráfico figurativo).\nFuncionalidad-Decoración: Esta es bastante auto-explicativa, y hace referencia a qué tantos elementos decorativos forman la visualización.\nDensidad-Ligereza: Refiere al número de elementos que conforman la visualización o, en otras palabras, a qué tan cargado está nuestro gráfico.\nDimensionalidad: Refiere al número de variables que se incluyen en el gráfico o, puesto de otra forma, a cuántas partes de la historia queremos narrar con un mismo gráfico. No es lo mismo hacer un gráfico de dispersión en el que los puntos sean todos del mismo color a hacer uno donde los puntos estén coloreados según una tercera variable.\nOriginalidad-Familiaridad: Refiere a si los elementos utilizados son familiares para el observador o son elementos nuevos. Una forma fácil de entender este eje es poner lado a lado un gráfico de frecuencias y un gráfico de densidad. Ambos cumplen con el mismo objetivo (uno es más adecuado para variables continuas), pero es bastante probable que los gráficos de frecuencias te sean más familiares.\nRedundancia-Novedad: Referente al número de elementos que ayudan a soportar una misma parte de la historia. Un ejemplo sería tener un gráfico de frecuencias para distintas clases, donde cada barra tiene un color distinto (como en la Figura 5.7). Ahí tanto las clases en el eje x como el color de las barras nos indican que son cosas diferentes.\n\nSi analizamos un poco la distribución de las características, los gráficos que tiendan más a estar en la mitad superior de la rueda son más complejos y profundos que los de la parte baja. De hecho, el mismo Cairo menciona que los científicos e ingenieros prefieren una rueda como esta:\n\n\n\n\n\n\nFigura 5.4: Rueda preferida por científicos e ingenieros\n\n\n\nMientras que los artistas, diseñadores gráficos y periodistas preferirían una esta otra:\n\n\n\n\n\n\nFigura 5.5: Rueda preferida por diseñadores gráficos\n\n\n\n¿Esto quiere decir que forzosamente debamos de hacer gráficos abstractos, con muchos elementos “originales” pero mínimas decoraciones, que representen múltiples variables y que no sean redundantes? PARA NADA. Recuerda, esto es solo una heurística, y el hacia dónde te inclines en cada eje dependerá de qué objetivo tengas con tu visualización, a quién vaya dirigida e, incluso, en qué medio va a ser observada.\n\n5.2.1.1 Minard y la (fallida) invasión Napoleónica\nPara cerrar con esta heurística házme un favor y observa con atención el siguiente gráfico:\n\n\n\n\n\n\nFigura 5.6: Gráfico de Minard sobre la invasión Napoleónica a Rusia\n\n\n\nEvidentemente es un gráfico muy cargado de información, pero es también considerado por muchos como la “mejor visualización que se ha creado”. ¿Cuántos elementos lo conforman y qué parte de la historia cuentan?\n\n\n\n5.2.2 Tufte y sus tintas\nAhora hablemos de una heurística propuesta por una de las figuras seminales en la visualización de datos: Edward Tufte. Él propone que una buena visualización debería tener una alta proporción de tinta de datos a tinta total (Tufte, 1983), donde la tinta de datos es la tinta gastada para imprimir el núcleo del gráfico; es decir, la información mínima necesaria para transmitir el mensaje, mientras que la tinta total es, como el nombre sugiere, la cantidad de tinta empleada para imprimir el gráfico completo. ¿Qué implica una alta proporción de tinta de datos a tinta total? Que reduzcamos lo más posible el número de elementos en el gráfico.\n\n5.2.2.1 Darkhorse Analytics: Data looks better naked\nPara esta parte sigamos el ejemplo de Darkhorse Analytics, en el cual se mejora la siguiente figura:\n\n\n\n\n\n\nFigura 5.7: Gráfico aparentemente inofensivo (Calorías por 100g de alimento)\n\n\n\nEl gráfico así como está presentado puede encajar en mayor o menor medida con tus gustos, pero lo cierto es que es un gráfico con una gran cantidad de elementos. Bajo la heurística de las tintas de Tufte quitar es mejorar, así que vayamos elemento a elemento. El primero es el color de fondo. No entraré en el debate de si el color es bonito o no, sino más bien quiero que te preguntes ¿qué me dice el color de fondo sobre lo que se está graficando? La respuesta es: nada; por lo tanto, hay que eliminarlo:\n\n\n\n\n\n\nFigura 5.8: ¿Realmente es necesario el color de fondo?\n\n\n\nEl siguiente elemento son, en realidad, varios. La visualización muestra el número de calorías por 100g de distintos alimentos. Eso aparece repetido en el título, el título del eje x, el título del eje \\(y\\) y, tal vez de forma menos obvia, en la acotación de colores y las etiquetas del eje x. ¿Qué hacer? Una posible solución Es reducir el título a “calorías por 100g”. ¿100 g de qué? de lo que tenemos en las etiquetas del eje x. Esto nos permite no solo reducir el texto en el título, sino también eliminar los títulos de ambos ejes y la acotación:\n\n\n\n\n\n\nFigura 5.9: ¿Calorías por 100 g de qué?\n\n\n\nYa “adelgazamos” nuestro gráfico, pero aún podemos continuar. El siguiente elemento son las cuadrículas que delimitan el área del gráfico y el área de graficado. Nuevamente la pregunta es ¿nos dicen algo sobre las calorías de los alimentos? Y nuevamente la respuesta es no. Aunado a esto, desde un punto de vista psicológico, el tener esas delimitaciones puede “limita” la imaginación del observador. Estés o no de acuerdo con este último punto, no cambia el que no aportan nada, así que podemos quitarlas:\n\n\n\n\n\n\nFigura 5.10: ¿Libertad?\n\n\n\nPuede que estés pensando que hasta este punto ya eliminamos muchas cosas, pero aún hay un par de elementos más. El primero son los colores de las barras. Al igual que la acotación son un elemento redundante que manda el mensaje de que estamos tratando con cosas diferentes, pero eso ya está definido claramente en el eje x. En este caso sería más interesante resaltar alguna categoría en particular, tal vez para responder a la pregunta ¿qué tan calóricos son estos alimentos en relación al tocino?, lo que nos permitiría resaltar al tocino en rojo si ponemos a las demás en un color “neutro” como el gris:\n\n\n\n\n\n\nFigura 5.11: ¿Más o menos calorías que el tocino?\n\n\n\n¿Esto es todo? Aún no. Tenemos en las barras un elemento derivado de las décadas de los 80s-90s cuando se masificó el uso de las computadoras y los modelos tridimensionales: las sombras y el volumen de las barras. Realmente no aportan nada a la narrativa, sino que solo “cargan” más el gráfico, así que también las eliminamos:\n\n\n\n\n\n\nFigura 5.12: Un look más minimalista\n\n\n\nEn este punto hay algo que destaca tanto o incluso más que las barras: el texto del título y de las etiquetas del eje \\(y\\). Evidentemente no podemos eliminarlos porque ya no sabríamos qué es lo que representa el gráfico, pero lo que sí podemos hacer es cambiar las negritas por gris claro, con lo cual cambiamos el punto de anclaje de la figura a la barra roja con el tocino:\n\n\n\n\n\n\nFigura 5.13: ¡Tocino!\n\n\n\nAhora ya estamos muy cerca, pero hay algo que se ve fuera de lugar. Ese “algo” está relacionado con el eje \\(y\\): las etiquetas y las líneas guía. En este punto seguramente me dirás “si quitamos esos elementos ya no vamos a saber”qué tanto es tantito” o cuántas calorías tiene cada cosa”, y tendrías la razón, pero podemos sustituirlo poniendo directamente el número de calorías por 100g de cada alimento sobre su barra, lo cual resulta en un gráfico no solo más simple sino también más preciso:\n\n\n\n\n\n\nFigura 5.14: ¿Cuántas calorías tienes?\n\n\n\nSi comparas esta última visualización con la Figura 5.7 notarás que hubo un cambio notable. El gráfico nuevo es más simple, más fácil de leer e incluso más preciso que el primero, con todo y que tiene muchos menos elementos.\n¿Con esto quiero decir que siempre debamos de tomar esta aproximación minimalista? No, para nada. De hecho, hay casos en los cuales el tener muchos elementos visuales puede ayudar a llamar la atención del lector, a expensas de la precisión del gráfico. Tomemos el siguiente ejemplo:\n\n\n\n\n\n\nFigura 5.15: Costos monstruosos\n\n\n\nAmbos gráficos presentan exactamente la misma información, pero estarás de acuerdo conmigo en que el dibujo con el monstruo es mucho más llamativo que el simple gráfico de barras, lo cual lo hace más adecuado para el medio en el que fue distribuido: una revista/periódico. Evidentemente no es compatible con una publicación científica, pero si presentas el segundo gráfico es bastante probable que la gente no voltee a verlo.\n\n\n\n5.2.3 Cairo y sus principios\nEsta siguiente heurística también fue propuesta por Cairo, y son cinco sencillos principios que debemos de seguir para llevar a una visualización altamente efectiva. El primero es el principio de funcionalidad, en el que los elementos del gráfico deben de ayudar a transmitir la historia que estamos contando. Volvamos a la Figura 5.7. Las barras, sus colores y la acotación eran funcionales, pero no lo eran el color del fondo o las cuadrículas de graficado.\nEl segundo principio es el de veracidad. Este es auto-explicativo, pero es posiblemente el más importante de todos: que los datos presentados sean veraces, y que sean presentados de una forma veraz o, en otras palabras: NO CUCHAREAR DATOS NI SU APARIENCIA. Ejemplos de visualizaciones donde este principio no se cumple abundan en los medios de comunicación, algunas veces sin que sea el objetivo, pero muchas otras intencionalmente para forzar una narrativa. Un caso particular es la Figura 5.16, en donde tenemos el gráfico presentado en los medios de comunicación durante las elecciones entre Nicolás Maduro y Henrique Capriles. En apariencia la diferencia en votos era gigantesca, acrecentado no solo por la diferencia de alturas de los cilindros, sino también por ser figuras tridimensionales. Si los seres humanos somos malos juzgando áreas (razón por la que no se recomienda hacer gráficos de pastel), somos peores aún juzgando volúmenes.\n\n\n\n\n\n\nFigura 5.16: A que no me alcanzas\n\n\n\nVolviendo a la diferencia de alturas, aquí hay una “trampa” más: el eje \\(y\\) se encuentra truncado. ¿Qué tanto? Lo suficiente para que una diferencia de 1.59% se vea como una ventaja abrumadora de Maduro sobre Capriles. ¿Cómo se vería el gráfico si se presentara de forma veraz? Bastante menos dramático, eso es seguro:\n\n\n\n\n\n\nFigura 5.17: Tú y yo no somos tan diferentes\n\n\n\nRecuerda: no porque estemos contando una historia tenemos porque forzar una narrativa o solo mostrar lo que nos conviene. Nuevamente, ejemplos como este abundan en los medios de comunicación, y aquí puedes ver algunos otros.\nEl tercer principio es el de belleza. También es autoexplicativo: que el gráfico sea “bonito”. El problema con este principio es que es sumamente subjetivo. Lo que puede ser bonito para mi puede no serlo para ti, y viceversa, pero podemos valernos de las dos heurísticas anteriores para llegar a un gráfico que sea atractivo y, sobre todo, legible.\nSi estos tres principios se cumplen y van de la mano, llegamos de forma automática al cuarto: el principio de comprensibilidad. Cairo menciona que una visualización comprensible es aquella que permite que el lector/observador pueda ver el gráfico, analizarlo y que pueda tener un momento de “¡EUREKA!”, sin que nosotros describamos el gráfico.\n\n\n5.2.4 Del dicho al hecho hay mucho trecho\nEstas son algunas de las consideraciones más básicas que debemos de tener al realizar una visualización de datos, pero no son las únicas. Te recomiendo leer el artículo de Rougier, Droettboom & Bourne (2014), el cual tiene otras guías para mejorar nuestras figuras. Ahora bien, una cosa es conocer la teoría, pero de nada sirve si no podemos llevarlo a la práctica, y es aquí donde entra ggplot2.",
    "crumbs": [
      "Fundamentos del análisis de datos",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Principios de visualización de datos y `ggplot2`</span>"
    ]
  },
  {
    "objectID": "c05_ggplot2.html#introducción-a-ggplot2",
    "href": "c05_ggplot2.html#introducción-a-ggplot2",
    "title": "5  Principios de visualización de datos y ggplot2",
    "section": "5.3 Introducción a ggplot2",
    "text": "5.3 Introducción a ggplot2\nRecordarás de las primeras sesiones que un componente muy importante de R es la realización de gráficos, lo cual quiere decir que R base nos permite realizar visualizaciones de datos. De hecho, el graficador por defecto es sumamente potente, pero desafortunadamente no es precisamente intuitivo. En este sentido, la librería ggplot2 es una (¿mejor?) alternativa en la que los gráficos se crean de manera declarativa, y está basada en el libro “Grammar of Graphics” (Wilkinson, 2005). Digo “¿mejor?” porque, como en todo, es una cuestión de gustos y costumbre; sin embargo, pedagógicamente es bastante amable. La creación de gráficos se realiza mediante capas, donde tenemos nuestros datos, en un data.frame, luego indicamos qué queremos que grafique y en dónde, luego cómo queremos que lo grafique, y luego cómo queremos los demás elementos. En código tendríamos algo así:\n```{r}\nggplot(data = datos, mapping = aes(x, y)) + geom_*() + ...\n```\nPodría explicarte qué es cada cosa aquí, o podemos mejor aprender haciendo y crear una visualización. Antes de comenzar una visualización es necesario saber qué queremos responder con ella. En este caso, utilizaremos la base de datos mpg incluída en ggplot2. El primer paso es, entonces, conocer la información que contiene. Para ello guardaremos la base en una variable que llamaremos df1:\n\nlibrary(ggplot2)\n\ndf1 &lt;- ggplot2::mpg\ndf1\n\n\n  \n\n\n\nUna manera rápida de tener una idea de cómo está dispuesta una base de datos es utilizando la función head(var). Esta nos mostrará solo las primeras instancias (renglones) del data.frame que estemos analizando. En la tabla inferior podemos ver que se trata de una base de datos sobre automóviles y que las columnas representan: el fabricante, el modelo, el desplazamiento de combustible (litros), el año del modelo, el número de cilindros, el tipo de transmisión, el tipo de tracción, los consumos en ciudad y autopista (en millas por galón, mpg), el tipo de combustible que utilizan y la clase a la que pertenencen. También nos plantearemos el objetivo de eliminar la mayor cantidad de elementos posibles hasta solo tener el esqueleto y de ahí agregar algunos elementos que favorezcan la interpretación.\n\nhead(df1)\n\n\n  \n\n\n\n\n5.3.1 ggplot() + ...\nA partir de esta información podemos tratar de responder si existe una relación apreciable entre el consumo de combustible (por ejemplo en autopista) y el desplazamiento del motor, considerando la clase del vehículo. Para atender a esta pregunta utilizaremos un gráfico de dispersión, con el desplazamiento en el eje x, el consumo en el eje \\(y\\) y la clase indicada por los colores de los puntos. Ahora que tenemos claro qué queremos visualizar y cómo lo vamos a visualizar podemos empezar a graficar. El primer paso es inicializar el espacio de graficado con la función ggplot() y pasarle los parámetros estéticos utilizando la función aes(x, y, colour). Es importante mencionar que en este momento aparecerá únicamente el espacio de graficado en blanco. Esto es normal, ya que únicamente definimos el “qué”, pero no el “cómo”.\n\nggplot(data = df1, aes(x = displ, y = cty, colour = class))\n\n\n\n\n\n\n\n\nYa que inicializamos el espacio gráfico podemos agregar la información que nos interesa. Para facilitar la construcción paso a paso y evitar el repetir código innecesariamente podemos almacenar la gráfica completa en una variable (por ejemplo plot2) e ir añadiendo capas (operador +) posteriormente. Para ver un gráfico guardado en una variable simplemente hay que llamar a esa variable. La primera capa que agregaremos será la que indicará el tipo de gráfico que deseamos (nombrados como geom_*), en este caso un gráfico de dispersión:\n\nplot1 &lt;- ggplot(data = df1, aes(x = displ, y = hwy, colour = class)) +\n         geom_point()\nplot1 # Imprime el gráfico\n\n\n\n\n\n\n\n\nAhora sí tenemos la información que necesitamos y podríamos comenzar a describir el gráfico, pero en realidad hay demasiados elementos que son innecesarios y otros que son poco informativos en su estado actual (etiquetas de ejes), entonces trabajemos uno por uno. Para modificar las etiquetas de los ejes podemos utilizar las funciones xlab() y ylab() como capas separadas; sin embargo, podemos modificar todas las etiquetas y títulos en un mismo paso utilizando la función labs(title, x, y, caption, colour, ...).\n\nplot2 &lt;- plot1 + labs(x = \"Desplazamiento (l)\",\n                      y = 'Consumo (mpg)',\n                      colour = 'Clase',\n                      title = \n                        'Tamaño del motor y Rendimiento de combustible',\n                      subtitle = 'Consumo en carretera',\n                      caption = 'Datos: ggplot2::mpg')\nplot2\n\n\n\n\n\n\n\n\n\n\n5.3.2 Tema de ggplot2\nAhora que está claro cuáles son las variables que estamos mostrando podemos empezar a modificar la estética. Recordemos que debemos mantener la relación datos/tinta lo más alta posible, y uno de los elementos más prevalentes del gráfico es el fondo gris con todo y cuadrículas. Para modificar esos elementos tenemos que modificar el “tema” de la gráfica, que no es otra cosa mas que utilizar una función que nos permita modificar en una sola línea la estética general del gráfico. Los temas se encuentran señalados con el nombre theme_*. Probemos con theme_minimal():\n\nplot2 + theme_minimal()\n\n\n\n\n\n\n\n\nLogramos eliminar el fondo gris y de paso las “espinas” (líneas de los ejes) y ahora el gráfico está en mucho mejor condición para ser presentado; sin embargo aún podemos ir más lejos. El objetivo de esta gráfica no es ver los detalles precisos de la información, si no extraer la información más relevante, por lo que la cuadrícula es un elemento que no aporta nada a la visualización. Para retirarla utilizaremos la función theme(), la cual permite modificar el aspecto de todos los elementos del gráfico. En realidad, las funciones theme_*() son aplicaciones de theme() con diferentes valores por defecto, por lo que podemos replicar el efecto de theme_minimal() e incluir otras modificaciones. Otra función muy útil para este procedimiento es la función element_blank(), la cual le indica a ggplot2 que no debe mostrar ese elemento. Otra cuestión importante que debemos de considerar es la relación de aspecto. Debido a que esta puede modificar enormemente la percepción de los datos, su selección no es algo trivial. En general, la proporción áurea (1:1.61) es un buen punto de partida y en series de tiempo es la proporción que menos deforma los datos. Una proporción cuadrada tiene sentido únicamente en aquellos casos en los que ambos ejes tengan la misma magnitud de variación y procuraremos que el eje más largo sea aquel con la variación más pequeña. En este caso, la variación del eje \\(y\\) (5 a 45) es mucho mayor que la del eje x (1.5 a 7), por lo cual una proporción cuadrada no sería una buena alternativa. En su lugar, utilicemos la proporción áurea. El último elemento que eliminaremos aquí son las marcas de los ejes, ya que realmente no aportan demasiada información.\n\nplot2 &lt;- plot2 + \n         # Eliminamos la cuadrícula menor\n         theme(panel.grid.minor = element_blank(),\n               # Eliminamos la cuadrícula mayor\n               panel.grid.major = element_blank(),\n               # Eliminamos el color de fondo\n               panel.background = element_blank(),\n               # Eliminamos las líneas de los ejes\n               axis.line = element_blank(),\n               # Eliminamos la leyenda\n               legend.key = element_blank(),\n               # Cambiamos la relación de aspecto\n               aspect.ratio = 1/1.61,\n               # Eliminamos las marcas de los ejes\n               axis.ticks = element_blank()\n                       )\nplot2\n\n\n\n\n\n\n\n\n\n\n5.3.3 Personalizar los ejes\nAhora que nos deshicimos del fondo, la cuadrícula y las líneas y marcas de los ejes podemos trabajar en los valores de los ejes. Una de las mejores maneras de hacerlo es utilizando las funciones scale_x_*() o scale_y_*(), sustituyendo el * por continuous o discrete dependiendo del tipo de variable con el que estemos trabajando. En este caso, eliminaremos por completo las marcas del eje \\(y\\) y dejaremos únicamente los desplazamientos más comunes en el eje x.\n\nplot2 &lt;- plot2 + scale_x_continuous(breaks = c(1.8, 2.5, 5, 7)) +\n                 scale_y_continuous(breaks = NULL)\nplot2\n\n\n\n\n\n\n\n\n\n\n5.3.4 Añadir líneas de referencia\nAhora que nos deshicimos de los valores del eje la gráfica ya no es entendible debido a que no sabemos cuál es la orientación o la escala de los datos. Una alternativa es añadir un par de líneas de referencia. Esto lo haremos con la función geom_hline(), la cual nos permite añadir líneas horizontales a través de todo el gráfico que cruzan al eje \\(y\\) en una posición que nosotros determinamos:\n\n# Valores de referencia como el mínimo, la media y\n# el máximo de los consumos\n\nrefs &lt;- c(round(min(df1$hwy),0),\n          round(mean(df1$hwy),0),\n          round(max(df1$hwy),0))\n\n# Líneas de referencia, una verde para el mejor consumo,\n# una gris para el consumo promedio y una roja para el peor consumo\nplot2 &lt;- plot2 + geom_hline(yintercept = refs[1],\n                            colour = 'firebrick', alpha = 0.5,\n                            linetype = 'dashed') +\n                 geom_hline(yintercept = refs[2],\n                            colour = 'lightslategrey', alpha = 0.5,\n                            linetype = 'dashed') +\n                 geom_hline(yintercept = refs[3],\n                            colour = 'forestgreen', alpha = 0.5,\n                            linetype = 'dashed')\nplot2\n\n\n\n\n\n\n\n\nAhora el gráifico ya cuenta nuevamente con un sentido de dimensión, pero no tenemos los valores de referencia, entonces habrá que poner esas anotaciones con la función geom_text(), utilizando como valores de posición en y los mismos que las líneas de referencia + un pequeño valor:\n\n# Líneas de referencia con los mismos colores\nplot2 &lt;- plot2 + annotate('text', x = 1.3, y = refs[1]+1, \n                          label = as.character(refs[1]),\n                          colour = 'firebrick') +\n                 annotate('text', x = 1.3, y = refs[2]+1,\n                          label = as.character(refs[2]),\n                          colour = 'lightslategrey') +\n                 annotate('text', x = 7, y = refs[3]+1,\n                          label = as.character(refs[3]),\n                          colour = 'forestgreen')\nplot2\n\n\n\n\n\n\n\n\nCon esta última modificación terminamos de explorar algunas de las funciones más básicas e importantes para personalizar los elementos que más impactan en una visualización, pero antes de terminar de discutir este punto me gustaría terminar el objetivo que nos propusimos al inicio de sacar información de la gráfica. En general, existe una tendencia a que el consumo de combustible incremente conforme incrementa el desplazamiento, lo cual es de esperarse, ya que el desplazamiento es una medida de el volumen máximo de combustible que puede entrar al motor en un momento dado; sin embargo, podemos también observar que, independientemente del desplazamiento, las SUVs y pickups tienden a tener los peores rendimientos de combustible, mientras que los subcompactos tienden al otro extremo. Podemos también analizar a los vehículos de dos plazas y ver que aún cuando tienen desplazamientos altos, sus rendimientos son mejores que los de las SUVs.\n\n\n5.3.5 Conclusión y ejercicio\nEn cuanto a la parte visual, se podría argumentar que esta visualización final no es tan precisa como la primera, que algún elemento podría embellecerse, o que podriamos eliminar la leyenda y poner etiquetas de texto en algunos puntos para indicar las clases. Todos estos argumentos y muchos otros serían válidos ya que la estética es algo subjetivo; sin embargo, las decisiones que tomemos deberán estar en función del medio de distribución de la visualización (no es lo mismo una página web que en un medio impreso, por ejemplo) y sobre todo del público objetivo. Esta visualización en particular funciona para los fines didácticos que tenía en mente, es adecuada para una presentación de resultados de manera electrónica como este video, pero no es una visualización adecuada para una publicación científica. Arreglar eso será tu ejercicio para esta sesión.\nPara finalizar con el objetivo principal de la clase te presento la visualización inicial y la final, una junto a la otra, para ver en dónde comenzamos, dónde terminamos y cómo llegamos hasta aquí. También te sugiero revises y descargues el PDF de esta página, que es un acordeón donde se encuentran los gráficos y funciones más comunes. Más adelante revisaremos algunos de ellos pero es un recurso que vale la pena tener a la mano.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.3.5.1 Gráfico final\nY también el código necesario para el gráfico final, todo en un solo bloque de código.\n\n# Valores de referencia para utilizar en la gráfica\nrefs &lt;- c(round(min(df1$hwy),0),  # Valor mínimo = peor consumo\n          round(mean(df1$hwy),0), # Valor promedio\n          round(max(df1$hwy),0))  # Valor máximo = mejor consumo\n\n# Objeto con todos los pasos para llegar a la gráfica final\n# Inicializamos el espacio gráfico\nfinal.plot &lt;- ggplot(data = df1, aes(x = displ, y = hwy,\n                                     colour = class)) +\n              # Gráfico de dispersión\n              geom_point() +\n              # Establecemos los títulos, subtítulos y un pie de foto\n              labs(x = 'Desplazamiento (l)',\n                   y = 'Consumo (mpg)',\n                   colour = 'Clase',\n                   title = 'Tamaño del motor y Rendimiento de combustible',\n                   subtitle = 'Consumo en carretera',\n                   caption = 'Datos: ggplot2::mpg'\n                   ) +\n              #Eliminamos la cuadrícula menor\n              theme(panel.grid.minor = element_blank(),\n                    #Eliminamos la cuadrícula mayor\n                    panel.grid.major = element_blank(),\n                    #Eliminamos el color de fondo\n                    panel.background = element_blank(),\n                    #Eliminamos las líneas de ejes\n                    axis.line = element_blank(),\n                    #Eliminamos el fondo de la leyenda\n                    legend.key = element_blank(),\n                    #Establecemos la rel. de aspecto\n                    aspect.ratio = 1/1.61,\n                    #Eliminamos las marcas de los ejes\n                    axis.ticks = element_blank(),\n                    #Cambiamos el tipo de letra\n                    text = element_text(family = 'Times',\n                                        colour = 'gray50')\n                    ) + \n              # Reducimos las divisiones del eje ex a 4 valores\n              scale_x_continuous(breaks = c(1.8, 2.5, 5, 7)) +\n              # Eliminamos las divisiones del eje $y$\n              scale_y_continuous(breaks = NULL) +\n              # Añadimos una línea roja en el peor consumo\n              geom_hline(yintercept = refs[1],\n                         colour = 'firebrick', alpha = 0.5, \n                         linetype = 'dashed') +\n              # Añadimos una línea gris en el consumo promedio\n              geom_hline(yintercept = refs[2],\n                         colour = 'lightslategrey', alpha = 0.5, \n                         linetype = 'dashed') +\n              # Añadimos una línea verde en el mejor consumo\n              geom_hline(yintercept = refs[3],\n                         colour = 'forestgreen', alpha = 0.5,\n                         linetype = 'dashed') +\n              # Etiqueta del peor consumo\n              annotate('text', x = 1.3, y = refs[1]+1,\n                       label = as.character(refs[1]),\n                       colour = 'firebrick') +\n              #Etiqueta del consumo promedio\n              annotate('text', x = 1.3, y = refs[2]+1,\n                       label = as.character(refs[2]),\n                       colour = 'lightslategrey') +\n              # Etiqueta del mejor consumo\n              annotate('text', x = 1.3, y = refs[3]+1,\n                       label = as.character(refs[3]),\n                       colour = 'forestgreen',\n                       size = 30) +\n              theme(axis.text.x = element_text(size = 20))\n              \nfinal.plot\n\n\n\n\n\n\n\n# ggsave(\"final.pdf\", height = 4, width = 3, family = \"Arial\")\n\n\n\n\n5.3.6 Extras\nAunque estas modificaciones no necesariamente forman parte del proceso necesario para la visualización que era de nuestro interés, sí que son rutinarias, por lo que vale la pena echarles un ojo.\n\n5.3.6.1 Colores de puntos\nModificar los colores de los puntos. Podemos utilizar la función randomColor(n) de la librería con el mismo nombre. Esta función solamente recibe el número de colores que queremos y los generará de manera aleatoria:\n\naleat &lt;- randomcoloR::randomColor(7)\nfinal.plot + scale_color_manual(name = \"Clase\", values = aleat)\n\n\n\n\n\n\n\n\nPodemos también especificar una paleta predefinida, utilizando la capa scale_color_brewer():\n\nfinal.plot + scale_color_brewer(type = \"spe\", palette = \"RdYlGn\")\n\n\n\n\n\n\n\n\nOtra opción es directamente pasar un vector con los nombres de los colores que sean de nuestro interés:\n\ncolor_names &lt;- c(\"red\", \"blue\", \"yellow\", \"black\",\n                 \"dodgerblue\", \"pink\", \"gray\")\nfinal.plot + scale_color_manual(name = \"Clase\", values = color_names)\n\n\n\n\n\n\n\n\n\n\n5.3.6.2 Tamaño de los puntos\nPara modificar el tamaño de los puntos solamente hay que agregar el argumento size a la capa geom_point, en el cuál indicaremos qué tamaños tomarán los puntos. Puede ser un solo valor:\n\nfinal.plot + geom_point(size = 10)\n\n\n\n\n\n\n\n\nO también a partir de una columna de la base de datos (dividida entre 5 para no obtener únicamente “manchas”):\n\nfinal.plot + geom_point(size = df1$hwy/5)\n\n\n\n\n\n\n\n\n\n\n5.3.6.3 Tipografías y Exportación de gráficos\nEl manejo de las tipografías en R es un poco especial, por ello usualmente recomiendo generar el gráfico en R, exportarlo como PDF (cairo_pdf(\"filename.pdf\", width, height, family)) y agregar las cursivas donde sea necesario; sin embargo, un paquete que puede resultar especialmente útil es ggtext. Este añade un nuevo tipo de “elemento” de texto que recibe formato Markdown (element_markdown()); es decir, podemos agregar itálicas o negritas. Para poder utilizarlo, sin embargo, es necesario modificar ligeramente nuestros datos de antemano. Para facilitarnos las cosas agregaremos una nueva columna a df1 que contenga las clases en itálicas y extraeremos los valores únicos (algo más eficiente sería hacerlo al revés, pero es más lógico de esta manera):\n\ndf1$clase &lt;- paste0(\"*\",df1$class,\"*\")\nclases &lt;- unique(df1$clase)\nclases\n\n[1] \"*compact*\"    \"*midsize*\"    \"*suv*\"        \"*2seater*\"    \"*minivan*\"   \n[6] \"*pickup*\"     \"*subcompact*\"\n\n\nFinalmente lo agregaremos a la gráfica. ¡OJO! Es necesario modificar el tema para que entienda el formato markdown:\n\n# if(!require(ggtext)) {install.packages(\"ggtext\", dependencies = T)}\n\nfinal.plot + scale_color_discrete(name = \"Clase\", labels = clases) +\n             theme(legend.text = ggtext::element_markdown())\n\n\n\n\n\n\n\n\nCon este elemento podemos modificar también fracciones de cualquier texto de nuestra gráfica, por ejemplo carretera en negritas:\n\nfinal.plot + labs(subtitle = \"Consumo en **carretera**\") +\n             theme(plot.subtitle = ggtext::element_markdown())\n\n\n\n\n\n\n\n\nMezclando ambas modificaciones:\n\nfinal.plot + scale_color_discrete(name = \"Clase\",labels = clases) +\n             labs(subtitle = \"Consumo en **carretera**\") +\n             theme(plot.subtitle = ggtext::element_markdown(),\n                   legend.text = ggtext::element_markdown())\n\n\n\n\n\n\n\n\nAhora sí, esto es todo para esta clase. ¡Nos vemos en la siguiente!",
    "crumbs": [
      "Fundamentos del análisis de datos",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Principios de visualización de datos y `ggplot2`</span>"
    ]
  },
  {
    "objectID": "c05_ggplot2.html#ejercicio",
    "href": "c05_ggplot2.html#ejercicio",
    "title": "5  Principios de visualización de datos y ggplot2",
    "section": "5.4 Ejercicio",
    "text": "5.4 Ejercicio\nAjusta la visualización de los datos mpg para que pueda ser publicable en una revista científica (de tu interés) y responde:\n\n¿Qué elementos quitarías?\n¿Qué elementos cambiarías?\n¿Qué elementos agregarías?\n¿Crees que en su estado actual cumple con los criterios de Tufte y Cairo que revisamos en clase? (Explica tu respuesta).\n\nNOTA: En vez de los datos mpg puedes utilizar datos propios o los datos de pingüinos de Palmer.\n\n\n\n\nCairo A. 2012. The functional art. Berkeley, USA: New Riders, Pearson Education.\n\n\nRougier NP, Droettboom M, Bourne PE. 2014. Ten Simple Rules for Better Figures. PLoS computational biology 10:e1003833-7. DOI: 10.1371/journal.pcbi.1003833.\n\n\nTufte E. 1983. The Visual Display of Quantitative Information. Cheshire, Connecticut: Graphics Press.\n\n\nWilkinson L. 2005. The Grammar of Graphics. USA: Springer.",
    "crumbs": [
      "Fundamentos del análisis de datos",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Principios de visualización de datos y `ggplot2`</span>"
    ]
  },
  {
    "objectID": "c06_prob.html",
    "href": "c06_prob.html",
    "title": "6  Probabilidad",
    "section": "",
    "text": "6.1 Definiciones básicas\nHabrás notado que he evitado utilizar la palabra probabilidad hasta este momento. Esto es porque la probabilidad es justamente el asignarle un número a las posibilidades y, por lo tanto, podemos pensar en la probabilidad com una medida de incertidumbre. ¿Cómo la expresamos numéricamente? La manera más sencilla de entender a la probabilidad es desde un punto de vista geométrico; es decir, como una proporción o una frecuencia relativa, de la forma ¿cuántas veces ha ocurrido B en relación al número de veces que han ocurrido tanto A como B? Pero vayamos paso a paso.\nPrimero, algunas (tediosas y obligadas) definiciones:\nOtros eventos son:",
    "crumbs": [
      "Fundamentos del análisis de datos",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probabilidad</span>"
    ]
  },
  {
    "objectID": "c06_prob.html#definiciones-básicas",
    "href": "c06_prob.html#definiciones-básicas",
    "title": "6  Probabilidad",
    "section": "",
    "text": "Experimento aleatorio: Es, como su nombre lo indica, un experimento, prueba (o como quieras llamarlo) en el cual no puedes saber con certeza cuál va a ser el resultado. Un resultado es, entonces, la “salida” o lo que pasa al realizar el experimento una sola vez, y aquí seguramente te perdí, entonces un ejemplo: Si lanzar una moneda es nuestro experimento, sus resultados son cara y cruz. Imagina que lanzo una moneda al aire, ¿cuál crees que haya sido el resultado?\nEspacio muestreal: Son todos y cada uno de los posibles resultados de un experimento.\nEvento: Podemos pensar en un evento como un conjunto de datos/resultados, por lo tanto, el espacio muestreal es un evento.\n\n\n\nUniverso: Que incluye al espacio muestreal y el conjunto vacío (\\(\\varnothing\\)). Como te imaginarás, este conjunto está vacío, no tiene nada.\nUnión: Denotada como \\(\\cup\\) (diferente de u y U), representa la unión de dos conjuntos/eventos; es decir, el caso donde nos interesa encontrar A o B.\nIntersección: Denotada como \\(\\cap\\) (diferente de n), representa el traslape entre dos conjuntos/eventos; es decir, el caso donde nos interesa encontrar A y B.\nComplemento: Denotado como \\(\\tilde{A}\\), representa lo que no es ese conjunto, en este caso, lo que NO es A, que es el área restante de B y el conjunto vacío.\n\n\n\n\n\n\n\nFigura 6.2: Eventos y diagramas de Venn",
    "crumbs": [
      "Fundamentos del análisis de datos",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probabilidad</span>"
    ]
  },
  {
    "objectID": "c06_prob.html#axiomas-de-la-probabilidad",
    "href": "c06_prob.html#axiomas-de-la-probabilidad",
    "title": "6  Probabilidad",
    "section": "6.2 Axiomas de la probabilidad",
    "text": "6.2 Axiomas de la probabilidad\n¿Qué tiene que ver esto con probabilidad? Pues que estas denominaciones dan lugar sus leyes/reglas; es decir los Axiomas de la Probabilidad:\n\n\\(P(A) \\geq 0\\); es decir, todo evento tiene una probabilidad positiva y no mayor a 1. No necesita mayor explicación, simplemente ¿cómo interpretarías una probabilidad negativa?\n\\(P(U) = 1\\); es decir, la probabilidad de nuestro espacio muestreal y el conjunto vacío es 1. Eso tiene sentido, si hacemos un experimento aleatorio vamos a tener un resultado, independientemente de cuál sea, lo cual está relacionado con el tercer axioma:\nSi A y B son mútuamente excluyentes: \\(P(A \\cup B) = P(A) + P(B)\\). Básicamente, si nos interesa saber cuál es la probabilidad de que ocurran dos resultados, en donde si ocurre uno ya no ocurre el otro, lo único que tenemos que hacer es sumar la probabilidad de cada uno de ellos. Si lanzo una moneda al aire, la probabilidad de que caiga cara es del 50% y la probabilidad de que caiga cruz es del 50%, pero la probabilidad de que caiga es del 100%.\n\nAdicionales a estos tres axiomas tenemos dos casos especiales y una generalización:\n\n\\(P(\\varnothing) = 0\\); el cual es autoexplicativo, la probabilidad de que ocurra el evento vacío es 0.\n\\(P(\\tilde{A}) = 1 - P(A)\\); es decir, si \\(\\tilde{A}\\) representa lo que no es A y dado que \\(P(U) = 1\\), solo debemos de restarle a ese universo la probabilidad de A.\n\\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\). Te darás cuenta que este se parece al tercer axioma y es porque es una generalización al caso donde A y B no son mútuamente excluyentes. ¿De dónde sale la resta? De que si A y B se encuentran unidos (el diagrama de Venn inferior) y sumamos el área de A con el área de B terminamos sumando dos veces la zona en la que están sobrelapados (intersección); por lo tanto, tenemos que quitarlo una vez para no sobre estimar. A esta generalización se le conoce como la regla aditiva de la probabilidad.\n\n\n\n\n\n\n\nFigura 6.3: Axiomas de la probabilidad",
    "crumbs": [
      "Fundamentos del análisis de datos",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probabilidad</span>"
    ]
  },
  {
    "objectID": "c06_prob.html#probabilidades-marginales-y-condicionales",
    "href": "c06_prob.html#probabilidades-marginales-y-condicionales",
    "title": "6  Probabilidad",
    "section": "6.3 Probabilidades marginales y condicionales",
    "text": "6.3 Probabilidades marginales y condicionales\nAhora entendimos que podemos hacer operaciones con la probabilidad, y eso nos lleva a los siguientes dos conceptos que son sumamente importantes: las probabilidades marginal y condicional.\nHablamos de una probabilidad marginal cuando nos interesa la P(A) cuando \\(A \\cup B\\). En este caso, podemos expresar al conjunto A como \\(A = (A \\cap B) \\cup (A \\cup \\tilde{B})\\). ¿En Español? El conjunto A está dado por la unión de la intersección de A y B y la intersección de A con el complemento de B. ¿Aún menos rebuscado? Es la suma de las partes no unidas. El procedimiento aquí es justamente un caso similar a la regla aditiva de la probabilidad. Estamos sumando la zona traslapada entre A y B con lo que no es B, que nos deja únicamente con A. Te preguntarás por qué se denomina marginal. Para esto primero necesitamos definir una tabla de contingencia. Esta es simplemente una tabla en la que cada renglón tiene frecuencias relativas de los distintos niveles de una variable categórica y las columnas tienen las frecuencias relativas de cada nivel de otra variable categórica. En los márgenes de la tabla tenemos los totales para cada nivel (evento o conjunto) y de ahí viene el nombre.\nPor otra parte, la probabilidad condicional nos permite responder a la pregunta ¿cuál es la P(A) si ya sé que B ocurrió?. Matemáticamente la representamos como \\(P(A|B)\\) (probabilidad de A dado B), y es una razón de la probabilidad conjunta de A y B (\\(P(A,B)\\) o \\(P(A \\cap B)\\)) y la probabilidad marginal de B (\\(P(B)\\)); es decir: \\(P(A|B) = \\frac{P(A,B)}{P(B)}\\). La probabilidad conjunta representa la probabilidad de que dos eventos ocurran al mismo tiempo y puede llegar a ser un poco problemática. Si ambos eventos son independientes, obtenerla es sencillo: \\(P(A,B) = P(A)P(B)\\). El problema surge si A y B no son independientes, en cuyo caso: \\(P(A,B) = P(A)P(B|A)\\), lo cual nos lleva a una referencia cruzada. Por practicidad, y porque el interés del curso no es que sepas hacer estas cosas a mano, obtengamos la probabilidad conjunta desde su posición en la tabla de contingencia; es decir, cada una de sus celdas.\n\n\n\n\n\n\nFigura 6.4: Probabilidades marginales y condicionales\n\n\n\nEn el ejemplo de la diapositiva (OJO: los datos no son representativos de ninguna población) calculamos la \\(P(Blond|Blue)\\); es decir, la probabilidad de que alguien sea rubio si sabemos que tiene los ojos azules, dada por la división de la probabilidad conjunta \\(P(Blond,Blue) = 0.16\\) y la marginal \\(P(B) = 0.36\\) que resulta en \\(P(Blond|Blue) \\approx 0.44\\). ¿Cuál sería entonces la \\(P(Green|Red)\\)?",
    "crumbs": [
      "Fundamentos del análisis de datos",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probabilidad</span>"
    ]
  },
  {
    "objectID": "c06_prob.html#distribuciones-de-probabilidad",
    "href": "c06_prob.html#distribuciones-de-probabilidad",
    "title": "6  Probabilidad",
    "section": "6.4 Distribuciones de probabilidad",
    "text": "6.4 Distribuciones de probabilidad\n\n\n\n\n\n\nFigura 6.5: Estadísticos, probabilidades y distribuciones (Fuente: xkcd)\n\n\n\nDejando los memes de las viñetas (la inferior es bastante trágica), hablemos de cómo escalar de valores puntuales a algo más aplicado a la investigación. Podemos utilizar nuestra intución de probabilidad de manera cotidiana (e.g., probabilidad de lluvia), pero en cuestiones académicas tenemos una hipótesis de trabajo, la cual trasladamos a pruebas de significancia para realizar inferencias. Eso es algo que abordaremos más a detalle en la siguiente sección; sin embargo, vamos a tener múltiples datos, cuya distribución probabilidad es lo que va a moldear nuestros análisis. Es necesario, entonces, definir qué es una distribución de probabilidad.\nEn pocas palabras, una distribución de probabilidad es una lista con todos los resultados de un evento y sus probabilidades correspondientes. Hay una gran diversidad de distribuciones teóricas de probabilidad, cada una con sus peculiaridades, parámetros, momentos y lugares para utilizarlas. No te preocupes por aprenderlas todas, hablaremos de las distribuciones relevantes para cada modelo que apliquemos. Por ahora solo es importante que conozcas que, si hablamos de distribuciones discretas, hablamos entonces de la probabilidad de cada resultado. Si tenemos una distribución continua, podemos partirla en intervalos para discretizarla y hablar de la probabilidad de que una observación pertenezca a ese intervalo. Sea cual sea el caso, estas son masas de probabilidad, las cuales suman a 1, tal que:\n\\[\n\\sum_{i = 1}^n P(x_i) = 1\n\\]\n\n\n\n\n\n\nFigura 6.6: Algunas distribuciones de probabilidad y sus relaciones. ¿Te gusta ver símbolos? La imagen de la izquierda va a ser de tu agrado. ¿Eres más visual? Toma la de la derecha.\n\n\n\n\n\n\n\n\n\nNota\n\n\n\n¿A qué me refiero con discreta o continua? A los valores que pueden tomar los resultados que dan forma a una distribución. Una distribución de probabilidad discreta solo toma valores enteros o categóricos, mientras que una continua puede tomar valores decimales. La relación de esto con nuestros datos la veremos en la siguiente sesión: muestreo.\n\n\nPero volvamos al tema de las distribuciones continuas, porque tienen una cualidad bastante interesante. Resulta que en una distribución continua la probabilidad de cada valor (\\(P(x)\\)) es 0. ¿Por qué? Porque, por definición, todos los valores en el intervalo de la distribución son posibles y hay una cantidad infinita de ellos (de aquí sale también el problema de la precisión de punto flotante, pero esa es otra historia). ¿Cómo contender con esto? Podemos discretizar la distribución y hablar de masas de probabilidad de los intervalos resultantes (regla de Sturgess, por ejemplo); sin embargo, estos intervalos son, por mucho apellido de autor que lleven, arbitrarios. ¿Entonces? Podemos hacerlos infinitesimalmente pequeños; es decir, aproximar la amplitud de los intervalos a 0 (pero no exactamente 0) y entonces tenemos densidades de probabilidad. ¿Por qué densidad? Porque dividimos la masa de ese intervalo infinitesimalmente pequeño entre su amplitud, lo cual nos deja con una definición similar a \\(densidad = \\frac{masa}{área}\\). Si hacemos eso, nuestras densidades pueden ser mayores a 1, lo cual indica que tenemos una alta masa en relación a la escala. El otro cambio es que, como recordarás de tus clases de cálculo, al pasar de una variable discreta a una continua pasamos de una sumatoria a una integral:\n\\[\n\\sum_{i = 1}^n \\frac{p([x_i, i_i+\\Delta x])}{\\Delta x} \\Rightarrow \\int dxp(x) = 1\n\\]\nNo es necesario que memorices esto, solo que tengas en cuenta la diferencia entre masas y densidades de probabilidad. Como añadido, este mismo problema es lo que causa que un gráfico de frecuencias (histograma) no sea la mejor solución para ver la distribución de una variable continua. En su lugar podemos utilizar gráficos de densidad, los cuales hacen lo que acabamos de mencionar (al menos en escencia). ¿Cómo los hacemos? Eso lo veremos más adelante.\nDejando las ecuaciones de lado, la selección de la distribución de probabilidad que utilizaremos depende del problema. ¿Tienes datos de conteos? Puedes utilizar las distribuciones Poisson o binomial negativa. ¿Tienes datos continuos en el intervalo 0-1? Vale la pena echarle un ojo a la distribución Beta. ¿Tienes datos binarios? Deberías voltear hacia la distribución binomial y, de hecho, vamos a explorarla un poco para entender cómo funcionan las distribuciones de probabiliad y cómo podemos aprovechar sus implementaciones en R.\n\n\n\n\n\n\nNota\n\n\n\nAunque en el curso vamos a ver distintas ecuaciones y funciones matemáticas, NO es necesario que las memorices. Veremos solo las más indispensables y solo en los momentos en los que sean útiles para dar sentido a los procedimientos que estamos realizando. En general, prefiero utilizar pruebas visuales para que desarrolles una intuición sobre qué es lo que se está haciendo, por qué y para qué, aunque no tengas tatuadas las ecuaciones y resuelvas los problemas a mano. Todos hemos utilizado un microondas y sabemos que no debemos de meterle objetos metálicos pero ¿sabes exactamente cómo ensamblar uno?",
    "crumbs": [
      "Fundamentos del análisis de datos",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probabilidad</span>"
    ]
  },
  {
    "objectID": "c06_prob.html#distribuciones-de-probabilidad-y-volados",
    "href": "c06_prob.html#distribuciones-de-probabilidad-y-volados",
    "title": "6  Probabilidad",
    "section": "6.5 Distribuciones de probabilidad y volados",
    "text": "6.5 Distribuciones de probabilidad y volados\nEsta no sería una sesión de probabilidad si no habláramos de volados, así que hagamos justo eso. ¿Qué es un volado? Un experimento en el cual lanzamos una moneda y obtenemos uno de dos resultados: cara o cruz. Este resultado podemos verlo de otra manera: éxito (cara) o fracaso (cruz). Si estamos en un escenario de este tipo, en el cual solo podemos tener dos resultados, estamos hablando de ensayos de Bernoulli, y su formalización matemática se conoce como proceso Bernoulli. No voy a entrar en esos detalles porque es innecesario para los fines del curso, solo es un breviario cultural, lo que no es un breviario cultural es que también se les conoce como ensayos binomiales. Sí, binomial como en la distribución que mencioné antes, así que formulémosla paso a paso.\n\n\n\n\n\n\nNota\n\n\n\nDependiendo del software o referencia que estés consultando, a la distribución binomial también se le conoce como distribución Bernoulli.\n\n\nRetomemos la definición de una distribución de probabilidades: “una lista de resultados posibles y sus probabilidades correspondientes”. Entonces la distribución binomial tiene exactamente dos resultados posibles y (por lo tanto) dos probabilidades correspondientes. Ni una más, ni una menos. Si pensamos en un volado tendríamos lo siguiente:\n\nDistribución de probabilidades de un volado.\n\n\nResultado\nProbabilidad\n\n\n\n\nCara\n0.5\n\n\nCruz\n0.5\n\n\n\nPero si pensamos un poco más a profundidad en lo que implica la tabla anterior, es decir, solo dos resultados posibles mutuamente excluyentes, y recordamos nuestros axiomas de la probabilidad, podemos generalizarla de la siguiente manera:\n\nGeneralización a la distribución binomial.\n\n\nResultado\nProbabilidad\n\n\n\n\nÉxito\n\\(p\\)\n\n\nFracaso\n\\(q = 1 - p\\)\n\n\n\nEn otras palabras, dado que solo tenemos dos resultados posibles, la probabilidad de fracaso \\(q\\) siempre es el complementario del éxito \\(p\\) (\\(1 - p\\)). Por esta razón, la distribución binomial tiene un solo parámetro, que representa la probabilidad de éxito.\n\n\n\n\n\n\nNota\n\n\n\n¿Qué es un parámetro de una distribución? Un número que controla su forma. Piensa en ellos como perillas que cambian la distribución como las perillas de la estufa cambian la intensidad de la flama.\n\n\nAhora bien, recordarás que la probabilidad vista desde un punto de vista geométrico representa una proporción; por lo tanto, realmente (y formalmente) tenemos otro parámetro: el número de experimentos (\\(n\\)). Una forma común de representar textualmente las distribuciones es utilizando notación probabilística:\n\\[\nY \\sim Binom(n, p)\n\\]\nEsto se lee: \\(Y\\) es una variable aleatoria con distribución (sí, todo eso está contenido en el \\(\\sim\\)) binomial (\\(Binom\\)), con parámetros \\(n\\) y \\(p\\). Si sustituimos nuestros volados:\n\\[\nY \\sim Binom(n, p = 0.5)\n\\]\nTe estarás preguntando: ¿y la \\(n\\)? Pues en realidad puede ser cualquier número entero &gt; 0. Podemos pensar en 4 volados, y esto quedaría de la siguiente manera:\n\\[\nY \\sim Binom(n = 4, p = 0.5)\n\\]\nSi lanzáramos los cuatro volados esperaríamos obtener dos caras y dos cruces, pero la realidad es demasiado caprichosa como para ajustarse a la teoría Podemos decirle a R que exprese ese capricho, utilizando la función rbinom(n, size, prob), donde rbinom es un acrónimo para “random binomial” (binomial aleatoria). Como el nombre sugiere, esta función permite generar variantes aleatorias de cierta cantidad de experimentos aleatorios (size), obtenidos de una distribución binomial con parámetros n y prob. En nuestro caso, nos interesa generar 4 volados (n) con una probabilidad de cara (éxito, prob) de 0.5. ¿Y size? En nuestro ejemplo es 1, pues hicimos un solo experimento donde “lanzamos” 4 volados:\n\nrbinom(n = 4, size = 1, prob = 0.5)\n\n[1] 1 0 1 1\n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nVoy a presentarte la expresión matemática de la distribución binomial. No te preocupes, no hay que resolver nada a mano, ni es necesario que te la aprendas, solo lo hago como recurso didáctico.\n\n\nDependiendo de tu suerte habrás obtenido exactamente dos caras (unos) y dos cruces (ceros), o un poco más de alguno. ¿La razón? Una variable aleatoria es una función que transforma la realidad a números. En el caso de nuestra distribución binomial está en términos de obtener la probabilidad de obtener EXACTAMENTE \\(k\\) éxitos en \\(n\\) ensayos de Bernoulli independientes, dada por la función de masas de probabilidad:\n\\[\nP(k|n,p) = P(X = k) = \\binom{n}{k}p^k(1-p)^{n-k}\n\\]\npara \\(k = 0, 1, 2, \\dots, n\\), donde \\(\\binom{n}{k}\\) representa el coeficiente binomial:\n\\[\n\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\n\\]\n\n\n\n\n\n\nNota\n\n\n\n¿Variante aleatoria es lo mismo que variable aleatoria? NO, pero sí están relacionadas. Una variable aleatoria es, como dije arriba, una función que transforma la realidad a números o, en otras palabras, una variable cuyo valor numérico depende de la salida de un fenómeno aleatorio (lanzar un volado), mientras que una variante aleatoria es un resultado particular obtenido de esa función (cara o cruz).\n\n\n¿Cómo traducimos esto al Español? Nuestros \\(k\\) éxitos suceden con probabilidad \\(p^k\\) y \\(n-k\\) fracasos suceden con probabilidad \\((1-p)^{n-k}\\); sin embargo, estos \\(k\\) éxitos pueden ocurrir en cualquiera de nuestros \\(n\\) experimentos, y hay \\(\\binom{n}{k}\\) formas diferentes (combinaciones) de distribuir esos \\(k\\) éxitos en la secuencia de \\(n\\) experimentos. Vale, no es tan tangible como me hubiera gustado, y mi objetivo no es saturarte de símbolos, solo quiero que veas que estas ecuaciones, por más abstractas que parezcan, tienen un sentido lógico, y cada elemento que las conforma nos dice algo sobre la intuición detrás de ellas.\nAhora bien, en este caso hemos asumido que la moneda que hemos estado lanzando es justa; es decir, que la probabilidad de que caiga cara o cruz es la misma, pero ¿qué pasa si la moneda está cargada hacia que caiga más de alguna manera? Es ahí donde entran las frecuencias a largo plazo.\n\n6.5.1 Frecuencia a largo plazo\nRecordarás que uno de los parámetros de la distribución binomial es \\(p\\), la probabilidad de éxito, y ese es el parámetro que queremos aproximar. Si bien es cierto que hay distintos métodos para abordar este tipo de problemas (estimación de parámetros), podemos aproximarlo con un poco de voluntad y fuerza bruta. ¿Cómo? Repitiendo el experimento que nos interesa una cantidad suficiente de veces. En nuestro caso queremos evaluar si una moneda es justa o no (si \\(p ≈ 0.5\\)), para lo cual realizaremos una serie de volados y ver cuál es la probabilidad de que caiga cara (\\(P(H)\\)), lo cual es lo mismo que la proporción caras (H) a cruces (T; H:T) final. Pongamos también en práctica nuestro R y generemos una función que reciba un número \\(N\\) de volados a realizar y que regrese un data.frame con el registro de cada resultado:\n\nvolado &lt;- function(N){\n  \n  set.seed(0)\n  \n  prob &lt;-  round(runif(n = 1), 2)\n  \n  # Realizar N lanzamientos\n  sec &lt;- rbinom(n = N,\n                size = 1,\n                prob = prob)\n  \n  # Suma acumulativa:\n  # Si solo obtenemos 1s el resultado final será 500.\n  # Si solo obtenemos 0s el resultado final será 0.\n  suma_acum &lt;- cumsum(sec)\n  \n  # Generamos un identificador para cada lanzamiento\n  lanzamiento &lt;- 1:N\n  \n  # Calculamos la proporción H:T acumulada a cada lanzamiento\n  prop_acum &lt;- suma_acum/lanzamiento\n  \n  # data.frame con resultados\n  resultados &lt;- data.frame(lanzamiento, sec, prop_acum)\n  \n  return(list(prob, prop_acum, resultados))\n}\n\nEsta función tiene una pequeña “trampa”, o un pequeño “truco”, según como quieras verlo. Si pones atención, la primera línea es prob &lt;- runif(n = 1), y ese objeto es el que se pasa a la función rbinom como argumento para prob. Estas funciones se ven sospechosamente similares, y con justa razón: ambas codifican distribuciones de probabilidad. Mientras que la función rbinom nos permite obtener variantes aleatorias de una distribución bionomial, la función runif nos permite obtener variantes aleatorias de una distribución uniforme. Una de las aplicaciones prácticas de esta distribución es la generación de números aleatorios, por lo que podemos generar una una “carga” aleatoria (prob) para nuestra “moneda virtual” (rbinom).\n\n\n\n\n\n\nNota\n\n\n\n¿Qué es una distribución uniforme? Una distribución en la que todos los elementos tienen la misma probabilidad. La función runif tiene tres argumentos: runif(n, min, max), donde n es el número de variantes a obtener, min es el valor mínimo y max el valor máximo. Por defecto da un número aleatorio entre 0 y 1.\n\n\nHabiendo dicho esto, lancemos 500 volados y veamos qué “carga” tiene la moneda, utilizando un gráfico de líneas:\n\nlibrary(ggplot2)\n\n# 500 lanzamientos\nN &lt;- 500\n# Realizar los lanzamientos\nvolados &lt;- volado(N = N)\n# Extraemos los resultados\n## P(H) \"real\"\nprob &lt;- volados[[1]]\n## Data.frame\nres_volados &lt;- volados[[3]]\n\n\n# Inicialización del espacio gráfico\nprop_plot &lt;- ggplot(data = res_volados,\n                    aes(x = lanzamiento,\n                        y = prop_acum)) +\n              # Gráfico de líneas\n              geom_line(colour = \"deepskyblue4\",\n                        linetype = \"solid\",\n                        size = 0.7) +\n              # Marcador en los puntos\n              geom_point(colour = \"deepskyblue4\",\n                         alpha = 0.1,\n                         fill = NA,\n                         shape = \"circle\",\n                         stroke = 1,\n                         size = 4) +\n              # Modificar etiquetas\n              labs(x = \"# de lanzamiento\",\n                   y = element_blank(), \n                   title = \"Frecuencia a largo plazo\",\n                   subtitle = paste(\"P(H) final:\",\n                                    round(res_volados$prop_acum[N], 2)),\n                   caption = \"\") +\n              # Cambiar el tema por defecto\n              theme_bw() +\n              # Escala del eje y con tres valores de interés\n              scale_y_continuous(breaks = c(0, 0.5, 1),\n                                 labels = c(\"T\", \"50%\", \"H\")) +\n              # Cinco etiquetas en el eje x\n              scale_x_continuous(n.breaks = 5) +\n              # Líneas de referencia en los puntos de interés\n              geom_hline(yintercept = 1,\n                         colour = rgb(118,78,144, maxColorValue = 255),\n                         alpha = 0.9, linetype = \"dashed\") +\n              geom_hline(yintercept = 0.5,\n                         colour = \"lightslategray\",\n                         alpha = 0.9, linetype = \"dashed\") +\n              geom_hline(yintercept = 0,\n                         colour = rgb(118,78,144, maxColorValue = 255),\n                         alpha = 0.9, linetype = \"dashed\") +\n              # Línea de referencia con el valor \"real\"\n              geom_hline(yintercept = volados[[1]],\n                         colour = \"forestgreen\",\n                         alpha = 0.9, linetype = \"dashed\") +\n              # Anotación con el valor \"real\"\n              annotate(\"text\",\n                       x = N*0.9,\n                       y = ifelse(volados[[1]] &gt; 0.5,\n                                  volados[[1]] - 0.05,\n                                  volados[[1]] + 0.05),\n                       label = paste(\"P(H) = \", volados[[1]]),\n                       colour = \"forestgreen\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\nprop_plot\n\n\n\n\n\n\n\nFigura 6.7: Frecuencia a largo plazo de 500 volados para aproximar la probabilidad de obtener una cara en un volado (\\(P(H)\\)) con una moneda con “carga” (sesgo) aleatorio.\n\n\n\n\n\nTe darás cuenta de que nuestro valor final fue muy cercano al esperado. Ahora intenta hacer esto mismo, pero con 5 lanzamientos. ¿Qué obtuviste? Es bastante posible que la \\(P(H)\\) haya quedado bastante lejos de el objetivo. Esto resalta la importancia de algo que abordaremos más a profundidad la siguiente sesión: la representatividad.\nVeamos ahora la distribución de probabiliades de manera gráfica. Esta es una distribución discreta, por lo que utilizaremos un gráfico de frecuencias:\n\n# Inicialización del espacio gráfico\nfreq_binom &lt;- ggplot(data = res_volados,\n                     aes(x = as.character(sec))) +\n              # Graficar las frecuencias absolutas\n              geom_bar(colour = \"deepskyblue4\",\n                       fill = \"deepskyblue4\") +\n              # Cambiar el tema por defecto\n              theme_bw() +\n              # Transformar el eje x a categórico y asignar etiquetas\n              scale_x_discrete(name = \"Resultado\",\n                               labels = c(\"1\" = \"H\",\n                                          \"0\" = \"T\")) +\n              # Modificar el resto de las etiquetas del gráfico\n              labs(title = \"Resultados de volados\",\n                   y  = \"Frecuencia\")\n\nfreq_binom\n\n\n\n\n\n\n\nFigura 6.8: Distribución de frecuencias de los volados con la moneda con carga aleatoria.\n\n\n\n\n\n\n\n\n\n\n\nImportante\n\n\n\nUna forma relativamente común de representar proporciones es utilizando gráficos de “pie” o “rebanadas”. EVITA hacerlo lo más que puedas. Los seres humanos somos, en general, bastante malos para evaluar diferencias de áreas, además de que la misma información puede ser representada en un gráfico de frecuencias, sean absolutas o relativas.\n\n\n¿Qué concluimos de estos resultados? Eso dependerá del “humor” de tu generador de números aleatorios. Puede que tu moneda sí haya sido justa (\\(P(H) ≈ 0.5\\)), o puede que haya estado fuertemente “cargada”. Independientemente de esto podemos hacer un ejercicio para calcular la probabilidad de que la moneda sea justa, considerando un márgen de error del 5% alrededor del 50%; es decir, si obtenemos una \\(P(H)\\) entre 45% y 55% la vamos a dar por “justa”, y esto me lleva a hablar de distribuciones continuas.",
    "crumbs": [
      "Fundamentos del análisis de datos",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probabilidad</span>"
    ]
  },
  {
    "objectID": "c06_prob.html#distribuciones-continuas-y-pingüinos",
    "href": "c06_prob.html#distribuciones-continuas-y-pingüinos",
    "title": "6  Probabilidad",
    "section": "6.6 Distribuciones continuas y pingüinos",
    "text": "6.6 Distribuciones continuas y pingüinos\nEntonces, ¿cómo calculamos la probabilidad? Muy sencillo, el “sesgo” de la moneda proviene de una distribución uniforme, por lo que podemos partir el intervalo \\([0,1]\\) en partes iguales de \\(0.05\\) (5%):\n\ninterv &lt;- seq(0, 1, 0.05)\ninterv\n\n [1] 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70\n[16] 0.75 0.80 0.85 0.90 0.95 1.00\n\n\nLuego podemos simplemente sacar la proporción de los casos que caen en nuestro intervalo aceptable con respecto al total: Si partimos en “pasos” de 5% entonces solamente 0.45, 0.5 y 0.55 son aceptables, por lo que sería 3 con respecto al número total de intervalos:\n\n3/length(interv)\n\n[1] 0.1428571\n\n\n\n\n\n\n\n\nTip\n\n\n\nRecuerda: la función length() nos da el número de elementos en objeto dado.\n\n\n\n\n\n\n\n\nNota\n\n\n\nResponde: ¿Qué tipo de probabilidad estamos calculando aquí? ¿Qué axioma estamos utilizando?\n\n\nEs decir que la probabilidad de que nuestra moneda sea justa es del 14.2% Si reflexionas un poco, esto no tiene sentido. ¿Cómo es posible que un 10% de una distribución uniforme tenga un 14% de probabilidad de ocurrir? Recuerda, discretizar una distribución continua resulta en intervalos arbitrarios y, aún cuando nuestro valor de “sesgo” viene de una distribución uniforme, no es lo mismo partirla en 20 pedazos a partirla en 100. De hecho, comprobémoslo: partamos nuestro intervalo \\([0,1]\\) en 100, y obtengamos la proporción de los intervalos que son mayores o iguales a 0.45 y menores o iguales a 0.55:\n\ninterv_100 &lt;- seq(from = 0, to = 1, length.out = 100)\nlength(interv_100[interv_100 &gt;= 0.45 & interv_100 &lt;= 0.55])/length(interv_100)\n\n[1] 0.1\n\n\n\n\n\n\n\n\nTip\n\n\n\nSi tienes duda de qué hace length.out de diferente a lo que hicimos en los intervalos de 0.05% recuerda que siempre puedes darte una vuelta por la documentación de la función seq.\n\n\nAhora sí, nuestro intervalo de “aceptación” del 10% se ve reflejado en una probabilidad del 10%. Esto nos abre la puerta a un par de ejercicios interesantes. Primero, retomemos nuestros datos de pingüinos y quedémonos solo con los datos de los pingüinos Adelie:\n\npenguins &lt;- subset(palmerpenguins::penguins, species == \"Adelie\")\npenguins &lt;- na.omit(penguins)\n\nAhora veamos la distribución de la longitud del pico en gramos:\n\nggplot(data = penguins, aes(x = bill_length_mm)) +\n  geom_density(colour = \"deepskyblue4\") +\n  theme_bw() +\n  labs(title = \"Distribución de la longitud del pico (mm)\",\n       subtitle = \"Pingüinos Adelie\",\n       x = element_blank(),\n       y = \"Densidad\")\n\n\n\n\n\n\n\nFigura 6.9: Distribución empírica (observada) de la distribución del pico de pingüinos Adelie.\n\n\n\n\n\n\n\n\n\n\n\nNota\n\n\n\nEn la sesión de estadística descriptiva ahondaremos un poco más en la diferencia entre un gráfico de densidad y un gráfico de frecuencias; sin embargo, si recuerdas la definición de densidad de probabilidad te será fácil saber que entre más alta sea la densidad en un punto dado, mayor es su probabilidad.\n\n\nAhora imaginemos que estamos en campo, medimos un individuo de 44 mm, y nos interesa saber cuál es la probabilidad de encontrar un al menos tan grande como él. Pues hacemos exactamente lo mismo que en el caso anterior:\n\nbill_lengths &lt;- penguins$bill_length_mm\nlength(bill_lengths[bill_lengths &gt;= 36])/length(bill_lengths)\n\n[1] 0.8561644\n\n\nLo cual nos da una probabilidad del 3.4%. Esto que hicimos aquí es el fundamento de las pruebas de significancia (pruebas de hipótesis), solo que hay una pequeña diferencia: el contraste no lo hacemos con la distribución de nuestros datos, sino con una distribución teórica (usualmente la distribución normal). En la sesión de técnicas paramétricas hablaremos duro y tendido de la distribución normal, sus características y cómo aprovecharla para probar “hipótesis” (vamos a ver también cuál hipótesis es la que estamos probando, porque posiblemente no sea la que tienes en mente). Veamos entonces, asumiendo una distribución normal, cuál es la probabilidad de encontrarnos con un idividuo de al menos 44 mm. Para esto utilizaremos la función pnorm(q, mean, sd, lower.tail), la cual calcula la función de densidad acumulada para los parámetros dados. ¿Y eso con qué se come? Pues bien, es ahí donde entra la acumulación: estamos integrando (sumando, vamos) las densidades hasta un punto (cuantil, más adelante hablaremos de esto) dado, considerando o no la cola (el lado) inferior (izquierda) o superior (derecha) de la distribución. Si nos interesan preguntas de “menor a” utilizamos lower.tail = TRUE, pues nos interesa toda la distribución hasta ese punto. Si nos interesa “superior a” utilizamos lower.tail = FALSE, pues nos interesa la distribución de ese punto en adelante:\n\npnorm(q = 44,\n      mean = mean(bill_lengths),\n      sd = sd(bill_lengths),\n      lower.tail = FALSE)\n\n[1] 0.0259491\n\n\n\n\n\n\n\n\nNota\n\n\n\nEl fundamento de esto es que si integramos nuestra curva de densidad (acumulamos/sumamos el área bajo la curva) obtenemos la probabilidad correspondiente.\n\n\n¡Es más baja! Veamos cómo quedaría nuestro individuo de 44 mm bajo el supuesto de una distribución normal:\n\nggplot() +\n   stat_function(fun = dnorm, n = 100,\n                 args = list(mean = mean(bill_lengths),\n                             sd = sd(bill_lengths)),\n                 colour = \"deepskyblue4\") +\n  xlim(min(bill_lengths),\n       max(bill_lengths)) +\n  geom_vline(xintercept = 44,\n             linetype = \"dashed\",\n             colour = \"firebrick\") +\n  theme_bw() +\n  labs(title = \"Distribución de longitudes de picos\",\n       subtitle = \"Pingüinos Adelie, distribución normal\",\n       x = element_blank(),\n       y = \"Densidad\")\n\n\n\n\n\n\n\nFigura 6.10: Distribución normal teórica de la distribución del pico de pingüinos Adelie, con media y desviación estándar de los datos observados. La línea vertical muestra la posición de un individuo con un pico de 44 mm en la distribución teórica.\n\n\n\n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nEsto es solo un ejemplo de cómo trabajamos con distribuciones continuas, y cómo podemos utilizar R para calcular probabilidades bajo esas distribuciones. Si bien es la misma intuición detrás de las pruebas de significancia, no estamos haciendo una.\n\n\nEn efecto, se encuentra muy cerca del límite de la distribución. Ahora puede que te estés preguntando el por qué del “un individuo de al menos 44 mm”. Recuerda que la probabilidad de un valor individual en una distribución continua es 0, por lo que hay que a) establecer un intervalo alrededor de ese valor, o b) poner la pregunta en términos de si es mayor o menor a ese valor. En la sesión de pruebas de hipótesis veremos cómo se relaciona esto con los valores de p, que no están muy lejos de lo que hicimos aquí. Ahora cambiemos la pregunta para ver cuál es la probabilidad de encontrar un individuo menor a 44 mm:\n\npnorm(q = 44,\n      mean = mean(bill_lengths),\n      sd = sd(bill_lengths),\n      lower.tail = TRUE)\n\n[1] 0.9740509\n\n\n¡Es altísima! El complementario del caso anterior, de hecho, pues solamente volteamos la pregunta a responder, pero tenía que hacerla de emoción :P.\nHasta este punto creo que ya te he torturado lo suficiente con la teoría alrededor de la probabilidad y las distribuciones de probabilidad. Podríamos seguir y hablar de las más comunes/utilizadas, pero creo que te será más digerible que en cada tema hablemos de las distribuciones de interés, especialmente de la distribución normal.\nEsto sería todo para esta sesión (en compensación por lo tedioso de la teoría no hay ejercicio). ¡Nos vemos en la siguiente!",
    "crumbs": [
      "Fundamentos del análisis de datos",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Probabilidad</span>"
    ]
  },
  {
    "objectID": "c07_muestreo.html",
    "href": "c07_muestreo.html",
    "title": "7  Introducción al muestreo",
    "section": "",
    "text": "7.1 Datos y variables\nEn una investigación reunimos datos con el objetivo de obtener alguna conclusión o predicción. Pues bien comencemos definiendo un dato como una representación puntual de la realidad. Son un solo valor, sea una sola medición, un promedio, una desviación estándar, una proporción de sexos, etc, y el conjunto de datos de un mismo atributo medidos en distintos individuos nos dan una variable. Es decir, en nuestros conjuntos de datos cada valor es un dato, y cada columna (usualmente) es una variable. ¿Por qué es importante conocer esto? Porque hay distintos tipos de variables, los cuales definen cómo es que vamos a graficar y tratar esos datos:",
    "crumbs": [
      "Fundamentos del análisis de datos",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introducción al muestreo</span>"
    ]
  },
  {
    "objectID": "c07_muestreo.html#datos-y-variables",
    "href": "c07_muestreo.html#datos-y-variables",
    "title": "7  Introducción al muestreo",
    "section": "",
    "text": "Cualitativas: hacen referencia a las cualidades de nuestros individuos, y tienen dos escalas:\n\nNominal: hace referencia a categorías en las que no hay un orden o distintas importancias. Ejemplos pueden el sexo o el color.\nOrdinal: aquí hay un órden, y un ejemplo muy claro son las encuestas: 0 es nunca, 1 es casi nunca, 2 es ocasionalmente, 3 es casi siempre y 4 es siempre. Aunque son categorías bien definidas, 2 &lt; 3 y 3 &lt; 4. No son cuantitativas porque las respuestas están sujetas a la interpretación personal, pero descartar el órden en el análisis sería un error.\n\nCuantitativas, que hacen referencia a atributos cuantificables de manera objetiva. Hay dos tipos, cada uno con dos escalas.\n\nTipos:\n\nDiscretas: Son solo números enteros. Un ejemplo cotidiano es la edad, que usualmente la expresamos en años. No vamos por la vida diciendo tengo 18.5 años o 18 años con 6 meses, solo decimos tengo 18 años.\nContinuas: Es el caso contrario, son números fraccionarios. Se les denomina continuas porque hay un número infinito de valores posibles entre un valor y el otro, un ejemplo es la temperatura (35.1ºC, 100 K, etc.)\n\nEscalas:\n\nIntervalo: La escala de intervalo es aquella en donde el 0 NO es absoluto o, mejor dicho, donde el 0 es arbitrario. La temperatura expresada en grados centígrados es un ejemplo claro, 0ºC no indica ausencia de movimiento molecular, solo toma como referencia arbitraria el punto de congelación del agua.\nRazón: Aquí el 0 sí es absoluto y representa la ausencia del atributo en cuestión. La longitud es un ejemplo, si algo tiene longitud 0 más bien no tiene longitud, o si algo tiene una temperatura de 0 K quiere decir que no tiene movimiento molecular (~-273.15ºC).\n\n\n\n\n\n\n\n\n\nFigura 7.1: Tipos y escalas de variables",
    "crumbs": [
      "Fundamentos del análisis de datos",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introducción al muestreo</span>"
    ]
  },
  {
    "objectID": "c07_muestreo.html#qué-datos-obtener",
    "href": "c07_muestreo.html#qué-datos-obtener",
    "title": "7  Introducción al muestreo",
    "section": "7.2 ¿Qué datos obtener?",
    "text": "7.2 ¿Qué datos obtener?\nAlgo que es muy importante tener siempre bien presente es que, aún cuando existen herramientas y técnicas que nos permiten procesar múltiples variables, en cualquier procedimiento de ciencia de datos es INDISPENSABLE que los datos sean de excelente calidad y, sobre todo, que sean adecuados para responder la pregunta que nos interesa, lo cual nos debe de llevar, invariablemente, a preguntarnos “¿qué datos debo de obtener?” O, en otras palabras, “¿qué debo medir?” Una frase que se me quedó marcada de mis clases de la licenciatura es “La investigación inicia y termina en el escritorio del investigador”; es decir, no salimos a hacer trabajo de campo y a registrar todo lo que se nos atraviese, caso contrario podemos terminar en una conclusión como “los bebés son traídos por cigüeñas”, o podemos tomar una decisión equivocada.\n\n7.2.1 Coincidencias\nSituémonos en la Alemania de 1960-1980, cuando estaban pasando por una crisis de natalidad. Sies (1988) quiso encontrar una medida que permitiera entender el problema y salió a buscar respuestas. En esta búsqueda se encontró con algo que le pareció sumamente interesante: había una relación notablemente alta entre la cantidad de pares de cigüeñas reproductoras y la cantidad de bebés nacidos.\n\n\n\n\n\n\nFigura 7.2: Bebes, cigüeñas y casualidad\n\n\n\nPor muy inverosímil que esto pueda parecernos, lo cierto es que este tipo de relaciones altas entre variables que no están relacionadas existen y en ocasiones puede ser muy difícil identificar si en efecto la relación es causal, casual, o si obedece a que ambas dependen de una tercera variable latente (no observada). Sobre este tema te recomiendo el artículo de Höfer, Przyrembel & Verleger (2004) sobre la “teoría de la cigüeña”, el de Haig (2010) sobre qué es una correlación espuria y también revisar esta página de internet con otras correlaciones curiosas.\n\n\n\n\n\n\nNota\n\n\n\nSi bien es cierto que las correlaciones espurias es algo que debíamos de ver en el tema de correlación, es importante reconocer que un muestreo bien planeado, basado en ciencia, es lo que minimiza la probabilidad de que describamos una relación de este tipo. Matemáticamente es prácticamente identificar si la relación que tenemos es causal o casual, por lo que es mejor evitarlas con un poco de planeación y sentido común.\n\n\n\n\n7.2.2 Contradicciones\nOtro ejemplo de la importancia de la selección de variables es el “Sesgo de supervivencia”. Situémonos ahora en la Segunda Guerra Mundial, en los cuarteles de la Fuerza Aérea de Estados Unidos. A un grupo de matemáticos le fue dada la tarea de designar qué partes de los aviones debían ser reforzadas para incrementar la tasa de supervivencia. El grupo entonces analizó los aviones que volvían a la base y generaron un diagrama como este, en el cual los puntos rojos representan las áreas con mayores daños balísticos.\n\n\n\n\n\n\nFigura 7.3: ¿Dónde reforzamos?\n\n\n\nLa primera aproximación que viene a la cabeza es reforzar esas zonas, pero Abraham Wald tuvo la suficiente visión para darse cuenta de un problema fundamental con lo que estaban midiendo: los aviones que volvían; es decir, aquellos que no habían sido derribados. Entonces propuso que en su lugar se reforzaran aquellas zonas donde NO había daños, ya que esos aviones fueron los que no volvieron a su base. El resultado: se incrementó la supervivencia como se había solicitado.",
    "crumbs": [
      "Fundamentos del análisis de datos",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introducción al muestreo</span>"
    ]
  },
  {
    "objectID": "c07_muestreo.html#un-último-paréntesis",
    "href": "c07_muestreo.html#un-último-paréntesis",
    "title": "7  Introducción al muestreo",
    "section": "7.3 Un último paréntesis",
    "text": "7.3 Un último paréntesis\nHablemos ahora sobre el uso de las palabras colecta y recolecta en el contexto biológico, que en ocasiones de utilizan de manera intercambiable al referirse a la obtención de muestras. Si revisamos la definición de colectar (RAE) veremos que esta es “recaudar (cobrar dinero)”, mientras que la definición de recolectar es “Recoger los frutos de una cosecha” o “Reunir cosas o personas de procedencia diversa”. Bajo esta luz, está claro que la terminología correcta es recolectar; sin embargo, el problema no termina ahí. En México, los permisos para reunir muestras con fines científicos tienen el nombre legal de “Licencias de colecta científica” (NOM-126-SEMARNAT-2000) y en el artículo 3 de la Ley General de Vida Silvestre se nombra a “la extracción de ejemplares, partes o derivados de vida silvestre del hábitat en que se encuentran” como colecta. Entonces, ¿cuál utilizar? Aunque pueda parecer algo trivial, lo correcto es utilizar cada una en su contexto, recolectar en la descripción del método de muestreo y colecta al declarar que a) se realizó el trámite correspondiente de acuerdo con el marco legal y b) los números de las licencias de colecta. Aunque pudiera parecer que esta discusión NO está relacionada con el curso, considero importante también el no olvidarnos del marco legal (y ético) involucrado en el desarrollo de la investigación, desde el muestreo hasta el análisis de los datos y el reporte de los resultados.",
    "crumbs": [
      "Fundamentos del análisis de datos",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introducción al muestreo</span>"
    ]
  },
  {
    "objectID": "c07_muestreo.html#muestreemos-orcas",
    "href": "c07_muestreo.html#muestreemos-orcas",
    "title": "7  Introducción al muestreo",
    "section": "7.4 Muestreemos orcas",
    "text": "7.4 Muestreemos orcas\n\n\n\n\n\n\nNota\n\n\n\nEl diseño experimental y el diseño de muestreo son temas lo suficientemente grandes como para formar un curso con ellos. Únicamente revisaremos los conceptos más importantes, de manera que puedas reflexionar un poco más sobre qué debes de tomar en cuenta en tus diseños.\n\n\nAhora sí, podemos entrar a la teoría del muestreo. Lo primero es introducir algunos conceptos básicos, aunque intentemos hacerlo de una manera más dinámica que solo dar sus definiciones. Imaginemos que el siguiente escenario. Queremos saber cuál es la longitud promedio de las orcas de cierta localidad. ¿Para qué? Para facilitar la explicación, por supuesto, aunque esto puede ser fácilmente extrapolado a cualquier investigación, sea experimental u observacional.\nBien, entonces estudiaremos la longitud de las orcas de la costa central de Oaxaca. Esto conforma nuestra población objetivo o población estadística; es decir, el conjunto de individuos que vamos a estudiar. Nota que estos individuos están bien delimitados; es decir, no vamos a considerar otras especies de delfines, ni fijarnos en el peso o en orcas de otra localidad. Únicamente vamos a medir orcas que nos encontremos al navegar en la costa central de Oaxaca. Formalmente:\n\nPoblación estadística: Conjunto de todos los individuos a estudiar. Esta puede empatar o no con una población biológica. En este caso, lo más probable es que las orcas de la CCO sean parte de una población de orcas transeúntes; sin embargo, forman nuestra población estadística.\nIndividuo: Objeto, ente, planta, animal, quimera, célula o cualquier unidad sobre la cual se realiza la observación y que, consecuentemente, tiene el atributo que queremos medir.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Individuo\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Población estadística y población biológica\n\n\n\n\n\n\n\nFigura 7.4: De individuo a población.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nLa Figura 7.4 (b) tiene algunos símbolos que no hemos mencionado, todos referentes a la simbología/notación de conjuntos:\n\n\\(\\{\\}\\) Indican que es un conjunto\n\n\\(\\lor\\) es el “ó lógico” (uno u otro).\n\\(\\subset\\) indica que lo que está a la izquierda es un subconjunto de lo que está a la derecha.\n\n\n\nAmbos conceptos van de la mano para definir las generalidades de nuestros métodos en campo o laboratorio, pues llevan la información que delimita nuestro esfuerzo y que, entonces, define nuestro marco de muestreo. No tiene ningún sentido medir orcas de La Paz, BCS, si nuestro interés son las de la CCO, Formalmente:\n\nMarco de muestreo: Lista de todas las unidades de muestreo, donde una unidad de muestreo es la unidad básica para la captación de información de la población de interés. En palabras menos rebuscadas: el conjunto de qué vamos a medir y a quién (o a qué) se lo vamos a medir.\n\nUna vez definimos nuestro marco de muestreo podemos salir a campo. En un mundo ideal tendríamos acceso a la población completa, por lo que la estimación de la longitud promedio que realizaramos sería correcta pra ese momento. ¿Por qué solo en ese momento? Puede que los individuos juveniles crezcan, que los más viejos mueran, que nazcan nuevos, que se vayan algunos, etc., y que, entonces el parámetro poblacional (la longitud promedio) que queremos medir cambie. Esto complica un poco las cosas, pues es logísticamente imposible acceder a todas las orcas de la CCO. Si lo hiciéramos tendríamos un censo, de lo contrario tenemos un muestreo; es decir, vamos a medir únicamente una fracción de la población estadística, la cual va a conformar nuestra muestra y cuyos estadísticos van a ser nuestros estimadores de los parámetros poblacionales. Formalmente (Figura 7.5):\n\nParámetro poblacional: Función definida sobre los valores de las características medibles de una población. En palabras ménos técnicas: el valor real de lo que queremos saber de la población. Representados usualmente con letras griegas o mayúsculas (\\(\\mu\\) o \\(M\\), \\(\\sigma\\) o S, por ejemplo).\nCenso: Medición de toda la población. En otras palabras: un muestreo de toda la población en un momento determinado en el tiempo.\nMuestreo: Medición de un atributo en individuos de una población.\nMuestra: Conjunto de individuos de la población que son medidos (formalmente hace referencia a las mediciones en sí mismas).\nEstadístico: Función definida sobre los valores medibles de una muestra. En otras palabras, la estimación del atributo de interés a partir de la muestra. Representados con letras latinas minúsculas (\\(\\bar{x}\\), \\(s\\)).\n\n\n\n\n\n\n\nFigura 7.5: Estadístico es a muestra como parámetro es a población.",
    "crumbs": [
      "Fundamentos del análisis de datos",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introducción al muestreo</span>"
    ]
  },
  {
    "objectID": "c07_muestreo.html#un-buen-muestreo",
    "href": "c07_muestreo.html#un-buen-muestreo",
    "title": "7  Introducción al muestreo",
    "section": "7.5 Un buen muestreo",
    "text": "7.5 Un buen muestreo\nVolvamos a nuestro ejemplo con las orcas. Realizamos cierto número de navegaciones en la temporada seca (tal vez por limitaciones logísticas), en las cuales encontramos y medimos 50 orcas. Ese número de individuos medidos es nuestro tamaño de muestra. Ese número es sumamente importante, pues define la representatividad del muestreo; es decir, qué tan buen “resumen” de la población es nuestra muestra. OJO: Esto solo aplica si el muestreo tuvo ciertas cualidades:\n\nAleatorio: Es decir, nuestro diseño de muestreo/experimental debe de permitir que todos los individuos de nuestra población sean medidos con la misma probabilidad. Esto es sumamente difícil de conseguir y podemos entrar a detalles filosófico de qué podemos considerar aleatorio y qué no, pero sí podemos tratar de hacerlo:\n\n\nExhaustivo: Muy de la mano (de hecho una consecuencia) de la aleatoriedad. El muestreo debe de considerar todos los posibles valores o atributos de la variable a medir. En nuestro ejemplo esto se reduce a que midamos orcas de todos los tamaños y que no sesguemos el muestreo a individuos únicamente grandes o pequeños, salvo que así lo definiéramos en nuestro marco de muestreo.\n\n\nExclusivo: Atributos o valores de un indicador deben ser mutuamente excluyentes; es decir, que no tengamos un individuo que mida 8 y 9.3 m, por ejemplo. Esto es un poco redundante en este escenario, pero si fuéramos a establecer rangos de edad como cría, juvenil, hembra adulta y macho adulto, que todos estén en una sola categoría, por lo que hay que decidir qué hacer con un juvenil con características de adulto. También tiene que ver con tener un marco de muestreo bien definido.\nPreciso: Tratar de tener el mayor número de distinciones posibles para tener una mejor descripción. En nuestras orcas es mejor medir en 830 cm que redondear a 8 m.\n\n\n7.5.1 Errores de muestreo/medición\nEsa “mejor descripción” me lleva también a hablar sobre el tema de los errores de muestreo o medición. Por más que querramos evitarlo, siempre habrá errores dentro de nuestro muestreo o dentro de nuestra medición. Tal vez utilizamos una regla de 30 cm para medir organismos que miden más de 30 cm, o tal vez utilizamos una regla de metal que se contrae si hace frío y se expande si hace calor. De cualquier manera, siempre vamos a ser “víctimas” de alguno de dos tipos de errores (Figura 7.6):\n\nError sistemático: Un error que ocurre cada que realizamos la medición. Es constante y consistente. Si no somos conscientes de este error, nuestros resultados son inválidos. Pensemos en nuestras orcas, y que estamos utilizando fotogrametría aérea (fotos de dron) para hacer nuestros registros. Si calibramos nuestra medición al ángulo de visión que obtenemos a 10 m de altura y tomamos todas nuestras fotos a 11 m, el error va a ser constante y podemos corregirlo con una re-calibración. El problema está cuando no sabemos que este error está sucediendo; es decir, asumir que nuestras fotos se tomaron a 10 m y hacer la estimación con esa calibración.\nError aleatorio: Un error que no es constante. Volviendo a la fotogrametría, pensemos que obtuvimos un dron cuyo altímetro salió defectuoso, que no lo sabemos y que aunque marque 10 m de altura puede estar a ± 1m de ahí. Ese error es aleatorio. No podemos predecir si en un momento dado va a sobre o subestimar la altura. En este caso, los resultados son no confiables.\n\n\n\n\n\n\n\nFigura 7.6: Tipos de errores representados con dianas de tiro.\n\n\n\nPosiblemente esos términos de no confiable o inválido te suenen alarmantes. La clave para poder contender con ellos es a) saber que existen y b) ser conscientes de su magnitud. Si sabemos qué tanto estamos subestimando por la diferencia de alturas de 10 a 11 m, podemos corregir el valor. Si sabemos cuál es la distribución del error de medición del altímetro, podemos modelarlo (la inferencia Bayesiana se presta muy bien para eso). ¿Con eso quiero decir que entonces no hay que cuidar estos detalles? PARA NADA, por el contrario, hay que cuidarlos tanto que hay que saber cómo remediarlos o incluirlos en nuestros análisis de datos.\n\n\n\n\n\n\nAdvertencia\n\n\n\nRecuerda que, desde un punto de vista práctico, la estadística es muy simple: le das un conjunto de datos y te regresa algunos números. Que lo resultante sea confiable no depende solo de utilizar una técnica adecuada, sino de que la materia prima (los datos) sea buena. ¿Has probado un jugo de naranja hecho con naranjas “pasadas”?",
    "crumbs": [
      "Fundamentos del análisis de datos",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introducción al muestreo</span>"
    ]
  },
  {
    "objectID": "c07_muestreo.html#tipos-de-muestreo-y-representatividad",
    "href": "c07_muestreo.html#tipos-de-muestreo-y-representatividad",
    "title": "7  Introducción al muestreo",
    "section": "7.6 Tipos de muestreo y representatividad",
    "text": "7.6 Tipos de muestreo y representatividad\nEn nuestro ejemplo tenemos mediciones de 50 orcas, asumamos que controlamos nuestros errores y que, por lo tanto, esos valores son válidos y confiables. ¿Podemos estar seguros de que esas 50 orcas nos dan una representación adecuada de la población? En otras palabras, nuestra muestra es representativa? Para explicar esto, generemos una población hipotética de 1000 orcas, en la cual hay 500 hembras y 500 machos y, para simplificarnos la existencia, solo hay individuos adultos. Para seguir con la simplificación, asumiremos que la longitud en ambos sexos tiene una distribución normal, con un promedio de 9.6 m para machos y 8.2 m para hembras y una dispersión de 0.1 m (más adelante hablaremos de todos estos detalles). En este escenario, podemos asumir que la media poblacional (el valor que queremos inferir) es el promedio de los promedios de ambos sexos:\n\nmachos &lt;- rnorm(500, mean = 9.6, sd = 0.1)\nhembras &lt;- rnorm(500, mean = 8.2, sd = 0.1)\nmedia_real &lt;- mean(c(9.6, 8.2))\npobl &lt;- data.frame(sexo = c(rep(\"M\", 500), rep(\"H\", 500)),\n                   lt = c(machos, hembras))\n\nSiempre es más fácil ver un gráfico:\n\nlibrary(ggplot2)\npobl_dens &lt;- ggplot(data = pobl, aes(x = lt)) +\n             geom_density(color = \"dodgerblue4\") +\n             theme_bw() +\n             geom_vline(xintercept = media_real,\n                        color = \"forestgreen\") +\n             labs(title = \"Densidad de la LT de orcas\",\n                  x = element_blank(),\n                  y = element_blank())\npobl_dens\n\n\n\n\n\n\n\n\nY ahora hablemos de los diseños probabilísticos de muestreo, partiendo de este ejemplo.\n\n7.6.1 Muestreo Aleatorio Simple\nEs, como el nombre lo indica, el más simple de los diseños. La idea es que haremos el muestreo de manera que la probabilidad de muestrear cualquier individuo de la población es uniforme para todos los individuos; es decir, la misma. PREGUNTA: ¿Esto se sostiene para nuestro ejemplo? Cuando tenemos diseños observacionales como este, siempre vamos a infringir, de una manera u otra, con la parte aleatoria. Estamos sujetos a si encontramos o no a los animales, si la “curiosidad” por la embarcación es variable, etc., etc., etc., por lo que, dependiendo de lo que estemos realizando, podemos solo “asumir” que el muestreo fue aleatorio. En este caso de ejemplo, por fortuna, podemos darnos el “lujo” de hacerlo de manera adecuada; es decir:\n\nDefinir la población estadística (hecho)\nDefinir un conjunto de muestras con la misma probabilidad de ser escogidas. Son nuestras 50 muestras a obtener, para lo cual\nSeleccionaremos una de las muestras utilizando números aleatorios.\n\nHagamos entonces el ejercicio para una muestra y luego dejemos que R haga lo demás:\n\nind &lt;- round(runif(1, min = 1, max = 1000))\nind\n\n[1] 110\n\npobl$lt[ind]\n\n[1] 9.786141\n\n\nAquí lo que hicimos fue tomar un individuo de nuestra población de manera uniformemente “aleatoria”, en este caso el número 267 y medirlo. Ahora tenemos que repetir esa acción otras 49 veces, una a una, o podemos decirle a R que lo haga en un solo paso (imaginemos que somos nosotros quienes lo hacemos):\n\nmuestras &lt;- sample(pobl$lt, size = 50, replace = FALSE)\nhead(muestras)\n\n[1] 8.121116 9.777177 8.106572 8.293091 8.206649 8.103461\n\n\nNotarás dos cosas: 1) no sabemos a qué individuos corresponden esas muestras, lo cual sería problemático si no tuviéramos 2) el argumento replace = FALSE. Con esto lo que hicimos fue un muestreo sin reemplazo; es decir, una vez que un individuo fue medido se “retiró” de la población, en el sentido de que no puede ser vuelto a medir. Este tipo de muestreo es más preciso, pues no tenemos duplicados o pseudo-réplicas. Por otra parte, el muestreo con reemplazo es útil en otro tipo de situaciones, por ejemplo cuando queremos aproximar frecuencias a largo plazo como hicimos en la sesión anterior.\n\n\n\n\n\n\nFigura 7.7: ¿Con o sin reemplazo?\n\n\n\nEn nuestro ejemplo de las orcas es correcto asumir que esto puede ser así, pues podemos saber quién es quien con fotos de sus aletas dorsales y, con ello, descartar las mediciones duplicadas. Desafortunadamente, esto no siempre es así, y es algo que debemos de considerar al diseñar nuestro muestreo. Dicho esto, ya tenemos nuestras 50 muestras, entonces podemos realizar la estimación de nuestro promedio (media):\n\nmean(muestras)\n\n[1] 8.781358\n\n\nPongámosla en nuestro gráfico inicial como una línea vertical para ver en dónde quedó:\n\npobl_dens + geom_vline(xintercept = mean(muestras),\n                       color = \"firebrick\")\n\n\n\n\n\n\n\n\nNo está exactamente donde debería de estar. Para complicar más las cosas, añadamos un pequeño sesgo a nuestro muestreo: No es completamente aleatorio, sino que hubo algún factor que hiciera que muestreáramos preferencialmente a los machos. Tal vez eran más curiosos y era más fácil tomar una foto con la embarcación como referencia. Simulemos ese escenario. Nuestra muestra seguirá siendo de 50, pero esta vez tendremos 35 machos y 15 hembras:\n\nmuestras &lt;- c(sample(machos, size = 35, replace = F),\n              sample(hembras, size = 15, replace = F))\npobl_dens + geom_vline(xintercept = mean(muestras),\n                       color = \"Firebrick\")\n\n\n\n\n\n\n\n\nAquí la estimación ya se aleja mucho más de nuestra media “real”. La razón es, sin duda, el sesgo que añadimos hacia los machos.\n\n\n7.6.2 Muestreo Aleatorio Estratificado\nEn este caso, donde la población se encuentra sub-dividida en estratos no traslapados y estos son muestreados de manera independiente, estamos hablando de un Muestreo Aleatorio Estratificado. Este es simplemente una “mezcla” de MAS, en el sentido que haremos un muestreo aleatorio simple dentro de cada estrato, aunque es importante conocer su tamaño de antemano. Para obtener la media poblacional solo obtenemos el promedio de ambos promedios:\n\npobl_dens + geom_vline(xintercept = mean(c(mean(muestras[1:35]),\n                                           mean(muestras[36:50]))),\n                       color = \"firebrick\")\n\n\n\n\n\n\n\n\nMatemáticamente la ecuación es más complicada, primero obtendríamos el peso de cada estrato (\\(W_s\\)), donde \\(n_s\\) es el tamaño del estrato \\(s\\) y \\(N\\) es el tamaño poblacional total:\n\\[\nW_s = \\frac{n_s}{N}\n\\]\nLuego, la media de la población estratificada está dada por:\n\\[\n\\mu = \\frac{\\sum_{s = 1}^{S} n_s*\\bar{x}_s}{N} \\\\\n\\therefore \\\\\n\\mu = \\sum_{s = 1}^{S} W_s*\\bar{x}_s\n\\]\nComprobémoslo:\n\n# Peso de cada estrato:\nw &lt;- c((500/1000), (500/1000))\nm &lt;- c(mean(muestras[1:35]), mean(muestras[36:50]))\n\npobl_dens + geom_vline(xintercept = sum(w*m), color = \"Firebrick\")\n\n\n\n\n\n\n\n\nLa igualdad podría demostrarse matemáticamente (solo aplica con estratos de igual tamaño), pero eso queda como ejercicio si te da curiosidad. OJO: Hacer esta estimación de la media total solo es válida si a) tiene sentido recuperarla o estimarla, lo cual depende totalmente de la pregunta de investigación (puede que tenga más sentido estimar la media para cada grupo) y b) si el muestreo es representativo. Veamos qué pasa si muestreamos únicamente 10 y 5 individuos:\n\nmuestras &lt;- c(sample(machos, size = 10, replace = F),\n              sample(hembras, size = 5, replace = F))\n\npobl_dens + geom_vline(xintercept = mean(c(muestras[1:10],\n                                           muestras[11:15])),\n                       color = \"Firebrick\")\n\n\n\n\n\n\n\n\nComo era de esperarse, la estimación está sumamente sesgada. Esto es por el problema de la representatividad; es decir, nuestra muestra es muy pequeña (especialmente para las hembras) y, por lo tanto, no es una buena imagen de lo que pasa a nivel población. Como ejercicio, estima la media de la población estratificada con las ecuaciones que vimos arriba. ¿Cambió el resultado?\nEste problema tiene una “solución”: muestrear más. ¿Qué tanto? Depende de la precisión (grado de error) que querramos. La relación entre el tamaño de muestra, la Varianza poblacional y la precisión está dada por:\n\\[\nn = \\frac{1.96^2 S^2}{d^2}\n\\]\n¿De dónde sale ese 1.96? Es el límite al 95% de confianza (en pruebas de hipótesis hablaremos de qué es eso) de una distribución normal, dado en términos de desviaciones estándar, por lo que podríamos cambiarlo por cualquier otro número para representar cualquier intervalo de confianza. ¿Qué pasa si la población no tiene una distribución normal? A ese 1.96 tenemos aproximadamente el 75% de los datos poblacionales (Desigualdad de Chebyshev), independientemente de su distribución. No entraré en esos detalles, lo que es realmente importante es que debemos de conocer la varianza poblacional, lo cuál es un problema y de los grandes. Gigante, de hecho. Podemos estimarla a partir de la muestra, pero resulta que, si no es representativa, nuestra estimación de la varianza tampoco es confiable. En este caso, para tener un muestreo representativo al 95% de confianza, considerando la varianza poblacional de \\(0.1^2\\) y con una precisión de 0.05m, necesitaríamos un tamaño de muestra de mínimo 15 individuos (por sexo).\n\nn &lt;- ((1.96^2)*(0.1^2))/0.05^2\nround(n)\n\n[1] 15\n\n\nO podemos ver cuál es la precisión de la estimación de la media de las hembras con nuestra muestra de 5 individuos:\n\nd &lt;- sqrt(((1.96^2)*(0.1^2))/5)\nd\n\n[1] 0.08765386\n\n\nEs decir, tenemos un márgen de error aproximado de 0.87m para la media de las hembras. A esto lo acompaña algo que se conoce como pruebas de potencia, sobre lo cual encontrarás referencias en la sección de lecturas recomendadas del servidor de Discord. También es importante mencionar que en la sesión de pruebas de hipótesis hablaremos de cómo representar la incertidumbre en nuestras estimaciones (intervalos de confianza). Por el momento basta que te lleves el mensaje de que es importante considerar qué factores pueden estar generando “ruido” en tu diseño. Si tu pregunta se puede responder mediante un experimento en condiciones controladas, controla toda posible fuente de variación externa al factor que te interesa, de manera que lo que midas refleje únicamente lo que quieras responder. Si es un estudio observacional, acota tu marco de muestreo lo más posible. En ambos casos, y en la medida de lo posible, mide todas las fuentes de variación que pudieran estar influenciando tus observaciones e inclúyelas en tus análisis.\nEsto sería todo para esta sesión. Espero que haya sido de tu agrado y, sobre todo, que aunque no haya podido resolver todas tus incertidumbres, te haya motivado a reflexionar sobre la importancia de un buen muestreo. La inferencia estadística NO es magia negra que pueda resolver nuestros problemas, simplemente nos ayuda a tomar decisiones en situaciones de incertidumbre pero, que esas decisiones sean buenas (o no) depende totalmente de los datos que tengamos disponibles (y un poco de suerte).\n\n\n\n\nHaig BD. 2010. What Is a Spurious Corelation? Understanding Statistics 2:125-132. DOI: 10.1207/S15328031US0202_03.\n\n\nHöfer T, Przyrembel H, Verleger S. 2004. New evidence for the Theory of the Stork. Paediatric and Perinatal Epidemiology 18:88-92.\n\n\nSies H. 1988. A new parameter for sex education. Nature 332:495.",
    "crumbs": [
      "Fundamentos del análisis de datos",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introducción al muestreo</span>"
    ]
  },
  {
    "objectID": "s03_basics.html",
    "href": "s03_basics.html",
    "title": "Técnicas básicas",
    "section": "",
    "text": "Objetivo de aprendizaje\nEl objetivo de esta sección es que te familiarices con el análisis de datos, comenzando con la estadística descriptiva. Posteriormente que ahondes en las técnicas (posiblemente) más comunes y socorridas en análisis bioestadísticos: \\(t\\) de Student y ANOVA, no solo con su aplicación, sino también con sus fundamentos e intuiciones. Revisarás qué es son los intervalos de confianza y valor de p, cómo y cómo no interpretarlos, revisarás los supuestos de normalidad y homogeneidad de varianzas, desde una perspectiva teórica y con pruebas visuales para facilitar su entendimiento.",
    "crumbs": [
      "Técnicas básicas"
    ]
  },
  {
    "objectID": "c08_descriptiva.html",
    "href": "c08_descriptiva.html",
    "title": "8  Estadística descriptiva",
    "section": "",
    "text": "8.1 Descripciones o exploraciones\nYa fuimos a campo o al laboratorio, realizamos nuestro muestreo o experimiento, y tenemos nuestros datos. ¿Ya podemos empezar a hacer pruebas de significancia y regresiones, no? Técnicamente sí, pero paremos nuestro tren y no querramos correr antes de saber caminar. Al momento de realizar el análisis de nuestros datos es “sano” que nos familiaricemos con ellos antes de realizar cualquier proceso de estadística inferencial. El objetivo de la estadística descriptiva es, como el nombre indica, describir la información contenida en los datos, para lo cual utiliza métodos que resumen la información: medidas de tendencia central/dispersión y gráficos. ¿Por qué es importante? Nos permite familiarizarnos con la información con la que contamos, ya sea para identificar patrones, seleccionar variables importantes, visualizar inconsistencias en los datos, detectar anomalías y mucho más. Notarás conforme avancemos en el curso que la visualización de nuestros datos juega un papel importantísimo durante la interpretación, pero empecemos desde abajo.\nEstoy seguro de que algunos de los conceptos que revisaremos en esta sesión los has revisado ya con anterioridad, por lo que no te haré el cuento largo, sino que me enfocaré más en las peculiaridades de cada una de las medidas y gráficos, en qué escenarios son útiles, cuándo no lo son tanto y lo enlazaremos con temas que veremos más adelante.",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Estadística descriptiva</span>"
    ]
  },
  {
    "objectID": "c08_descriptiva.html#medidas-de-tendencia-central",
    "href": "c08_descriptiva.html#medidas-de-tendencia-central",
    "title": "8  Estadística descriptiva",
    "section": "8.2 Medidas de tendencia central",
    "text": "8.2 Medidas de tendencia central\n\nlibrary(ggplot2)\nlibrary(GGally)\nlibrary(palmerpenguins)\n\nComencemos hablando con lo que, por lo general, es nuestro mayor interés: hacia dónde tienden nuestros datos. Para ello tenemos algunas medidas de tendencia central; es decir, literalmente describimos nuestros datos a partir de dónde se acumulan más.\n\n8.2.1 Media\nLa primera de estas medidas es, posiblemente, la más común de todas: la media o el promedio. En su forma más simple; es decir, la media aritmética la obtenemos sumando todos nuestros datos y dividiéndolos entre su número:\n\\[\n\\overline{x} = \\frac{\\sum_{i = 1}^n x_i}{n}\n\\]\nEsto es conocimiento general, y es algo con lo que todos los estudiantes somos torturados eventualmente. Bueno, más que recordar cómo calcularlo e implementarlo en R (mean(x), donde x es un vector de observaciones), pensemos en qué representa. La media es, literalmente, un indicador de hacia qué valor se están acumulando nuestros datos, tal y como si colgáramos cosas en un tendedero (házme un favor y piensa en que el siguiente gráfico está invertido verticalmente):\n\nset.seed(0)\nx &lt;- data.frame(x = rnorm(100, mean = 0))\nplot1 &lt;- ggplot(data = x, aes(x = x)) +\n         geom_density(color = \"dodgerblue4\") +\n         theme_bw() +\n         geom_vline(xintercept = mean(x$x),\n                    color = \"dodgerblue4\")\nplot1\n\n\n\n\n\n\n\n\nEn este caso, la mayor parte de nuestros datos están acumulados alrededor de 0. ¿Qué crees que pase si añadimos otros 20 datos, esta vez acumulados en 5? Veámos el cambio:\n\nset.seed(0)\nx2 &lt;- rbind(x, data.frame(x = rnorm(20, mean = 5)))\nplot2 &lt;- plot1 +  geom_density(aes(x = x),\n                               data = x2,\n                               color = \"firebrick\")\nplot2 + geom_vline(xintercept = mean(x2$x),\n                   color = \"firebrick\")\n\n\n\n\n\n\n\n\nComo era de esperarse, la media se “jaló” a la derecha, y de manera bastante notable. Esos 5 valores extremos tuvieron un peso bastante importante. Literalmente fue como si hubiéramos colgado cinco cosas más en nuestro tendedero, pero alejadas de la ropa que colgamos en un inicio. Aunque la media es una manera muy efectiva de resumir nuestros datos, esto solo es cierto si estos se parecen a una distribución normal (en la sesión de técnicas paramétricas hablaremos duro y tendido con respecto a esto), pero si no, como en nuestro caso con los nuevos valores, es necesario buscar una alternativa.\n\n8.2.1.1 Media ponderada\nUna de ellas es una modificación a la media, en la cuál cada valor tiene su propia ponderación o su propia importancia. Esa es la media con la que más padecemos los estudiantes de secundaria hacia arriba, pues el examen tiene una ponderación distinta a las tareas, por ejemplo. En mi caso personal, era un tormento cuando se ponderaba más las tareas que el examen, pero eso es otra historia. ¿Por qué es importante? Porque podemos utilizarla para “regresar” nuestra media a su lugar; de hecho, esta es la base fundamental detrás de las regresiones robustas, en donde el peso de cada observación disminuye según incrementa su distancia de 0:\n\n# Pesos: fracción de la distancia máxima a 0\nw &lt;- max(abs(0 - x2$x))/abs(0 - x2$x)\n# Media ponderada\nwmean &lt;- weighted.mean(x2$x, w)\n\nplot2 + geom_vline(xintercept = wmean,\n                   color = \"firebrick\")\n\n\n\n\n\n\n\n\n\n\n8.2.1.2 Media geométrica\nEsta es menos conocida, y representa el promedio de porcentajes, razones o tasas de crecimiento. Se expresa como la raiz n-ésima del producto de los n valores:\n\\[MG = \\sqrt[n]{\\Pi_i^nx_i}\\]\nPensemos en que estimamos la tasa de crecimiento poblacional (\\(\\lambda\\)) anual de ballenas jorobadas en tres años seguidos, a partir de un modelo de marca-recaptura, y los valores que obtuvimos fueron 1.03, 0.98, 1.4, 0.94:\n\nlamb &lt;- c(1.03, 0.98, 1.4, 0.94)\nprod(lamb)^(1/length(lamb))\n\n[1] 1.073569\n\n\nEs decir, el crecimiento poblacional promedio fue del 7%.\nOtra manera de calcularla es:\n\\[\nMG = e^\\overline{log(\\lambda)}\n\\]\n\nexp(mean(log(lamb)))\n\n[1] 1.073569\n\n\n¿Te animas a encontrar la igualdad matemática?\n\n\n\n8.2.2 Mediana\nUna alternativa más es la mediana. A diferencia de la media, que “busca” hacia donde se están acumulando los datos, la mediana nos indica exactamente que valor se encuentra en el centro de nuestra base de datos. Si partimos nuestra base de datos en 100 partes iguales (100%), cada parte representa un cuantil (1%). Cada diez cuantiles tenemos un decil, cada 25 cuantiles tenemos un cuartil (cuartiles 25%, 50% y 75%), y en el cuartil 50 tenemos la mediana. Esta es, entonces, mucho menos sensible a valores extremos:\n\nplot2 + geom_vline(xintercept = median(x2$x),\n                   color = \"firebrick\")\n\n\n\n\n\n\n\n\nLa estimación no es exactamente la misma que la media de los datos originales o de la media ponderada según su distancia a 0; sin embargo, el efecto es notablemente menor que con la media tradicional. Esta resistencia a valores extremos es lo que hace que las técnicas no paramétricas estén basadas en la mediana, en vez de la media.\n\n\n8.2.3 Moda\nLa moda corresponde al valor que más se repite en un conjunto de datos. Con datos continuos en el sentido estricto no existe; sin embargo, en muchos casos sí que podemos tener repetidos dependiendo de la escala y la precisión de nuestro instrumento. Otra manera de calcularla es discretizando nuestros datos y encontrar el intervalo más frecuente. Una propiedad interesante de la moda es que su valor corresponde con el valor que tiene la mayor probabilidad dentro de la distribución, por lo que puede ser útil en ciertos casos de Inferencia Bayesiana. Para calcularla podemos utilizar la función Mode(x) de la librería DescTools:\n\nletmode &lt;- DescTools::Mode(c(\"a\", \"a\", \"b\", \"c\", \"d\"))\nletmode\n\n[1] \"a\"\nattr(,\"freq\")\n[1] 2\n\n\nA lo largo de este curso no aplicaremos la moda, solo la agregué para que la tengas presente.",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Estadística descriptiva</span>"
    ]
  },
  {
    "objectID": "c08_descriptiva.html#medidas-de-dispersión",
    "href": "c08_descriptiva.html#medidas-de-dispersión",
    "title": "8  Estadística descriptiva",
    "section": "8.3 Medidas de dispersión",
    "text": "8.3 Medidas de dispersión\nAl igual que en el caso anterior, repasaremos rápidamente las medidas de dispersión, con el objetivo de explorar la intuición detrás de ellas y su relación con otros conceptos que revisaremos más adelante. Independientemente de cuál utilicemos, todas las medidas de dispersión indican justamente eso, qué tan grande es la variabilidad de una distribución, ya sea de nuestros datos o la distribución muestral del parámetro que estemos estimando.\n\n8.3.1 Desviación estándar y Varianza\nEn pocas palabras, la varianza es una medida de la dispersión promedio de los datos; es decir, cuál es el área promedio que abarca la dispersión de los datos. En la sección de Multivariado vamos a ver cuál es la relación entre la varianza y la covarianza, a entender a la varianza como un caso especial de la covarianza y ver de dónde sale esa suma de cuadrados. Esto último también me lleva a que cada que leas suma de cuadrados pienses en una medida de dispersión o en la varianza de los datos.\n\\[\n\\sigma^2 = \\frac{\\sum{(x_i - \\mu)^2}}{N}\n\\]\nLa desviación estándar, por otra parte, es simplemente la raíz cuadrada de la varianza. Si la varianza representa un área, la desviación estándar representa una distancia, la distancia promedio que existe entre cada uno de los datos y la media. Estas dos medidas (la desviación estándar y la varianza) son sumamente útiles y utilizadas en los procesos estadísticos; de hecho, junto con la media, son los principales parámetros poblacionales que usualmente queremos estimar a partir de nuestra muestra. Una aclaración es que la ecuación de arriba es para calcular la varianza poblacional, mientras que si queremos calcular la varianza muestral aplicaremos una corrección con los grados de libertad (que definiremos más adelante)\n\\[\ns^2 = \\frac{\\sum(x_i - \\overline{x})^2}{n-1}\n\\]\nEsta varianza muestral se considera un estimador insesgado de la varianza poblacional.\n\n8.3.1.1 Estimadores\nEste es un buen momento para hablar de los estimadores. ¿Qué es un estimador? Una medida que utilizaremos para estimar un parámetro poblacional. Evidentemente, no puede ser cualquier número ni cualquier medida, debe de tener ciertas características. Particularmente:\n\nInsesgado: Es decir, que la media de la distribución del estimador sea igual al parámetro. De nuevo, en la sesión de técnicas paramétricas vamos a hablar sobre distribuciones muestrales, el teorema del límite central y su implicación para el supuesto de normalidad que a veces puede ser un dolor de cabeza. Por lo pronto entiende que “la media de la distribución del estimador” hace referencia a que, si hicieramos una cierta cantidad de muestreos y calculamos algún parámetro para cada muestreo, el promedio de esas estimaciones debe ser igual al parámetro poblacional.\nConsistencia: Es la propiedad en la que un estimador se aproxima al valor del parámetro conforme incrementa el tamaño de muestra, lo cual también tiene que ver con el teorema del límite central que revisaremos más adelante.\nEficiencia: La estimación tiene el error estándar más pequeño cuando se compara con otro estimador. Por ejemplo, en una distribución Normal, la media y la mediana son prácticamente iguales; sin embargo, el error estándar de la media es \\(\\frac{\\sigma}{\\sqrt{n}}\\), mientras que el de la mediana es \\(\\approx 1.25\\) veces ese valor.\nSuficiencia: El estimador es, por sí mismo, capaz de transmitir toda la información disponible en la muestra sobre el valor del parámetro.\n\nPor estas 4 razones es que la mayor parte de nuestras inferencias están en relación a la media poblacional, estimada a partir de la media muestral. La mediana no se considera un buen estimador de la media poblacional si la distribución no es simétrica, pues, como vimos arriba, está sesgada en relación a la media poblacional. Por otra parte, su error estándar es mayor que el error estándar de la media (en términos de sus distribuciones muestrales), ni utiliza todos los datos (solo el cuantil 50).\n\n\n\n8.3.2 Coeficiente de variación\nEl coeficiente de variación, también conocido como la desviación estándar relativa, es la relación que existe entre la desviación estándar y la media de los datos, tal que:\n\\[\nCV = \\frac{\\sigma}{\\mu}*100\n\\]\nEste es especialmente útil cuando queremos comparar las dispersiones de dos cosas que están en distinta escala, o expresar en un porcentaje qué tan grande es nuestra variabilidad en relación a nuestra estimación. Si hacemos una estimación de tamaño poblacional de 1000 individuos, con un coeficiente de variación del 50% quiere decir que nuestra variabilidad es de la mitad de nuestra estimación.\n\n\n8.3.3 Error estándar\nEsta medida es especialmente útil en la estimación de los intervalos de confianza de cualquier parámetro, y representa la desviación estándar de su distribución muestral, el cuál podemos estimar como:\n\\[\n\\sigma_{\\overline{x}} \\approx \\frac{\\sigma_x}{\\sqrt{n}}\n\\]\nNormalmente nosotros no estimaremos o interpretaremos esta medida, sino que iremos directamente a los intervalos de confianza para expresar la incertidumbre alrededor de nuestras estimaciones.\n\n\n\n\n\n\nNota\n\n\n\n¿Qué son los intervalos de confianza? Son una medida de la incertidumbre en la estimación de nuestro parámetro de interés, pero hablaremos de ellos más a detalle en las siguientes sesiones.",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Estadística descriptiva</span>"
    ]
  },
  {
    "objectID": "c08_descriptiva.html#gráficos",
    "href": "c08_descriptiva.html#gráficos",
    "title": "8  Estadística descriptiva",
    "section": "8.4 Gráficos",
    "text": "8.4 Gráficos\nEn la sesión de visualización vimos las consideraciones que debemos de tener en cuenta para hacer una visualización efectiva, pero no hablamos de los tipos de gráficos que podemos realizar. En esta sesión, entonces, no entraremos a ver los detalles de la visualización, simplemente hablaremos de los gráficos más comunes, en qué escenarios son útiles y cómo construirlos utilizando ggplot2. Para esta parte utilizaremos datos de biometrías de pinguinos, contenidos en la librería palmerpenguins\n\n8.4.1 Gráfico de frecuencias\nTambién nombrado a veces histograma. Es, posiblemente, el gráfico más sencillo de todos, pues lo único que hacemos es poner una barra a la altura del número de individuos que hay en una clase o intervalo. Para construirlo en ggplot2 utilizaremos la capa geom_histogram:\n\nggplot(data = penguins,\n       aes(x = bill_length_mm)) +\n  geom_histogram(fill = \"dodgerblue4\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nSi tenemos más de un grupo y queremos ver todas las distribuciones de frecuencia podemos pasar el argumento fill dentro de los argumentos de estética (aes())\n\nggplot(data = penguins,\n       aes(x = bill_length_mm, fill = species)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nHabrás notado que ggplot2 nos dio una notificación con respecto al número de intervalos (cajas) utilizados para estimar las frecuencias, en particular que utilizó 30. Podemos seleccionar una cantidad manualmente, por ejemplo 100, y especificarla con el argumento bins, o especificar la amplitud del intervalo con el argumento binwidth:\n\nggplot(data = penguins,\n       aes(x = bill_length_mm, fill = species)) +\n  geom_histogram(bins = 100)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n¿El problema? ¿Cómo sabemos cuántos intervalos utilizar? Si ya hay intervalos establecidos, por ejemplo longitudes para estados de madurez sexual, podemos establecer esos intervalos, pero si nuestra distribución es continua podemos utilizar una mejor alternativa.\n\n\n8.4.2 Gráfico de densidad\nLos gráficos de densidad nos permiten, justamente, representar gráficamente la distribución de una variable continua. Recordarás de la clase de probabilidad que la probabilidad de un valor continuo es teóricamente 0, pero podemos estimar la densidad de probabilidad de un punto determinado si hacemos el intervalo infinitesimalmente pequeño, y es esto lo que representa un gráfico de densidad. Piensa en él como una versión suavizada de un gráfico de frecuencias:\n\nggplot(data = penguins,\n       aes(x = bill_length_mm, fill = species)) +\n  geom_density(color = NA)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\nEso se ve bastante mejor, pues ya no tenemos la incertidumbre de cuántas “cajas” formar para encajonar nuestros datos, pero no es lo más adecuado para comparar distribuciones de manera gráfica. Afortunadamente, también tenemos alternativas para eso.\n\n\n8.4.3 Gráfico de cajas y bigotes\nUna es el clásico gráfico de cajas y bigotes, en el cuál hay (por grupo) un indicador (punto, línea horizontal o muesca) de la tendencia central (media, mediana), una caja que indica una medida de dispersión (desviación estándar, rango intercuantil, error estándar) y dos “bigotes” (líneas verticales) que indican los límites “aceptables” de la distribución. Adicionalmente tenemos puntos “libres” que indican valores extremos. En ggplot2 podemos construirlos con la capa geom_boxplot, siempre que especifiquemos en aes() una variable de agrupamiento para x, la variable de la que queremos ver el interés en y y, opcionalmente, un argumento de color que corresponda con los grupos en x.\n\nggplot(data = penguins,\n       aes(x = species, y = bill_length_mm,\n           color = species)) +\n  geom_boxplot(fill = NA)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nInfortunadamente perdemos mucha información si lo comparamos con el gráfico de densidad. Si tan solo hubiera una manera de mezclarnos… ¡Espera! Sí que la hay.\n\n\n8.4.4 Gráfico de violín\nLos gráficos de violín son, justamente, una alternativa más “fina” a los gráficos de cajas y bigotes, en los cuales podemos mostrar la distribución completa de cada grupo. En ggplot2 utilizaremos la capa geom_violin, que requiere exactamente la misma información que el gráfico de caja y bigotes:\n\nggplot(data = penguins,\n       aes(x = species, y = bill_length_mm,\n           color = species)) +\n  geom_violin(fill = NA)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_ydensity()`).\n\n\n\n\n\n\n\n\n\nAfortunadamente, uno no está peleado con el otro, y podemos añadir la información resumida disponible en el gráfico de cajas y bigotes:\n\nggplot(data = penguins,\n       aes(x = species, y = bill_length_mm,\n           color = species)) +\n  geom_violin(fill = NA) +\n  geom_boxplot(fill = NA, width = 0.1)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_ydensity()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nCon estos gráficos vemos la relación entre una variable categórica y una variable numérica, pero ¿qué pasa si tenemos dos variables numéricas?\n\n\n8.4.5 Gráfico de dispersión\nEl gráfico más básico es el gráfico de dispersión. Simplemente es un gráfico de las coordenadas dadas por una variable x con respecto a una variable y. En ggplot2 utilizaremos la capa geom_point:\n\nggplot(data = penguins,\n       aes(x = flipper_length_mm,\n           y = bill_length_mm)) +\n  geom_point(color = \"dodgerblue4\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nAl igual que en el gráfico de frecuencias, podemos colorear los puntos según una variable categórica:\n\nggplot(data = penguins,\n       aes(x = flipper_length_mm,\n           y = bill_length_mm)) +\n  geom_point(aes(color = species))\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nE, incluso, según una variable continua:\n\nggplot(data = penguins,\n       aes(x = flipper_length_mm,\n           y = bill_length_mm)) +\n  geom_point(aes(color = body_mass_g))\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n¿Y si no me interesa ver todos los puntos, sino las distribuciones bivariadas? También tenemos alternativas.\n\n\n8.4.6 Gráfico de densidad bivariado\nPodemos utilizar un gráfico de densidad bivariado, el cual muestra contornos correspondientes a la densidad de los puntos. Su interpretación es exactamente igual a un mapa topográfico. Los contornos más pequeños representan la mayor densidad, mientras que los más grandes una menor densidad.\n\nggplot(data = penguins,\n       aes(x = flipper_length_mm,\n           y = bill_length_mm)) +\n  geom_density2d(aes(color = species))\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density2d()`).\n\n\n\n\n\n\n\n\n\n\n\n8.4.7 Heatmaps\n¿Y si mis dos variables son discretas? No temas, para eso tenemos heatmaps:\n\nggplot(data = penguins,\n       aes(x = species,\n           y = island)) +\n  geom_tile(fill = \"dodgerblue4\",\n            color = \"dodgerblue4\")\n\n\n\n\n\n\n\n\nSi tienes una variable adicional (pueden ser frecuencias o alguna variable continua) puedes agregarla con el argumento fill dentro de aes:\n\nggplot(data = penguins,\n       aes(x = species,\n           y = island)) +\n  geom_tile(aes(fill = bill_length_mm))\n\n\n\n\n\n\n\n\n¿Hay algún equivalente para tres variables numéricas? Por supuesto.\n\n\n8.4.8 Gráfico de contornos\nPodemos generar un gráfico de contornos, utilizando la capa geom_contour. OJO: para esta hay que hacer cierto procesamiento de los datos, en el sentido que hay que generar una malla uniforme de coordenadas x, y, z. Esto queda fuera de la discusión por este momento, pero lo revisitaremos cuando grafiquemos la zona de decisión de un análisis de funciones discriminantes lineales en la sesión de clasificación.\n\n\n8.4.9 Otros gráficos\nEstos gráficos están lejos de ser los únicos a nuestra disposición. Para comparar distribuciones también tenemos forest plots o ridgelines. Si queremos ver la relación entre más de dos variables, considerando cada individuo, podemos utilizar gráficos de coordenadas paralelas, para lo cual utilizaremos la función ggparcord de la librería GGally:\n\nGGally::ggparcoord(data = penguins,\n                   groupColumn = \"species\",\n                   columns = 3:6)\n\n\n\n\n\n\n\n\nY muchos otros más que no tendríamos tiempo de revisar en una sola sesión, cada uno con un uso particular. Algunos de ellos los veremos durante el curso, mientras que algunas otras alternativas las puedes encontrar en este enlace. También es importante mencionar que R base y ggplot2 no son las únicas librerías para generar gráficos en R. Podemos utilizar D3 o plotly, por ejemplo, para generar no solo otro tipo de gráficos, sino también hacerlos interactivos.\nEsto sería todo para esta sesión. Espero que haya sido un buen recordatorio y que te sea de utilidad en algún futuro.",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Estadística descriptiva</span>"
    ]
  },
  {
    "objectID": "c09_ph0.html",
    "href": "c09_ph0.html",
    "title": "9  Pruebas de significancia estadística",
    "section": "",
    "text": "9.1 Hipótesis\nSi yo te preguntara ¿qué es una hipótesis? Yo esperaría una respuesta cercana a “una posible explicación a un fenómeno”. La definición “completa” que me gustaría que recordaras es “una serie de premisas concatenadas para dar una explicación a un fenómeno”, de la forma “Dado que A, entonces B” (¿recuerdas la probabilidad condicional?). Bueno, pues es importante reconocer que esa NO es la hipótesis que probamos con las pruebas de significancia, o al menos no directamente. Si a nosotros nos interesa saber si un nuevo alimento, llamémosle A, es capaz de darnos peces más grandes que nuestro alimento actual (control, C), entonces diseñaríamos un experimento con dos grupos: un grupo experimental al que le daríamos el alimento A, y un grupo control al que se le daría el alimento C. Nuestra hipótesis sería algo como “Dadas las diferentes composiciones de los alimentos A y C, la talla final será diferente entre ambos grupos”. Esa es nuestra hipótesis de trabajo; es decir, nuestra posible explicación; sin embargo, esta no es una hipótesis estadística.\n¿Qué es entonces una hipótesis estadística? Sería más correcto hablar de juegos de hipótesis estadísticas, el cual quedaría de la siguiente manera:\nEntonces, es un juego porque hay más de una, y representan un escenario de igualdad o de diferencia. Por cada juego de hipótesis hay al menos una hipótesis de nulidad (\\(H_0\\)) y una hipótesis alternativa (\\(H_1\\)). ¿Cómo llegamos a estas hipótesis y qué representan? Bien, es ahí donde debemos de hablar de las pruebas de significancia estadística.",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Pruebas de significancia estadística</span>"
    ]
  },
  {
    "objectID": "c09_ph0.html#hipótesis",
    "href": "c09_ph0.html#hipótesis",
    "title": "9  Pruebas de significancia estadística",
    "section": "",
    "text": "\\(H_0: \\mu_1 = \\mu_2; \\overline{X_A} = \\overline{X_C}\\)\n\\(H_1: \\mu_2 ≠ \\mu_2; \\overline{X_A} ≠ \\overline{X_C}\\)\n\n\n\n\n\n\n\n\nNota\n\n\n\nLos términos “pruebas de hipótesis de nulidad” y “pruebas de significancia estadística” hacen referencia a lo mismo; es decir, son sinónimos para referirnos al proceso mediante el cual utilizaremos a la estadística para tomar una decisión.",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Pruebas de significancia estadística</span>"
    ]
  },
  {
    "objectID": "c09_ph0.html#pruebas-de-hipótesis-de-nulidad",
    "href": "c09_ph0.html#pruebas-de-hipótesis-de-nulidad",
    "title": "9  Pruebas de significancia estadística",
    "section": "9.2 Pruebas de hipótesis de nulidad",
    "text": "9.2 Pruebas de hipótesis de nulidad\nEn el Capítulo 1 dimos algunas definiciones de estadística, pero hay una que, toma mucho sentido en el tema que vamos a abordar hoy. Esta fue propuesta por Savage (1954): “[La estadística] es la ciencia de tomar decisiones bajo situaciones de incertidumbre.\nPartiendo de esa definición, entendamos qué es la inferencia estadística y las pruebas de hipótesis, utilizando una analogía propuesta por Cassie Kozyrkov (científica jefa de decisión en Google). Pensemos que logramos contratarnos en una empresa de exploración espacial, y que nuestra actividad principal es salir al espacio en una nave espacial, visitar planetas y reportar si hay vida o no.\n\n\n\n\n\n\nFigura 9.1: A buscar planetas.\n\n\n\n¿Bonito? Tal vez demasiado para ser verdad y, como era de esperarse, lo es. Tenemos un jefe con una actitud bastante mejorable, pero dejando eso de lado, tenemos algunas limitaciones logísticas:la primera es que nuestro reporte es a través de un aparatito que cuenta con dos botones uno para decir que sí hay vida y otro para decir que no hay vida en un planeta dado, otra limitación es que tenemos recursos limitados y que únicamente podemos hacer caminatas de dos horas. Esto se traduce en que estamos en una situación de incertidumbre.\n\n\n\n\n\n\nFigura 9.2: Exploración espacial e incertidumbre.\n\n\n\nUn desafío más es que NO podemos ver un planeta y no dar un reporte. Debemos, sí o sí, decir si hay vida en el planeta o no. Esto lleva a que necesitemos establecer una decisión por defecto. ¿Por defecto para qué? Para aquellos casos en los que NO podamos aterrizar en el planeta y explorarlo (tal vez hay una tormenta eléctrica, o alguna otra situación). Estarás de acuerdo conmigo en que decir que NO hay vida en el planeta tiene más sentido que decir que sí, y no por la situación del planeta en si misma, sino porque de lo contrario no tiene caso hacer una exploración espacial. Me explico. Si la acción por defecto fuera decir que sí hay vida en el planeta no necesitariamos ni siquiera hacer exploraciones, simple y sencillamente presionaríamos el botón correspondiente cada que nos avisaran de un nuevo planeta.\n\n\n\n\n\n\nFigura 9.3: Decisiones por defecto y alternativa.\n\n\n\nUna vez planteadas nuestras decisiones por defecto, podemos pensar en probar si hay vida en el planeta o no. Esto es un escenario típico de una prueba de hipótesis de nulidad, lo que nos lleva a definir una hipótesis nula y una alternativa. Para nuestra hipótesis nula nos podemos preguntar:\n“Si supiera todo sobre este planeta, ¿qué me inclinaría a presionar el botón X?”.\nEspero que tu respuesta haya sido “Que no haya vida en el planeta”. Simple, ¿no? A final de cuentas lo sabemos TODO sobre el planeta, incluyendo si hay vida o no y en consecuencia presionaremos el botón X si y solo si no hay vida. ¿Y la hipótesis alternativa? Pues está dada por todas las situaciones en las que la hipótesis nula sea falsa, en este caso que sí haya vida en el planeta.\nEntonces, definimos nuestras hipótesis:\n\nNula (\\(H_0\\)): No hay vida en el planeta.\nAlternativa(\\(H_A\\)): Sí la hay.\n\n\n\n\n\n\n\nFigura 9.4: Hipótesis de nulidad e hipótesis alternativa.\n\n\n\nSalimos entonces a explorar el universo, encontramos un planeta, aterrizamos y damos nuestra caminata. El resultado: 0 organismos en las dos horas que caminamos. Te pregunto: ¿qué aprendimos que sea de interés? Y aquí espero que tu respuesta sea Nada. Me explico, tuvimos una muestra de 0 organismos, no sabemos de qué tamaño es la población (si es que la hay). ¿Explicaciones para el resultado? Bastantes, pero todo se reduce a que tuvimos que tomar nuestra decisión por defecto y, por lo tanto, no aprendimos nada del planeta. Literalmente obtuvimos el mismo resultado que si hubieramos pasado de largo y eso está bien. No entraré en la discusión de por qué el buscar siempre aprender algo es un sinsentido, solo diré que quien quiera hacerlo es porque tiene demasiada energía, pero sigamos con nuestro ejemplo.\n\n\n\n\n\n\nFigura 9.5: No sabes nada, Jon Snow.\n\n\n\nRecordemos la definición de Savage y tomemos una decisión. Para ello cambiemos nuestra pregunta a lo que debería de ser el mantra detrás de todas las pruebas de hipótesis: ¿Mi evidencia deja en ridículo a mi hipótesis de nulidad? La respuesta debería de ser no, pues no encontramos nada que la contradijera y, por lo tanto, presionaremos nuestro botón de no hay vida en el planeta.\n\n\n\n\n\n\nFigura 9.6: ¿Nuestra evidencia hace quedar en ridículo a nuestra \\(H_0\\)?\n\n\n\nSalimos del planeta y llegamos a otro. Repetimos el proceso, solo que aquí sí nos encontramos una forma de vida alienígena en forma de exactamente un individuo. Recordemos nuestro marco de decisión. Nuestra acción por defecto es presionar el botón X si no hay vida (hipótesis nula) y acabamos de encontrar un individuo (solo uno). ¿El tamaño de la población? No lo sabemos. ¿Nuestra evidencia deja en ridículo a nuestra hipótesis nula? Por supuesto que sí, entonces la rechazamos y tomamos nuestra acción alternativa: presionar el botón con la palomita verde. ¿Qué aprendimos que sea de interés? Que hay vida en el planeta.\n\n\n\n\n\n\nFigura 9.7: ¡Aprendimos algo!\n\n\n\nEste ejemplo es lo que hacemos o deberíamos de hacer al aplicar una prueba de hipótesis de nulidad. Establecer nuestra acción por defecto y la alternativa, definir nuestra hipótesis de nulidad, ir a tomar datos y luego preguntarnos si esa evidencia deja en ridículo a nuestra hipótesis nula. ¿Cómo definimos la acción por defecto? Eso es tarea del tomador de decisiones, y en la academia muy pocas veces las tenemos definidas. ¿Cómo concluimos? Si nuestra evidencia ridiculiza a nuestra hipótesis nula, tendremos una conclusión a favor de la hipótesis alternativa. ¿Si no? No aprendimos nada y tomamos la decisión por defecto. ¿Es la correcta? No lo sabemos, pero tampoco nos interesa… o al menos hasta cierto punto. Es aquí donde entran los tipos de errores y los valores de p, solo recuerda: buscar tus llaves antes de salir de casa y no encontrarlas después de 5 minutos NO indica que no estén, solo no sabes dónde están.\n\n\n\n\n\n\nFigura 9.8: Alimento experimental vs. control",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Pruebas de significancia estadística</span>"
    ]
  },
  {
    "objectID": "c09_ph0.html#tipos-de-errores",
    "href": "c09_ph0.html#tipos-de-errores",
    "title": "9  Pruebas de significancia estadística",
    "section": "9.3 Tipos de errores",
    "text": "9.3 Tipos de errores\nAhora bien, el no poder responder inequívocamente a si nuestra decisión fue la correcta o no es lo que nos lleva a hablar de los tipos de errores estadísticos. En el Capítulo 7 hablamos de algunos errores de muestreo (sistemático y aleatorio), pero estos son diferentes a los errores que podemos cometer al momento de realizar nuestras pruebas de significancia. Estos errores los vamos a expresar en términos de probabilidades. ¿Probabilidades de qué? Por muy “obvio” que parezca, probabilidades de equivocarnos al tomar una decisión sobre nuestra hipótesis de nulidad. Te voy a presentar estos errores de dos maneras: una formal, otra que pudiera resultarte más intuitiva, y una analogía. Formalmente:\n\nError de tipo I (\\(\\alpha\\)): Probabilidad de rechazar la hipótesis nula cuando es verdadera.\nError de tipo II (\\(\\beta\\)): Probabilidad de NO rechazar la hipótesis nula cuando es falsa.\n\nSi recuerdas nuestro viaje espacial y el proceso mediante el cual decidíamos si había vida en un planeta o no, todo se reduce a si nos quedamos con nuestra decisión por defecto (no rechazar la hipótesis de nulidad) o si nuestra evidencia la dejó en ridículo (rechazar la hipótesis de nulidad). Si te es posible, procura recordar estas definiciones. ¿Por qué digo que si te es posible? Porque puede ser que no te acomode pensar en términos de “rechazo” y que prefieras pensar en términos de aceptación. Bajo ese esquema tendríamos:\n\nError de tipo I (\\(\\alpha\\)): Probabilidad de aceptar la hipótesis aternativa siendo falsa.\nError de tipo II (\\(\\beta\\)): Probabilidad de aceptar la hipótesis nula siendo falsa.\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nLa definición de los tipos de errores estadísticos está formalmente en términos de rechazar o no la hipótesis de nulidad. Este re-planteamiento en forma de aceptación es solo una herramienta didáctica. ¿Por qué? Porque nunca “aceptamos” o damos algo por cierto si es que estamos en un escenario de incertidumbre. Este re-planteamiento tiene solo un fin didáctico.\n\n\nAhora bien, hay una tercera forma de entender estos errores, la cual simplifica mucho las cosas. Imagina un escenario en el que van una mujer evidentemente embarazada y, por alguna extraña razón, un hombre a hacerse una prueba de embarazo. El doctor puede, entonces, dar dos veredictos por caso: la persona está embarazada o no está embarazada. Gráficamente esto podemos ponerlo de la siguiente manera:\n\n\n\n\n\n\nFigura 9.9: Errores y falsedades.\n\n\n\nQue un hombre esté embarazado es imposible, por lo que decirle a nuestro curioso paciente que lo está es un falso positivo o un error de tipo I. Por otra parte, decirle a la mujer que está embarazada que no lo está es un falso negativo, o un error de tipo II.\nAunque estos son los dos tipos de errores estadísticos formales, tenemos un tercero que, aún no siendo normal, es necesario que cuidemos:\n\nError Tipo III: Rechazar correctamente la hipótesis nula equivocada.\n\n¿A qué me refiero con esto? A utilizar la matemática correcta para responder una pregunta equivocada; es decir, utilizar pruebas que no responden o atacan directamente nuestro problema. Un ejemplo de esto puede ser utilizar un modelo lineal simple para describir una relación exponencial, pero hablaremos de estos problemas más adelante.\nAhora bien, hablamos de probabilidades, mientras que en el ejemplo de la Figura 9.9 vimos errores particulares. Pues es aquí donde entra el famosísimo (¿infame?) valor de p.",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Pruebas de significancia estadística</span>"
    ]
  },
  {
    "objectID": "c09_ph0.html#nivel-de-significancia",
    "href": "c09_ph0.html#nivel-de-significancia",
    "title": "9  Pruebas de significancia estadística",
    "section": "9.4 Nivel de significancia",
    "text": "9.4 Nivel de significancia\nSi te pregunto ¿qué es el valor de p y qué representa? Es posible que busques la definición funcional y que me digas que es un valor que si es menor a 0.05 indica que la prueba es “significativa” y que si es mayor no lo es. Eso es solo la regla de decisión, pero no define al valor de p. \n¿Qué es entonces? Retrocedamos un poco a nuestro ejemplo de la exploración espacial. Partíamos del supuesto (decisión por defecto) de que no hay vida en ningún planeta, y que nuestra evidencia debe de ridiculizar a nuestra hipótesis de nulidad para que presionemos el botón de que sí hay vida en el planeta. En nuestras pruebas de significancia es exactamente lo mismo. Partimos del supuesto de que no hay un efecto (o diferencia) significativa. Esto lo representamos con un modelo teórico de distribución de probabilidades, el cuál representa un universo donde la hipótesis nula siempre es verdadera. Es decir, esta distribución teórica de probabilidades son todos los casos en los cuales NO existe ningún efecto o diferencia entre nada. Un mundo gris y aburrido, vamos.\nEn este escenario entonces nosotros buscamos evidencia que nos haga cambiar de opinión, y lo evaluamos en términos probabilísticos lo evaluamos como la probabilidad de que el modelo teórico haya generado los datos que estamos viendo. ¿A que esto tiene más sentido que solo la regla de decisión?\nAhora puede que te preguntes: ¿de dónde sale esta probabilidad? Pues está definida por un estadístico de prueba. Tomando nuestros datos vamos a calcular el valor que les corresponde según la función del modelo teórico. Luego, vamos a obtener la probabildiad de encontrar, de forma aleatoria, ese valor, o uno con la misma probabilidad, o una menor. Ahora bien, si recuerdas algo de la sesión de probabilidad puede que esta última definición no te cuadre, y es que si tenemos una distribución continua no podemos buscar la probabilidad individual de un valor, por lo que la reformulamos como la probabilidad de encontrar otro valor al menos igual de grande, y esto es lo que conocemos como el nivel de significancia.\n\n\n\n\n\n\nImportante\n\n\n\nValor de p, p-value y nivel de significancia son exactamente lo mismo, solo que algunas personas utilizan “nivel de significancia” para referirse al valor de \\(\\alpha\\); e.g., “las pruebas se consideraron significativas a un nivel de significancia \\(\\alpha = 0.05\\). Esto es erróneo, pues \\(\\alpha\\) representa la probailidad de cometer un error de tipo 1 con la que estamos dispuestos a vivir, pero vamos a entrar a más detalles un poco más adelante.\n\n\nSé que esta última parte es sumamente abstracta, así que grafiquemos una distribución normal con un algunas áreas bajo la curva (probabilidades) de referencia:\n\nlibrary(ggplot2)\n\nn &lt;- 100000\nv1 &lt;- data.frame(var = rnorm(n))\nsds &lt;- data.frame(xf = c(3, 1.96, 1))\n\nsds[\"AUC\"] &lt;- NA\n\nfor (i in seq_along(sds$xf)) {\n  sds$AUC[i] &lt;-\n    as.character(round(1 -length(v1$var[(v1$var &lt; -sds$xf[i]) |\n                                        (v1$var &gt; sds$xf[i])]) /n,\n                       2))\n}\n\nuni_norm &lt;- ggplot() + \n            geom_rect(data = sds, aes(xmin = -xf, xmax = xf, \n                                      ymin = 0, ymax = Inf, \n                                      fill = AUC),  alpha = 0.3) +\n            geom_density(data = v1, aes(var),\n                         kernel = \"gaussian\", \n                         colour = \"deepskyblue4\",\n                         fill = \"deepskyblue4\", \n                         alpha = 0.6) +\n            labs(x = \"Z\",\n                 y = element_blank(),\n                 title =\n                   \"Gráfico de densidad de una distribución normal\",\n                 subtitle = expression(paste(\"n = 100000; \",\n                                             mu, \" = 0; \",\n                                             sigma, \" = 1\")),\n                 caption = \"Datos simulados\") +\n            theme(panel.grid.minor = element_blank(),\n                  panel.grid.major = element_blank(),\n                  panel.background = element_blank(),\n                  axis.line = element_blank(),\n                  aspect.ratio = 1/1.61,\n                  axis.ticks = element_blank(),\n                  text = element_text(colour = \"gray50\")\n                  ) +\n            scale_y_continuous(breaks = NULL) +\n            scale_x_continuous(breaks = c(-sds$xf, sds$xf))\n            \nuni_norm\n\n\n\n\n\n\n\nFigura 9.10: Áreas bajo la curva (AUC) para tres valores de Z de referencia. Estas representan las zonas de aceptación, con su amplitud (confianza) dada por el valor de AUC.\n\n\n\n\n\n¿Qué nos dice este gráfico? Imagina que hacemos el procedimiento estadístico correspondiente y obtenemos un valor de \\(Z = 1.0\\). El área comprendida entre \\(Z = 1\\) y \\(Z = -1\\) es del 0.68. ¿Esto quiere decir que nuestro valor de p es de 0.68? NO, todo lo contrario. Recuerda, buscamos un valor al menos igual de grande; por lo tanto, nos interesa lo que está fuera de la zona sombreada (buscamos valores \\(|Z| \\geq 1\\)) y entonces nuestro valor de \\(p = 1-0.68 = 0.32\\). ¿Es suficiente una probabilidad del 32% para decir que nuestra evidencia dejó en ridículo a nuestra hipótesis de nulidad? Creo que estarás de acuerdo conmigo en que no.\nPero volvamos a nuestro gráfico. Las zonas sombreadas representan las zonas de no rechazo a distintos niveles de “confianza”, de manera que si obtenemos un valor de Z (estadístico de prueba) \\(\\leq 1\\) estamos en el intervalo de confianza del 68%, \\(\\leq 1.96\\) del 95% y \\(\\leq 3\\) de \\(\\approx\\) 99%.\nImagina que ahora obtuvimos un valor de \\(Z = 1.97\\), recuerda lo que vimos en la sesión de probabilidad sobre el uso de la distribución normal en R. ¿Qué valor de \\(p\\) le corresponde? ¿Es un resultado “significativo? ¿Qué tal con \\(Z = 1.95\\)?\nEsto me llevaría al siguiente punto de discusión, sobre el obscurantismo alrededor del valor de p, pero tomemos un desvío para hablar sobre otro concepto muy relacionado con este último: los intervalos de confianza",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Pruebas de significancia estadística</span>"
    ]
  },
  {
    "objectID": "c09_ph0.html#intervalos-de-confianza",
    "href": "c09_ph0.html#intervalos-de-confianza",
    "title": "9  Pruebas de significancia estadística",
    "section": "9.5 Intervalos de confianza",
    "text": "9.5 Intervalos de confianza\nLos intervalos de confianza, como recordarás que mencionamos someramente en el Capítulo 8, representan una medida de incertidumbre en alguna estimación, pero vayamos más a fondo. Estos intervalos, al igual que el valor de p, abordan el mismo problema: las estimaciones puntuales no son infalibles ni siempre son verídicas, por lo que necesitamos una referencia de “certeza” alrededor de ellas. Si lo piensas detenidamente eso justamente es lo que hacemos con el valor de p y, de hecho, podemos pensar en los intervalos de confianza como el valor de p visto desde otra perspectiva.\nEntonces, ¿qué son los intervalos de confianza? Son una referencia de la “certeza” que tenemos alrededor de una estimación. Es muy posible que te hayas encontrado con la notación \\(\\overline{x} ± SD\\), en la cual resumimos nuestros datos con el promedio y la desviación estándar de los datos. Esto está bien siempre y cuando solo nos interese describir todos nuestros datos. Si nos interesa dar información sobre nuestra estimación, entonces haríamos algo tal que \\(\\overline{x}; [IC_i, IC_s]\\), donde \\(IC_i\\) representa el límite inferior del intervalo de confianza e \\(IC_s\\) el límite superior.\nEstos límites indican entre dónde y dónde (o, mejor dicho, entre qué valores) se puede encontrar la estimación. Si son equivalentes al valor de p, entonces siguen también un modelo teórico de distribución de probabilidades, y su amplitud está dada por un porcentaje de esa distribución (igual que en la zona de no rechazo de una hipótesis de nulidad).\n\n\n\n\n\n\nAdvertencia\n\n\n\nDada esta descripción, es muy posible que quieras interpretarlos como “hay X% de probabilidad de que la estimación se encuentre en este intervalo”. NO SE INTERPRETAN DE ESA MANERA. La forma adecuada es, si realizo \\(N\\) muestreos independientes, cada uno con tamaño de muestra \\(n\\), y para cada uno estimo la media y sus intervalos de confianza, ≈ 95% de estas estimaciones incluirán la media “real” (poblacional; Figura 9.11).\n\n\n\n\n\n\n\n\nFigura 9.11: Representación gráfica de la interpretación de los intervalos de confianza: 100 muestreos independientes con tamaño de muestra 20. Para cada uno se estima la media (x) y sus intervalos de confianza (líneas verticales). Solo seis (rojo) no incluyeron la media real (línea azul):\n\n\n\n\n9.5.1 Cálculo de los intervalos de confianza\nTe mencioné anteriormente que los IC siguen un modelo de distribución de probabilidades. Los modelos más comunes son las distribuciones \\(t\\) de Student y normal. Por el momento no es necesario que te preocupes por el modelo subyacente; sin embargo, lo más común es que utilicemos la distribución \\(t\\). ¿La razón? La veremos más adelante cuando expliquemos propiamente la pruena \\(t\\) de Student. Las ecuaciones correspondientes las puedes encontrar en un libro de estadística básica (e.g., Zar, 2010), así que te ahorraré el que las leas aquí y mejor vayamos directamente a cómo calcularlos con R.\nPara ejemplificarlo utilicemos 15 muestras de nuestros 10,000 datos simulados bajo una distribución normal estándar (v1$var):\n\nset.seed(0)\nv2 &lt;- sample(v1$var, size = 15)\n\nLa estimación de la media ya la conocemos. En este caso fue de -0.27, que podríamos decir se encuentra alejado de la media poblacional 0 que establecimos, pero ¿qué tanto es tantito? Es ahí donde entran los IC.\n\nmean(v2)\n\n[1] 0.3629882\n\n\nPero para calcularlos, posiblemente sin que sea sorpresa, tenemos distintas maneras de hacerlo. Comenzando con R base podemos utilizar la funciónt.test(x, conf.level), donde x es el vector de observaciones y conf.level el nivel de confianza deseado:\n\nt.test(v2, conf.level = 0.95)\n\n\n    One Sample t-test\n\ndata:  v2\nt = 1.3242, df = 14, p-value = 0.2067\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.2249449  0.9509213\nsample estimates:\nmean of x \n0.3629882 \n\n\nEsta función aplica una prueba \\(t\\) de Student para una sola muestra, por lo que da un valor del estadístico de prueba \\(t\\), los grados de libertad df y un valor de p. Por el momento olvidemos eso y quedémonos con el intervalo de confianza al 95%: \\([-0.73, 0.18]\\). Este intervalo es grande en relación a nuestra media (\\(|IC_s - IC_i| = |0.18-0.73| = 0.55 &gt; |\\overline{x}| = 0.27\\)), lo cual nos dice que no “confiamos” mucho en ese valor promedio, lo cual tiene sentido porque la distribución poblacional es una normal estandar (\\(\\mu = 0; \\sigma = 1\\)). Ahora bien, no podemos descartar que esa media sea diferente de 0, pues el IC incluye al 0 (ojo también al valor de p).\n\n\n\n\n\n\nTip\n\n\n\nTanto los IC como el valor de p pueden utilizarse para “probar hipótesis” (sensu pruebas de significancia). Si tus intervalos de confianza con amplitud \\(1-\\alpha\\) para un efecto dado incluyen al 0, entonces vas a tener un valor de \\(p &gt; \\alpha\\).\n\n\nOtra forma de calcularlos es con la función Rmisc::CI(x, ci), donde x es el vector de observaciones y ci el nivel de confianza:\n\nRmisc::CI(x = v2, ci = 0.95)\n\n     upper       mean      lower \n 0.9509213  0.3629882 -0.2249449 \n\n\nUna alternativa más es utilizar la función rcompanion::groupwiseMean(formula, data, conf, R), la cual tiene dos funcionalidades interesantes: i) permite calcular los IC para varios grupos, y ii) poder estimar los intervalos a partir de remuestreos Bootstrap. En esta función: formula es lo que vimos en la función aggregate, donde indicamos al mismo tiempo la variable numérica y la variable de agrupamiento, tal que: var~grupo; data indica el data.frame que contiene la información, conf la amplitud de los intervalos y R el número de réplicas Bootstrap a realizar. Sustituyendo:\n\nrcompanion::groupwiseMean(v2~1,\n                          data = as.data.frame(v2),\n                          conf = 0.95,\n                          R = NA)\n\n\n  \n\n\n\nAquí notarás algunas cosas interesantes:\n\nEn formula pasamos v2~1, porque no tenemos una columna de agrupamiento (solo tenemos 1 grupo).\nEn data pasamos nuestro vector como un data.frame utilizando as.data.frame(v2).\nEn R pasamos NA para indicar que NO queremos que los IC se estimen utilizando réplicas Bootstrap.\n\n\n\n\n\n\n\nNota\n\n\n\nLas réplicas Bootstrap son útiles cuando tenemos distribuciones muy sesgadas y no normales, a diferencia de nuestros datos de ejemplo. Cambia a R = 500. ¿Cambió algo?\n\n\nDespués de este pequeño rodeo en el cual vimos cómo los IC y los valores de p están íntimamente relacionados, volvamos a hablar de estos últimos y qué consideraciones debemos de tener al utilizarlos e interpretarlos.",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Pruebas de significancia estadística</span>"
    ]
  },
  {
    "objectID": "c09_ph0.html#valores-de-p-usos-y-abusos",
    "href": "c09_ph0.html#valores-de-p-usos-y-abusos",
    "title": "9  Pruebas de significancia estadística",
    "section": "9.6 Valores de p: usos y abusos",
    "text": "9.6 Valores de p: usos y abusos\nDependiendo de cuántas veces hayas llevado una materia de estadística estarás más o menos familiarizado con la frase “No se encontraron diferencias significativas (\\(p &gt; \\alpha = 0.05\\)).” Pregunta: ¿de dónde salió ese \\(\\alpha = 0.05\\)? Ojo, no te estoy preguntando qué representa \\(\\alpha\\), sino de dónde salió el criterio de tomar 0.05 como el umbral para decidir si algo es significativo o no. Según qué tanto hayas leído (o qué tanto hayan hecho su tarea tus profesores), puedes dar una de tres respuestas:\n\n“Porque así me lo enseñaron” (sensu convención).\n“Porque es lo aceptado en el área de investigación” (idem).\n“Porque así lo planteó Ronald Fisher, pues consideró que 1/20 falsos positivos eran pocos”.\n\nY las tres respuestas tienen algo de razón, pero también ambas están equivocadas. Me explico. Si bien es cierto que en el área de ecología y biología en general un \\(\\alpha\\) de 0.05 se ha considerado como “suficiente”, justo porque estamos dispuestos a aceptar 1/20 falsos positivos, en otras áreas se requiere de mayor certeza para tomar una decisión. En el área de investigación médica y farmacéutica, por ejemplo, usualmente toman valores de 0.01 e, incluso, 0.001. La razón es que el nivel de \\(\\alpha\\) debe de decidirse a priori, según el problema que tengamos entre manos y cuántos falsos positivos estemos dispuestos a aceptar.\nVeamos el caso de la siguiente figura (Armhein, Greenland & McShane, 2019):\n\n\n\n\n\n\nFigura 9.12: Valores de p, intervalos de confianza y conclusiones erróneas.\n\n\n\nAquí tenemos dos estudios que, en realidad, dieron resultados equivalentes o, cuando menos, que no están en conflicto uno con el otro. El estudio azul tuvo el mismo efecto positivo promedio que el rojo, aunque con un IC mucho más angosto y, en consecuencia, un valor de p “signficativo” (pequeño). El estudio rojo, por el contrario tiene un intervalo de confianza mucho más amplios que incluyeron el 0 y, consecuentemente, un valor de p “no significativo”. ¿Cuál sería la forma correcta de interpretar estos resultados? Primero, no descartar al estudio rojo o tacharlo de “no significativo”. Mejor, pensemos en los IC como intervalos de compatibilidad, describamos los resultados en función de las consecuencias de estos intervalos, con especial énfasis en la estimación puntual, pues es la más compatible con los datos. La razón de esto es que, por puro efecto del azar, el estudio azul podría replicar exactamente el mismo estudio y esta vez obteer un IC similar al rojo o viceversa.\n\n\n\n\n\n\nAdvertencia\n\n\n\n¡Significancia estadística NO IMPLICA significancia biológica!\n\n\n¿A qué me refiero con esta advertencia? A que no porque la prueba arroje efectos o diferencias significativas quiere decir que esas diferencias sean importantes para el fenómeno que estemos analizando y, de hecho, en la siguiente sesión vamos a ver un ejemplo donde la diferencia es muy pequeña en magnitud, pero es estadísticamente significativa. Sin más preámbulo, vayamos a aplicar una prueba de significancia.",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Pruebas de significancia estadística</span>"
    ]
  },
  {
    "objectID": "c09_ph0.html#prueba-básica-t-de-student",
    "href": "c09_ph0.html#prueba-básica-t-de-student",
    "title": "9  Pruebas de significancia estadística",
    "section": "9.7 Prueba básica: \\(t\\) de Student",
    "text": "9.7 Prueba básica: \\(t\\) de Student\nLa prueba de hipótesis más conocida es la prueba \\(t\\) de Student, y con justa razón. Esta prueba se construye a partir de una distribución \\(t\\) de Student, la cual tiene tres parámetros: i) \\(\\mu\\): media o centro de la distribución (posición del punto de mayor densidad en el eje x), ii) \\(\\sigma\\): escala de la distribución (desviación estándar) y iii) \\(\\nu\\): grados de libertad. Recordarás que los parámetros de una distribución afectan su forma. En este caso, el parámetro “más importante” son los grados de libertad, pues conforme estos se acerquen a infinito más se acercará la distribución a una normal. De hecho, con \\(n \\geq 30\\) se considera que la distribución es prácticamente normal. ¿Qué es lo que cambia? La altura o el peso de las colas:\n\n\n\n\n\n\nFigura 9.13: Distribución \\(t\\) de Student con diferentes grados de libertad \\(k\\) (\\(\\nu\\)).\n\n\n\nEste cambio en la altura de las colas es sumamente importante, y es lo que hace que la distribución \\(t\\) sea uno de los “caballitos de batalla” de la estadística, pues literalmente estamos siendo conscientes que, conforme disminuye el tamaño de muestra (grados de libertad, pero ahorita los definimos), incrementa la probabilidad de tener valores extremos (el peso de las colas). Con esto también establecemos que no queremos que esos valores extremos desvíen nuestras estimaciones.\nEn este punto estarás pensando “todo eso está muy bien, pero ¿qué son los grados de libertad y con qué se comen?” Y este concepto es uno que se entiende mejor si lo deducimos con un ejercicio extremadamente simple. Si yo te digo que obtengas el promedio de los números 1, 2 y 3 harías esto:\n\nmean(c(1, 2, 3))\n\n[1] 2\n\n\nY si te digo ahora que hagas eso mismo para los números 1 y 3:\n\nmean(c(1, 3))\n\n[1] 2\n\n\nObtenemos exactamente el mismo resultado que antes, pues retiramos la media del conjunto de datos original. Eso son los grados de libertad: el número de observaciones independientes de la media, tal que \\(\\nu = n - 1\\). Es sumamente importante que tengas presente este concepto, pues un montón de pruebas involucran grados de libertad.\n\n\n\n\n\n\nNota\n\n\n\nEn algunas pruebas verás que los grados de libertad se estiman de distintas maneras, según qué involucre la prueba y qué grados de libertad sean los que se están calculando, pero la escencia siempre es la misma: retirar la media de los datos.\n\n\nVolviendo a la prueba \\(t\\), tenemos dos variantes y media: una prueba para una muestra, una para dos muestras independientes o una para dos muestras dependientes. ¿Por qué dos variantes y media? Ya lo verás con el caso de dos muestras dependientes (pareadas). En general, esta prueba nos permite comparar dos valores (un promedio contra una referencia o dos promedios). Sus supuestos son:\n\nNormalidad (si son dos grupos cada grupo debe de estar normalmente distribuido)\nMuestras independientes; es decir que la observación \\(x_i\\) sea independiente de la observación \\(x_j\\), que es diferente de tener muestras pareadas.\nHomogeneidad de varianzas.\n\n\n\n\n\n\n\nNota\n\n\n\nEn la siguiente sesión hablaremos largo y tendido de estos supuestos, pero por el momento entiéndelos como “requisitos” o “características” que deben de tener nuestros datos para que podamos confiar en los resultados de la prueba.\n\n\nAdicionalmente se “recomienda” que se utilice con tamaños de muestra menores a 30, por aquello de que a partir de 30 la distribución se vuelve prácticamente normal, pero también es recomendada si no conocemos la varianza poblacional.\n\n9.7.1 Prueba \\(t\\) para una muestra\nEsta prueba nos permite comparar la media de una muestra con un valor de referencia. La función de la prueba \\(t\\) para una muestra está dada por:\n\\[\nt = \\frac{\\overline{x}- \\mu_0}{\\frac{s}{\\sqrt{n}}}\n\\] Donde: \\(\\overline{x}\\) es el promedio de nuestra muestra, \\(\\mu_0\\) es un valor de referencia con el que queremos comparar \\(\\overline{x}\\), \\(s\\) es la desviación estándar de la muestra y \\(n\\) es el número de observaciones. Esta ya la aplicamos antes cuando estimamos los intervalos de confianza con la función t.test, pero volvamos a visitarla:\n\nt.test(v2, mu = 0, conf.level = 0.95)\n\n\n    One Sample t-test\n\ndata:  v2\nt = 1.3242, df = 14, p-value = 0.2067\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.2249449  0.9509213\nsample estimates:\nmean of x \n0.3629882 \n\n\nAhora sí podemos revisar la salida completa. La primera línea nos dice el tipo de prueba \\(t\\) que estamos aplicando. En este caso pasamos un solo vector, por lo que la prueba a aplicar es para una sola muestra. La segunda línea nos da el nombre de los datos. La tercera nos da los resultados de la prueba: el valor del estadístico \\(t\\), los grados de libertad d.f. (degrees of freedoom) y el valor de p (p-value), mientras que la cuarta nos da la hipótesis alternativa que estamos evaluando (la nula no cambia, ¿o sí?) ¿Contra qué valor está contrastando? Por defecto contra 0 (podemos cambiarlo con el argumento mu).\n¿Cómo reportamos estos resultados? “La media no fue signficativamente diferente de 0 (\\(t = -1.29\\), \\(d.f. = 14\\), \\(p = 0.21 &gt; \\alpha = 0.05\\)).\n\n\n\n\n\n\nImportante\n\n\n\nSiempre que hagas una prueba estadística es importante reportar el valor del estadístico de prueba y el/los parámetro(s) de la distribución, además del valor de p. ¿La razón? Para allá vamos.\n\n\nAhora bien, ¿por qué da una hipótesis alternativa? Porque podemos probar una de tres:\n\nLa media es diferente de \\(\\mu_0\\), en donde se aplica una prueba de dos colas.\nLa media es menor a \\(\\mu_0\\), en donde se aplica una prueba de una sola cola (considera la cola derecha).\nLa media es mayor a \\(\\mu_0\\), en donde se aplica también una prueba de una cola (la cola izquierda)\n\nLo cual me lleva a hablar de pruebas de dos o una cola. El qué hipótesis alternativa se prueba ya lo definimos, pero ¿a qué se refiere eso de una o dos colas? A qué cola(s) integramos. En la Figura 9.10 vimos que sumabamos lo que estaba fuera de nuestra área de aceptación, tanto a la derecha como a la izquierda; es decir, las colas de la distribución. Si solo sumamos lo que está a la derecha estamos preguntándonos si nuestra media es menor a \\(\\mu_0\\).\nEsto podemos modificarlo con el argumento alternative, que puede ser \"two.sided\" (por defecto), \"greater\" o \"less\". Veamos el resultado con la hipótesis alternativa de que la media es menor a 0:\n\nt.test(v2, conf.level = 0.95, alternative = \"greater\")\n\n\n    One Sample t-test\n\ndata:  v2\nt = 1.3242, df = 14, p-value = 0.1033\nalternative hypothesis: true mean is greater than 0\n95 percent confidence interval:\n -0.1198256        Inf\nsample estimates:\nmean of x \n0.3629882 \n\n\nAquí, además de la hipótesis alternativa, hay dos cambios que es importante notar: el valor de p y el intervalo de confianza. Empecemos con el segundo, donde tenemos [-Inf, 0.099]. Esto no es ningún error. Simple y sencillamente refleja nuestra hipótesis alternativa: Si la media es menor no tiene caso que consideremos la cola izquierda, por lo que el límite inferior se va a -Inf, mientras que el límite superior se reduce a 0.099. Compara ese valor de 0.099 con el resultado de la prueba de dos colas. ¿Qué notas? Es exactamente la mitad. Estos intervalos de confianza son simétricos, por lo que si solo estamos considerando un lado es lógico que la amplitud se reduzca a la mitad y eso me lleva al otro cambio: el valor de p. También es exactamente la mitad del que nos dio la prueba para dos colas, y es por la misma razón, estamos considerando la mitad del área bajo la curva. Pero aquí hay una advertencia muy, pero muy importante:\n\n\n\n\n\n\nAdvertencia\n\n\n\nLa decisión de si se aplica una prueba de una o dos colas es algo que se realiza a priori; es decir, desde un inicio seleccionamos nuestra hipótesis alternativa. No se vale aplicar la prueba de dos colas (hay diferencias significativas), ver que nuestro valor de p &gt; 0.05, pero no es tan grande y luego aplicar una prueba de una cola para forzar un resultado significativo.\n\n\nEsta es la razón por la que hay que reportar la prueba, el valor del estadístico de prueba, los parámetros involucrados y el valor de p: podemos verificar que el resultado corresponda con la hipótesis que se dice se está probando. Me ha tocado saber de casos donde en la sección de métodos ponen la hipótesis alternativa en términos de “es diferente” (prueba de dos colas) y a la hora de presentar los resultados quieren “colar” una prueba de una cola. Pensemos que alguien nos dice en la sección de métodos: “Para evaluar si la media de la variable X fue significativamente diferente de 0 se utilizó una prueba \\(t\\) de Student (\\(\\alpha = 0.05\\))”, y en los resultados: “La media fue significativamente diferente de 0 (\\(\\mu = 0.25; SD = 0.2\\); t = -1.7, df = 14, p = 0.027). Ese valor de t es”grande”, pero no lo suficiente para dar un valor de p tan “lejano” de 0.05, por lo que nos damos a la tarea de corroborarlo:\n\npt(q = -1.7, df = 14)\n\n[1] 0.05561478\n\n\n¡Sorpresa! El valor sí que era más pequeño (la mitad), por lo que esta persona aplicó una prueba de una cola y quiso darla como una prueba de dos colas. Otra forma de hacerlo sería comprobar el valor de t que le correspondería a ese valor de p:\n\nqt(p = 0.027, df = 14)\n\n[1] -2.103324\n\n\nMoraleja: dejemos las pruebas de una sola cola para cuando realmente sea nuestro interés probar si algo es mayor o menor que otra cosa que, te adelanto, son muy pocos casos en donde tenemos suficiente información para sospechar eso desde nuestro diseño experimental.\n\n\n9.7.2 Prueba para muestras independientes\nLa siguiente variación la tenemos cuando queremos comparar las medias de dos muestras independientes o, mejor dicho, dos grupos independientes. Es decir, los individuos que forman al grupo 1 son diferentes de los que conforman al grupo 2. La ecuación original sufre una ligera modificación, en donde ahora se considera la desviación estándar mancomunada (\\(sp\\)) de las muestras; es decir, la variación en el valor dada por ambos grupos. Esto es importante, pues no es lo mismo tener una diferencia promedio de 10g con una desviación mancomunada de 100 g a tener esos mismos 10g de diferencia con una desviación mancomunada de 10g. La función queda entonces:\n\\[\nt = \\frac{\\overline{x_1} - \\overline{x_2}}{sp\\cdot\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}}\n\\]\nDonde:\n\\[\nsp = \\sqrt{\\frac{(n_1 - 1)s_{x_1}^2 + (n_2 - 1)s_{x_2}^2}{n_1 + n_2 -2}}\n\\]\nAfortunadamente para implementarla en R solo vamos a utilizar la función t.test(formula, data) que ya conocíamos, solo que añadiremos dos argumentos adicionales: var.equal = T y paired = F. var.equal hace referencia al supuesto de homogeneidad de varianzas que mencionamos antes. Si no se cumple podemos pasar var.equal = F y entonces se aplicará la prueba \\(t\\) de Welch, la cual modifica el modo en el que se estima la varianza mancomunada y permite contender con varianzas desiguales. paired, por otra parte, define si es una prueba para muestras independientes (F) o muestras dependientes (pareadas, T). Esto último lo veremos más adelante.\n\n\n\n\n\n\nNota\n\n\n\n¿Qué prueba aplicar? ¿\\(t\\) de Welch o \\(t\\) de Student? Por defecto la función t.test aplica la prueba de Welch, la cual es más poderosa que la de Student cuando no se cumple el supuesto de homogeneidad de varianzas y al menos tan poderosa como la de Student cuando se cumplen todos sus supuestos. La decisión es tuya, pero la prueba \\(t\\) de Welch cubre la mayor parte de los casos.\n\n\nCreemos primero un conjunto de datos y luego apliquemos la prueba:\n\n# Datos del grupo A\nA &lt;- rnorm(10, 10, 0.1)\n# Datos del grupo B\nB &lt;- rnorm(10, 11, 0.1)\n\n# Formar un solo data.frame\ndf1 &lt;- data.frame(grupo = \"A\", v1 = A)\ndf1 &lt;- rbind(df1, data.frame(grupo = \"B\", v1 = B))\n\n# Aplicar la prueba\nt.test(v1~grupo, data = df1, var.equal = T, paired = F)\n\n\n    Two Sample t-test\n\ndata:  v1 by grupo\nt = -28.643, df = 18, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group A and group B is not equal to 0\n95 percent confidence interval:\n -1.0535072 -0.9095223\nsample estimates:\nmean in group A mean in group B \n       10.00929        10.99081 \n\n\nLa salida es muy parecida al caso anterior: el tipo de prueba, el valor del estadístico de prueba, los grados de libertad, el valor de p, la hipótesis alternativa, un intervalo de confianza y los promedios de cada grupo. ¿Qué representa ese intervalo de confianza? Es el intervalo de confianza para la diferencia de medias, que es el cómo estamos comparando los grupos.\nPodemos también presentar los resultados de manera gráfica. Para ello necesitaremos guardar los resultados de nuestra prueba t.test en un objeto y extraer la información de ahí. ¿Cómo verificamos cuál es el tipo de objeto?\n\nttest &lt;- t.test(v1~grupo, data = df1, var.equal = T, paired = F)\ntypeof(ttest)\n\n[1] \"list\"\n\n\nEl objeto es una lista NOMBRADA, por lo que podemos acceder a su contenido utilizando el operador de [[]] (posición numérica o \"nombre\") o el operador $. Guardemos el valor de p en un nuevo objeto para incluirlo en la gráfica:\n\np_val &lt;- ttest$p.value\np_val\n\n[1] 1.814597e-16\n\n\nConstruyamos y grafiquemos los intervalos de confianza para la media de cada grupo (95%):\n\nICs &lt;- rcompanion::groupwiseMean(v1~grupo, data = df1, conf = 0.95)\n\nerror.plot &lt;- ggplot(data = ICs, aes(x = grupo, y = Mean)) +\n              geom_point(color = \"deepskyblue4\") + \n              geom_errorbar(aes(ymin = Trad.lower, ymax = Trad.upper),\n                            color = \"deepskyblue4\") +\n              theme(panel.grid.minor = element_blank(),\n                    panel.grid.major = element_blank(),\n                    panel.background = element_blank(),\n                    axis.line = element_blank(),\n                    aspect.ratio = 1/1.61,\n                    axis.ticks = element_blank(),\n                    text = element_text(colour = \"gray50\"),\n                    legend.position = \"none\") +\n              labs(x = \"Grupo\",\n                   y = \"x\",\n                   title = \"¿Diferencias significativas?\",\n                   subtitle = \n                     \"µ e IC para una variable aleatoria\",\n                   caption =  paste(\"t(v = \",\n                                    ttest$parameter, \", 0.05) = \",\n                                    round(ttest[[\"statistic\"]] ,2), \n                                    \"; \",\n                                    ifelse(p_val &lt; 0.0001,\n                                           \"p &lt; 0.0001\",\n                                           paste(\"p = \", p_val))))\n              \nerror.plot\n\n\n\n\n\n\n\n\n\n\n9.7.3 Prueba para muestras pareadas\nEsta prueba nos permite comparar los promedios de una variable de un mismo grupo en dos momentos diferentes en el tiempo, por ejemplo, comparar si la frecuencia cardiaca promedio de un grupo de participantes fue diferente antes y después de hacer ejercicio; es decir tendremos datos pareados cuando tengamos mediciones de los mismos individuos (identificados) en dos momentos diferentes. La prueba para muestras pareadas es la media variante que mencioné al inicio. ¿Por qué media? Porque en realidad aplica la prueba de una sola muestra, solo que con un paso previo: restamos los valores de cada individuo para quedarnos con un solo vector de diferencias y con ese aplicar la prueba para una muestra:\n\\[\nt = \\frac{\\overline{X}_D - \\mu_o}{\\frac{S_D}{\\sqrt n}}\n\\]\nDonde \\(\\overline{X_D}\\) y \\(S_D\\) son el promedio y la desviación estándar de las diferencias. Hagamos el ejercicio. Primero, carguemos los datos, que en este caso están contenidos en un archivo de excel:\n\ndependientes &lt;- openxlsx::read.xlsx(\"datos/datos_t.xlsx\", sheet = 2)\ndependientes\n\n\n  \n\n\n\nNotar que está en formato compacto (no codificado), por lo tanto hay que transformarla:\n\ndependientes_m &lt;- reshape2::melt(dependientes, value.name = \"FC\",\n                                 na.rm = T, variable.name = \"periodo\")\n\nNo id variables; using all as measure variables\n\ndependientes_m\n\n\n  \n\n\n\nAplicamos la prueba:\n\nt.test(FC~periodo, data = dependientes_m, paired = T)\n\n\n    Paired t-test\n\ndata:  FC by periodo\nt = -6.1115, df = 9, p-value = 0.0001768\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -24.11459 -11.08541\nsample estimates:\nmean difference \n          -17.6 \n\n\nComo era de esperarse, hubo diferencias en la frecuencia cardiaca. Ahora comprobemos que solo se hizo la resta de inicial vs final:\n\ndif_FC &lt;- dependientes$Después - dependientes$Antes\nt.test(dif_FC)\n\n\n    One Sample t-test\n\ndata:  dif_FC\nt = 6.1115, df = 9, p-value = 0.0001768\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 11.08541 24.11459\nsample estimates:\nmean of x \n     17.6",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Pruebas de significancia estadística</span>"
    ]
  },
  {
    "objectID": "c09_ph0.html#ejercicio",
    "href": "c09_ph0.html#ejercicio",
    "title": "9  Pruebas de significancia estadística",
    "section": "9.8 Ejercicio",
    "text": "9.8 Ejercicio\nEn esta sesión hay dos ejercicios:\n\nCarga el archivo Datos1 2.csv (ojo con los nombres de las columnas al cargarlo) y realiza la estimación de la media, la desviación estándar y los intervalos de confianza (calquiera de las formas) para al menos 3 tamaños de muestra diferentes (considera que tienes la población completa). El objetivo es que veas y describas cómo cambian tanto la estimación puntual como la amplitud de los intervalos de confianza al incrementar el tamaño de muestra. Puedes utilizar cualquiera de las variables LT o PT.\nRealiza la prueba t con los datos de la hoja 1 del archivo datos_t.xlsx, la cual contiene datos de dos muestras independientes. La tarea consiste en cargar los datos, realizar la prueba y presentar un gráfico en el que se reporten los resultados.\n\n\n\n\n\nArmhein V, Greenland S, McShane B. 2019. Scientists rise up against statistical significance. Nature 567:305-307. DOI: 10.1038/d41586-019-00857-9.\n\n\nSavage LJ. 1954. The Foundations of Statistics. John Wiley & Sons.\n\n\nZar JH. 2010. Biostatistical Analysis. Prentice Hall.",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Pruebas de significancia estadística</span>"
    ]
  },
  {
    "objectID": "c10_param.html",
    "href": "c10_param.html",
    "title": "10  Técnicas paramétricas",
    "section": "",
    "text": "10.1 Librerías\nlibrary(ggplot2)\nlibrary(corrplot)\nlibrary(gridExtra)\nlibrary(rstatix)\nlibrary(dplyr)\nlibrary(ggpubr)\nlibrary(patchwork)",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Técnicas paramétricas</span>"
    ]
  },
  {
    "objectID": "c10_param.html#introducción",
    "href": "c10_param.html#introducción",
    "title": "10  Técnicas paramétricas",
    "section": "10.2 Introducción",
    "text": "10.2 Introducción\nPor fin dejaremos atrás los fundamentos de la estadística. Tal vez 10 sesiones con fundamentos se te hayan hecho largas, pero todos los conceptos que hemos visto tienen una razón de ser. No podía hablar de pruebas de hipótesis sin que antes supiéramos qué es la probabilidad y qué representa. No podíamos empezar a aplicar procedimientos estadísticos sin antes revisar las bases de R. No podíamos hacer visualizaciones efectivas sin hablar de las heurísticas correspondientes. En fin, creo que ya me entiendes. A partir de la siguiente sesión vamos a revisar solo la teoría correspondiente a cada prueba/técnica, asumiendo que ya cuentas con las bases que establecimos en las sesiones anteriores.",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Técnicas paramétricas</span>"
    ]
  },
  {
    "objectID": "c10_param.html#por-qué-paramétricas",
    "href": "c10_param.html#por-qué-paramétricas",
    "title": "10  Técnicas paramétricas",
    "section": "10.3 ¿Por qué paramétricas?",
    "text": "10.3 ¿Por qué paramétricas?\nLo primero que tenemos que abordar es ¿por qué se les conoce como pruebas paramétricas? Porque las inferencias que vamos a realizar son sobre los parámetros poblacionales, usualmente la media (\\(\\mu\\)) y la varianza (\\(\\sigma\\)). Ejemplos de pruebas paramétricas tenemos muchos, entre los más famosos tenemos la \\(t\\) de Student que revisamos en la sesión anterior, los análisis de la varianza (ANOVAs) que revisaremos en esta sesión, la correlación de Pearson y la regresión lineal que veremos en la siguiente, entre otros.\n\n\n\n\n\n\nNota\n\n\n\nNota para breviario cultural: todas las pruebas que mencioné con anterioridad son aplicaciones del modelo lineal general. ¿Eso qué significa? Que se asume que los grupos son separables linealmente, o que la relación entre dos o más variables es lineal.\n\n\nIndependientemente de la prueba o de la técnica, todas las técnicas paramétricas tienen al menos tres supuestos; i.e, requieren de al menos tres cosas:\n\nQue la distribución muestreal del valor de interés sea normal.\nQue las varianzas sean homogéneas.\nQue las muestras sean independientes.\n\nLa tercera sale sola si planteamos bien nuestro diseño experimental; es decir, que cada dato sea “independiente” de los demás. En otras palabras, que no sean mediciones de los mismos individuos, por ejemplo, o que las respuestas de cada individuo subsecuente no dependan del que estamos midiendo ahorita. Los primeros dos requieren de un poco más de explicación, así que vayamos paso a paso y hablemos de cada uno, empezando por la mayor tortura: el supuesto de normalidad.",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Técnicas paramétricas</span>"
    ]
  },
  {
    "objectID": "c10_param.html#supuesto-de-normalidad",
    "href": "c10_param.html#supuesto-de-normalidad",
    "title": "10  Técnicas paramétricas",
    "section": "10.4 Supuesto de Normalidad",
    "text": "10.4 Supuesto de Normalidad\nExiste por ahí un artículo titulado “Normalidad estadística y biología: una relación tortuosa”. Si bien es cierto que el cómo se desarrollaron los análisis, y por lo tanto sus conclusiones, no tiene ni pies ni cabeza, ni ningún tipo de fundamento, el título me parece bastante llamativo porque desde que nos empezamos a aproximar a las pruebas de hipótesis se nos habla del supuesto de normalidad o de que “nuestros datos deben de ser normales”. Bueno, hablemos a fondo de este supuesto, de qué es, qué no es, y por qué es importante.\n\n10.4.1 Versión corta\nSi no quieres entrar en demasiados detalles puedes quedarte con lo que voy a decir aquí y seguir con tu camino. Si te interesa conocer un poco más puedes leer después la versión larga.\nEn pocas palabras, el supuesto de normalidad se lee “La distribución muestral de la media es normal”. Si nosotros realizamos muestreos con tamaños de muestra \\(n\\) y para cada uno estimamos la media, conforme \\(n \\rightarrow \\infty\\) la distribución de esa media se volverá normal. Esa es la distribución muestreal de la media. Esta es una relación no controversial pues, aunque con \\(n \\geq 30\\) prácticamente lo garantizamos tenemos un problema: debemos de generalizar para cualquier \\(n\\) y simplificar, lo que nos lleva a que “las poblaciones muestreadas sigan una distribución normal”. Esto puede no ser práctico probarlo, pero sí que podemos utilizar nuestra muestra y ver si esta proviene de una distribución normal; ergo, aplicamos una prueba de normalidad para ver si nuestros datos se ajustan o no a una distribución normal.\n\n\n10.4.2 Versión larga\nPosiblemente la versión corta haya sido demasiado corta, y fue a propósito, pues me interesa que le des al menos una hojeada a la versión larga.\nVolvamos al asunto de que estamos tratando con la distribución muestral de la media. Esto es un problemón, porque una muestra NO contiene información sobre ella. A final de cuentas, una distribución muestral es el producto de distintas muestras, entonces debemos de llenar esos “vacíos” de información utilizando el centro, la dispersión y otros estadísticos descriptivos, y asumimos una distribución normal. ¿Por qué una distribución normal? Podría ser extremadamente breve y decirte que porque muchísimos atributos se distribuyen naturalmente de manera estadísticamente normal, pero en realidad es debido a otro concepto: el Teorema del Límite Central.\n\n10.4.2.1 Teorema del Límite Central\nEste teorema establece que: “Dadas muestras aleatorias e independientes con N observaciones cada una, la distribución de sus medias se aproxima a una distribución normal conforme N incrementa, INDEPENDIENTEMENTE de la distribución poblacional”; es decir, mientras N sea grande, \\(\\bar{x} \\sim Normal\\). Para probar esto podemos hacer un ejercicio en el cual simulemos una población con distribución Gamma, cuya zona de mayor densidad se encuentra desplazada a la izquierda:\n\nset.seed(0)\ndatos &lt;- data.frame(x = 1:1000, y = rgamma(1000, 1))\ngamma &lt;- ggplot(data = datos, aes(y)) + \n         geom_density(fill = rgb(118,78,144,\n                                 maxColorValue = 255),\n                      alpha = 0.5, colour = \"white\") +\n         theme_bw() +\n         labs(title = \"Distribución Gamma\",\n              x = element_blank(),\n              y = element_blank()) +\n         theme(text = element_text(colour = \"gray40\"))\n\n#cairo_pdf(\"gamma.pdf\", family = \"Montserrat\",\n#          height = 5, width = 5*1.6, pointsize = 20)\ngamma\n\n\n\n\n\n\n\n#dev.off()\n\nCon nuestra población definida, podemos seleccionar algunos tamaños de muestra, realizar 1000 muestreos aleatorios, obtener la media de cada muestreo y graficar su distribución. Primero para N = 3:\n\nN = 3\nmedias &lt;- data.frame(x = 1:1000,\n                     y = replicate(1000,\n                                   mean(sample(datos$y, N))))\n\n\ndist_n3 &lt;- ggplot(data = medias, aes(y)) +\n           geom_density(fill = \"dodgerblue4\",\n                        alpha = 0.5, colour = \"white\") +\n           theme_bw() +\n           labs(title = sprintf(\"Distribución muestreal con N = %d\", N),\n                x = element_blank(),\n                y = element_blank()) +\n           theme(text = element_text(colour = \"gray40\"))\n\n#cairo_pdf(\"n_3.pdf\", family = \"Montserrat\",\n#          height = 5, width = 5*1.6, pointsize = 20)\ndist_n3\n\n\n\n\n\n\n\n#dev.off()\n\nAhora para N = 10. La distribución se aproxima más a una distribución normal:\n\nN = 10\nmedias &lt;- data.frame(x = 1:1000,\n                     y = replicate(1000,\n                                   mean(sample(datos$y, N))))\n\ndist_n10 &lt;- ggplot(data = medias, aes(y)) +\n            geom_density(fill = \"dodgerblue4\",\n                         alpha = 0.5, colour = \"white\") +\n            theme_bw() +\n            labs(title = sprintf(\"Distribución muestreal con N = %d\",\n                                 N),\n                 x = element_blank(),\n                 y = element_blank()) +\n            theme(text = element_text(colour = \"gray40\"))\n\n#cairo_pdf(\"n_10.pdf\", family = \"Montserrat\",\n#          height = 5, width = 5*1.6, pointsize = 20)\ndist_n10\n\n\n\n\n\n\n\n#dev.off()\n\nCon N = 30 la distribución es más cercana a una normal que a la gamma, por lo que usualmente se acepta que: con N≥30 la distribución muestreal de la media DEBERÁ ser normal:\n\nN = 30\nmedias &lt;- data.frame(x = 1:1000,\n                     y = replicate(1000, mean(sample(datos$y, N))))\n\n\ndist_n30 &lt;- ggplot(data = medias, aes(y)) +\n            geom_density(fill = \"dodgerblue4\",\n                         alpha = 0.5, colour = \"white\") +\n            theme_bw() +\n            labs(title = sprintf(\"Distribución muestreal con N = %d\",\n                                 N),\n                 x = element_blank(),\n                 y = element_blank()) +\n            theme(text = element_text(colour = \"gray40\"))\n\n#cairo_pdf(\"n_30.pdf\", family = \"Montserrat\",\n#           height = 5, width = 5*1.6, pointsize = 20)\ndist_n30\n\n\n\n\n\n\n\n#dev.off()\n\nPara comprobar, hagámos el ejercicio con una distribución uniforme; es decir, en la cual todos los valores tienen la misma probabilidad de ser obtenidos (desviaciones debido al generador de números “aleatorios”):\n\nN = 30\ndatos &lt;- data.frame(x = 1:1000, y = runif(1000))\nunif &lt;- ggplot(data = datos, aes(y)) + \n        geom_density(fill = rgb(118,78,144,\n                                maxColorValue = 255),\n                     alpha = 0.5, colour = \"white\") +\n        theme_bw() +\n        labs(title = \"Distribución \\\"uniforme\\\"\",\n             x = element_blank(),\n             y = element_blank()) +\n        theme(text = element_text(colour = \"gray40\"))\n\n#cairo_pdf(\"unif.pdf\", family = \"Montserrat\",\n#           height = 5, width = 5*1.6, pointsize = 20)\nunif\n\n\n\n\n\n\n\n#dev.off()\n\n\nmedias &lt;- data.frame(x = 1:1000,\n                     y = replicate(1000,\n                                   mean(sample(datos$y, N))))\n\ndist_n30 &lt;- ggplot(data = medias, aes(y)) +\n            geom_density(fill = \"dodgerblue4\",\n                         alpha = 0.5,\n                         colour = \"white\") +\n            theme_bw() +\n            labs(title = sprintf(\"Distribución muestreal con N = %d\",\n                                 N),\n                 x = element_blank(),\n                 y = element_blank()) +\n            theme(text = element_text(colour = \"gray40\"))\n\n#cairo_pdf(\"n_30u.pdf\", family = \"Montserrat\",\n#          height = 5, width = 5*1.6, pointsize = 20)\ndist_n30\n\n\n\n\n\n\n\n#dev.off()\n\nUn aspecto importante a considerar es la “Primera Propiedad Conocida” de la distribución normal: dadas muestras aleatorias e independientes con N observaciones cada una (tomadas de una distribución normal), la distribución de medias muestreales es normal e insesgada (i.e., centrada en la media poblacional), independientemente del tamaño de N. Por lo tanto, aún con un tamaño de muestra de 1 debería dar una distribución parecida a la normal. Comprobemos:\n\nN &lt;-  1\ndatos &lt;- data.frame(x = 1:1000, y = rnorm(1000))\nnorm &lt;- ggplot(data = datos, aes(y)) + \n        geom_density(fill = rgb(118,78,144,\n                                maxColorValue = 255),\n                     alpha = 0.5,\n                     colour = \"white\") +\n        theme_bw() +\n        labs(title = \"Distribución Normal\",\n             x = element_blank(),\n             y = element_blank()) +\n        theme(text = element_text(colour = \"gray40\"))\n\n#cairo_pdf(\"norm.pdf\", family = \"Montserrat\",\n#          height = 5, width = 5*1.6, pointsize = 20)\nnorm\n\n\n\n\n\n\n\n#dev.off()\n\n\nmedias &lt;- data.frame(x = 1:1000,\n                     y = replicate(1000,\n                                   mean(sample(datos$y, N))))\n\ndist_n1 &lt;- ggplot(data = medias, aes(y)) +\n           geom_density(fill = \"dodgerblue4\",\n                        alpha = 0.5, colour = \"white\") +\n           theme_bw() +\n           labs(title = sprintf(\"Distribución muestreal con N = %d\",\n                                N),\n                x = element_blank(),\n                y = element_blank()) +\n           theme(text = element_text(colour = \"gray40\"))\n\n#cairo_pdf(\"n_1.pdf\", family = \"Montserrat\",\n#          height = 5, width = 5*1.6, pointsize = 20)\ndist_n1\n\n\n\n\n\n\n\n#dev.off()\n\nLa implicación de esta propiedad es que entre menos “normal” (en términos de su distribución estadística) sea nuestra población de estudio, necesitaremos un mayor tamaño de muestra para que nuestra distribución muestral de la media sea normal. El problema surge cuando nos debemos de enfrentar a tamaños de muestra pequeños (n &lt; 30).\nAunque siempre podemos asumir (literalmente) que nuestra población se encuentra normalmente distribuida y “capitalizar en la robustez del modelo estadístico subyacente”, abusando del TLC, o reconocer que tamaños de muestra más pequeños nos pueden acercar lo suficiente (n &gt; 30 es para casos extremos) podemos hacerlo mejor. La tercera opción es la evaluación formal, la cual consiste en hacer una prueba de bondad de ajuste para conocer si nuestros datos se desvían o no de una distribución normal teórica. Antes de entrar a esos métodos, analicemos la prueba de bondad de ajuste más conocida: la prueba \\(\\chi^2\\) de independencia.",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Técnicas paramétricas</span>"
    ]
  },
  {
    "objectID": "c10_param.html#pruebas-de-bondad-de-ajuste",
    "href": "c10_param.html#pruebas-de-bondad-de-ajuste",
    "title": "10  Técnicas paramétricas",
    "section": "10.5 Pruebas de bondad de ajuste",
    "text": "10.5 Pruebas de bondad de ajuste\n\n10.5.1 \\(\\chi^2\\) de independencia\nEsta prueba nos permite probar si la distribución de nuestros datos (frecuencias de variables nominales) son iguales a una distribución teórica. El ejemplo más sencillo lo tenemos al evaluar si la distribución de sexos en una población es diferente de 1:1. En este caso, la distribución de nuestros datos es binomial (dos categorías, verdadero/falso, éxito/fracaso, macho/hembra, etc.). En nuestro muestreo contamos 142 machos y 190 hembras. Coloquemos esos datos en un objeto y realicemos la prueba:\n\nsexos &lt;- c(machos = 142, hembras = 190)\nsex_chi &lt;- chisq.test(sexos)\nsex_chi\n\n\n    Chi-squared test for given probabilities\n\ndata:  sexos\nX-squared = 6.9398, df = 1, p-value = 0.00843\n\n\nVeamos la distribución teórica gráficamente y veamos la ubicación del estadístico de prueba:\n\nchi_data &lt;- data.frame(x = rchisq(1000, 1))\n\nchisq_plot &lt;- ggplot(data = chi_data, aes(x)) +\n              geom_density(fill = rgb(118,78,144,\n                                      maxColorValue = 255),\n                           alpha = 0.5, colour = \"white\") +\n              geom_vline(xintercept = sex_chi$statistic,\n                         color = \"firebrick\") +\n              annotate(geom = \"text\",\n                       x = sex_chi$statistic+1.1, y = 1,\n                       label = sprintf(\"X^2 = %.2f\",\n                                       round(sex_chi$statistic, 2))) +\n              theme_bw() +\n              labs(title = sprintf(\"Distribución X^2 teórica (g.l = %d)\",\n                                   sex_chi$parameter),\n                   x = element_blank(),\n                   y = element_blank()) +\n             theme(text = element_text(colour = \"gray40\"))\n#cairo_pdf(\"chi_plot.pdf\", family = \"Montserrat\",\n#           height = 5, width = 5*1.6, pointsize = 20)\nchisq_plot\n\n\n\n\n\n\n\n#dev.off()\n\nPartiendo del valor de p podemos concluir que la proporción fue diferente de nuestro modelo teórico 1:1, pero ¿qué pasa si nos interesara comprobar si es diferente a otra proporción, por ejemplo 40% machos y 60% hembras? En ese caso únicamente debemos de proporcionar un vector p en el cual establezcamos la probabilidad correspondiente a cada grupo:\n\nchisq.test(sexos, p = c(0.4, 0.6))\n\n\n    Chi-squared test for given probabilities\n\ndata:  sexos\nX-squared = 1.0622, df = 1, p-value = 0.3027\n\n\nAquí nuestros datos no ridiculizan a nuestra hipótesis de nulidad, por lo que no podemos rechazarla. Un ejemplo más complejo es el de la presentación, en donde tratamos de probar si el proceso de vacunación hizo alguna diferencia en el estado de salud de los empleados o, en otras palabras, ¿la incidencia de pneumonía fue la misma, INDEPENDIENTEMENTE de si los empleados se vacunaron o no? Al igual que en el caso anterior, coloquemos los datos en un objeto:\n\nvacunas &lt;- data.frame(no_vacuna = c(23, 8, 61),\n                      vacuna = c(5, 10, 77),\n                      row.names = c(\"neumococo\", \"otra_neumonia\",\n                                    \"sin_neumonia\"))\nvacunas\n\n\n  \n\n\n\nAhora apliquemos la prueba:\n\nvacs &lt;- chisq.test(vacunas)\nvacs\n\n\n    Pearson's Chi-squared test\n\ndata:  vacunas\nX-squared = 13.649, df = 2, p-value = 0.001087\n\n\nComo era de esperarse al ver las frecuencias, la incidencia de pneumonía aparentemente no fue la misma entre los empleados vacunados y los que no se vacunaron. En este caso, podemos extraer aún más información, tal y como la dependencia entre las variables. Para ello accederemos al atributo residuals de la salida de chisq.test, el cual representa los residuales de Pearson para cada celda:\n\nvacs$residuals\n\n               no_vacuna     vacuna\nneumococo      2.4053512 -2.4053512\notra_neumonia -0.3333333  0.3333333\nsin_neumonia  -0.9630868  0.9630868\n\n\nValores positivos muestran una asociación positiva entre las variables correspondientes; es decir, la incidencia de neumonía por neumococo aumentó (signo positivo) en aquellos empleados que no fueron vacunados y viceversa, valores negativos muestran una asociación negativa; es decir, la incidencia disminuyó en aquellos que sí fueron vacunados. Si nuestro interés fuera saber qué tanto contribuyó cada celda al valor de \\(\\chi^2\\) podemos elevar cada residual al cuadrado y dividirlo entre el valor de \\(\\chi^2\\) observado, tal que:\n\ncontrib &lt;- 100*((vacs$residuals^2)/vacs$statistic)\ncontrib\n\n              no_vacuna    vacuna\nneumococo     42.390150 42.390150\notra_neumonia  0.814077  0.814077\nsin_neumonia   6.795773  6.795773\n\n\nEvidentemente, los residuales más grandes tuvieron la mayor contribución que, en este caso, estuvo dada por la incidencia de neumonía por neumococo en ambos grupos. Podemos ver estos resultados de manera gráfica utilizando la librería corrplot:\n\ncorrplot::corrplot(contrib, is.corr = F)\n\n\n\n\n\n\n\n\nAhora que tenemos una idea sobre cómo funcionan las pruebas de bondad de ajuste, podemos regresar a hablar sobre las pruebas de normalidad.\n\n\n10.5.2 Supuesto de Normalidad\nComo imaginarás, las pruebas de normalidad son pruebas de bondad de ajuste en donde la distribución teórica es una distribución normal, aunque el modo en el cual se evalúan las desviaciones de la normalidad (i.e., las diferencias) es diferente para cada prueba. Para aplicarlas, utilizaremos la base de datos de muestras independientes del archivo datos_t, particularmente la columna DC:\n\ndc &lt;- openxlsx::read.xlsx(\"datos/datos_t.xlsx\", sheet = 1)\ndc\n\n\n  \n\n\n\nPodemos hacer una primera valoración utilizando un gráfico de densidad con un gráfico de densidad normal teórico superpuesto:\n\nset.seed(0)\nnorm_plot &lt;- ggplot(data = dc, aes(DC)) +\n             geom_density(fill = rgb(118,78,144,\n                                     maxColorValue = 255),\n                          colour = \"white\", alpha = 0.5) +\n             stat_function(fun = dnorm, n = 100,\n                           args = list(mean = mean(dc$DC),\n                                       sd = sd(dc$DC))) +\n             # Límites expandidos para visualizar el\n             # kde normal \"completo\".\n             # El kde observado se encuentra extendido más allá\n             # de los límites de los datos:\n             xlim(c(40, 65)) + \n             theme_bw() +\n             labs(title = \"Gráfico de densidad de DC vs. normal teórica\",\n                  x = element_blank(),\n                  y = element_blank()) +\n             theme(text = element_text(colour = \"gray40\"))\n\n#cairo_pdf(\"norm_plot.pdf\", height = 5, width = 5*1.6, pointsize = 20)\nnorm_plot\n\n\n\n\n\n\n\n#dev.off()\n\n¿Qué opinas? Apliquemos ahora las pruebas de normalidad:\n\n10.5.2.1 Prueba de Shapiro-Wilk\nLa prueba más conocida para evaluar la normalidad de un conjunto de datos es la prueba de Shapiro-Wilk. Su estadístico de prueba (W) se calcula de una manera poco amigable, pero conceptualmente implica ordenar los valores de la muestra y evaluar las desviaciones (diferencias) con respecto a la media, la varianza y su covarianza (este concepto se retoma más adelante) esperadas. En pocas palabras, la covarianza indica cuánto cambia una variable (la media) con respecto a otra (la varianza).\n¿Qué tiene que ver la covarianza con el Supuesto de Normalidad? Tiene que ver con la Segunda Propiedad Conocida de la Distribución Normal, la cual establece que Dadas observaciones aleatorias e independientes (de una distribución normal), la media muestral y la varianza muestral son independientes. En otras palabras, cuando tomas una muestra y la usas para estimar tanto la media como la varianza de la población, qué tanto puedes equivocarte sobre la media es independiente de qué tanto puedes equivocarte sobre la varianza. Esta es una característica única de la distribución normal y es una de las razones por la que la prueba de S-W es de las más (por no decir la más) utilizada y recomendada, especialmente para muestras pequeñas. En algunos estudios de simulación como este ha demostrado ser más sensible a las desviaciones de la normalidad que la prueba de Kolmogorov-Smirnov, aunque antes de explicarla apliquemos la prueba de S-W:\n\nshapiro.test(dc$DC)\n\n\n    Shapiro-Wilk normality test\n\ndata:  dc$DC\nW = 0.95125, p-value = 0.6833\n\n\nEl valor de p no nos permite rechazar nuestra hipótesis de nulidad a un \\(\\alpha = 0.05\\), por lo que podemos concluir que los datos se ajustan a una distribución normal. Vuelve al gráfico de densidad normal, ¿qué opinas?\nComo añadido, visualicemos la segunda propiedad conocida de la distribución normal:\n\nmeans &lt;- NA\nsds &lt;- NA\n\nfor (i in 1:1000) {\n  norm_data &lt;- rnorm(10)\n  means[i] &lt;- mean(norm_data)\n  sds[i] &lt;- sd(norm_data)\n}\n\nmean_sd &lt;- data.frame(mean = means, sd = sds)\n\nprop_2 &lt;- ggplot(data = mean_sd, aes(x = mean, y = sd)) +\n          geom_point(color = \"dodgerblue4\", size = 2, alpha = 0.5) +\n          theme_bw() +\n          labs(title = \n                 \"Segunda Propiedad Conocida de la Distribución Normal\",\n               subtitle = \"1000 muestreos de una población normal\",\n               x = \"Media\",\n               y = \"Desviación Estándar\") +\n          theme(text = element_text(colour = \"gray40\"))\n#cairo_pdf(\"prop_2.pdf\", family = \"Montserrat\",\n#          height = 5, width = 5*1.6, pointsize = 20)\nprop_2\n\n\n\n\n\n\n\n#dev.off()\n\nCon una distribución Gamma:\n\nmeans &lt;- NA\nsds &lt;- NA\n\nfor (i in 1:1000) {\n  gamma_data &lt;- rgamma(10, shape = 1)\n  means[i] &lt;- mean(gamma_data)\n  sds[i] &lt;- sd(gamma_data)\n}\n\nmean_sd &lt;- data.frame(mean = means, sd = sds)\n\nprop_g &lt;- ggplot(data = mean_sd, aes(x = mean, y = sd)) +\n          geom_point(color = \"dodgerblue4\", size = 2, alpha = 0.5) +\n          theme_bw() +\n          labs(title =\n                 \"Segunda Propiedad Conocida de la Distribución Normal\",\n               subtitle = \"1000 muestreos de una población gamma\",\n               x = \"Media\",\n               y = \"Desviación Estándar\") +\n          theme(text = element_text(colour = \"gray40\"))\n\n#cairo_pdf(\"prop_g.pdf\", family = \"Montserrat\",\n#           height = 5, width = 5*1.6, pointsize = 20)\nprop_g\n\n\n\n\n\n\n\n#dev.off()\n\nEjericio: Realiza el mismo gráfico para la columna DC y para la columna CH.\n\n\n10.5.2.2 Prueba Kolmogorov-Smirnov\nA diferencia de la prueba S-W, la prueba K-S compara las función de densidad acumulada empírica (observada) vs. una función de densidad acumulada teórica (no necesariamente normal), lo cual causa que sea sensible a desviaciones en el centro de la distribución pero no en las colas; sin embargo, es importante mencionar, que la prueba K-S es convergente; es decir, que conforme \\(N \\rightarrow \\infty\\) la prueba converge a la “respuesta verdadera” en términos de probabilidad. Esta razón hace que esta prueba no se recomiende para tamaños de muestra pequeños. Para aplicarla:\n\nks.test(dc$DC, \"pnorm\")\n\n\n    Exact one-sample Kolmogorov-Smirnov test\n\ndata:  dc$DC\nD = 1, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided\n\n\nA diferencia del caso anterior, esta prueba si tuvo evidencia suficiente para ridiculizar nuestra hipótesis nula, por lo que podemos concluir que nuestros datos no se ajustan a una distribución normal. Vuelve nuevamente al gráfico KDE. ¿Qué opinas?\nVeamos las densidades acumuladas:\n\n# Generamos una cdf normal teórica:\ncdf &lt;- data.frame(norm = rnorm(1000,\n                               mean = mean(dc$DC),\n                               sd = sd(dc$DC)))\n\n# Graficamos una vs. la otra:\ncdfplot &lt;- ggplot(data = dc, aes(DC)) +\n           stat_ecdf(geom = \"step\",\n                     colour = rgb(118, 78, 144,\n                                  maxColorValue = 255),\n                     alpha = 1) +\n           stat_ecdf(data = cdf, aes(norm),\n                     geom = \"line\", colour = \"black\") +\n           theme_bw() +\n           labs(title = \"Densidades acum. empírica y teórica para DC\",\n                x = element_blank(),\n                y = element_blank())\n\n#cairo_pdf(\"cdf.pdf\", family = \"Montserrat\",\n#          height = 5, width = 5*1.6, pointsize = 20)\ncdfplot\n\n\n\n\n\n\n\n#dev.off()\n\nConjuntando con el gráfico kde original podemos ver por qué la prueba K-S arrojó un resultado significativo, ya que hubo desviaciones importantes en la zona central. Interpretar correctamente un gráfico CDF NO es sencillo y requiere de experiencia, por lo que únicamente lo incluí para acompañar a la prueba que se basa en la densidad acumulada.\nHabiendo explicado dos de las pruebas de normalidad más comunes, pasemos a los análisis paramétricos. El primero de ellos lo revisamos durante la clase de pruebas de hipótesis: la prueba t de Student, por lo que pasaremos directamente al Análisis de la Varianza.",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Técnicas paramétricas</span>"
    ]
  },
  {
    "objectID": "c10_param.html#análisis-de-la-varianza",
    "href": "c10_param.html#análisis-de-la-varianza",
    "title": "10  Técnicas paramétricas",
    "section": "10.6 Análisis de la Varianza",
    "text": "10.6 Análisis de la Varianza\nEn términos simples, podemos pensar en el ANOVA como una extensión de la prueba t-Student a más de dos grupos a comparar. Durante la clase de Comparaciones Multivariadas abordamos el riesgo que conlleva realizar múltiples pruebas de hipótesis (comparaciones) en nuestros datos; es decir, el problema de realizar dos o más comparaciones entre grupos como si se tratara de pruebas independientes. Por el momento, solo ten en mente que se incrementa la posibilidad de obtener un falso positivo únicamente por azar, por lo que hay que utilizar una técnica adecuada y es ahí donde entra el ANOVA o, mejor dicho, los ANOVAs. Como te imaginarás, estas pruebas nos permiten comparar medias entre más de dos grupos, aunque aquí la comparación se realiza de manera global y la hipótesis alternativa se expresa como “Al menos una de las medias es diferente”. Esto quiere decir que el ANOVA no nos dirá entre qué par(es) de grupos se encontraron las diferencias, sino que habrá que acompañarlo de una prueba post-hoc. Esta prueba es la prueba de diferencias honestas (HSD) de Tukey, la cual se encuentra basada en la distribución de los rangos estudentizados y fue diseñada para no incrementar la probabilidad de falsos positivos al realizar múltiples comparaciones. En esta sesión revisaremos tres modaliades de ANOVA: de una vía, de dos vías y factorial, de menor a mayor complejidad, aunque estos no son los únicos. Entre los demás diseños de ANOVA se encuentran el ANOVA de medidas repetidas (estudios de crecimiento en laboratorio con medidas intermedias entre el inicio y el final, por ejemplo) o el ANOVA anidado, en el cual el diseño es similar a una muñeca rusa.\nAntes de aplicar y explicar los modelos de ANOVA, es necesario desarrollar una intuición sobre el procedimiento. El nombre “Análisis de Varianza” viene de que, literalmente, se utilizan las varianzas para comparar las medias. Aunque el proceso matemático implica calcular promedios de promedios, varias sumas de cuadrados y cuadrados medios del error, podemos resumirlo para fines prácticos en que la comparación se realiza mediante una razón/cociente, tal que:\n\\[F = \\frac{\\sigma^2_{entre}}{\\sigma^2_{dentro}}\\]\nSé que esto puede sonar muy poco intuitivo, pero si nos detenemos un poco a analizar la ecuación podemos darle mucho sentido. La varianza dentro de los grupos podemos considerarla como la varianza “promedio” de cada grupo (razón por la que es importante que estas sean homogéneas entre todos nuestros grupos), mientras que la varianza entre los grupos representa la “separación” (dispersión) entre los grupos (sin considerar el error). Partiendo de esto, es evidente que si la varianza entre grupos es muy grande en relación a la varianza dentro de los grupos podemos inferir que existe un efecto del factor de agrupamiento pues “no hay” (ojo a las comillas y los supuestos) otra forma de que las distribuciones de los grupos se desplacen.\nGráficamente la varianza dentro de los grupos se representaría de la siguiente manera:\n\nanov_sim &lt;- data.frame(grupo = as.factor(c(rep(\"A\", 1000),\n                                           rep(\"B\", 1000))),\n                       y = c(rnorm(1000, mean = 10, sd = 1),\n                             rnorm(1000, mean = 20, sd = 1)))\n\ndentro_plot &lt;- ggplot(data = anov_sim,\n                      aes(y, fill = grupo, alpha = 0.5)) +\n               geom_density(trim = T, show.legend = F,\n                            colour = \"white\") +\n               theme_minimal() +\n               labs(title = \"Varianza dentro de los grupos\",\n                    x = element_blank(),\n                    y = element_blank()) +\n               scale_y_continuous(labels = NULL) +\n               xlim(c(5, 25))\ndentro_plot\n\n\n\n\n\n\n\n\nMientras que la varianza entre los grupos podemos, para fines de interpretación, visualizarla como la varianza dada por ambos grupos. En realidad esto representaría la varianza total y la varianza entre los grupos es el resultado de eliminar la varianza dada por el error, pero sigamos con el ejemplo:\n\nanov_sim$tot &lt;- rnorm(200, mean = 15, sd = sd(anov_sim$y))\nentre_plot &lt;- ggplot(data = anov_sim, aes(tot)) + \n              geom_density(fill = \"dodgerblue4\",\n                           alpha = 0.5, colour = \"white\") +\n              theme_minimal() +\n              labs(title = \"Varianza entre los grupos\",\n                   x = element_blank(),\n                   y = element_blank()) +\n              scale_y_continuous(labels = NULL) +\n              xlim(c(5, 25))\nentre_plot\n\nWarning: Removed 90 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\nVisualizándolas como si de un cociente se tratara es posible darse cuenta cómo la varianza “entre” los grupos es mucho mayor que la varianza dentro de los grupos, lo cual daría un valor de la razón de varianzas muy alto, sugiriendo un efecto del factor de agrupamiento.\n\n#cairo_pdf(\"anova_plot.pdf\", family = \"Montserrat\",\n#           height = 5, width = 5*1.6, pointsize = 20)\nentre_plot/dentro_plot\n\nWarning: Removed 90 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n#dev.off()\n\nVeamos qué pasa cuando las medias son más cercanas entre sí:\n\nanov_sim2 &lt;- data.frame(grupo = as.factor(c(rep(\"A\", 1000),\n                                            rep(\"B\", 1000))),\n                        y = c(rnorm(1000, mean = 10, sd = 1),\n                              rnorm(1000, mean = 11, sd = 1)))\nanov_sim2$tot &lt;- rnorm(2000, mean(10.5), sd(anov_sim2$y))\ndentro_plot2 &lt;- ggplot(data = anov_sim2,\n                       aes(y, fill = grupo, alpha = 0.5)) +\n               geom_density(trim = T, show.legend = F,\n                            colour = \"white\") +\n               theme_minimal() +\n               labs(title = \"Varianza dentro de los grupos\",\n                    x = element_blank(),\n                    y = element_blank()) +\n               scale_y_continuous(labels = NULL) +\n               xlim(c(5, 15))\nentre_plot2 &lt;- ggplot(data = anov_sim2, aes(tot)) + \n              geom_density(fill = \"dodgerblue4\",\n                           alpha = 0.5, colour = \"white\") +\n              theme_minimal() +\n              labs(title = \"Varianza entre los grupos\",\n                   x = element_blank(),\n                   y = element_blank()) +\n              scale_y_continuous(labels = NULL) +\n              xlim(c(5, 15))\n#cairo_pdf(\"anova_plot2.pdf\", family = \"Montserrat\",\n#           height = 5, width = 5*1.6, pointsize = 20)\nentre_plot2/dentro_plot2\n\n\n\n\n\n\n\n#dev.off()\n\n\n10.6.1 Supuesto de Homogeneidad de Varianzas\nComo podrás imaginar, el que las varianzas de los grupos no sean homogéneas generará un sesgo al momento de calcular el cociente y, en consecuencia, el nivel de significancia de la prueba. Esto es lo que da origen al Supuesto de Homogeneidad de Varianzas. Existe una gran diversidad de pruebas, cada una con sus consideraciones, fortalezas y desventajas, pero analizaremos únicamente las (posiblemente) más conocidas.\n\n10.6.1.1 Prueba de Bartlett\nLa prueba de Bartlett se considera como la prueba Uniformemente Más Poderosa; es decir, la que es menos propensa a cometer un falso negativo para cualquier valor de \\(\\alpha\\). Este poder, sin embargo, tiene sus bemoles o su bemol, mejor dicho. Esta prueba se apoya TOTALMENTE en que la variable de interés en cada factor se encuentra normalmente distribuída (¡Hola de nuevo, Supuesto de Normalidad!). De violarse este supuesto el valor de \\(\\alpha_v\\) (verdadero) para la prueba puede ser mayor o menor al definido por nosotros (\\(\\alpha_n\\), nominal). De manera particular, si la distribución de la variable analizada presenta una curtosis negativa el \\(\\alpha_v\\) será menor al nominal, mientras que con una curtosis positiva será el caso contrario. Esto lleva a que hagamos una prueba más o menos estricta de lo que habíamos planeado originalmente y que nuestros resultados no sean confiables. De cualquier manera, veamos cómo aplicarla:\n\nbartlett.test(y~grupo, data = anov_sim)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  y by grupo\nBartlett's K-squared = 0.4309, df = 1, p-value = 0.5115\n\n\nEn este caso, no podemos ridiculizar nuestra hipótesis de nulidad, por lo que podemos concluir que las varianzas entre nuestros grupos son homogéneas (y deben serlo, pues así las especificamos).\n\n\n10.6.1.2 Prueba de Levene\nEs la alternativa recomendada por muchos a la prueba de Bartlett. Aunque no es tan poderosa, sí es robusta a las violaciones al supuesto de normalidad, de modo que el \\(\\alpha\\) verdadero es muy similar al nominal para una gran cantidad de distribuciones, aunque es insensible a distribuciones simétricas con colas altas como la t de Student o doble exponencial (también conocida como distribución de Laplace). Aplicarla también es sumamente sencillo:\n\ncar::leveneTest(y~grupo, data = anov_sim)\n\n\n  \n\n\n\nComo era de esperarse, el resultado es consistente con la prueba de Bartlett para este caso.\n\n\n\n10.6.2 ANOVA de una sola vía\nHabiendo revisado los conceptos básicos detrás del ANOVA, podemos pasar a aplicar algunos modelos. El más sencillo es el ANOVA de una sola vía, el cual es el caso más sencillo; es decir, comparamos una sola variable numérica entre los niveles de un solo factor (pesos finales para tres alimentos distintos, por ejemplo). Para ejemplificarlo utilizaremos la base datos1 2.csv que se trabajó para la tarea de Intervalos de confianza, con una columna extra: id, el cual es un identificador para cada individuo. Esta columna fue añadida únicamente para ejemplificar un caso de ANOVA posterior. En este ejemplo, compararemos los pesos totales entre los tres periodos (OJO: este es un diseño para un ANOVA factorial, únicamente lo utilizaremos como ejemplo).\nEl primer paso es, evidentemente, cargar la base de datos:\n\ndf &lt;- read.table(\"datos/Datos1 2.csv\", header = F, skip = 1, sep = \",\")\ncolnames(df) &lt;- c(\"Dieta\", \"Periodo\", \"Rep\", \"LT\", \"PT\", \"id\")\ndf$Periodo &lt;- factor(df$Periodo, levels = c(\"I\", \"M\", \"F\"))\nhead(df)\n\n\n  \n\n\n\n\n10.6.2.1 Aplicación del ANOVA\nEl siguiente paso es aplicar el ANOVA. El valor de p es bastante bajo, lo cual ridiculiza nuestra hipótesis de nulidad y concluimos que al menos un par de medias son significativamente diferentes entre sí (\\(F_{2, 155} = 574.3; p &lt; 0.0001\\)).\n\nuna_via &lt;- aov(PT~Periodo, data = df)\nsummary(una_via)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nPeriodo       2 10.600   5.300   574.3 &lt;2e-16 ***\nResiduals   155  1.431   0.009                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n22 observations deleted due to missingness\n\n\nTe has de preguntar, ¿las medias de qué grupos son diferentes? Eso no lo podemos saber con un ANOVA, sino que hay que aplicar una prueba post-hoc: una prueba que evalúe las diferencias entre cada par de grupos.\n\n\n10.6.2.2 Prueba post-hoc\nEsta prueba se construye a partir de la distribución de rangos estudentizados, y fue diseñada para evitar el conflicto entre el \\(\\alpha\\) y el número de comparaciones, por lo que la interpretación del valor de p es directa (sin correcciones).\n\n\n\n\n\n\nImportante\n\n\n\n¿Por qué no aplicar una prueba \\(t\\) para cada par de grupos? Porque en ese caso inflaríamos nuestro \\(\\alpha\\). Hablaremos más de esta relación y algunas correcciones al valor de p en el Capítulo 16. Por el momento quédate con que cada prueba para comparar múltiples grupos (tipo ANOVA) tiene su prueba post-hoc que permite realizar comparaciones pareadas.\n\n\nEn este caso, el valor de p fue muy pequeño para las tres comparaciones, por lo que rechazamos nuestra hipótesis de nulidad en los tres casos. El resto de la tabla es también informativo, pues nos indica la magnitud de las diferencias y sus intervalos de confianza (tal y como en la prueba t de Student):\n\nTukeyHSD(una_via)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = PT ~ Periodo, data = df)\n\n$Periodo\n         diff       lwr       upr p adj\nM-I 0.2949615 0.2518882 0.3380349     0\nF-I 0.6376957 0.5931429 0.6822484     0\nF-M 0.3427341 0.2967181 0.3887501     0\n\n\n\n\n10.6.2.3 Comprobación de supuestos: ANOVA como un modelo lineal\nEl ANOVA es, en realidad, un modelo lineal disfrazado de otra cosa. Este “disfraz” se lo puso Ronald Fisher cuando desarrolló una prueba para estudiar genética y rendimientos de cultivos (el origen de las pruebas de hipótesis de nulidad); sin embargo, la regresión lineal surgió casi 100 años antes (en el siglo 19), gracias a que Legendré y Gauss quisieron predecir movimientos planetarios. Dejando el breviario cultural a un lado, hoy en día los resultados de un ANOVA son, justamente, un “maquillaje” que se aplica a una regresión lineal. Tan es así que podemos extraer coeficientes de nuestro objeto de ANOVA:\n\nuna_via$coefficients\n\n(Intercept)    PeriodoM    PeriodoF \n  0.5510000   0.2949615   0.6376957 \n\n\nComparemos estos con los coeficientes de un modelo lineal que tenga la misma fórmula que nuestro modelo ANOVA:\n\nlm(PT~Periodo, data = df)$coefficients\n\n(Intercept)    PeriodoM    PeriodoF \n  0.5510000   0.2949615   0.6376957 \n\n\nComo era de esperarse, son exactamente los mismos y, seguramente, en este punto te estés preguntando cómo es que ajustamos un modelo de regresión lineal si nuestros predictores son categóricos. Esa es una pregunta que dejaremos para otro momento, lo importante es que entonces el supuesto de normalidad aplica sobre los residuales, no sobre nuestros valores crudos. ¿Por qué? La explicación es bastante intuitiva: Un modelo perfecto tiene residuales exactamente iguales a 0; es decir, todas las diferencias entre los valores observados y predichos son iguales a 0. Evidentemente esto usualmente no sucede (más adelante veremos que debemos de desconfiar de un modelo con un error muy pequeño), pero esperamos que estos residuales estén mayormente acumulados alrededor de 0 o, en otras palabras, que tengamos muy pocos valores que se encuentren lejos de nuestra predicción. Apliquemos entonces una prueba de Shapiro-Wilk a nuestros residuales:\n\nshapiro.test(una_via$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  una_via$residuals\nW = 0.96043, p-value = 0.0001769\n\n\n¿Qué pasa con la homogeneidad de varianzas? En cualquier modelo (lineal o no), la homocedasticidad hace referencia a que la dispersión de los residuales sea homogénea a lo largo del intervalo del predictor; es decir, que no tengamos áreas donde el error sea más grande. Aquí nuestro predictor (el periodo) es categórico, por lo que tiene sentido evaluar si las varianzas de todos nuestros grupos son iguales. A final de cuentas, el que un grupo tenga una mayor dispersión va a causar que los residuales en ese grupo tengan una mayor dispersión. Comprobamos antes que nuestros residuales no son normales, por lo que podemos aplicar la prueba de Levene:\n\ncar::leveneTest(PT~Periodo, data = df)\n\n\n  \n\n\n\nNos llevamos una bastante desagradable sorpresa: nuestras varianzas no son homogéneas. Esto, como sabes, hace que el valor de p de nuestro ANOVA no sea confiable, pues las varianzas dentro de los grupos contribuyen de diferente manera a la varianza entre los grupos.\n\n\n\n\n\n\nImportante\n\n\n\nEsta última explicación seguramente te hizo preguntarte si la misma lógica no aplica también para la normalidad de cada grupo y la respuesta es “sí y no”. Para explicarlo primero realicemos las pruebas de normalidad para cada uno de nuestros grupos, utilizando la función shapiro_test de rstatix:\n\ndf |&gt; group_by(Periodo) |&gt; shapiro_test(PT)\n\n\n  \n\n\n\nPodemos considerar que la distribución de PT en cada grupo proviene de una distribución normal y, por lo tanto, que cumplimos con el supuesto de normalidad, ¿no? Pues no. En realidad, la distribución total de PT es una mezcla de tres distribuciones normales, pero la distribución de la variable en sí misma no necesariamente debe de ser normal. De hecho, si los grupos están bien segregados, no tiene sentido esperar que la distribución marginal (independientemente del grupo) sea normal. ¿Qué si tiene sentido? esperar que la distribución condicional (considerando los grupos) sea normal, lo que nos lleva de nuevo a los residuales. En otras palabras, la normalidad de los datos crudos, no es relevante, sino la normalidad de los residuales. Puede ser que, como en este caso, los grupos tengan distribuciones normales pero no los residuales. El caso contrario también puede darse: que cada grupo no siga una distribución normal pero sí los residuales.\n\n\nPor otra parte, al tratarse de un modelo lineal podemos aprovechar el paquete performance para evaluar el ajuste de nuestro modelo y verificar sus supuestos:\n\n# cairo_pdf(\"model1.pdf\")\nperformance::check_model(una_via)\n\n\n\n\n\n\n\n# dev.off()\n\nCuando lleguemos a modelo lineal vamos a explicar a más detalle cada una de estas gráficas, pero la interpretación está en el subtítulo de cada una. También podemos hacer las pruebas correspondientes utilizando este paquete. Desafortunadamente los resultados son solo la decisión (sin el valor del estadístico de prueba y los parámetros de la distribución):\n\nperformance::check_normality(una_via)\n\nWarning: Non-normality of residuals detected (p &lt; .001).\n\nperformance::check_homogeneity(una_via, method = \"auto\")\n\nWarning: Variances differ between groups (Fligner-Killeen Test, p = 0.000).\n\n\nDicho esto, subamos un escalón más en la escalera que es ANOVA.\n\n\n\n10.6.3 ANOVA de dos vías\nSi una vía es a un factor, dos vías es a dos factores. En este análisis compararemos el efecto de dos factores simultáneamente, pero de manera independiente; es decir, compararemos los grupos de cada factor sin considerar que tenga una interacción con el otro.\n\n\n\n\n\n\nNota\n\n\n\n¿Qué es la interacción? Eso lo veremos más adelante, pero se resume a que el efecto de un factor dependa del efecto de otro.\n\n\nNuestro segundo factor será la Dieta. Los pasos son exactamente los mismos que en el anterior:\n\n10.6.3.1 Aplicación del ANOVA\nEl ANOVA de dos vías es un caso especial del ANOVA factorial, en el cuál únicamente hay dos factores y NO se considera su interacción, por lo que el modo de declararlo es una fórmula en la cuál los factores se consideran de manera aditiva: Respuesta~Factor1+Factor2. La forma tradicional de reportar los resultados de este ANOVA sería: hubo un efecto significativo de las dietas (F(2, 153) = 11.45; p &lt; 0.0001) y de los periodos (F(2, 153) = 560.42; p &lt; 0.0001).\n\ndos_vias &lt;- aov(PT~Dieta+Periodo, data = df)\nsummary(dos_vias)\n\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nDieta         2  0.212   0.106   11.45 2.33e-05 ***\nPeriodo       2 10.399   5.199  560.42  &lt; 2e-16 ***\nResiduals   153  1.419   0.009                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n22 observations deleted due to missingness\n\n\n\n\n10.6.3.2 Comprobación de Supuestos\nNo es una sorpresa que al añadir un factor más los residuales tampoco sean normales, ni que las varianzas de nuestros grupos no sean homogéneas:\n\nperformance::check_normality(dos_vias)\n\nWarning: Non-normality of residuals detected (p &lt; .001).\n\nperformance::check_homogeneity(dos_vias, method = \"auto\")\n\nWarning: Variances differ between groups (Fligner-Killeen Test, p = 0.020).\n\n\n\n\n10.6.3.3 Prueba post-hoc.\nAfortunadamente esto es solo un ejemplo (:P) y podemos seguir con la prueba post-hoc. La implementación es la misma que en el caso anterior. Algo a notar es que la salidaes una lista y que podríamos acceder a los resultados de cualquier factor utilizando el operador $ (TukeyHSD(aov_obj)$factor). Aquí, las diferencias se encontraron entre la dieta C y las otras dos, pero no entre A y B.\n\nTukeyHSD(dos_vias)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = PT ~ Dieta + Periodo, data = df)\n\n$Dieta\n           diff         lwr         upr     p adj\nB-A -0.02402094 -0.06672932  0.01868745 0.3801380\nC-A -0.09036834 -0.13594353 -0.04479315 0.0000176\nC-B -0.06634740 -0.11227232 -0.02042249 0.0023101\n\n$Periodo\n         diff       lwr       upr p adj\nM-I 0.2912145 0.2480228 0.3344062     0\nF-I 0.6266462 0.5819709 0.6713214     0\nF-M 0.3354316 0.2892891 0.3815741     0\n\n\n¿Cómo interpretarías estos resultados? ¿podemos confiar en ellos? Veamos qué pasa con las distribuciones utilizando un gráfico de interacción:\n\nggplot(data = df, aes(x = Dieta, y = PT, fill = Periodo)) +\n  geom_violin(alpha = 0.5, show.legend = T) +\n  labs(title = \"Distribución de PT en los tres momentos de medición\",\n       x = element_blank(),\n       y = element_blank()) +\n  theme_bw()\n\nWarning: Removed 22 rows containing non-finite outside the scale range\n(`stat_ydensity()`).\n\n\n\n\n\n\n\n\n\nEs evidente que en los tres tratamientos hubo un crecimiento, el cual además parece haber sido bastante similar. Este es un ejemplo del error de tipo III que mencionaba en la clase de pruebas de hipótesis: utilizar la matemática correcta para responder la pregunta equivocada. Veamos qué pasa si realizamos un ANOVA factorial.\n\n\n\n10.6.4 ANOVA factorial\nComo te podrás imaginar a partir de lo mencionado sobre el ANOVA de dos vías, este ANOVA es la versión más generalizada en la cual podemos utilzar más de dos factores y analizar su interacción; es decir, su interdependencia. Sigamos con la base anterior, en este caso considerando también el factor réplica:\n\ndf$Rep &lt;- factor(df$Rep, levels = c(\"A\", \"B\"))\n\n\n10.6.4.1 Aplicación del ANOVA\nLa única diferencia con el caso anterior es que esta vez utilizaremos el operador * para añadir los nuevos términos, en vez de hacerlo de forma aditiva. Haciendo esto la tabla del ANOVA cambia, en donde primero aparece el efecto de cada factor analizado de manera independiete (como si hubieramos hecho un ANOVA de “tres vías”) y después los términos de interacción. La interacción entre dos factores representa un efecto combinado de los factores involucrados en la variable analizada; es decir, cuando hay interacción entre dos factores el efecto de uno “depende” del el nivel del otro. Aquí no tuvimos interacciones significativas. Tal vez la interacción entre los tres factores se vea sospechosamente cerca de nuestro umbral de significancia, pero recuerda que no hay “efectos casi significativos”.\n\nfact &lt;- aov(PT~Dieta*Periodo*Rep, data = df)\nsummary(fact)\n\n                   Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nDieta               2  0.212   0.106  12.185 1.32e-05 ***\nPeriodo             2 10.399   5.199 596.509  &lt; 2e-16 ***\nRep                 1  0.060   0.060   6.874  0.00971 ** \nDieta:Periodo       4  0.053   0.013   1.522  0.19912    \nDieta:Rep           2  0.001   0.000   0.038  0.96240    \nPeriodo:Rep         2  0.048   0.024   2.755  0.06703 .  \nDieta:Periodo:Rep   4  0.038   0.009   1.076  0.37066    \nResiduals         140  1.220   0.009                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n22 observations deleted due to missingness\n\n\nLo que sí llama la atención es el resultado “significativo” para las réplicas. La prueba de diferencias honestas de Tukey sugiere que rechacemos la hipótesis nula; es decir, que la media de la réplica A fue superior a la media de la réplica B en un promedio de 0.038 g. Tomando eso en cuenta, ¿son realmente diferencias significativas?\n\nTukeyHSD(fact)$Rep\n\n           diff         lwr          upr   p adj\nB-A -0.03871758 -0.06808625 -0.009348919 0.01014\n\n\nSi analizamos el gráfico de interacción correspondiente podemos ver que, en efecto, la réplica B presentó menores pesos totales, especialmente en el periodo final. El problema es que las distribuciones no están ni remotamente cerca de ser normales. Recordarás que en su momento dijimos que el ANOVA es relativamente robusto a violaciones del supuesto de normalidad o, mejor dicho, que era más importante el supuesto de homogeneidad de varianzas. Esto es cierto pero tampoco debemos de olvidar que el objetivo es comparar las medias, por lo que tener distribuciones amorfas como en este caso puede sesgar nuestra inferencia:\n\nggplot(data = df, aes(x = Periodo, y = PT, fill = Rep)) +\n  geom_violin(alpha = 0.5, show.legend = T) +\n  labs(title = \"Distribución de PT en los tres momentos de medición\",\n       x = element_blank(),\n       y = element_blank()) +\n  theme_bw() + facet_wrap(~Dieta)\n\n\n\n\n\n\n\n\n\n\n10.6.4.2 Comprobación de supuestos\nEl supuesto de normalidad, como era de esperarse, sigue sin sostenerse pero ¡sorpresa! Ahora no hay evidencia para rechazar que las varianzas de los grupos sean iguales entre sí. Esto se debe a que, a diferencia de los casos anteriores, ahora estamos capturando la variación a más niveles. La pregunta a realizarse aquí es, ¿podemos confiar en TODOS los resultados de este ANOVA?\n\nperformance::check_normality(fact)\n\nWarning: Non-normality of residuals detected (p = 0.003).\n\nperformance::check_homogeneity(fact, method = \"auto\")\n\nOK: There is not clear evidence for different variances across groups (Fligner-Killeen Test, p = 0.104).\n\n\nAunque me encantaría seguir exprimiento estos datos, si regresamos brevemente a la tabla del ANOVA veremos que hubo 22 observaciones faltantes, correspondientes a la mortalidad durante el experimento. Debido a la impraciticidad/imposibilidad de marcar o identificar cada guppy no es posible aplicar un anova de medidas repetidas; sin embargo, podemos ejemplificarlo con otros datos.\n\n\n\n10.6.5 ANOVA de medidas repetidas\nEl ANOVA de medidas repetidas es otro de los modelos de ANOVA, el cual podemos considerar como una extensión de la prueba t para muestras dependientes; es decir, en la cual los mismos individuos fueron medidos en más de dos ocasiones, denominado ANOVA de medidas repetidas de una vía. Si tenemos no solo los distintos tiempos de medición sino también factores adicionales entonces tendremos ANOVAs de medidas repetidas de dos vías (tiempo y un factor adicional) o de tres vías (tiempo y dos factores adicionales). Al igual que en el ANOVA “normal” comencemos desde abajo con el de una vía.\n\n10.6.5.1 ANOVA de medidas repetidas de una vía\nCarguemos los datos de ejemplo (selfesteem de la librería datarium), los cuales son una medida de autoestima medida en tres ocasiones distintas:\n\nselfesteem &lt;- datarium::selfesteem\nhead(selfesteem)\n\n\n  \n\n\n\nLa base se encuentra en formato corto, por lo que habrá que pasarla a formato largo:\n\nestima &lt;- selfesteem |&gt;\n          gather(key = \"time\",\n                 value = \"score\",\n                 t1, t2, t3) |&gt;\n          convert_as_factor(id, time)\nhead(estima)\n\n\n  \n\n\n\n\n10.6.5.1.1 Aplicación del ANOVA\nEsta vez no utilizaremos la notación de fórmula ni tan siquiera la función aov, sino que recurriremos a la función anova_test() de la librería rstatix para hacer más intuitiva la declaración, donde data es el data.frame con los datos (dah!), dv es la variable dependiente; es decir, nuestra variable a comparar, wid es un identificador único para cada individuo y within el factor dentro del cual queremos hacer las comparaciones:\n\nanova_rep1 &lt;- rstatix::anova_test(data = estima,\n                                  dv = score,\n                                  wid = id,\n                                  within = time, type = 3)\nget_anova_table(anova_rep1)\n\n\n  \n\n\n\n\n\n\n\n\n\nImportante\n\n\n\nLos grados de libertad del numerador son k - 1, mientras que los del denominador son (k - 1)(n-1), donde k es el número de medidas repetidas y n el número de individuos.\n\n\nAquí, además de los resultados esperados en nuestra tabla de ANOVA, tenemos un término adicional: ges. Este hace referencia a una medida del tamaño del efecto conocida como \\(\\eta^2\\) (eta cuadrada) generalizada. Esta va de 0 a 1, donde 0 es nulo efecto y 1 es que toda la varianza de los datos puede ser explicada por el factor. ¿Te suena a \\(R^2\\)? Son bastante similares. Un problema es que mide la varianza en la muestra, no la población. Corolario: es mejor analizar las diferencias entre los grupos, especialmente en el contexto de la pregunta que queremos resolver. El modo de reportar estos resultados sería algo como:\n\n“Las medidas de autoestima a través del tiempo fueron significativamente diferentes (\\(F_{2,18}\\) = 55.5, p &lt; 0.0001; \\(\\eta^2\\) generalizado = 0.82)”\n\nComo breviario, el término \\(\\eta^2\\) generalizado lo puedes encontrar también como \\(\\hat{\\epsilon}\\).\n\n\n10.6.5.1.2 Comprobación de supuestos\n¿Cómo comprobamos los supuestos? Comencemos con el supuesto de homogeneidad de varianzas o, en el caso de ANOVA de medidas repetidas, el supuesto de esfericidad. Este supuesto es una “extensión” del supuesto de homogeneidad de varianzas. Definimos esfericidad como la condición en la que las varianzas de las diferencias entre todas las combinaciones de los niveles de interés son iguales. La violación de este supuesto conlleva un incremento en la probabilidad de un falso positivo; es decir, vuelve a la prueba demasiado “liberal” o “crédula”. Aunque este supuesto es sumamente importante, si utilizamos rstatix::anova_test como arriba no necesitamos probarlo directamente, pues la función hace la prueba correspondiente (prueba de Mauchly para esfericidad) y, además, aplica una corrección (corrección de Greenhouse-Geisser) a los grados de libertad. Si te interesa ver los resultados de esta prueba puedes verla en los atributos de anova_rep1. En este caso no podemos rechazar la hipótesis de nulidad de la esfericidad.\n\nanova_rep1$`Mauchly's Test for Sphericity`\n\n\n  \n\n\n\n¿Qué pasa con el supuesto de normalidad? Se vuelve ligeramente problemático, pues la gente de rstatix no devuelve los residuales, entonces toca ajustar el modelo “a mano”:\n\nanova_rep1 &lt;- aov(score~time+Error(id), data = estima)\nsummary(anova_rep1)\n\n\nError: id\n          Df Sum Sq Mean Sq F value Pr(&gt;F)\nResiduals  9   4.57  0.5078               \n\nError: Within\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ntime       2 102.46   51.23   55.47 2.01e-08 ***\nResiduals 18  16.62    0.92                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNotarás que la salida es ligeramente diferente. De entrada incluye un término para la variación a nivel individual. Ese término solo “estabiliza” la varianza tomando en cuenta la autocorrelación a nivel de individuo, por lo que no es el que nos interesa. El que interesa es el error Within, que corresponde al (los) factor(es) de interés. Como era de esperarse, los resultados son idénticos. Ahora comprobemos la normalidad:\n\nperformance::check_normality(anova_rep1$Within)\n\nOK: residuals appear as normally distributed (p = 0.094).\n\n\nComo buenos datos de libro, ambos supuestos se cumplen.\n\n\n10.6.5.1.3 Prueba post-hoc\nDebido a que las medidas son repetidas no podemos aplicar la prueba de diferencias honestas de Tukey, pero sí podemos aplicar pruebas t de Student pareadas y corregir el valor de p con una corrección de Bonferroni. En la sección de multivariado se abordará esta corrección, pero entiéndela en este momento como el modo de evitar que incrementemos la probabilidad de un falso positivo y, en consecuencia, deberemos de interpretar los valores de la columna p.adj. Viendo la tabla, es posible concluir que hubo diferencias entre las medidas de autoestima en los tres periodos.\n\npwt &lt;- pairwise_t_test(data = estima,\n                       score~time,\n                       paired = T,\n                       p.adjust.method = \"bonferroni\")\npwt\n\n\n  \n\n\n\n\n\n\n10.6.5.2 ANOVA de medidas repetidas de dos vías\nAl igual que en el ANOVA “normal”, hablamos de dos vías cuando tenemos dos factores, en este caso son el tiempo y alguno adicional. Para ejemplificarlo utilizaremos la base de datos selfesteem2 de datarium.\n\nselfesteem2 &lt;- datarium::selfesteem2\nestima2 &lt;- selfesteem2 |&gt;\n           gather(key = \"time\",\n                  value = \"score\", t1, t2, t3) |&gt;\n           convert_as_factor(id, time)\nhead(estima2)\n\n\n  \n\n\n\n\n10.6.5.2.1 Aplicación del ANOVA\nLa declaración de la prueba es la misma que en el caso anterior, solamente agregaremos un factor adicional a within:\n\nanova_rep2 &lt;- anova_test(data = estima2,\n                         dv = score,\n                         wid = id,\n                         within = c(treatment, time))\nget_anova_table(anova_rep2)\n\n\n  \n\n\n\nNotarás que aquí también tenemos un efecto de la interacción. Esto quiere decir que la evaluación de la autoestima depende de la combinación de tratamiento y tiempo, pero antes de interpretar comprobemos los supuestos.\n\n\n10.6.5.2.2 Comprobación de supuestos\nPrimero, el de esfericidad:\n\nanova_rep2$`Mauchly's Test for Sphericity`\n\n\n  \n\n\n\nTenemos una violación al supuesto de esfericidad en el tiempo, por lo que los grados de libertad fueron corregidos a un factor de 0.653 (GGe):\n\nanova_rep2$`Sphericity Corrections`\n\n\n  \n\n\n\nAl igual que en el ANOVA de medidas repetidas de una vía, para comprobar la normalidad toca declarar el modelo a mano:\n\nanova_rep2_ &lt;- aov(score~treatment*time + Error(id/(time*treatment)),\n                   data = estima2)\nsummary(anova_rep2_)\n\n\nError: id\n          Df Sum Sq Mean Sq F value Pr(&gt;F)\nResiduals 11   4641   421.9               \n\nError: id:time\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ntime       2  258.7  129.35   27.37 1.08e-06 ***\nResiduals 22  104.0    4.73                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nError: id:treatment\n          Df Sum Sq Mean Sq F value Pr(&gt;F)   \ntreatment  1  316.7   316.7   15.54 0.0023 **\nResiduals 11  224.2    20.4                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nError: id:time:treatment\n               Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ntreatment:time  2 266.36  133.18   30.42 4.63e-07 ***\nResiduals      22  96.31    4.38                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSi pones atención a la salida de id:time te darás cuenta del efecto de la corrección de Greenhouse-Geiser. Los grados de libertad sin corregir eran 2, después de la corrección son 2*0.653 = 1.31. Dejando ese ejemplo de lado, comprobamos la normalidad para todos los niveles:\n\nperformance::check_normality(anova_rep2_$`id:time`)\n\nOK: residuals appear as normally distributed (p = 0.964).\n\nperformance::check_normality(anova_rep2_$`id:treatment`)\n\nOK: residuals appear as normally distributed (p = 0.055).\n\nperformance::check_normality(anova_rep2_$`id:time:treatment`)\n\nOK: residuals appear as normally distributed (p = 0.875).\n\n\n\n\n10.6.5.2.3 Pruebas post-hoc\nA diferencia del caso anterior, y debido a la significancia del término de interacción, las pruebas post-hoc se vuelven más complejas. En el sentido estricto no podemos interpretar los efectos principales (de cada variable por sí sola) per-se, pues dependen del otro factor. ¿Qué hacemos? Tenemos dos formas de descomponer el análisis:\n\nEfecto principal simple; es decir, un modelo de una vía de la primera variable para cada nivel de la segunda. En otras palabras, comparamos el efecto del tratamiento en cada tiempo del experimento. Debido a que hacerlo a mano es un poco tedioso, encadenemos el proceso:\n\n\nanova_rep2_post1 &lt;- estima2 |&gt; \n                    # Agrupa la base por cada nivel de tiempo\n                    group_by(time) |&gt;\n                    # ANOVA de medidas repetidas para cada nivel\n                    anova_test(dv = score,\n                               wid = id, \n                               within = treatment) |&gt;\n                    # Extrae los resultados\n                    get_anova_table() |&gt; \n                    # Ajusta los valores de p\n                    adjust_pvalue(method = \"bonferroni\")\n\nanova_rep2_post1\n\n\n  \n\n\n\n\nAplicar una prueba t de Student para datos dependientes en los términos significativos. Debido a que tratamiento tiene solo dos niveles, realizar este proceso es redundante; de hecho, los valores de p serán iguales a los mostrados atrás; sin embargo, hagámoslo con fines demostrativos:\n\n\npwt_2 &lt;- estima2 |&gt;\n         group_by(time) |&gt;\n         pairwise_t_test(score~treatment,\n                         paired = T,\n                         p.adjust.method = \"bonferroni\")\npwt_2\n\n\n  \n\n\n\nEsto es todo para la clase de hoy. Es una clase bastante extensa y aún con ello se quedaron fuera algunas variantes de ANOVA (jerárquico, por ejemplo); sin embargo, creo que estos cubren los casos más generales. ¡Nos vemos en la siguiente!",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Técnicas paramétricas</span>"
    ]
  },
  {
    "objectID": "c11_rls.html",
    "href": "c11_rls.html",
    "title": "11  Modelo lineal",
    "section": "",
    "text": "11.1 Librerías\nlibrary(ggplot2)\nlibrary(performance)\nlibrary(stats4)\nlibrary(Metrics)",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modelo lineal</span>"
    ]
  },
  {
    "objectID": "c11_rls.html#introducción",
    "href": "c11_rls.html#introducción",
    "title": "11  Modelo lineal",
    "section": "11.2 Introducción",
    "text": "11.2 Introducción\nEn muchísimos lugares encontramos, de forma completamente natural, patrones que se repiten una y otra vez, tal y como en la música. El mundo de la estadística y del aprendizaje automatizado se construye de la misma manera, partiendo de pequeños motivos que aparecen una y otra vez. En esta sesión vamos a hablar de el, posiblemente, más popular de todos: el modelo lineal. Hablaremos entonces del caso más básico y escalaremos paso a paso en la complejidad.",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modelo lineal</span>"
    ]
  },
  {
    "objectID": "c11_rls.html#regla-de-tres-y-el-modelo-lineal",
    "href": "c11_rls.html#regla-de-tres-y-el-modelo-lineal",
    "title": "11  Modelo lineal",
    "section": "11.3 Regla de tres y el modelo lineal",
    "text": "11.3 Regla de tres y el modelo lineal\nAntes de empezar a hablar propiamente de la regresión lineal, sus diferencias con la correlación, en qué consiste, cómo aplicarlas y demás detalles, demos un paso hacia atrás y expliquemos el fundamento con manzanitas (literalmente).\nImagina que te digo que gasté 50 pesos para comprar 10 manzanas y luego te pregunto ¿cuánto cuesta una? Para responder a la pregunta aplicarás, aunque no seas consciente, el modelo lineal, pero primero resolvamos el problema con una regla de tres\n\\[\\begin{align*}\n10 🍎 = \\$50 \\\\\n1🍎 = ?\n\\end{align*}\\]\nAquí multiplicaríamos \\(1🍎 \\times 50\\$\\) y dividimos entre \\(10🍎\\), lo cual nos lleva a decir que una manzana me costó 5\\(\\$\\):\n\\[\n\\frac{1🍎 \\times 50 \\$}{10🍎} = 5\\frac{\\$}{🍎}\n\\]\nHasta aquí nada nuevo, así que hagamos ese resultado a un lado por el momento y volvamos al problema del modelo lineal. ¿En qué consiste un modelo lineal? En utilizar la ecuación de la recta (\\(y = a + bx\\)) para establecer una relación (lineal, dah) entre dos variables. Con nuestras manzanas podemos representarlo de la siguiente manera, donde \\(y\\) es el dinero (\\(\\$\\)) gastado para alguna cantidad de manzanas (🍎):\n\\[\n\\$ = a + b*🍎\n\\]\n¿Qué representan \\(a\\) y \\(b\\) Empecemos, por simplicidad didáctica, definiendo \\(b\\):\n\\[\\begin{align*}\nSi \\\\\n\\$ = a + b*🍎 \\\\\n\\Rightarrow \\$ - a = b*🍎 \\\\\n\\therefore \\frac{\\$ - a}{🍎} = b\n\\end{align*}\\]\nEste pequeño ejercicio algebráico nos dice que \\(b\\) es el resultado de dividir nuestro precio (menos \\(a\\)) entre el número de manzanas. ¿Te suena? ¡Es el precio por una manzana! Te preguntarás: ¿entonces **qué es \\(a\\) y por qué no lo consideramos antes? Para darle sentido pensemos en qué haría que ambas aproximaciones nos lleven al mismo resultado: que \\(a\\) fuera 0, ¿no? Pues eso tiene todo el sentido del mundo, pues es el precio de 0 manzanas. Tomando esto en cuenta podemos asignarle nombre a nuestros distintos elementos:\n\n\\(y\\) es nuestra variable dependiente (lo que queremos predecir); es decir, el número de pesos gastados.\n\\(x\\) es nuestra variable independiente (con lo que vamos a predecir); es decir, el número de manzanas compradas.\n\\(a\\) es la ordenada al origen o intercepto, representada como \\(\\beta_0\\) o \\(\\alpha\\), indica el precio de 0 manzanas. Matemáticamente esto lo definimos como el punto donde la recta corta a la ordenada (eje y) o, en palabras más sencillas, el punto donde x = 0.\n\\(b\\) es la pendiente, representada como \\(\\beta_1\\) o \\(\\beta\\), e indica el precio de una manzana. Formalmente es la tasa de cambio que existe del eje \\(x\\) al eje \\(y\\); es decir, “cuántas unidades nos vamos a mover en el eje y por una unidad en el eje x”.\n\n\n\n\n\n\n\nFigura 11.1: De la regla de tres al modelo lineal\n\n\n\n¿Por qué se le denomina lineal? Porque si lo graficamos tendremos una línea recta:\n\nmanzanas &lt;- seq(1,10)\nb &lt;- 5\na &lt;- 0\nprecio &lt;- data.frame(manzanas, precio = a+b*manzanas)\n\nggplot(data = precio, aes(x = manzanas, y = precio)) +\n  geom_line(color = \"dodgerblue4\") +\n  geom_point(color = \"dodgerblue4\") +\n  scale_x_continuous(breaks = scales::pretty_breaks()) +\n  see::theme_lucid() +\n  labs(title = \"Precio por cantidad de manzanas (n)\",\n       subtitle = \"Modelo: $ = 0 + 5*n\",\n       x = element_blank(),\n       y = element_blank())\n\n\n\n\n\n\n\nFigura 11.2: Modelo lineal con el precio por cantidad de manzanas.\n\n\n\n\n\nAunque esto nos lleva a un pequeño inconveniente o una consideración. Si utilizamos un modelo lineal estamos asumiendo explícitamente (aunque a veces inconscientemente) que el cambio entre nuestras variables es constante, determinado por \\(\\beta\\). Reflexiona: ¿en la naturaleza cuántos procesos crees que sean realmente lineales? Con esto no quiero decir que debamos de olvidarnos del modelo lineal y que, entonces, esta sesión es una pérdida de tiempo, no. Quiero decir que debemos de ser consciente del supuesto bajo el cual estamos trabajando, muchas veces para simplificarnos la existencia.\n\n\n\n\n\n\nNota\n\n\n\n¿Recuerdas la definición formal de un modelo? Es una representación SIMPLIFICADA de la realidad; es decir, siempre que involucremos un modelo, sea cual sea, estamos dejando cosas en el tintero. Recuerda: “Todos los modelos están equivocados, pero algunos son útiles” (Box, 1976).",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modelo lineal</span>"
    ]
  },
  {
    "objectID": "c11_rls.html#regresión-lineal-simple",
    "href": "c11_rls.html#regresión-lineal-simple",
    "title": "11  Modelo lineal",
    "section": "11.4 Regresión lineal simple",
    "text": "11.4 Regresión lineal simple\nAntes mencioné que la regla de tres es una aplicación del modelo lineal, lo que no dije es que es, en realidad, un ejercicio de regresión. ¿Qué es una regresión? Es una parte del aprendizaje automatizado supervisado, del cual hablaremos con más lujo de detalle en la sección correspondiente, pero podemos definirla como el proceso de estimar los parámetros de un modelo matemático a partir de ciertos datos. En una regresión la variable dependiente “siempre” es numérica. ¿Qué pasa si tenemos una variable dependiente categórica? Entonces estamos en un escenario de clasificación, pero no nos adelantemos.\n\n\n\n\n\n\nNota\n\n\n\n¿Qué es el aprendizaje automatizado? Por el momento entiendelo como la filosofía de “enseñar con ejemplos” llevada a modelos matemáticos y predicciones. En cualquier modelo de aprendizaje automatizado supervisado tenemos una serie de ejemplos (datos), en los cuales una o más variables sirven como predictoras de otra(s); es decir, el objetivo es generar buenas predicciones.\n\n\n\n\n\n\n\n\nNota\n\n\n\n¿Qué es un parámetro en este contexto? Una manera fácil de entender los parámetros de un modelo es verlos como perillas que regulan cómo se transforma lo que está a la derecha del símbolo de igualdad para llegar a lo que está a la izquierda. Si te das cuenta es básicamente lo que sucede con las distribuciones de probabilidad, y con justa razón: las distribuciones de probabilidad son, en sí mismas, modelos. No confundas esta definición con la definición de parámetro poblacional.\n\n\nAhora bien, hay una gran cantidad de métodos de auste de una regresión, pero todos se reducen a una cosa: minimizar una función de pérdida. ¿Qué es una función de pérdida y con qué se come? Es una forma rebuscada de llamarle a la distancia que existe entre nuestros valores observados y los valores predichos por el modelo o, en palabras más sencillas, qué tan lejos quedó la flecha del blanco; es decir, al ajustar un modelo estamos minimizando su error. ¿Cuáles valores predichos? Aaaah, que bueno que preguntaste. Si vuelves a la Figura 11.2 te darás cuenta que, utilizando los parámetros estimados con nuestra regla de tres (\\(a\\) = 0 y \\(b\\) = 5) calculamos cuánto gastaríamos si compraramos desde 1 hasta 10 manzanas. De esos costos solo teníamos el dato de que \\(10 🍎 = \\$50\\), todo lo demás es una predicción.\n\n\n\n\n\n\nNota\n\n\n\nLo que hicimos fue, de hecho, una extrapolación, pues predijimos valores fuera del alcance de nuestros datos observados. Si tuvieramos “huecos” en nuestros datos y quisiéramos rellenarlos con el modelo tendríamos una interpolación; es decir, predeciríamos valores dentro del alcance de nuestros datos observados.\n\n\nPero volvamos a nuestra regresión lineal. El modelo más simple es el que veremos en esta sesión: la regresión lineal simple. En esta, tal y como en nuestro ejemplo con las manzanas, describimos la relación entre dos variables continuas utilizando la ecuación de la recta. Formalmente este lo expresamos como:\n\\[\nY = \\beta_0 + \\beta_1*x + \\epsilon\n\\]\nMencioné que existen distintas formas de ajustar sus parámetros, entre las que tenemos (ordenadas de mayor a menor complejidad):\n\nMínimos cuadrados, que veremos a continuación.\nMáxima verosimilitud, que también veremos a continuación.\nInferencia Bayesiana, que en realidad incluye como caso especial a la máxima verosimilitud.\nDescenso estocástico de gradiente, en ciertos escenarios de redes neuronales.\n\nEmpecemos entonces con el ajuste por mínimos cuadrados.\n\n11.4.1 Mínimos cuadrados\nEn este método de ajuste la función de pérdida es la función cuadrática:\n\\[\nD(y_i - \\hat{y_i}) = \\sum_{i=1}^n(y_i - \\hat{y_i})^2 = \\sum_{i=1}^n \\epsilon^2\n\\]\nEs decir, minimizamos la distancia (diferencia) cuadrática entre los valores observados y los valores predichos. Analíticamente (cálculo) el proceso consiste en obtener la derivada parcial de \\(D(y_i - \\hat{y_i})\\), igualarla a 0, y encontrar una expresión para \\(\\beta_0\\) y \\(\\beta_1\\). Te voy a ahorrar toda la matemática correspondiente y te daré, solo como referencia, estas últimas expresiones. Para \\(\\beta_1\\):\n\\[\n  \\beta_1 = \\frac{\\Sigma_{i = 1}^n(x_i - \\overline{x})(y_i - \\overline{y})}{\\Sigma_{i = 1}^n(x_i - \\overline{x})^2}\n\\]\nY para \\(\\beta_0\\):\n\\[\n\\beta_0 = \\overline{y}- \\beta_1*\\overline{x}\n\\]\n¿Por qué solo como referencia? Porque (afortunadamente para nosotros) R ya tiene codificado todo el proceso en la función lm(formula, data) y no tenemos que preocuparnos por nada de eso.\n\n\n\n\n\n\nNota\n\n\n\nRecordarás que en el Capítulo 4 hablé (y ejemplifiqué) sobre cómo ajustar una regresión lineal simple, tanto con R como con tidymodels. En esta sesión tidymodels aún nos queda “un poco grande”, en el sentido de que aún no podemos aprovechar todo lo que ofrece, por lo que me voy a limitar a utilizar la forma de R base.\n\n\nPara ejemplificarlo carguemos los datos contenidos en example_data.csv:\n\ndf_reg1 &lt;- read.csv(\"datos/example_data.csv\")\n\nLuego grafiquémoslos:\n\nplot_data_reg1 &lt;- ggplot(data = df_reg1,\n                         aes(x = v1, y = v2)) +\n                  geom_point(color = \"dodgerblue4\",\n                             alpha = 0.7,\n                             size = 2) +\n                  labs(title = \"Relación entre v1 y v2\") +\n                  see::theme_lucid()\nplot_data_reg1\n\n\n\n\n\n\n\n\n\n11.4.1.1 Ajuste y Bondad de ajuste\nAhora ajustemos el modelo de mínimos cuadrados (lm()) a los datos y veamos los resultados de la regresión (summary()):\n\nreg1 &lt;- lm(v2~v1, data = df_reg1)\nsummary(reg1)\n\n\nCall:\nlm(formula = v2 ~ v1, data = df_reg1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.8956 -1.9924 -0.5525  1.5351 15.3006 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -3.72654    0.73213   -5.09 1.81e-06 ***\nv1           1.17765    0.08141   14.47  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.095 on 95 degrees of freedom\nMultiple R-squared:  0.6878,    Adjusted R-squared:  0.6845 \nF-statistic: 209.3 on 1 and 95 DF,  p-value: &lt; 2.2e-16\n\n\nDescribamos la salida elemento por elemento:\n\nCall: es el cómo llamamos a la función. ¿La razón? En caso de que estemos llamando a la función lm dentro de otra función, cosa que no hicimos. De cualquier manera, sirve como una forma de verificar que pusimos las cosas en orden. -Residuals: Nos da información sobre la distribución de los residuales (la diferencia entre observado y predicho). Esta es útil con fines diagnósticos, pero hablaremos más a fondo de ellos más adelante.\nCoefficients: Nos da una tabla con los valores de los parámetros del modelo, donde la pendiente tiene el nombre de la variable predictora, su error estándar y una prueba \\(t\\) para cada uno. ¿Contra qué está comparando? Contra un modelo nulo; es decir, contra un modelo donde ese parámetro tenga un valor = 0.\nResidual standard error: Más información sobre los residuales, aunque en este caso es el error estándar. Este es sumamente útil para darnos una idea de qué tan preciso es el modelo, pues indica en cuántas unidades, en promedio, se desvía la predicción de los datos observados. Este valor, dividido entre el promedio de la variable predicha nos da la tasa de error del modelo.\nMultiple R-squared: Es el valor del famosísimo (¿infame?) coeficiente de determinación (\\(R^2\\)). Si ya has llevado clases de estadística y de regresión lineal es muy posible que lo entiendas como “la varianza de los datos explicada por el modelo”. ¿Por qué digo infame? Porque, al igual que el valor de p, es un valor del cuál se abusa. Nuevamente, los seres humanos somos flojos por naturaleza, por lo que nos gusta resumir las cosas en un solo número. En este sentido, el \\(R^2\\) es una medida de bondad de ajuste; es decir, de qué tan bien ajustado está el modelo. Es muy práctico, pues está contenido en el intervalo \\([0,1]\\) y representa un porcentaje; sin embargo, para que podamos confiar en él debemos de haber cubierto con el resto de supuestos de la RLS. Personalmtente te sugiero mejor tomar el RSE como medida de ajuste, aunque la interpretación no sea tan directa.\nAdjusted R-squared: Es un ajuste al \\(R^2\\) que lo hace menos optimista, especialmente diseñado para escenarios de regresión múltiple (de ahí el “Multiple” del punto anterior). Por el momento lo vamos a ignorar.\nF-statistic: Resultados de un ANOVA que, al igual que el punto anterior vamos a ignorar porque es informativo solo en regresiones múltiples. Es un ANOVA para comparar todo el modelo contra un modelo que tiene solo un intercepto, por lo que aquí solo es redundante con la prueba \\(t\\) para la pendiente.\n\n\n\n\n\n\n\nNota\n\n\n\n¿Qué es la bondad de ajuste? Como dice el nombre, qué tan bien ajustado está el modelo, en el sentido de qué tan buenas son las predicciones.\n\n\nLos resultados del modelo los podemos reportar como:\n\nEn el modelo de regresión lineal simple tanto el intercepto (\\(\\beta_0 = -3.72\\)) como la pendiente (\\(\\beta_1 = 1.17\\)) son significativamente diferentes de 0 (\\(\\beta_0: t_{\\nu = 95} = -5.09; p &lt; 0.0001\\); \\(\\beta_1: t_{\\nu = 95} = 14.47; p &lt; 0.0001\\)). El valor de \\(R^2\\) indica que el modelo explica alrededor del 70% de la varianza de los datos, lo cual sugiere un ajuste aceptable, con un error estándar de los residuales de 3.095 unidades.\n\nUsualmente acompañaríamos este reporte de un gráfico, el cual podemos construir con la capa geom_smooth(method = \"lm\", se = FALSE), tal que:\n\nplot_reg1 &lt;- plot_data_reg1 +\n             geom_smooth(method = \"lm\",\n                         se = F,\n                         colour = rgb(118,78,144,\n                                      maxColorValue = 255)) +\n             labs(caption = paste(\"Modelo ajustado: v2 = \", \n                                   round(reg1$coefficients[1],2), \n                                   \" + \",\n                                   round(reg1$coefficients[2],2),\n                                   \"*v1 + e\"))\n\nplot_reg1\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAhora bien, recordarás que ninguna estimación es infalible, por lo que tanto el reporte como el gráfico están incompletos. Hablemos entonces de los intervalos de confianza.\n\n11.4.1.1.1 Intervalos de confianza para los parámetros\nEn un modelo de regresión lineal tenemos “dos” intervalos de confianza: los intervalos de confianza para la estimación de los parámetros y el intervalo de confianza para la regresión. Los primeros, como te imaginarás, representan la incertidumbre alrededor de la estimación de nuestros parámetros de regresión. Como recordarás de lo que mencioné en el capítulo Capítulo 8, estos se construyen a partir de su error estándar (\\(IC_{95\\%} = \\beta ± 1.96*EE\\)) y, a diferencia de lo que vimos en el Capítulo 9, ahora los obtenemos con la función confint():\n\nconfint_reg1 &lt;- confint(reg1, level = 0.95)\nconfint_reg1\n\n                2.5 %    97.5 %\n(Intercept) -5.179989 -2.273085\nv1           1.016035  1.339263\n\n\nCon esta información podemos realizar un gráfico donde representemos esta incertidumbre, en donde obtengamos dos límites para la línea de regresión utilizando esos valores, tal que:\n\n# Construimos el límite inferior con el límite del 2.5% de los intervalos\ndf_reg1[\"inf_int\"] &lt;- confint_reg1[1,1] + confint_reg1[2,1]*df_reg1$v1\n# Construimos el límite superior con el límite del 97.5% de los intervalos\ndf_reg1[\"sup_int\"] &lt;- confint_reg1[1,2] + confint_reg1[2,2]*df_reg1$v1\n\nAñadiéndolos al gráfico de los datos:\n\nplot_reg1 +\n  geom_ribbon(data = df_reg1,\n              aes(ymin = inf_int,\n                  ymax = sup_int),\n              fill = \"gray60\",\n              alpha = 0.3) +\n  labs(subtitle = \"Intervalos de confianza para los parámetros\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nPero este no es el gráfico que usualmente veremos. Si deseáramos inlcuir esta información solo añadiríamos los IC al reporte, tal que:\n\nEn el modelo de regresión lineal simple tanto el intercepto (\\(\\beta_0 = -3.72\\); \\(IC_{95\\%}: [-5.18, -2.27]\\)) como la pendiente (\\(\\beta_1 = 1.17\\); \\(IC_{95\\%}: [1.02,  1.34]\\)) son significativamente diferentes de 0 (\\(\\beta_0: t_{\\nu = 95} = -5.09; p &lt; 0.0001\\); \\(\\beta_1: t_{\\nu = 95} = 14.47; p &lt; 0.0001\\)). El valor de \\(R^2\\) indica que el modelo explica alrededor del 70% de la varianza de los datos, lo cual sugiere un ajuste aceptable, con un error estándar de los residuales de 3.095 unidades.\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nEn este caso nuestros datos son “adimensionales”; es decir, no tenemos unidades de ninguna variable. Si nuestras variables no son adimensionales debemos de incluir también las unidades correspondientes (pesos, manzanas y pesos/manzana, si volvemos a nuestro ejemplo).\n\n\n\n\n11.4.1.1.2 Intervalo de confianza para la regresión\n¿Si el gráfico anterior no es el que presentamos, cuál es? Uno que incluya el intervalo de confianza para la regresión per-se (también llamado para la recta). ¿Recuerdas el RSE? Pues se construye con ese valor. Para incluirlo en el gráfico solo tenemos que modificar ligeramente geom_smooth() y hacer se = TRUE:\n\nplot_reg1 + \n  geom_smooth(method = \"lm\",\n              se = TRUE,\n              colour = rgb(118,78,144, maxColorValue = 255))\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigura 11.3: Modelo de regresión lineal simple de v2 y v1. La línea morada representa el ajuste lineal, y el área sombreada el intervalo de confianza para la regresión\n\n\n\n\n\n\n\n\n\n\n\nNota\n\n\n\nEstamos obteniendo dos mensajes de geom_smooth() porque en plot_reg1 ya teníamos esa capa (con se = FALSE). Pudiera llegar a sonar obvio, pero cuando hagas tus gráficos NO es necesario que repitas capas.\n\n\nNuestro reporte completo quedaría entonces como:\n\nEn el modelo de regresión lineal simple (Figura 11.3) tanto el intercepto (\\(\\beta_0 = -3.72\\); \\(IC_{95\\%}: [-5.18, -2.27]\\)) como la pendiente (\\(\\beta_1 = 1.17\\); \\(IC_{95\\%}: [1.02,  1.34]\\)) son significativamente diferentes de 0 (\\(\\beta_0: t_{\\nu = 95} = -5.09; p &lt; 0.0001\\); \\(\\beta_1: t_{\\nu = 95} = 14.47; p &lt; 0.0001\\)). El valor de \\(R^2\\) indica que el modelo explica alrededor del 70% de la varianza de los datos, lo cual sugiere un ajuste aceptable, con un error estándar de los residuales de 3.095 unidades.\n\nY ahora toca abordar el “elefante en el cuarto” y comprobar que nuestra regresión sea confiable.\n\n\n\n\n11.4.2 Supuestos de la Regresión Lineal\n¿Qué no ya habíamos evaluado la bondad del ajuste? Sí y no. En realidad hice una pequeña trampa para que veas por qué el \\(R^2\\) no cuenta toda la historia, y por qué no debemos de confiar ciegamente en él. La regresión lineal simple, como buena técnica paramétrica, tiene sus supuestos:\n\nLinealidad: Existe una relación lineal entre las variables involucradas.\nIndependencia: El error es independiente; i.e., no hay correlación entre el error de puntos consecutivos (aplica para series de tiempo).\nNormalidad: El error sigue una distribución normal.\nHomocedasticidad: El error tiene una varianza constante para cada valor de \\(X\\).\n\n\n\n\n\n\n\nNota\n\n\n\nTe darás cuenta de que todo está en términos del “error”, tal y como hablábamos de distribuciones muestrales de la media en el capítulo Capítulo 10. Siguiendo la misma lógica, nuestras inferencias para comprobar los supuestos las haremos sobre los residuales.\n\n\nSi no cumplimos con uno, varios, o ninguno, la confiabilidad de nuestro modelo de regresión para fines de interpretación va disminuyendo. Los primeros dos son bastante lógicos. El primero es auto-explicativo: si la relación no es lineal, el modelo lineal no es suficiente para describirla. El segundo tiene que ver con datos de series de tiempo y algo que se conoce como autocorrelación, pero esto se reduce a que el error del punto \\(t_i\\) no dependa del punto \\(t_{i-1}\\).\nEl supuesto de normalidad, para variar, requiere de un poco más de explicación: la distribución que debe ser normal es la del error. En ningún lugar se habla de que \\(Y\\) o \\(X\\) deban de estar normalmente distribuidos, solamente el error. Esto puede sonarte extraño, pero tiene todo el sentido del mundo: si estamos optimizando el modelo a partir de los residuales, ¿por qué habría de importarnos la distribución cruda de las variables?\nEl supuesto de homocedasticidad, por otra parte, es análogo al supuesto de homogeneidad de varianzas pero, nuevamente, nos interesa qué pasa con el error, no con las variables originales (por favor, no hagas una prueba de Levene con tus variables como grupos). En este caso lo importante es que el error sea parejo, independientemente de si tenemos valores pequeños o grandes del predictor.\n¿Cómo evaluamos estos supuestos? Mayoritariamente con gráficos de dispersión, pero vamos uno a uno:\n\nLinealidad: Con un gráfico de residuales. En este gráfico tenemos los residuales estandarizados (cada residual menos el promedio de los residuales, dividido entre la desviación estándar) en el eje \\(y\\), y los valores ajustados por el modelo (predicciones) en el eje \\(x\\). Se pueden añadir dos referencias: una línea de referencia horizontal en \\(y = 0\\) y una curva LOESS (hablaremos un poco de este modelo en el Capítulo 13). Si la relación entre nuestras variables predicha y predictora es perfectamente lineal, entonces todos los puntos caerán sobre la línea de referencia y la curva LOESS será completamente horizontal en 0. Entre menos lineal sea la relación más se alejarán los puntos de la línea y mayores curvaturas tendrá el modelo LOESS. Adicionalmente, puede servir para identificar valores extremos (puntos que caigan fuera de \\([-1.96, 1.96]\\) si se trabaja a un 95% de confianza).\nIndependencia: Con un gráfico de autocorrelación. Lo vamos a obviar porque el análisis de series de tiempo está fuera del alcance de este curso.\nNormalidad: Una prueba de normalidad de los residuales y con un gráfico cuantil-cuantil (QQ plot). En este gráfico se grafican (valga la redundancia) los residuales en el eje \\(y\\), y cuantiles teóricos según una distribución normal en el eje \\(x\\). Además se traza una línea de referencia con una pendiente de 1, que representa una distribución normal perfecta. El objetivo es que la prueba de normalidad sea no significativa y que los residuales caigan lo más cercanamente posible a la línea de referencia.\nHomogeneidad de varianzas: Una prueba Breusch & Pagan (1979) y un gráfico de escala-locación (scale-location). La prueba, como te imaginarás, tiene la hipótesis de nulidad de que el error (residuales) está homogéneamente distribuido. En el gráfico tenemos en el eje \\(y\\) la raíz cuadrada del absoluto de los residuales estandarizados (\\(\\sqrt{|re|}\\)) y en el eje \\(x\\) los valores ajustados. ¿Por qué la raíz del absoluto? Ese detalle ya es clavarse demasiado, y prefiero que nos adentremos bien a otro tema un poco más adelante, así que conformemonos por saber que queremos que los puntos estén distribuidos de manera aleatoria (homogénea) en todo el eje \\(x\\). ¿Qué quiere decir esto? que no tengamos una mayor dispersión de los puntos (varianza de los residuales) en valores ajustados pequeños (a la izquierda del gráfico) que en los valores más grandes (derecha del gráfico), lo que se vería como un &gt; imaginario, o viceversa, o algún otro patrón.\n\n¿Muchos gráficos? Tal vez. Podríamos hacerlo a mano, o podemos aprovechar la librería performance y obtenerlos todos de una vez con la función check_model(object), donde object es el objeto con los resultados del ajuste. Esta función nos da todos los gráficos en un solo paso:\n\nreg1_diags &lt;- performance::check_model(reg1)\nreg1_diags\n\n\n\n\n\n\n\nFigura 11.4: Gráficos diagnósticos de la regresión.\n\n\n\n\n\nY se acabó el encanto. Resulta que no cumplimos con ninguno de los supuestos, y entonces nuestro \\(R^2 \\approx 0.7\\) nos mintió vilmente. Cada quien reacciona de forma diferente a cuándo alguien le miente, pero lo que es seguro que pase es que desconfiará, al menos un poco, de todo lo que esa persona le diga en un futuro. En defensa del \\(R^2\\), no es su culpa y, de hecho, estoy siendo demasiado duro con él. Me explico: el \\(R^2\\) es confiable como medida de ajuste sí y solo si se cumplen los mismos supuestos de la regresión lineal simple, lo cual no es el caso; ergo, no podíamos confiar en él desde un principio porque nuestros datos no lo permiten. El problema es que se ha malversado su uso, y muchas personas lo utilizan como si fuera el único sello de garantía, cuando en realidad es el último.\nPero volvamos a nuestros gráficos. En la Figura 11.4 se ven muy pequeños, y hay un par que no hemos explicado:\n\nPosterior predictive check: Este tipo de gráficos es uno de los más socorridos en inferencia Bayesiana, para comprobar que la distribución posterior obtenida por el modelo sea consistente con los datos observados. Aquí no tenemos un modelo Bayesiano, pero sí que podemos utilizar la información de la distribución de los parámetros (estimación y error estándar) para simular datos aleatorios. Si el modelo tiene una buena capacidad predictiva, entonces los datos observados \\(Y\\) deben de caer dentro del área comprendida por las líneas de los datos predichos \\(\\hat{Y}\\). En este caso, la capacidad predictiva del modelo no es del todo mala, solamente tenemos una sobre-estimación notable en valores cercanos a 10. Esto nos habla de la robustez del modelo lineal, pero no por ello hay que abusar de él.\n\n\nplot(performance::check_predictions(reg1)) +\n  see::theme_lucid()\n\n\n\n\n\n\n\n\n\nLinearity: Es el gráfico de residuales. Desafortunadamente no hay una forma de obtenerlo en una sola línea, así que tocará construirlo a mano y, de paso, modificar un par de cosas. La primera es graficar los residuales estandarizados (menos su media y divididos entre su desviación estándar) para poder darnos una idea de si tenemos valores extremos o no. La segunda es asignar una escala de colores para facilitarlo. Para poder hacer esto vamos a pasarle a ggplot() directamente el objeto de regresión, y en aes() vamos a utilizar dos atributos ocultos del objeto reg1: .fitted con los valores predichos y .stdresid con los residuales estandarizados. En el gráfico resultante podemos ver que hay varios valores con residuales altos (tonos rojizos), y algunos extremos (&gt; 1.96). La curva LOESS no se ve exageradamente desviada de la línea de referencia, lo cual indica que un modelo lineal puede no ser tan mala elección.\n\n\n# Inicializar el espacio gráfico\nggplot(data = reg1, # Objeto de regresión\n       # Datos ajustados y residuales estandarizados\n       aes(x = .fitted, y = .stdresid,\n           colour = .stdresid)) +\n  # Gráfico de dispersión\n  geom_point(size = 3,\n             alpha = 0.7,\n             show.legend = F) +\n  # Referencia en 0\n  geom_hline(yintercept = 0,\n             colour = \"black\",\n             linetype = \"dashed\") +\n  # Referencia loess\n  geom_smooth(method = \"loess\",\n              colour = \"#3aaf85\") +\n  # Gradiente de colores\n  # \"#cd201f\": color para los extremos (rojo)\n  # \"#1b6ca8\": color para el punto intermedio (0)\n  # Escala de -2 a 2 (debería ser 1.96)\n  # oob: ¿qué hacer con datos fuera de los límites?\n  # scales::squish : marcarlos como si estuvieran en el límite\n  scale_color_gradient2(low = \"#cd201f\",\n                        midpoint = 0, \n                        mid = \"#1b6ca8\", \n                        high = \"#cd201f\",\n                        breaks = c(-2, 0, 2),\n                        limits = c(-2, 2),\n                        oob = scales::squish) +\n  labs(title = \"Linearity\",\n       subtitle = \"Reference line should be flat and horizontal\",\n       y = \"Standardized residuals\",\n       x = \"Fitted values\") +\n  see::theme_lucid()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nHomogeneity of variance: Homocedasticidad/Heterocedasticidad. Con un poco de imaginación puedes trazar un &gt;, indicando que hay una mayor varianza en los valores pequeños que en los valores grandes. La prueba Breusch & Pagan (1979), por otra parte, sugiere que la varianza es homogénea. Este es un típico caso donde conviene errar en el lado de la precaución y profundizar en el análisis antes de sacar una conclusión. ¿Qué pasa con nuestro &gt; si quitamos el punto extremo con un residual estandarizado &gt; 4? Se vuelve mucho menos marcado, ¿no? Posiblemente sea eso lo que está viendo la prueba y que no esté siendo engañada por ese punto. Dada la estructura de los datos (muchos acumulados en valores pequeños y pocos en valores grandes), yo decidiría que no se cumple el supuesto de homogeneidad de varianzas.\n\n\nreg1_homoced &lt;- performance::check_heteroscedasticity(reg1)\nplot(reg1_homoced) +\n  see::theme_lucid()\n\n\n\n\n\n\n\nreg1_homoced\n\nOK: Error variance appears to be homoscedastic (p = 0.206).\n\n\n\nInfluential Observations: Este es otro gráfico diagnóstico que no había mencionado porque no está directamente relacionado con los supuestos. Este gráfico se conoce como gráfico de apalancamiento, y señala observaciones que pudieran estar “engañando” o afectando de manera importante la estimación de la recta. En otras palabras, que “jalen” la recta hacia ellos, por estar extremadamente lejos de la tendencia central de \\(y\\) para ese punto \\(x\\). En menos palabras: nos permite identificar “outliers”. Hay una gran cantidad de métodos, y la función performance::check_outliers() califica cada valor con una nota compuesta por el promedio de los resultados binarios (“outlier” o no, 1 o 0) de cada método. Representa la probabilidad de que cada observación sea clasificada como “outlier” por al menos un método. Se considera un “outlier” si su calificación es superior o igual a 0.5 (líneas verdes punteadas); es decir, vamos a buscar puntos que estén fuera del “cono” formado por los contornos. En este caso ninguno está fuera del contorno, pero el punto 1 se ve sospechoso.\n\n\nreg1_outliers &lt;- performance::check_outliers(reg1)\nplot(reg1_outliers)\n\n\n\n\n\n\n\n\n\nNormality: Por último, el gráfico de normalidad. No es de sorprender que tengamos “desviaciones de la normalidad” bastante marcadas en algunos puntos, especialmente cerca de la cola derecha de la distribución. La prueba de normalidad de los residuales también rechaza que se cumpla el supuesto.\n\n\nreg1_norm &lt;- performance::check_normality(reg1)\nplot(reg1_norm, type = \"qq\")\n\n\n\n\n\n\n\nreg1_norm\n\nWarning: Non-normality of residuals detected (p &lt; .001).\n\n\nIndependientemente de mi “bullying” al \\(R^2\\), ahora ya sabes todo lo que implica hacer una regresión lineal simple, y que es mucho más que simplemente picar botones en alguna suite estadística o utilizar la función lm en R o alguna otra función en otro lenguaje de programación.\n\n\n11.4.3 Predicción vs. Interpretación\nAhora bien, mencioné en varias ocasiones cosas relacionadas con la “predicción” y la “interpretación”. Pues resulta que, como vimos arriba, para fines predictivos no importan demasiado los supuestos, y antes de que agarres un trinche y una antorcha escucha lo que tengo que decir. Antes te dije que la regresión forma parte del aprendizaje automatizado supervisado y, como tal, su principal (por no decir único) objetivo es la predicción. Un modelo de regresión exitoso es un modelo que pueda predecir adecuadamente, punto. ¿Y la interpretación? Esa es otra cara de la moneda. De hecho, están inversamente correlacionadas, en el sentido de que entre más poderosa es una técnica, menos interpretable es. De todos estos detalles vamos a hablar más adelante en el Capítulo 17, pero por el momento quiero que te quedes con lo siguiente:\n\n\n\n\n\n\nImportante\n\n\n\nLa validación de supuestos es solo necesaria si nos interesa explicar los parámetros del modelo, no si solo nos interesan sus predicciones.\n\n\nSin ir demasiado lejos, el modelo que construimos arriba no cumple con el supuesto de normalidad (los demás están en la cuerda floja) y aún así las predicciones posteriores (en el posterior predictive check) se ven bastante aceptables. Desafortunadamente para nostros, usualmente nos interesa la interpretación, así que hay que hacer la tarea completa.\n¿Y si solo me interesan las predicciones? Bueno, igual hay que verificar algunas cosas, pero eso lo veremos en el Capítulo 17.\n\n\n11.4.4 Máxima verosimilitud\nBueno, ya sabemos cómo aplicar, interpretar, y validar los supuestos de una regresión lineal en R utilizando el método de mínimos cuadrados, pero antes te mencioné que también había otros métodos entre los que se encuentra el ajuste por máxima verosimilitud. Entonces es necesario explicar qué es la verosimilitud para luego ver cómo maximizarla, ¿no crees?\nRecordemos por un momento lo que revisamos en el Capítulo 6 sobre la probabilidad. Dijimos que, cuando tenemos resultados mutuamente excluyentes y exhaustivos (todos los resultados), la suma de todas sus probabilidades es exactamente 1, tal que:\n\\[\n\\sum_i^n p_i == 1\n\\]\nHasta aquí todo bien, pero ¿dónde entra la verosimilitud? Si buscas en el diccionario de la Real Academia Española te vas a encontrar con una de sus siempre útiles definiciones: “Cualidad de verosímil”, por lo que hay que definir verosímil: “que tiene apariencia de verdadero”. Eso ya tiene más sentido. En un escenario de investigación nosotros podemos plantear múltiples hipótesis, que no son necesariamente excluyentes entre sí, entonces no podemos simplemente utilizar la probabilidad. Entendamos la verosimilitud con un ejemplo:\nImagina que alguien a quien conoces te dice que uno de sus amigos tiene poderes psíquicos. Tú, como persona de ciencia, decides ponerlo a prueba. Acuerdan una reunión y le pones un “desafío” simple: vas a lanzar diez volados, y el debe de adivinar el resultado. Al final, él adivina correctamente 7/10 volados. Sin dejarte llevar por tu escepticismo planteas algunas hipótesis: i) simple coincidencia, el tamaño de muestra no es lo suficientemente grande; ii) la moneda no es del todo “justa”, sino que tiende a caer más hacia cierto lado; iii) esta persona tiene una visión cinética sobre-humana y puede ver qué es lo que está arriba antes de que atrapes la moneda, iv) esta persona realmente tiene algo de clarividente. ¿Cuál crees que sea más verosímil? Espero que me digas que la primera hipótesis, especialmente después de lo que vimos en el Capítulo 6. Si por el contrario hubieran sido 1000 lanzamientos y hubiera adivinado 700, la historia sería otra, pero con 7/10 puede ser un capricho del mundo. Eso que hicimos fue, justamente, un ajuste por máxima verosimilitud: seleccionar la hipótesis más verosímil de entre un conjunto dado, solo que vamos a cambiar hipótesis por valores de parámetros.\n¿Formalmente? Un ajuste por máxima verosimilitud consiste en estimar parámetros de un modelo, dado un conjunto de observaciones, en donde se encuentra valores que maximicen la verosimilitud de las observaciones dados los valores de los parámetros. Puesto de otra manera, buscamos el conjunto de valores que maximicen la probabilidad de que nos hayamos encontrado nuestros datos, según el modelo que escogimos. Esto se parece mucho a lo que vimos en el Capítulo 9 sobre el nivel de significancia, solo que nuestro modelo “deja de ser una distribución de probabilidades” para ser un modelo de regresión. ¿Por qué las comillas? Porque nuestro error no puede quedar “suelto”, pero en máxima verosimilitud podemos trabajar con cualquier distribución de probabilidad que se ajuste a nuestro problema; de hecho, esto es lo que da lugar a los modelos lineales generalizados, pero eso lo veremos en el Capítulo 19.\n¿Cómo lo llevamos a la práctica? Primero quiero que te des la oportunidad de ver una relación interesante que puede ahorrarte mucho trabajo, o que puede abrirte la puerta a otro tipo de análisis.\n\n11.4.4.1 Mínimos cuadrados, máxima verosimilitud e inferencia Bayesiana\nEsta parte es completamente teórica y asumo que el ver procedimientos algebraicos no te supone un problema. De no ser así, puedes saltar al final para obtener la idea clave. Si decides que te interesa, vamos allá.\nRecordemos que un problema de regresión consiste en estimar los parámetros de un modelo matemático dado un conjunto de observaciones, y que tenemos una gran diversidad de formas de hacerlo. Comencemos hablando del método de ajuste más común para una RLS: mínimos cuadrados. Como mencioné antes, con este método minimizamos la función de pérdida cuadrática; es decir:\n\\[\\begin{align*}\n\\epsilon = y - \\hat{y}\\\\\nL = \\epsilon^2\n\\end{align*}\\]\nCuando utilizamos este método asumimos algunas cosas, entre ellas que nuestros residuales (no nuestra variable) se encuentran normalmente distribuidos. Aunque este método funciona, se prefiere utilizar métodos probabilísticos (Gerrodette, 2011), tal como la aproximación por máxima verosimilitud. Antes utilizamos un ejemplo práctico para definirla, pero ahora aproximémosla desde el teorema de Bayes:\n\\[\np(\\theta|x) = \\frac{p(x|\\theta)p(\\theta)}{p(x)}\n\\]\nQue podemos simplificar como:\n\\[\nPosterior = \\frac{verosimilitud \\times previa}{evidencia}\n\\]\nEl teorema de Bayes es lo que da lugar al paradígma de la inferencia Bayesiana, el cual no vemos en este curso; sin embargo explicar el teorema es bastante sencillo: ¿qué tan probable es una hipótesis (\\(\\theta\\)), dada cierta evidencia (datos, \\(x\\))? (probabilidad posterior, \\(p(\\theta|x)\\)). Para responderlo vamos a obtener la relación que hay entre qué tan probable es la evidencia, dada la hipótesis (\\(p(x|\\theta)\\), nuestra verosimilitud), qué tan probable creamos nosotros que es nuestra hipótesis (probabilidad previa, \\(p(\\theta)\\)) y la probabilidad de la evidencia en sí misma (\\(p(x)\\)). Obtener la probabilidad de la evidencia es un tema en sí mismo (en realidad solo la aproximamos), así que lo vamos a obviarla de la ecuación. Recordarás que en la inferencia estadística frecuentista partimos del hecho de que no sabemos nada sobre nuestro problema, y podemos entonces, al menos de manera teórica, establecer eso en el teorema de Bayes, lo cual nos lleva a cancelar nuestros términos de previa y evidencia y terminar con la siguiente equivalencia:\n\\[\nPosterior \\equiv verosimilitud\n\\]\nEste caso especial de la inferencia Bayesiana tiene un nombre: Estimación Máxima A posteriori (Maximum A posteriori Estimate, MAP) y es equivalente a la estimación puntual de un ajuste por máxima verosimilitud. ¿Por qué? Porque en esa aproximación tratamos de encontrar valores de nuestros parámetros que maximicen la verosimilitud de las observaciones, dados los parámetros. De manera matemática definimos la equivalencia como:\n\\[\np(x|\\theta) \\equiv L(\\theta|x) \\implies p(x_1, x_2, ..., x_n|\\theta)\n\\]\nOtro de nuestros supuestos en este paradigma es que las muestras son independientes entre sí, por lo que podemos expandir nuestra probabilidad conjunta con \\(P(A,B) = P(A)P(B)\\):\n\\[\nL(\\theta|x_1, x_2, ..., x_n) \\equiv p(x_1|\\theta)p(x_2|\\theta),...,p(x_n|\\theta)= \\prod p(x_i|\\theta)\n\\]\nY este término es lo que queremos maximizar, por lo cual lo podemos escribir tal que:\n\\[\n\\begin{matrix} max \\\\ \\theta \\end{matrix}\n\\left\\{ \\prod p(x_i|\\theta) \\right\\}\n\\]\nEl problema es que ni a nosotros ni a las computadoras nos gusta hacer multiplicaciones, por lo que podemos aplicar un logaritmo para convertir el productorio en una sumatoria. Recuerda: el logaritmo de un producto es igual a la suma del logaritmo de cada uno de sus componentes, por lo tanto:\n\\[\n\\begin{matrix} max \\\\ \\theta \\end{matrix}\n\\left\\{ log \\left( \\prod p(x_i|\\theta) \\right) \\right\\} \\implies\n\\begin{matrix} max \\\\ \\theta \\end{matrix}\n\\left\\{ \\sum_i^n log(p(x_i|\\theta)) \\right\\}\n\\]\nEsta última parte era un poco innecesaria, nada más que un breviario cultural para que conocieras por qué utilizamos logaritmos de verosimilitud, cosa que haremos más adelante, pero ahora vayamos al meollo del asunto:\n\n\n\n\n\n\nImportante\n\n\n\nUno de los supuestos del ajuste por mínimos cuadrados es un error normalmente distribuido. En máxima verosimilitud podemos ajustar nuestro error a cualquier distribución de probabilidad. Si utilizamos a la distribución normal como la distribución del error, entonces mínimos cuadrados, máxima verosimilitud (distribución normal) e inferencia Bayesiana (verosimilitud normal y previas muy planas) dan estimaciones equivalentes.\n\n\n\n\n11.4.4.2 Ajuste por máxima verosimilitud\nEn este punto puedes estar en uno de estos escenarios: a) lograste seguir toda la explicación y se te hizo lógica (si fue así, ¡felicidades! Eres un tan ñoño o ñoña como yo); b) seguiste la explicación y se te hizo lógica, pero no terminaste de entender el teorema de Bayes (igualmente, ¡felicidades! Vas para ñoño/ñoña que chutas); c) lo leíste pero te perdiste solo con las ecuaciones (también ¡felicidades!, tienes la intención de convertirte en ñoño/ñoña); o d) saltaste directamente a lo importante (¡felicidades a ti también! Tienes una vida 🥲). Si estás en los casos c y d, y puede que b seguramente no estés del todo convencido de que mínimos cuadrados y máxima verosimlitud con un error normal sean equivalentes. Si estás en el caso a, te gustaría una demostración. ¿Y si no te interesa? Igual la vamos a hacer.\nA diferencia de la implementación de una regresión por mínimos cuadrados, ajustar el modelo mediante máxima verosimilitud no es tan intuitivo. El primer paso es establecer manualmente nuestra función de verosimilitud, ajustando una distribución normal a los residuales:\n\ndata &lt;- df_reg1[c(\"v1\", \"v2\")]\nLL &lt;- function(b0, b1,\n               mu, sigma){\n  # Encontrar los residuales. Modelo a ajustar\n  R = data$v2 - data$v1 * b1 - b0\n  \n  # Calcular la verosimilitud. Residuales con distribución normal.\n  \n  R = suppressWarnings(dnorm(R, mu, sigma))\n  \n  # Sumar el logaritmo de las verosimilitudes para\n  # todos los puntos de datos.\n  -sum(log(R))\n}\n\nAhora ajustemos el modelo que acabamos de crear, utilizando la función stats4::mle(fun, start = list()) (*maximum likelihood estimation”), donde fun es la función de verosimilitud a ajustar y start son los valores iniciales de los parámetros. En este paso lo que estamos haciendo es estimar los dos parámetros (media y desviación estándar) que mejor describen los datos:\n\nmle_fit &lt;- mle(LL, start = list(b0 = 1, b1 = 1, sigma = 1), \n               fixed = list(mu = 0), \n               nobs = length(data$v2))\n\nsummary(mle_fit)\n\nMaximum likelihood estimation\n\nCall:\nmle(minuslogl = LL, start = list(b0 = 1, b1 = 1, sigma = 1), \n    fixed = list(mu = 0), nobs = length(data$v2))\n\nCoefficients:\n       Estimate Std. Error\nb0    -3.726537 0.72453828\nb1     1.177649 0.08056368\nsigma  3.063056 0.21991454\n\n-2 log L: 492.4402 \n\n\n\n\n\n\n\n\nNota\n\n\n\nHay que definir valores iniciales para los parámetros porque, a diferencia de por mínimos cuadrados, el proceso de minimización del negativo de la suma del logaritmo de la verosimilitud es iterativo mediante un algoritmo de búsqueda. El más común es el algoritmo de búsqueda de Newton-Raphson (o Newton-Fourier). ¿A qué me refiero con iterativo? A que la computadora variará los parámetros hasta llegar a la solución “optima”:\n\n\n\n\n\n\nFigura 11.5: Ajuste iterativo de parámetros\n\n\n\n\n\nEsta salida fue un poco más simple que la salida de la función lm() pero, ¿fue de diferente la estimación? Si vemos los coeficientes de nuestro ajuste por mínimos cuadrados veremos que la estimación puntual es la misma, y los errores estándares son prácticamente iguales:\n\nsummary(reg1)$coefficients\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) -3.726537 0.73212521 -5.090027 1.805160e-06\nv1           1.177649 0.08140729 14.466134 9.513254e-26\n\n\nAdemás, el error estándar de la estimación por mínimos cuadrados es prácticamente igual al sigma de la estimación por máxima verosimilitud:\n\nsummary(reg1)$sigma\n\n[1] 3.095131\n\n\nMoraleja: no utilices estimación por máxima verosimilitud si vas a utilizar una distribución normal. Lo único que ganas es hacer pasos adicionales. ¿Cómo utilizar otras distribuciones? Eso lo veremos a detalle en el Capítulo 19.",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modelo lineal</span>"
    ]
  },
  {
    "objectID": "c11_rls.html#correlación-y-covarianza",
    "href": "c11_rls.html#correlación-y-covarianza",
    "title": "11  Modelo lineal",
    "section": "11.5 Correlación y covarianza",
    "text": "11.5 Correlación y covarianza\nSé que la sesión hasta este momento ha sido larga y tediosa, pero el tema de regresión merece entrar a la teoría para no obtener conclusiones equivocadas. Dejemos de lado esa parte y cerremos hablando de dos conceptos relacionados: la correlación y la covarianza.\nLa covarianza nos indica cómo varía una variable en relación a otra. La correlación describe la relación entre dos variables. Ya me imagino la expresión de confusión que tienes mientras te preguntas ¿entonces son lo mismo? Pues no, la diferencia es que la correlación es un índice; es decir, está contenida en el intervalo \\([-1, 1]\\), por lo que esta mide no solo la dirección, sino la “fuerza” o el grado de linealidad de la relación, donde 0 es una relación lineal nula. Matemáticamente la diferencia es que la correlación entre dos variables es su covarianza dividida entre el producto de sus desviaciones estándar:\n\\[\\begin{align*}\ncov(X,Y) = \\frac{\\Sigma_{i=1}^n(X_i-\\bar{X})(Y_i-\\bar{Y})}{n-1} \\\\\ncor(X,Y) = \\frac{cov(X,Y)}{\\sigma_x * \\sigma_y}\n\\end{align*}\\]\nEs decir, son conceptos que están muy relacionados entre sí. En ambos el signo del valor indica la dirección de la relación, mientras que la magnitud indica la fuerza de la relación. El problema con la covarianza es que no tiene límites, entonces no puedes saber si una covarianza de 500 es particularmente grande salvo que tengas otra covarianza con la cual comparar, mientras que una correlación de 0.7 es una correlación moderadamente fuerte.\nMuy seguramente por tu cabeza haya pasado la pregunta: “si ambas nos dicen cómo es la relación entre dos variables, ¿cuál es la diferencia con la regresión?”. Pues que la regresión es un modelo predictivo, mientras que la correlación/covarianza es un estadístico descriptivo. Aquí no estamos comprometiendo que haya una tasa de cambio de \\(X\\) hacia \\(Y\\), ni estamos interesados en qué valor de \\(X\\) le corresponde a \\(y = 0\\). Aquí no nos interesa predecir, sino describir. Si no quieres comprometerte con todo lo que implica un modelo predictivo (aún nos faltó ver el tema del sobre-ajuste), solo calcula el coeficiente de correlación correspondiente.\n¿Por qué correspondiente? Porque tenemos más de una forma de calcular la correlación, y cada una tiene sus bemoles. La ecuación que vimos arriba es para el coeficiente de correlación de Pearson, el cual elevado al cuadrado nos da el coeficiente de determinación que vimos antes. Como tal, es un coeficiente paramétrico y tiene algunos supuestos:\n\nVariables en escala de intervalo o razón\nRelación lineal entre ambas variables. Sí, asume que el cambio de una variable a otra es constante.\nNormalidad Ambas variables deben de tener una distribución aproximadamente normal, por lo que aquí sí nos interesa la distribución de nuestras variables.\nCada observación debe de tener el par de datos \\((v1, v2)\\).\n\nEstos supuestos, a estas alturas, son autoexplicativos. ¿Qué pasa si no cumplimos con alguno de los primeros tres? Podemos utilizar la alternativa no paramétrica: el coeficiente de correlación \\(\\rho\\) de Spearman. En este los supuestos son:\n\nVariables al menos en escala ordinal\nRelación monótona entre ambas variables; es decir, que la relación vaya en un solo sentido (conforme aumenta una aumenta la otra, o conforme aumenta la otra disminuye), independientemente de que la tasa de cambio de una a otra no sea constante.\nCada observación debe de tener el par de datos \\((v1, v2)\\)\n\nBastante más relajado, ¿no? Pero esto no quiere decir que solo debas de utilizar este coeficiente, utiliza el que más se ajuste a tus datos particulares.\n\n\n\n\n\n\nNota\n\n\n\nEn el Capítulo 12 vamos a hablar de las técnicas no paramétricas y sus ventajas y desventajas con respecto a las técnicas paramétricas.\n\n\nAhora bien, ¿cómo obtenemos estos coeficientes en R? Muy sencillo, con las funciones cor(X, Y) y cov(X, Y):\n\n\n\n\n\n\nNota\n\n\n\nEstos coeficientes son simétricos, por lo que asignar variables \\(x\\) y \\(y\\) como dependientes e independientes es un error. ¿Notaste que arriba todo lo puse en términos de \\(v1\\) y \\(v2\\)?\n\n\nPrimero, generemos un par de variables donde la segunda sea una función lineal de la primera:\n\ndf1 &lt;- data.frame(v1 = -20:20)\ndf1[\"v2\"] &lt;- (10+2*df1$v1)\ndf1\n\n\n  \n\n\n\nAhora obtengamos la covarianza:\n\ncovar &lt;- cov(df1$v1, df1$v2)\ncovar\n\n[1] 287\n\n\nY el índice de correlación de Pearson:\n\ncorre &lt;- cor(df1$v1, df1$v2)\ncorre\n\n[1] 1\n\n\nAquí queda también demostrado por qué la covarianza es tan difícil de interpretar por sí sola, pues en este caso con una relación lineal perfecta fue de 287, pero si cambias los valores de v1 de alguna manera vas a obtener otro valor, mientras que la correlación seguirá siendo 1. Gráficamente:\n\nggplot(data = df1, aes(x = v1, y = v2)) +\n  geom_point(colour = \"dodgerblue4\", alpha = 0.8) +\n  labs(title = \"Relación entre v2 y v1\") +\n  annotate(\"text\", x = 15, y = 0,\n           label = paste(\"R = \", round(corre, 2))) +\n  annotate(\"text\", x = 15, y = -5,\n           label = paste(\"Cov. = \", round(covar, 2))) +\n  see::theme_lucid()\n\n\n\n\n\n\n\n\n¿Qué pasa cuando nos alejamos de esta relación lineal?\n\ndf1[\"v3\"] &lt;- (-df1$v1^2)\ncorre &lt;- cor(df1$v1, df1$v3)\ncovar &lt;- cor(df1$v1, df1$v3)\n\nggplot(data = df1, aes(x = v1, y = v3)) +\n  geom_point(colour = \"dodgerblue4\", alpha = 0.8) +\n  labs(title = \"Relación entre v2 y v1\") +\n  annotate(\"text\", x = 15, y = 0,\n           label = paste(\"R = \", round(corre, 2))) +\n  annotate(\"text\", x = 15, y = -50,\n           label = paste(\"Cov. = \", round(covar, 2))) +\n  see::theme_lucid()\n\n\n\n\n\n\n\n\nOtro ejemplo:\n\ndf1[\"v4\"] &lt;- sin(df1$v1)\ncorre &lt;- cor(df1$v1, df1$v4)\ncovar &lt;- cor(df1$v1, df1$v4)\n\nggplot(data = df1, aes(x = v1, y = v4)) +\n  geom_point(colour = \"dodgerblue4\", alpha = 0.8) +\n  labs(title = \"Relación entre v2 y v1\") +\n  annotate(\"text\", x = 15, y = 3,\n           label = paste(\"R = \", round(corre, 2))) +\n  annotate(\"text\", x = 15, y = 2,\n           label = paste(\"Cov. = \", round(covar, 2))) +\n  expand_limits(y = c(5, -3)) +\n  see::theme_lucid()\n\n\n\n\n\n\n\n\nAunque todas estas relaciones pueden ser predichas, un coeficiente de correlación lineal no es capaz de capturarlas y, por definición, tampoco un modelo de regresión lineal. ¿Qué hacer en estos casos? Veremos algunas alternativas en el Capítulo 13. Ahora ejemplifiquemos el coeficiente de correlación de Spearman. Primero, y para dejar más claro el concepto, veamos la diferencia entre una relación monótona y una no monótona:\n\ndf2 &lt;- data.frame(v1 = df1$v1[df1$v1 &gt; 0], \n                  v2 = df1$v1[df1$v1 &gt; 0]^2,\n                  mono = \"monótona\")\n\ndf2 &lt;- rbind(df2, data.frame(v1= df1$v1, \n                             v2 = df1$v3, \n                             mono = \"no monótona\"))\n\nggplot(data = df2, aes(x = v1, y = v2)) +\n  geom_point(colour = \"dodgerblue4\", alpha = 0.8) +\n  facet_wrap(~mono, nrow = 2, scales = \"free_y\") +\n  see::theme_lucid() +\n  theme(aspect.ratio = 1/1.61)\n\n\n\n\n\n\n\n\nY ahora calculemos los coeficientes de correlación de Pearson y Spearman para la relación monótona. La diferencia fue de 0.3, lo cual no es muy grande, pero conforme nos empezamos a alejar de escenarios ideales el coeficiente de Pearson empieza a perder sensibilidad y confiabilidad.\n\npaste(\"Pearson = \",\n      round((cor(df2$v1[df2$mono == \"monótona\"],\n                 df2$v2[df2$mono == \"monótona\"], \n                 method = \"pearson\")), 2)\n      )\n\n[1] \"Pearson =  0.97\"\n\npaste(\"Spearman = \",\n      round((cor(df2$v1[df2$mono == \"monótona\"],\n                 df2$v2[df2$mono == \"monótona\"], \n                 method = \"spearman\")), 2)\n      )\n\n[1] \"Spearman =  1\"\n\n\nYa para cerrar, ¿son estos los únicos coeficientes de correlación? Para nada, tenemos también: \\(\\tau\\) de Kendall, \\(\\phi k\\) (Baak et al. (2019)), V de Cramer, Predictive Power Score y el coeficiente de Máxima Información (Reshef et al. (2011)). Te invito a que leas más sobre ellos, sus ventajas y sus desventajas. Predictive Power Score no es direccional, por ejemplo (Figura 11.6).\n\n\n\n\n\n\nFigura 11.6: Predictive Power Score en una relación parabólica\n\n\n\nPor fin llegamos al final de esta extensa (pero espero no aburrida) sesión. Espero que la hayas encontrado útil, y que te motive a hacer toda la chamba detrás de un modelo de predicción como lo es la regresión lineal simple.",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modelo lineal</span>"
    ]
  },
  {
    "objectID": "c11_rls.html#ejercicio",
    "href": "c11_rls.html#ejercicio",
    "title": "11  Modelo lineal",
    "section": "11.6 Ejercicio",
    "text": "11.6 Ejercicio\nAunque me encantaría que para esta sesión no hubiera ejercicio, es importante que practiques lo que discutimos aquí. El ejercicio que vas a realizar es realizar una regresión entre las variables LT (longitud total) y AM (altura máxima) de los datos de peces haemúlidos de los datos Haem.csv. Responde:\n\nLa regresión asume un cambio direccional entre las variables \\(X\\) y \\(Y\\); es decir, el cambio en una modifica a la otra. ¿Cuál es la variable dependiente? ¿LT o AM? ¿Por qué?\nCalcula la correlación entre ambas variables. ¿Qué coeficiente utilizas y por qué?\nRealiza la regresión lineal simple. ¿Qué método de ajuste utilizas y por qué?\nReporta los resultados de la regresión (incluyendo el gráfico correspondiente).\nRealiza la comprobación de los supuestos. ¿Es confiable el modelo para fines de predicción? ¿Y para interpretación?\n\n\n\n\n\nBaak M, Koopman R, Snoek H, Klous S. 2019. A new correlation coefficient between categorical, ordinal and interval variables with Pearson characteristics. ArXiV. DOI: 10.48550/arxiv.1811.11440.\n\n\nBox GE. 1976. Science and Statistics. Journal of the American Statistical Association 71:791-799. DOI: 10.1080/01621459.1976.10480949.\n\n\nBreusch TS, Pagan AR. 1979. A simple test for heteroscedasticity and random coefficient variation. Econometrica 47:1287-1294.\n\n\nGerrodette T. 2011. Inference without significance: measuring support for hypotheses rather than rejecting them. Marine Ecology 32:404-418. DOI: 10.1111/j.1439-0485.2011.00466.x.\n\n\nReshef DN, Reshef YA, Finucane HK, Grossman SR, McVean G, Turnbaugh PJ, Lander ES, Mitzenmacher M, Sabeti PC. 2011. Detecting Novel Associations in Large Datasets. Science 334:1518-1524. DOI: 10.1126/science.1205438.",
    "crumbs": [
      "Técnicas básicas",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modelo lineal</span>"
    ]
  },
  {
    "objectID": "s03_noparnolin.html",
    "href": "s03_noparnolin.html",
    "title": "Técnicas no paramétricas y modelación no lineal",
    "section": "",
    "text": "Objetivo de aprendizaje\nEn esta sección del curso te adentrarás en las caras “opuestas” a las técnicas que has revisado hasta ahora; es decir, explorarás casos en los cuales no puedes o debes de aplicar una prueba paramétrica, así como casos en los cuales no asumirás que tus variables tienen una relación lineal, ni tampoco necesitas cumplir con el supuesto de normalidad.",
    "crumbs": [
      "Técnicas no paramétricas y modelación no lineal"
    ]
  },
  {
    "objectID": "c12_nopar.html",
    "href": "c12_nopar.html",
    "title": "12  Técnicas no paramétricas",
    "section": "",
    "text": "12.1 Librerías\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(moments)\nlibrary(DescTools)\nlibrary(patchwork)\ntheme_set(see::theme_lucid() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()))",
    "crumbs": [
      "Técnicas no paramétricas y modelación no lineal",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Técnicas no paramétricas</span>"
    ]
  },
  {
    "objectID": "c12_nopar.html#introducción",
    "href": "c12_nopar.html#introducción",
    "title": "12  Técnicas no paramétricas",
    "section": "12.2 Introducción",
    "text": "12.2 Introducción\nHasta ahora habíamos tratado con pruebas y técnicas que requieren ciertas características, particularmente normalidad y homocedasticidad, para dar resultados confiables. Pero, ¿qué pasa cuando nuestros datos no se ajustan a una distribución normal o nuestras varianzas no son homogéneas? Una opción es recurrir a algún método de transformación de datos, pero eso lo abordaremos en otra ocasión. Hoy hablaremos sobre una familia de técnicas cuyos requerimientos sobre las distribuciones de los datos no son tan estrictos: Las pruebas NO paramétricas o libres de Distribución. Para explicar en qué consisten y cuales son sus diferencias me gustaría hacerlo en forma de una sere de preguntas y respuestas:\n\n¿Por qué se llaman no paramétricas? Estas técnicas reciben esos nombres debido a que no están basadas en los Parámetros de una Población (\\(\\mu\\) o \\(\\sigma^2\\)) y que en consecuencia no requieren que los conozcamos o estimemos. También podemos decir que son no paramétricas porque no trabajan con los parámetros de ninguna distribución, sino que trabajan con la distribución empírica de los datos; es decir, con la distribución propia de los datos.\n¿Por qué no aplicarlas siempre? Una deducción rápida puede ser que, si estas pruebas no requieren de características especiales de nuestros datos, podemos utilizarlas siempre y dejar de lado las pruebas paramétricas. Esto se debe a que en general, las técnicas paramétricas son más precisas y tienen un mayor poder estadístico; es decir, que son mejores para encontrar un efecto realmente significativo (son más sensibles).\n¿En qué casos aplicarlas?\n\nCuando sepamos que la distribución de la variable en la población no es normal. Esta parte tiene sus bemoles porque el cubrir este único requisito dependerá de le exhaustividad de nuestro muestreo y si es realmente representativo de lo que sucede a un nivel poblacional. Dejando el tecnicismo de lado, recurriremos a estas técnicas en el mismo escenario en el que aplicaríamos alguna transformación a los datos: cuando tengamos evidencia lo suficientemente grande de que nuestros datos (muestras) no se ajustan a una distribución normal. Como ya habíamos comentado antes debemos sustentar nuestras conclusiones a partir de diversas evidencias y no solo un valor de p. Un análisis gráfico de la distribución de frecuencias (gráfico QQ, KDE o un gráfico de frecuencias), acompañado de una prueba de normalidad (e.g. Shapiro-Wilks) deberían ser lo mínimo realizable para evaluar la normalidad de nuestros datos. Adicionalmente podemos estimar la curtosis (altura de la curva) y el sesgo (simetría de la distribución) y ver qué tan diferentes son de el valor de 1 de una distribución normal.\nOtros casos a tener en cuenta son:\n\nNuestros datos están en escala nominal u ordinal.\nEl tamaño de muestra es muy pequeño para realizar una prueba paramétrica. ¿Qué tan pequeño? menos de 10-15 datos por muestra. Ën este punto es importante mencionar que con un n&lt;5 no es posible identificar desviaciones de una distribución normal.\nEl objetivo es realizar inferencias alrededor de la mediana y no de la media (útil con distribuciones muy sesgadas).\n\n\n¿Qué pasa si, aún violando los supuestos, empleo una prueba paramétrica? En general, la consecuencia es que se obtienen resultados no confiables. Para ejemplificarlo tomemos al Análisis de la Varianza (ANOVA) paramétrico. Recordemos que en este análisis comparamos la Varianza dentro de los grupos contra la Varianza entre los grupos, por lo que si uno o más grupos presentan desviaciones muy importantes de la normalidad el resultado será un análisis sesgado, causado por las diferencias en la distribución global de frecuencias. Adicionalmente, si no cumplimos con el supuesto de homogeneidad de varianzas, el resultado será que la varianza dentro de uno o más grupos será mayor y, en consecuencia, la razón de las mismas tenderá a ser más pequeña y el resultado será un mayor nivel de significancia (valor de p).\n\n\n\n\n\n\n\nFigura 12.1: Paramétrico vs. no paramétrico\n\n\n\nHabiendo tocado esos puntos, exploremos las alternativas no paramétricas a algunas de las pruebas que hemos visto en el curso.\nEn esta sesión utilizaremos los archivos Datos1 2.csv, Datos_bloques.csv y Abundancias.csv, por lo que primero tendremos que cargarlos:\n\n# Carga de datos\nguppys &lt;- read.csv(\"datos/Datos1 2.csv\",\n               header = F,\n               skip = 1,\n               sep = \",\") |&gt; select(1:5)\ncolnames(guppys) &lt;- c(\"Dieta\", \"Periodo\", \"Rep\", \"LT\", \"PT\")\nhead(guppys)\n\n\n  \n\n\n\n\nbloques &lt;- read.csv(\"datos/Datos_bloques.csv\")\nhead(bloques)\n\n\n  \n\n\n\n\nabundancias &lt;- read.csv(\"datos/Abundancias.csv\")\nabundancias\n\n\n  \n\n\n\nComencemos entonces a explorar algunas de las distintas herramientas que tenemos a nuestra disposición.",
    "crumbs": [
      "Técnicas no paramétricas y modelación no lineal",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Técnicas no paramétricas</span>"
    ]
  },
  {
    "objectID": "c12_nopar.html#comparación-de-muestras-independientes-u-de-mann-whitney",
    "href": "c12_nopar.html#comparación-de-muestras-independientes-u-de-mann-whitney",
    "title": "12  Técnicas no paramétricas",
    "section": "12.3 Comparación de muestras independientes: U de Mann-Whitney",
    "text": "12.3 Comparación de muestras independientes: U de Mann-Whitney\nTambién llamada prueba de Mann-Whitney-Wilcoxon o prueba de suma de rangos de Wilcoxon. Esta prueba es la alternativa a la prueba T de Student para muestras independientes, en donde la comparación se realiza en términos de las medianas (\\(\\tilde{x}\\)) y por tanto la hipótesis nula será \\(H_0: \\tilde{X}_1 = \\tilde{X}_2\\).\nAntes de pasar al modo de calcular el estadístico de prueba, hablemos sobre las consideraciones que debemos de tener para aplicar esta prueba:\n\nLa variable dependiente deberá de estar en escala ordinal o continua.\nLa variable independiente es una variable categórica con dos grupos.\nLas muestras deberán ser independientes; es decir, no deberá de existir una relación entre o dentro de los grupos (medidas repetidas)\nAunque los datos no se ajusten a una distribución normal, deberán de tener la misma forma (ambos con forma de campana y sesgo negativo, o ambos con sesgo positivo, etc.)\n\nEl estadístico de prueba es el estadístico U, el cual será el valor mínimo de alguno de los siguientes cálculos, donde R es la suma de rangos. El rango de cada dato se obtiene ordenando todos los datos de mayor a menor, teniendo cuidado de no perder a qué grupo pertenece cada uno de ellos, y numerarlos de 1 hasta el número de datos.\n\n\\(U_1 = R_1 - \\frac{n_1(n_1+1)}{2}\\)\n\\(U_2 = R_2 - \\frac{n_2(n_2 +1)}{2}\\)\n\nPara aplicarla en R utilizaremos la función wilcox.test(formula), pero primero exploremos nuestros datos. Nuestra variable de interés es la variable LT, por lo que realizaremos la evaluación de la normalidad, tanto de manera cuantitativa (prueba de Shapiro-Wilks, sesgo y curtosis), como de manera gráfica (Gráficos de densidad). Generemos una función que nos permita reunir estos datos en una sola tabla de manera rápida:\n\nguppys |&gt; group_by(Dieta) |&gt; rstatix::shapiro_test(LT)\n\n\n  \n\n\n\n\nnorms &lt;- data.frame(Dieta = NA, k = NA, s = NA)\ngrupos &lt;- unique(guppys$Dieta)\n\nfor (i in 1:length(grupos)) {\n  sub_set &lt;- na.omit(guppys$LT[guppys$Dieta == grupos[i]])\n\n  norms[i,] &lt;- c(as.character(grupos[i]),\n                 round(kurtosis(sub_set), 2),\n                 round(skewness(sub_set), 2)\n                 )\n}\nnorms\n\n\n  \n\n\n\nY gráficamente:\n\nkde_plots &lt;- ggplot(data = guppys, aes(LT, color = Dieta)) + \n             geom_density(show.legend = F) + \n             facet_wrap(~Dieta, nrow = 2) +\n             labs(title = \"Longitud Total de Guppys bajo tres dietas\",\n                  subtitle = \"Gráficos de densidad\",\n                  caption = \"Archivo: Datos1 2.csv\",\n                  x = element_blank(),\n                  y = element_blank()) +\n             scale_y_continuous(breaks = NULL)\nkde_plots\n\n\n\n\n\n\n\n\nDebido a que todas las distribuciones fueron leptocúrticas (k &gt; 1) y con sesgo negativo (s &lt; 1), todas las pruebas de normalidad tuvieron niveles de significancia pequeños y los gráficos KDE mostraron tendencias hacia la bimodalidad, concluimos que los datos no se ajustan a una distribución normal, ¿no? Sí y no. En efecto la distribución de cada grupo no es normal, pero recordarás que la decisión del tipo de prueba a aplicar no está en función de la distribución de cada grupo, sino de la normalidad de los residuales del modelo ANOVA.\n\n\n\n\n\n\nNota\n\n\n\nEsto era de esperarse debido a que hay una variable más de agrupamiento: el periodo; es decir, que hay información del inicio del experimento un periodo intermedio y un periodo final, solo se realiza este ejemplo con fines de enseñanza.\n\n\nImaginemos que nos interesa conocer si los organismos alimentados con la dieta B tuvieron una longitud total diferente a los alimentados con la dieta C, entonces apliquemos una prueba U-Mann-Whitney.\n\nu_data &lt;- subset(guppys, Dieta != \"A\")\nwilcox.test(LT~factor(Dieta), data = u_data)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  LT by factor(Dieta)\nW = 1408, p-value = 0.223\nalternative hypothesis: true location shift is not equal to 0\n\n\nEste resultado corresponde bastante bien con lo visto en los gráficos de densidad en el sentido de falta de diferencias, aunque podemos representarlo utilizando un gráfico de violín que es más adecuado para propósitos de comparación:\n\nu_mann_plot &lt;- ggplot(data = u_data, aes(x = Dieta, y = LT, fill = Dieta)) + \n               geom_violin(trim = T,\n                           alpha = 0.7,\n                           show.legend = F) + \n               geom_boxplot(width = 0.1,\n                            fill = \"white\",\n                            notch = T,\n                            show.legend = F) + \n               labs(title = \"Comparación de LT de guppys bajo las dietas B y C\",\n                    subtitle = \"Gráficos de violín\",\n                    caption = \"Archivo: Datos1 2.csv\",\n                    x = element_blank(),\n                    y = element_blank()\n                    ) +\n               scale_fill_manual(values = c(rgb(29,149,79,\n                                                maxColorValue = 255),\n                                            rgb(126,172,255,\n                                                maxColorValue = 255))\n                                 )\nu_mann_plot\n\n\n\n\n\n\n\n\n\n12.3.1 Comparación de dos muestras dependientes (pareadas): Prueba por rangos de Wilcoxon\nOtra pregunta que podemos estar interesados en responder con esta base de datos es si las Longitudes Totales al inicio del experimento fueron iguales o diferentes al final del experimento en la dieta A y asumamos, para fines de enseñanza, a) que los datos a este nivel no siguen una distribución normal y b) que tenemos mediciones pareadas; es decir, que la medición 1 del periodo inicial corresponde al mismo individuo que la medición 1 del periodo final. Bajo estos supuestos, realizaremos la prueba de rangos de Wilcoxon.\nEsta prueba es la alternativa no paramétrica a la prueba t para datos pareados y, al igual que en el caso de la prueba U de Mann-Whitney, compara si las medianas entre ambos grupos son diferentes entre sí. Por otra parte, y del mismo modo que la prueba t para datos pareados, el cálculo se realiza a partir de las diferencias entre ambos grupos, asignándoles un rango y luego sumarlas según su signo, tal que:\n\n\\(W^+ = \\sum R^+\\)\n\\(W^- = \\sum R^-\\)\n\nDonde R es el rango de cada diferencia y \\(+\\) y \\(-\\) representan su signo. El valor que utilizaremos durante la prueba será el más pequeño de los dos. La prueba de hipótesis se realiza utilizando la distribución normal, tal que:\n\n\\(Z = \\frac{min(W) - \\mu}{\\sigma - r}\\)\n\nDonde: \\(\\mu = \\frac{n(n+1)}{4}\\), \\(\\sigma = \\frac{\\sqrt{n(n+1)(2n+1)}}{24}\\) y r es un factor de reducción cuando existen t rangos empatados: \\(r = \\frac{t^3-t}{48}\\).\nEn R, por fortuna, la implementación es mucho más sencilla, solo hay que considerar que al ser una prueba pareada, el número de datos en ambos grupos debe ser igual.\n\n# Extraemos solo los datos que nos interesan\nwilcox_data &lt;- subset(guppys, (Dieta == \"A\" & Periodo != \"M\"))\n# Vemos que al final tenemos un individuo menos que al inicio,\n# por lo que hay que retirarlo de la base de datos (asumamos que es el último)\naggregate(LT~Periodo, data = wilcox_data, length)\n\n\n  \n\n\nwilcox_data &lt;- wilcox_data[-c(20),]\n#Ahora tenemos ns iguales, por lo que podemos aplicar la prueba pareada\naggregate(LT~Periodo, data = wilcox_data, length)\n\n\n  \n\n\n\nAhora apliquemos la prueba.\n\nwilcox.test(LT~Periodo, data = wilcox_data, paired = T)\n\n\n    Wilcoxon signed rank exact test\n\ndata:  LT by Periodo\nV = 190, p-value = 3.815e-06\nalternative hypothesis: true location shift is not equal to 0\n\n\nAl parecer las medianas son diferentes, lo cual se sostiene al analizar los resultados de manera gráfica:\n\nw_rank_plot &lt;- ggplot(data = wilcox_data,\n                      aes(x = Periodo, y = LT, fill = Periodo)) + \n               geom_violin(trim = T,\n                           alpha = 0.7,\n                           show.legend = F) + \n               geom_boxplot(width = 0.1,\n                            fill = \"white\",\n                            notch = T,\n                            show.legend = F) + \n               labs(title = \"Comparación de LT de guppys en dos periodos\",\n                    subtitle = \"Gráficos de violín\",\n                    caption = \"Archivo: Datos1 2.csv\",\n                    x = element_blank(),\n                    y = element_blank()\n                    ) +\n               scale_fill_manual(values = c(rgb(29,149,79,\n                                                maxColorValue = 255),\n                                            rgb(126,172,255,\n                                                maxColorValue = 255))\n                                 )\nw_rank_plot",
    "crumbs": [
      "Técnicas no paramétricas y modelación no lineal",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Técnicas no paramétricas</span>"
    ]
  },
  {
    "objectID": "c12_nopar.html#prueba-por-rangos-de-friedman-y-coeficiente-de-acuerdo-de-kendall",
    "href": "c12_nopar.html#prueba-por-rangos-de-friedman-y-coeficiente-de-acuerdo-de-kendall",
    "title": "12  Técnicas no paramétricas",
    "section": "12.4 Prueba por Rangos de Friedman y coeficiente de acuerdo de Kendall",
    "text": "12.4 Prueba por Rangos de Friedman y coeficiente de acuerdo de Kendall\nAntes de hablar de las siguientes dos técnicas, hablemos sobre los bloques aleatorizados completos. Este caso se da cuando contamos con un factor adicional a nuestros factores de interés. En este caso realizaremos bloques para poder reducir el error experimental derivado de dichos factores. Un ejemplo es tener una serie de votantes para evaluar una serie de aspectos, o evaluar la actividad proteolítica en distintas secciones del tracto digestivo de distintos peces, o diferentes mediciones de fósforo de distintas muestras realizadas por distintas personas.\n\n\n\n\n\n\nFigura 12.2: Bloques aleatorizados completos\n\n\n\nEn este sentido, la Prueba por Rangos de Friedman nos permite analizar a la vez las diferencias entre grupos, según los bloques. Para ello asignaremos rangos a las observaciones de cada bloque y compararemos el promedio de los rangos de cada grupo con respecto a la media y la varianza de los rangos de manera global. Pausa el video un momento, analiza la ecuación de \\(F_R\\) y trata de adivinar qué tipo de distribución sigue \\(F_R\\). Una pista: esa distribución ya fue vista durante el curso. La respuesta es una distribución \\(\\chi^2\\).\nOk, una vez que realizamos esos cálculos ya evaluamos las diferencias entre los grupos tomando en consideración a los bloques, pero ¿qué tan similares son estos bloques? o en otras palabras, ¿qué tan de acuerdo estuvieron los votantes? Para ello podemos utilizar el Coeficiente de acuerdo de Kendall (W), el cuál está en términos del error cuadrático de la asignación de rangos dentro de cada bloque y el número de bloques y grupos. W indica la magnitud del acuerdo, donde 0 representa total desacuerdo y 1 total acuerdo.\nAhora vayamos a R y apliquemos estas técnicas a nuestros datos por bloques. Para ello necesitaremos cambiar el formato de la base de datos de una base de datos tabular a una base de datos codificada utilizando la función melt():\n\nbloques_molten &lt;- reshape2::melt(bloques,\n                                 id.vars = \"Pez\",\n                                 variable.name = \"Secc\",\n                                 value.name = \"Act\")\nbloques_molten\n\n\n  \n\n\n\nAhora podemos aplicar la prueba de Friedman utilizando la función friedman.test(a ~ b|c), donde a representa la variable de medición, b los grupos y c los bloques.\n\nfriedman.test(Act~Secc|Pez, data = bloques_molten)\n\n\n    Friedman rank sum test\n\ndata:  Act and Secc and Pez\nFriedman chi-squared = 4.7586, df = 3, p-value = 0.1903\n\n\nRealicemos la comparación gráficamente:\n\nfried_plot &lt;- ggplot(data = bloques_molten,\n                     aes(x = Secc, y = Act, fill = Secc)) + \n              geom_violin(trim = T,\n                           alpha = 0.7,\n                           show.legend = F) + \n               geom_boxplot(width = 0.1,\n                            fill = \"white\",\n                            notch = F,\n                            show.legend = F) +\n              labs(title = \n                     \"Actividad proteolítica entre 4 secciones del tracto\n                   digestivo de peces\",\n                  # subtitle = \"Gráficos de violín\",\n                  caption = \"Archivo: Datos1 2.csv\",\n                  x = element_blank(),\n                  y = element_blank()\n                  )\nfried_plot\n\n\n\n\n\n\n\n\nDe estos resultados podríamos concluir que no hubo diferencias entre la actividad proteolítica entre las distintas secciones de los tractos digestivos, pero ¿qué tan diferentes entre sí fueron los peces? Utilicemos el coeficiente de acuerdo de Kendall utilizando la función KendallW(x, correct, test, na.rm) de la librería DescTools donde x representa una matriz de k grupos (columnas) y m bloques (renglones), correct es un argumento booleano si se debe de realizar la corrección por empates en los rangos y test para realizar una prueba \\(\\chi^2\\) con \\(H_0: W = 0\\); es decir, si hubo o no un desacuerdo total entre los bloques.\n\n#Esta función sí requiere los datos en forma tabular, por lo que x será nuestra variable bloques\nDescTools::KendallW(bloques, correct = T, test = T)\n\n\n    Kendall's coefficient of concordance Wt\n\ndata:  bloques\nKendall chi-squared = 7.5578, df = 8, subjects = 9, raters = 5, p-value\n= 0.4778\nalternative hypothesis: Wt is greater 0\nsample estimates:\n       Wt \n0.1889447 \n\n\nY comprobémoslo gráficamente (ojo, para graficar estamos utilizando la base codificada)\n\nkendall_plot &lt;- ggplot(data = bloques_molten,\n                       # OJO: Estamos transformando Pez a un factor\n                       aes(x = as.factor(Pez),\n                           y = Act,\n                           fill = as.factor(Pez))) +\n                geom_violin(trim = T,\n                           alpha = 0.7,\n                           show.legend = F) + \n               geom_boxplot(width = 0.1,\n                            fill = \"white\",\n                            notch = F,\n                            show.legend = F) +\n                labs(title = \n                       \"Actividad proteolítica de 9 peces\",\n                    # subtitle = \"Gráficos de violín\",\n                    caption = \"Archivo: Datos_bloques.csv\",\n                    x = element_blank(),\n                    y = element_blank()\n                    )\nkendall_plot",
    "crumbs": [
      "Técnicas no paramétricas y modelación no lineal",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Técnicas no paramétricas</span>"
    ]
  },
  {
    "objectID": "c12_nopar.html#anova-no-paramétrico-kruskall-wallis",
    "href": "c12_nopar.html#anova-no-paramétrico-kruskall-wallis",
    "title": "12  Técnicas no paramétricas",
    "section": "12.5 ANOVA no paramétrico: Kruskall-Wallis:",
    "text": "12.5 ANOVA no paramétrico: Kruskall-Wallis:\nEsta vez nos presentan la pregunta: ¿hubo diferencias entre las longitudes finales de los guppys, según la dieta con la que fueron alimentados?\nPara responder a esta pregunta podríamos utilizar un Análisis de la Varianza de una vía; sin embargo, recordemos que estamos asumiendo que se viola el supuesto de normalidad de los datos a ese nivel y, por tanto, habrá que realizar una prueba no paramétrica: ANOVA de Kruskal-Wallis. Esta prueba es una extensión de la prueba U de Mann-Whitney a más grupos, por lo tanto también trabajará con la suma de rangos de los datos y el procedimiento es muy similar:\n\nAsignar rangos a los datos unidos (sin perder el grupo)\nRealizar la suma de rangos de cada grupo y compararlas\n\nAl igual que en otras pruebas que hemos visto el día de hoy, la distribución de prueba es la distribución \\(\\chi^2\\) con \\(k-1\\) grados de libertad.\nUna vez que realizamos la prueba solo obtendremos si hubo diferencias en alguna de las medianas, pero no sabremos cuál o cuáles fueron diferentes; por lo tanto, habrá que realizar una prueba post-hoc: la prueba de comparaciones múltiples de Dunn, dada por:\nLa implementación de ambas pruebas en R es sumamente sencilla. Primero, el ANOVA Kruskal-Wallis utilizando la función kruskal.test(formula, data):\n\nH &lt;- kruskal.test(LT~Dieta, data = subset(guppys, Periodo == \"F\"))\nH\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  LT by Dieta\nKruskal-Wallis chi-squared = 3.975, df = 2, p-value = 0.137\n\n\nVemos que aparentemente no hay diferencias entre las medias; sin embargo, realicemos la prueba de Dunn para confirmar. Para ello utilizaremos la función DunnTest(formula, data) de la librería DescTools. Vemos que en efecto, pareciera no haber diferencias entre las longitudes finales de los guppys en función de las dieta con la que fueron alimentados.\n\nDescTools::DunnTest(LT~factor(Dieta),\n                    data = subset(guppys, Periodo == \"F\"))\n\n\n Dunn's test of multiple comparisons using rank sums : holm  \n\n    mean.rank.diff   pval    \nB-A      -7.105263 0.2578    \nC-A      -9.327485 0.2578    \nC-B      -2.222222 0.6851    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCorroboremos esto de manera gráfica:\n\nkruskal_plot &lt;- ggplot(data = subset(guppys, Periodo == \"F\"), \n                       aes(x = Dieta, y = LT, fill = Dieta)) +\n                geom_violin(trim = T,\n                           alpha = 0.7,\n                           show.legend = F) + \n               geom_boxplot(width = 0.1,\n                            fill = \"white\",\n                            notch = T,\n                            show.legend = F) + \n                labs(title = \"Comparación de LT finales de Guppys\",\n                    subtitle = \"Gráficos de violín\",\n                    caption = \"Archivo: Datos1 2.csv\",\n                    x = element_blank(),\n                    y = element_blank())\nkruskal_plot",
    "crumbs": [
      "Técnicas no paramétricas y modelación no lineal",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Técnicas no paramétricas</span>"
    ]
  },
  {
    "objectID": "c12_nopar.html#anova-de-dos-vías-no-paramétrico-scheirer-ray-hare",
    "href": "c12_nopar.html#anova-de-dos-vías-no-paramétrico-scheirer-ray-hare",
    "title": "12  Técnicas no paramétricas",
    "section": "12.6 ANOVA de dos vías no paramétrico: Scheirer-Ray-Hare",
    "text": "12.6 ANOVA de dos vías no paramétrico: Scheirer-Ray-Hare\nLa última prueba que veremos en esta sesión es la versión no paramétrica para problemas en los que tenemos dos factores y muestras independientes; i.e., la alternativa no paramétrica al ANOVA de dos vías. Para este caso cambiemos ligeramente los datos y utilicemos datos de concentraciones de Alanina (Ala) en milípedos, evaluadas por sexo y dos especies:\n\nmili &lt;- read.csv(\"datos/milipedos.csv\")\nhead(mili)\n\n\n  \n\n\n\nLos datos ya se encuentran en formato largo, pero veamos primero cuántas observaciones tenemos por cada combinación de Especie:Sexo:\n\nmili |&gt; group_by(Especie, Sexo) |&gt; summarise(Ala = n())\n\n`summarise()` has grouped output by 'Especie'. You can override using the\n`.groups` argument.\n\n\n\n  \n\n\n\nEl diseño es balanceado, sin embargo tenemos pocas muestras por grupo. Este problema es más de representatividad y qué tan válidas son las inferencias realizadas sobre tan pocos datos porque, si comprobamos nuestros supuestos, todo parece estar en orden:\n\nmili_aov &lt;- aov(Ala~Especie*Sexo, data = mili)\n# performance::check_model(mili_aov)\nperformance::check_normality(mili_aov)\n\nOK: residuals appear as normally distributed (p = 0.240).\n\nperformance::check_homogeneity(mili_aov)\n\nOK: There is not clear evidence for different variances across groups (Bartlett Test, p = 0.976).\n\n\nVeamos entonces el resultado de la prueba:\n\nsummary(mili_aov)\n\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nEspecie       2  55.26   27.63  13.082  0.00031 ***\nSexo          1 138.72  138.72  65.679 2.04e-07 ***\nEspecie:Sexo  2   6.89    3.45   1.631  0.22331    \nResiduals    18  38.02    2.11                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nConcluimos que, estadísticamente y a un \\(\\alpha = 0.05\\), parece haber diferencias significativas entre especies y entre sexos, y que la diferencia entre especies no depende del sexo (o viceversa). Podríamos confiar “ciegamente” en estos resultados y aplicar la prueba post-hoc correspondiente para las especies; sin embargo, veamos qué nos dice la prueba no paramétrica:\n\nmili_srh &lt;- rcompanion::scheirerRayHare(Ala~Especie+Sexo, data = mili)\n\n\nDV:  Ala \nObservations:  24 \nD:  0.9995652 \nMS total:  50 \n\nmili_srh\n\n\n  \n\n\n\nSe mantienen las diferencias entre los sexos, pero las diferencias entre especies no están en conflicto con nuestra hipótesis de nulidad a un \\(\\alpha = 0.05\\). ¿Qué prueba utilizar? Veamos la distribución de nuestros datos:\n\nala_sp_plot &lt;- ggplot(data = mili,\n                      aes(x = Especie, y = Ala,\n                          fill = Especie,\n                          color = Especie)) +\n               geom_violin(alpha = 0.2) +\n               geom_boxplot(fill = NA,\n                            width = 0.1)\nala_sx_plot &lt;- ggplot(data = mili,\n                      aes(x = Sexo, y = Ala,\n                          fill = Sexo,\n                          color = Sexo)) +\n               geom_violin(alpha = 0.2) +\n               geom_boxplot(fill = NA,\n                            width = 0.1)\nala_int_plot &lt;- ggplot(data = mili,\n                       aes(x = Sexo, y = Ala,\n                           fill = Sexo,\n                           color = Sexo)) +\n                geom_violin(alpha = 0.2) +\n                geom_boxplot(fill =NA,\n                             width = 0.1) +\n                labs(y = \"[Ala] (mg/L)\") +\n                theme(axis.text.x = element_text(angle = 90)) +\n                facet_wrap(~Especie)\n\n# Crear gráfico compuesto utilizando Patchwork:\n# Gráfico de interacción a la izquierda\n# Efectos principales a la derecha\n(ala_int_plot|(ala_sp_plot / ala_sx_plot) &\n    # Efectos principales sin etiqueta del eje y\n    labs(y = element_blank())) &\n  theme(legend.position = \"none\") &\n  labs(x = element_blank())\n\n\n\n\n\n\n\n\nComo era de esperarse, las diferencias entre sexos son las más marcadas: la concentración de alanina es consistentemente mayor en machos que en hembras, independientemente de la especie (no hay interacción). ¿Qué pasa entre las especies? Sin duda alguna estamos en la cuerda floja; sin embargo, al menos con los datos disponibles, la especie 2 tiende hacia los valores más bajos.\nLa pregunta que debemos de hacernos aquí es: ¿biológicamente tienen relevancia las diferencias observadas? La decisión dependerá totalmente de la experiencia del investigador con el problema. Fuera de este ejemplo no he tratado con datos de concentraciones de Alanina, ni tampoco he estudiado milípedos, por lo que no puedo dar una conclusión informada. ¿Entonces? Prefiero errar de precavido y decir que las diferencias entre especies no son significativas (especialmente considerando que las diferencias entre sexos son mayores) y hacer la recomendación de replicar el experimento con un tamaño de muestra más grande (OJO, no incrementar el tamaño de muestra, eso se conoce como p-hacking).\nCon esto llegamos al final de esta clase. Espero que haya sido de tu agrado y nos vemos en la siguiente.",
    "crumbs": [
      "Técnicas no paramétricas y modelación no lineal",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Técnicas no paramétricas</span>"
    ]
  },
  {
    "objectID": "c13_nolin.html",
    "href": "c13_nolin.html",
    "title": "13  Modelos no lineales",
    "section": "",
    "text": "13.1 Librerías\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(performance)\nlibrary(nlraa)",
    "crumbs": [
      "Técnicas no paramétricas y modelación no lineal",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Modelos no lineales</span>"
    ]
  },
  {
    "objectID": "c13_nolin.html#otro-tipo-de-relaciones",
    "href": "c13_nolin.html#otro-tipo-de-relaciones",
    "title": "13  Modelos no lineales",
    "section": "13.2 Otro tipo de relaciones",
    "text": "13.2 Otro tipo de relaciones\nEn las sesiones anteriores hablamos de modelos en los cuales asumimos que hay una relación lineal entre nuestras variables; es decir, una variable incrementa o disminuye de manera constante en relación a la otra, pero también mencionamos que esto no siempre es así. Aquí es justo donde entra la modelación no lineal. En un problema de regresión lineal simple utilizamos alguna función de pérdida y un algoritmo de optimización para encontrar los valores del intercepto y la pendiente, según nuestros datos. La regresión no lineal es, en escencia, lo mismo: ajustar parámetros de un modelo a partir de los datos. La única diferencia es que el modelo puede describir cualquier tipo de relación funcional entre las variables involucradas. Puede ser algo tan simple como un modelo potencial de un solo parámetro hasta algo tan complicado como el modelo de mortalidad de Chen & Watanabe (1989), o incluso más.\nAplicaciones biológicas de este tipo de modelos hay muchas: a) estimación de la riqueza específica de un sitio, b) modelar el crecimiento de organismos, c) relaciones talla-peso, etc., etc., etc. En esta sesión revisaremos algunos ejemplos de estos modelos y cómo aplicarlos e interpretarlos.\nSobre esto último, implementar estos modelos en R es sumamente sencillo. Los requerimientos son a) conocer la ecuación, b) escribirla como una operación y c) establecer algunos valores iniciales para iniciar la búsqueda. Para esto utilizaremos la función nls(formula, data, start) (Nonlinear Least Squares).",
    "crumbs": [
      "Técnicas no paramétricas y modelación no lineal",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Modelos no lineales</span>"
    ]
  },
  {
    "objectID": "c13_nolin.html#violación-del-supuesto-de-linealidad",
    "href": "c13_nolin.html#violación-del-supuesto-de-linealidad",
    "title": "13  Modelos no lineales",
    "section": "13.3 Violación del supuesto de linealidad",
    "text": "13.3 Violación del supuesto de linealidad\nTenemos dos razones básicas que nos pueden llevar a pensar en un modelo no lineal. La primera es que, de entrada, sepamos que nuestros datos deben de ser modelados de esa manera, principalmente debido a conocimiento previo del problema. La segunda es que se viole el supuesto de linealidad de los residuales. La primera es auto-explicativa, por lo que obviaré el escenario, pero la segunda no lo es tanto. Revisemos entonces un primer modelo para describir la longitud de mandíbulas de tiburones a partir de la edad de los mismos:\n\njaws &lt;- read.csv(\"datos/jaws.csv\")\nlm_jaws &lt;- lm(bone~age, data = jaws)\nperformance::check_model(lm_jaws)\n\n\n\n\n\n\n\n# ggsave(\"jaws_check.pdf\", height = 10, width = 6)\n\nPodemos ver que el supuesto de linealidad no se sostiene, y en consecuencia tenemos una pobre capacidad predictiva en el posterior predictive check. Si vemos nuestros datos:\n\njaws_plot &lt;- ggplot(data = jaws, aes(x = age, y = bone)) +\n             geom_point(color = \"dodgerblue4\") +\n             theme_bw() +\n             labs(title = \"Modelo lineal\",\n                  x = element_blank(),\n                  y = element_blank())\nlm_plot &lt;- jaws_plot + geom_smooth(method = \"lm\",\n                                   color = \"#ff7f0e\")\nlm_plot\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nNi por asomo nuestros residuales están cerca de la linea en 0, de hecho, forman una especie de “parábola”. Esto quiere decir que debemos de considerar otra aproximación, pues la descripción del modelo lineal no es adecuada. Ok, entonces ¿qué modelo aplicamos? Sabemos que son datos de edades y longitudes de mandíbulas, por lo que habrá que echar mano de algún modelo de crecimiento.",
    "crumbs": [
      "Técnicas no paramétricas y modelación no lineal",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Modelos no lineales</span>"
    ]
  },
  {
    "objectID": "c13_nolin.html#modelos-de-crecimiento",
    "href": "c13_nolin.html#modelos-de-crecimiento",
    "title": "13  Modelos no lineales",
    "section": "13.4 Modelos de crecimiento",
    "text": "13.4 Modelos de crecimiento\nEstos modelos se pueden construir con cualquier medida de longitud o peso, y en este caso utilizaremos longitudes de huesos de una especie de tiburón. Modelos para el crecimiento hay muchos, algunos son Gompertz (1832), crecimiento logístico, Morgan-Mercer-Flodin, Weibull o Richards, de los cuales algunos tienen variantes con más o menos parámetros. Te recomiendo ampliamente revisar este enlace para conocer un poco más sobre estos modelos (ecuaciones, aplicaciones, supuestos, etc.). Por desgracia, no tenemos tiempo suficiente para hablar de todos ellos, así que exploraremos el más conocido: el modelo de crecimiento individual de Von Bertalanffy.\n\n13.4.1 Modelo de Von Bertalanffy (1938)\nEste modelo lo derivó Von Bertalanffy (VB) desde algunos parámetros fisiológicos bastante simples. Es, posiblemente, el modelo de crecimiento más utilizado en estudios de pesquerías. Este modelo asume que la tasa de crecimiento de un organismo declina con la tasa de cambio de la longitud, y que puede ser descrito con el modelo:\n\\[\n\\frac{dl}{dt} = K(L_\\infty - l)\n\\]\ndonde \\(t\\) es el tiempo, \\(l\\) es la longitud (o cualquier otra medida de tamaño), \\(K\\) es la tasa de crecimiento, \\(L_\\infty\\), que representa la longitud asintótica a la que el crecimiento es 0. Si integramos esta ecuación, obtenemos:\n\\[\nL(t) = L_\\infty(1-e^{-K(t-t_0)})\n\\]\nEn donde el parámetro \\(t_0\\) se incluye para ajustar la ecuación para la talla inicial del organismo, y se interpreta como la edad a la cual el organismo tendría tamaño 0. Este modelo, entonces, consta de 3 parámetros (\\(L_\\infty\\), \\(K\\), y \\(t_0\\)); sin embargo, existe una parametrización alternativa que cambia \\(t_0\\) por \\(L_0\\); es decir, la talla del individuo al nacer:\n\\[\nL(t) = L_\\infty - (L_\\infty - L_0)e^{-Kt}\n\\]\nUtilicemos la primera variante por conveniencia al declarar el modelo:\n\nvb_jaws &lt;- nls(bone~L*(1-exp(-K*(age-t))),\n          data = jaws,\n          start = list(L = 140, K = 0.5, t = 0.1))\nvb_jaws\n\nNonlinear regression model\n  model: bone ~ L * (1 - exp(-K * (age - t)))\n   data: jaws\n       L        K        t \n115.2527   0.1235   0.2378 \n residual sum-of-squares: 8897\n\nNumber of iterations to convergence: 6 \nAchieved convergence tolerance: 2.091e-06\n\n\nGráficamente:\n\njaws$vb &lt;- predict(vb_jaws, jaws$age)\njaws_plot +\n  # Nota que los datos de y los pasamos directamente\n  # desde el data.frame:\n  geom_line(aes(y = jaws$vb),\n            color = \"#ff7f0e\") +\n  theme_bw() +\n  labs(title = \"Modelo Von Bertalanffy\")\n\n\n\n\n\n\n\n\nPodemos también utilizar directamente geom_smooth para construir el gráfico. Adicionalmente, podemos utilizar la librería patchwork para poner ambos gráficos (lineal y vb) en un solo gráfico:\n\n\n\n\n\n\nAdvertencia\n\n\n\nggplot pasa nuestros datos como x y y, no como los nombres de las variables originales.\n\n\n\nvb_plot &lt;- jaws_plot + \n           geom_smooth(method = \"nls\",\n                       formula = y~L*(1-exp(-K*(x-t))),\n                       method.args = list(start = list(L = 140,\n                                                       K = 0.5,\n                                                       t = 0.1)),\n                       se = F,\n                       color = \"#ff7f0e\",\n                       size = 0.5) +\n            labs(title = \"Modelo Von Bertalanffy\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\nlm_plot + vb_plot +\n  plot_annotation(title = \"Relación long. mandíbula ~ edad\",\n                  caption = \"Datos: JAWS\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nVisualmente es evidente que el modelo VB está mejor ajustado que el modelo lineal y, de hecho, podemos comparar ambos modelos utilizando teoría de la información, en particular el criterio de información de Akaike, del cual hablaremos con mayor detalle en el Capítulo 19. Por lo pronto, bástete saber que seleccionaremos el modelo con el menor AIC que, como esperábamos, es el modelo VB:\n\nAIC(lm_jaws, vb_jaws)\n\n\n  \n\n\n\nEl ajuste mejoró notablemente, pero ¿cómo interpretamos el modelo? La respuesta está en la descripción que vimos antes, pero trasladémosla a este caso particular:\n\nvb_jaws\n\nNonlinear regression model\n  model: bone ~ L * (1 - exp(-K * (age - t)))\n   data: jaws\n       L        K        t \n115.2527   0.1235   0.2378 \n residual sum-of-squares: 8897\n\nNumber of iterations to convergence: 6 \nAchieved convergence tolerance: 2.091e-06\n\n\nTenemos una \\(L_\\infty\\) de 115.25; es decir, la longitud a la cual la especie analizada deja de crecer (tasa de crecimiento = 0) es de 115.25 cm. Esto podemos verlo gráficamente:\n\npars_plot &lt;- vb_plot +\n             geom_hline(yintercept = coef(vb_jaws)[\"L\"],\n                        color = \"firebrick\",\n                        linetype = \"dashed\")\npars_plot\n\n\n\n\n\n\n\n\nEste parametro normalmente es de interés, pues representa la longitud máxima promedio, y puede ser útil en la evaluación de poblaciones sujetas a explotación, por ejemplo.\nEl siguiente parámetro es \\(K\\); es decir, la constante de crecimiento, cuyas unidades son unidades recíprocas de tiempo (e.g. años\\(^-1\\)). Esto hace que su interpretación sea muy poco amigable; sin embargo, podemos interpretarla en términos de vidas medias (ln 2/k) con unidades de tiempo. El origen del concepto de la vida media se encuentra en la química, particularmente en el decaimiento isotópico, de modo que la vida media de un isótopo representa el tiempo que toma a una concentración x del mismo en reducirse a la mitad, y ha sido extendida para expresar periodos de tiempo que expresen reducciones del 50% de muchas otras cosas. En este caso, la interpretación estaría más relacionada con la ecuación diferencial que vimos al inicio, en el sentido de que es el tiempo que toma que la tasa de crecimiento se reduzca en un 50%:\n\nhalf_life &lt;- unname(log(2/coef(vb_jaws)[\"K\"]))\nhalf_life\n\n[1] 2.784617\n\n\nDe aquí podemos también estimar el tiempo (promedio) que toma a un individo alcanzar la fracción \\(x\\) de \\(L_\\infty\\):\n\\[\nt_x = \\frac{1}{k}ln \\left( \\frac{L_\\infty - L_0}{L_\\infty(1-x)} \\right)\n\\] Pero no tenemos \\(L_0\\), ¿qué hacemos? Afortunadamente, ambas parametrizaciones (con \\(t_0\\) o \\(L_0\\)) son equivalentes, por lo que no necesitamos re-ajustar el modelo. Simplemente podemos estimarla a partir de los coeficientes de nuestro modelo:\n\\[\nL_0 = L_\\infty(1-e^{kt_0})\n\\] Entonces calculemos el tiempo promedio que toma a un individuo alcanzar el 95% de \\(L_\\infty\\), lo cual es una posible estimación de longevidad (Ricker, 1979):\n\nvb_coef &lt;- coef(vb_jaws)\nL &lt;- vb_coef[\"L\"]\nK &lt;- vb_coef[\"K\"]\nt0 &lt;- vb_coef[\"t\"]\nl0 &lt;- L*(1-exp(K*t0))\nt_95 &lt;- (1/K)*log((L-l0)/(L*(1-0.95)))\nunname(t_95)\n\n[1] 24.49364\n\n\nEs decir, la longevidad promedio de la especie es de 24.49 años. Otra forma de estimar la longevidad es con la fracción 0.9933 (Fabens 1965):\n\nt_99 &lt;- (1/K)*log((L-l0)/(L*(1-0.99)))\nunname(t_99)\n\n[1] 37.52495\n\n\nVisualmente:\n\npars_plot +\n  geom_vline(xintercept = t_99,\n             color = \"forestgreen\",\n             linetype = \"dashed\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nPara más detalles y una discusión sobre este modelo, te recomiendo ampliamente revisar el artículo de Cailliet et al. (2006).\n\n\nUna pregunta que muy seguramente tendrás es “¿qué pasa con los intervalos de confianza?”. Bueno, podemos construirlos de dos maneras: a) utilizando perfiles de verosimilitud con la función confint() o b) utilizar réplicas Bootstrap. La primera aproximación ya la conoces (utilizando errores estándar, recuperables con la función summary); sin embargo, la segunda merece que la exploremos.",
    "crumbs": [
      "Técnicas no paramétricas y modelación no lineal",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Modelos no lineales</span>"
    ]
  },
  {
    "objectID": "c13_nolin.html#bootstrap-e-intervalos-de-confianza",
    "href": "c13_nolin.html#bootstrap-e-intervalos-de-confianza",
    "title": "13  Modelos no lineales",
    "section": "13.5 Bootstrap e Intervalos de Confianza",
    "text": "13.5 Bootstrap e Intervalos de Confianza\nPrimero, ¿qué es Bootstrap y con qué se come? Mencionamos algo de esto en el Capítulo 9, pero no dimos mayores detalles más que podemos utilizarlo para calcular los intervalos de confianza. El muestreo Bootstrap (o bootstraping) consiste en remuestrear los datos, con el objetivo de crear una distribución empírica de algún estadístico. Es decir, NO utilizaremos ni asumiremos una distribución para nuestros residuales, sino que utilizaremos directamente nuestros datos para describir su distribución. Un escenario en el cual esto es útil es cuando el perfil de verosimilitud de nuestros parámetros no es simétrico, como es el caso de \\(t_0\\) en nuestro modelo:\n\nplot(profile(vb_jaws, \"L\"))\n\n\n\n\n\n\n\n\n¿Qué nos dice este gráfico? En pocas palabras, la forma de la distribución de la verosimilitud de nuestro parámetro, en términos de \\(\\tau\\), que representa el valor del estadístico \\(t\\) a cada punto, donde el valor de \\(t\\) se calcula como la raiz cuadrada del cambio en la suma de cuadrados, dividido por el error estándar residual. ¿En Español? Entre más grande sea \\(\\tau\\), más nos alejamos del punto con máxima verosimilitud (\\(\\tau = 0\\)). Piensa en este gráfico como un gráfico de densidad invertido.\nUtilicemos entonces la función boot_nls de la librería nlraa para hacer el muestreo de nuestros parámetros, y utilicemos la función car::Confint para obtener sus intervalos de confianza:\n\nboot_pars &lt;- nlraa::boot_nls(vb_jaws)\n\nNumber of times model fit did not converge 1 out of 999 \n\nboot_ci &lt;- car::Confint(boot_pars, type = \"perc\")\ndata.frame(Par = names(vb_coef), CI = boot_ci)\n\n\n  \n\n\n\nY podemos utilizar estas estimaciones para graficar nuestros IC:\n\njaws$boot_inf &lt;- boot_ci[1,1]*(1-exp(-boot_ci[2,1]*(jaws$age-boot_ci[3,1])))\njaws$boot_sup &lt;- boot_ci[1,2]*(1-exp(-boot_ci[2,2]*(jaws$age-boot_ci[3,2])))\nboot_plot &lt;- vb_plot +\n             geom_ribbon(aes(ymin = jaws$boot_inf,\n                             ymax = jaws$boot_sup,\n                             alpha = 0.5),\n                         fill = \"grey70\",\n                         show.legend = F) +\n             labs(title = element_blank(),\n                  subtitle = \"IC: Bootstrap\")\nboot_plot\n\n\n\n\n\n\n\n\nEn este caso particular, los resultados son similares a los obtenidos utilizando la función confint, solo que los de confint reflejan una “menor” incertidumbre en la estimación. En este caso particular, es importante considerar la asimetría en el perfil de verosimilitud de \\(t_0\\), por lo que yo preferiría los IC estimados con bootstrap:\n\nvb_ci &lt;- confint(vb_jaws)\n\nWaiting for profiling to be done...\n\njaws$inf &lt;- vb_ci[1,1]*(1-exp(-vb_ci[2,1]*(jaws$age-vb_ci[3,1])))\njaws$sup &lt;- vb_ci[1,2]*(1-exp(-vb_ci[2,2]*(jaws$age-vb_ci[3,2])))\nci_plot &lt;- vb_plot +\n           geom_ribbon(aes(ymin = jaws$inf,\n                           ymax = jaws$sup,\n                           alpha = 0.5),\n                       fill = \"grey70\",\n                       show.legend = F) +\n           labs(title = element_blank(),\n                subtitle = \"IC: Perfil de verosimilitud\")\nboot_plot + ci_plot + plot_annotation(title = \"Modelo VB y sus IC\")",
    "crumbs": [
      "Técnicas no paramétricas y modelación no lineal",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Modelos no lineales</span>"
    ]
  },
  {
    "objectID": "c13_nolin.html#relación-talla-peso",
    "href": "c13_nolin.html#relación-talla-peso",
    "title": "13  Modelos no lineales",
    "section": "13.6 Relación Talla-Peso",
    "text": "13.6 Relación Talla-Peso\nEn el caso anterior modelamos la tasa de crecimento individual (aunque puede ser cualquier otro crecimiento), pero otra aplicación interesante es modelar la relación talla-peso de alguna especie. Aunque esta sigue un modelo potencial sumamente simple, sirve para ejemplificar un par de cosas y aproximaciones. Para este ejemplo utilizaremos los datos RuffeSLRH92 contenidos en la librería FSAdata:\n\nruffe &lt;- FSAdata::RuffeSLRH92[, c(\"length\",\n                                  \"weight\")]\n# Eliminamos posibles NA\nruffe &lt;- na.omit(ruffe)\ncolnames(ruffe) &lt;- c(\"lt\", \"wt\")\nhead(ruffe)\n\n\n  \n\n\n\nGrafiquemos estos datos, pero primero reflexionemos un poco. ¿Qué tiene más sentido? ¿Que el peso dependa de la longitud? o, caso contrario, ¿que la longitud dependa del peso? Esta pregunta sobre qué variable depende de cuál es en apariencia trivial, pero que siempre debemos de plantearnos ante cualquier problema de regresión. Puedes ajustar tu modelo en ambos sentidos, pero ¿tiene sentido que un organismo crezca en longitud porque incrementó su peso? Habiendo definido eso, pongamos la talla (lt) en el eje x y el peso (wt) en el eje y:\n\nruffe_plot &lt;- ggplot(data = ruffe, aes(x = lt, y = wt)) +\n              geom_point(color = \"dodgerblue4\",\n                         alpha = 0.7) +\n              theme_bw() +\n              labs(title = \"Relación talla/peso\",\n                   x = element_blank(),\n                   y = element_blank())\nruffe_plot\n\n\n\n\n\n\n\n\nA simple vista es evidente que un modelo lineal no daría el ancho, pero tenemos una aproximación con una interpretación similar, el modelo potencial:\n\\[\nW_i = aL^b_ie^{\\epsilon_i}\n\\]\nDonde \\(A\\) y \\(B\\) son constantes, representando la ordenada al origen y la tasa de crecimiento, respectivamente, y \\(\\epsilon\\) es el error multiplicativo. Esto es sumamente similar a un modelo lineal, ¿no? Pues, en realidad, podemos llevar el modelo potencial a un modelo lineal si eliminamos el exponencial del lado derecho\n\\[\nlog(W_i) = log(\\alpha L^\\beta_ie^{\\epsilon_i}) \\\\\n\\therefore \\\\\nlog(W_i) = log(\\alpha) + \\beta*log(L_i) + \\epsilon_i\n\\]\nEsto simplifica mucho el problema de regresión, pues hace el error aditivo y estabiliza las varianzas del modelo por los logaritmos y, sobre todo, nos permite aplicar la RLS que ya conocemos. De hecho, este era el modo en el cuál se aplicaban antes las regresiones no lineales: linealizar la ecuación y luego utilizar RLS (o RLM) para estimar los coeficientes. Luego, re-convertir los coeficientes transformados (e.g. \\(log(\\alpha)\\)) para interpretar adecuadamente sus valores. Un proceso que se vuelve más engorroso conforme incrementa la complejidad (intenta hacerlo con el modelo VB), además de que no siempre es posible alcanzar un modelo lineal. Afortunadamente, podemos echar mano de las computadoras para no tener que resolverlo de esta manera. Apliquemos entonces el modelo potencial de relación talla/peso por ambas vías. Primero, el modelo linealizado:\n\nruffe_lm &lt;- lm(log(wt)~log(lt), data = ruffe)\nsummary(ruffe_lm)\n\n\nCall:\nlm(formula = log(wt) ~ log(lt), data = ruffe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.25499 -0.06744  0.00552  0.08638  1.08984 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -10.88841    0.05123  -212.5   &lt;2e-16 ***\nlog(lt)       2.92247    0.01131   258.4   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.164 on 734 degrees of freedom\nMultiple R-squared:  0.9891,    Adjusted R-squared:  0.9891 \nF-statistic: 6.678e+04 on 1 and 734 DF,  p-value: &lt; 2.2e-16\n\n\nSi graficamos este modelo directamente vamos a tener un problema:\n\nfitted &lt;- ruffe_lm$fitted.values\nruffe_plot + geom_line(aes(y = fitted),\n                       color = \"#ff7f0e\")\n\n\n\n\n\n\n\n\nHabrá entonces que re-convertir para pasar al espacio original de nuestro modelo:\n\nfitted_rec &lt;- exp(ruffe_lm$fitted.values)\nruffe_plot + geom_line(aes(y = fitted_rec),\n                       color = \"#ff7f0e\")\n\n\n\n\n\n\n\n\nComo te darás cuenta, el método funciona, pero es un poco convolucionado. Ajustemos ahora el modelo potencial con la función nls:\n\nruffe_pot &lt;- nls(wt~A*lt^{B},\n                 data = ruffe,\n                 start = c(A = 0.05,\n                           B = 3))\nruffe$pot_pred &lt;- predict(ruffe_pot, ruffe$lt)\nruffe_plot + geom_line(data = ruffe, aes(y = pot_pred),\n                       color = \"#ff7f0e\")\n\n\n\n\n\n\n\n\nMucho más simple, ¿no? Veamos nuestros coeficientes:\n\ncoef(ruffe_pot)\n\n           A            B \n3.213069e-05 2.813585e+00 \n\n\nY comparémoslos con los coeficientes del modelo lineal:\n\nexp(coef(ruffe_lm)[1])\n\n (Intercept) \n1.867349e-05 \n\ncoef(ruffe_lm)[2]\n\n log(lt) \n2.922466 \n\n\nHay ligeras diferencias en las estimaciones, lo cual es muy seguramente debido al cambio a escala logarítmica de los datos. ¿Cuál utilizar? Cualquiera de las aproximaciones es “correcta”, solo ten en presente que el utilizar una RLS implica jalar todos sus supuestos, por lo que hay que verificar el modelo:\n\nperformance::check_model(ruffe_lm)\n\n\n\n\n\n\n\nggsave(\"rlm_pot.pdf\", height = 10, width = 7)\nperformance::check_normality(ruffe_lm)\n\nWarning: Non-normality of residuals detected (p &lt; .001).\n\nperformance::check_heteroscedasticity(ruffe_lm)\n\nWarning: Heteroscedasticity (non-constant error variance) detected (p &lt; .001).\n\n\nY es aquí donde tenemos algunos “problemas”, los cuales se “eliminan” en la regresión no lineal. La RNL tiene únicamente dos requerimientos:\n\nHaber especificado una función que describa adecuadamente la relación entre las variables.\nSeleccionar puntos de inicio correctos. Aún con un modelo adecuado, si los puntos de inicio no son los correctos, puedes mandar la estimación a un “óptimo local” en vez el “óptimo global”; es decir, llegar a un “falso positivo”.\n\n¿Cómo seleccionar puntos de incio adecuados? Esa es la parte complicada. Una alternativa (engorrosa) es hacerlo desde los datos. En nuestro ejemplo particular, esto sería algo como lo siguiente:\n\nSeleccionar una observación por parámetro. En nuestro caso, tenemos 2: \\(\\alpha\\) y \\(\\beta\\), por lo que seleccionaremos dos individuos de nuestra base de datos. Una idea puede ser seleccionar los extremos en \\(x\\):\n\n\nmin_ruffe &lt;- which.min(ruffe$lt)\nmax_ruffe &lt;- which.max(ruffe$lt)\nruffe[min_ruffe,1:2]\n\n\n  \n\n\nruffe[max_ruffe,1:2]\n\n\n  \n\n\n\n\nGenerar una ecuación para cada observación:\n\n\\[\n0.1 = A * 13^B \\\\\n93.1 = A*192^B\n\\]\n\nResolvemos la primera ecuación para el parámetro que sea más sencillo despejar, en este caso A:\n\n\\[\nA = \\frac{0.1}{13^B}\n\\]\nA no puede ser mayor a 0.1, por lo que podemos seleccionar 0.05 como punto de inicio.\n\nUtilizar ese valor para “estimar” B:\n\n\\[\nlog(0.1) = log(0.05) + B*13 \\\\\n\\therefore \\\\\nB = \\frac{log(0.1)}{13} - log(0.05)\n\\]\n\nround((log(0.1)/13)-log(0.05))\n\n[1] 3\n\n\nEstos valores, de hecho, fueron los que le pasamos a la función nls. ¿Y la segunda observación? (el dato mayor) Esta puede ser útil para estimar un límite superior para el valor posible del parámetro. OJO: esto es solo si quieres evitar estar “jugando” con los valores. A final de cuentas, solo necesitamos acercar al algoritmo de búsqueda a la zona correcta, no darle la respuesta de antemano. Otra alternativa es linealizar el modelo y utilizar los parámetros re-convertidos como puntos de inicio. Una alternativa más es utilizar datos de la literatura. En este caso, estimaciones que hayan sido realizadas para otra localidad o en una especie cercana. Otra estrategia puede ser utilizar una RL tradicional y estimar los valores desde ahí. Por último, siempre podemos intentar adivinarlos a ojo, aunque eso es algo que requiere tener bastante experiencia y capacidad de abstracción.\nEstos fueron solo un par de ejemplos sobre el uso de modelos no lineales, pero hay muchos más. Me gustaría poder ejemplificar el uso de todos y cada uno de ellos, pues puede que ninguno de los que aquí presenté se ajuste a tus necesidades particulares; sin embargo, espero que haya sido lo suficientemente claro en cómo podemos maximizar su potencial y qué cosas debes de tener en cuenta, tanto para su implementación en R como para la interpretación de sus resultados.\nAhora bien, ¿qué pasa si no conocemos el modelo? Tienes que encontrar la ecuación, definir tus parámetros, su fundamento, luego validarlo y publicarlo. Nah, mentira. Aunque es una posibilidad, eso es trabajo de las personas del área de matemáticas. Podemos utilizar regresiones polinomiales o LOESS. Vayamos allá.",
    "crumbs": [
      "Técnicas no paramétricas y modelación no lineal",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Modelos no lineales</span>"
    ]
  },
  {
    "objectID": "c13_nolin.html#regresión-polinomial",
    "href": "c13_nolin.html#regresión-polinomial",
    "title": "13  Modelos no lineales",
    "section": "13.7 Regresión polinomial",
    "text": "13.7 Regresión polinomial\nAntes de entrar a la regresión polinomial hay que tener claro qué es un polinomio. Recordarás de tus clases de secundaria que se ve algo así:\n\\[\nY = \\beta_0x^0 + \\beta_1x^1 + \\cdots + \\beta_nx^n\n\\]\nEsto se ve sospechosamete parecido a nuestra regresión lineal, ¿no? Pues sí, en realidad lo único que estamos haciendo es agregar términos adicionales, donde nuestra variable se va elevando en potencias con cada nuevo término, y el número de términos define el orden del polinomio. De hecho, podemos considerar a nuestra regresión lineal simple como una regresión polinomial de primer orden, pero eso es otra historia. ¿Qué tiene que ver esto con los modelos no lineales? Que cada término añade una curvatura.\nTe voy a ser honesto, no soy muy fan de las regresiones polinomiales y me gustaría que tú tampoco lo fueras. ¿El motivo? Con un polinomio de orden suficiente podemos, literalmente, predecir a la perfección nuestros datos. Eso es excelente, ¿no? ¡Vamos a aplicar únicamente regresiones polinomiales de orden \\(n-1\\) y obtener ajustes perfectos! ¡PARA! Aunque podemos predecir a la perfección nuestros datos observados, ¿qué pasa con datos que NO observamos? Recordarás que el objetivo primordial de un modelo de regresión es predecir, no nuestros datos, sino datos que el modelo no ha visto. Usualmente, entre mejor reproduzca un modelo los datos con los que fue entrenado, peor es su capacidad predictiva real.\n\n\n\n\n\n\nNota\n\n\n\nHablaremos largo y tendido de esta relación complejidad del modelo vs. capacidad predictiva y del problema conocido como sobre ajuste en el Capítulo 17. Por el momento ten presente que buscamos un modelo que permita describir los datos de manera “suficiente”, sin llegar a ser perfecto.\n\n\nEjemplifiquemos una regresión polinomial de orden dos con el ejemplo de los datos jaws. Hacerlo es extremadamente sencillo, solo tenemos que añadir la función poly(x, degree) a nuestra formula en lm():\n\njaws_poly &lt;- lm(bone~poly(age, degree = 2), data = jaws)\nsummary(jaws_poly)\n\n\nCall:\nlm(formula = bone ~ poly(age, degree = 2), data = jaws)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-28.175  -9.360   1.275   8.089  37.905 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              93.979      2.062  45.585  &lt; 2e-16 ***\npoly(age, degree = 2)1  182.082     15.150  12.019  &lt; 2e-16 ***\npoly(age, degree = 2)2 -118.949     15.150  -7.852 2.48e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.15 on 51 degrees of freedom\nMultiple R-squared:  0.8016,    Adjusted R-squared:  0.7939 \nF-statistic: 103.1 on 2 and 51 DF,  p-value: &lt; 2.2e-16\n\n\nLa salida es prácticamente idéntica a la de la regresión lineal que vimos en el Capítulo 11, con la diferencia de que ahora tenemos un término adicional, de modo que nuestro modelo queda como:\n\\[\nbone = \\beta_0 + \\beta_1*age^1 + \\beta_2*age^2 + e\n\\]\nSustituyendo los coeficientes:\n\\[\nbone = 93.98 + 182.01*age^1 - 118.95*age^2 + e\n\\]\nPodemos añadir este ajuste al gráfico:\n\npoly_plot &lt;- jaws_plot + \n             geom_smooth(method = \"lm\",\n                         formula = y~poly(x, degree = 2),\n                         se = T,\n                         color = \"#ff7f0e\",\n                         size = 0.5) +\n              labs(title = \"Modelo polinomial (2)\")\nlm_plot + vb_plot + poly_plot +\n  plot_annotation(title = \"Relación long. mandíbula ~ edad\",\n                  caption = \"Datos: JAWS\")\n\n\n\n\n\n\n\n\nEl ajuste es “mejor” que el modelo lineal pero no tiene absolutamente ningún sentido biológico que la longitud de la mandíbula disminuya después de que los animales cumplan 35 años (¿tal vez menguan?). ¿Qué pasa con un polinomio de tercer orden? Veámoslo:\n\npoly_plot &lt;- jaws_plot + \n             geom_smooth(method = \"lm\",\n                         formula = y~poly(x, degree = 3),\n                         se = T,\n                         color = \"#ff7f0e\",\n                         size = 0.5) +\n              labs(title = \"Modelo polinomial (3)\")\nlm_plot + vb_plot + poly_plot +\n  plot_annotation(title = \"Relación long. mandíbula ~ edad\",\n                  caption = \"Datos: JAWS\")\n\n\n\n\n\n\n\n\nComo ya habíamos mencionado, se añade una curva más, ahora tenemos un cambio alrededor de los 25 años y un aumento después de los 45. Y así podríamos irnos hasta llegar a un polinomio de orden \\(u-1\\) (\\(u\\): número de valores únicos), pero eso no tiene ningún caso porque la interpretación se vuelve extremadamente compleja con más de dos o tres términos. En una regresión polinomial los coeficientes ya no son pendientes como las que vimos en la regresión lineal simple. Si quisiéramos saber cuántas unidades nos movemos en \\(y\\) por cambio unitario en \\(x\\) ya no podemos simplemente revisar \\(\\beta_1\\), pues la presencia de \\(\\beta_2\\) o más términos tienen un efecto en esa cantidad. En nuestro ejemplo de orden 2 el primer coeficiente es positivo, que indica el crecimiento en la primera parte de la curva, mientras que el segundo es negativo, e indica el decremento a partir de los 35 años.\nDesafortunadamente, el problema de la interpretación de los parámetros no es solo matemático. Si fuera el caso, con una inspección cuidadosa y entendimiento del modelo podríamos interpretarlo. El problema es que, en muchos casos, los parámetros no se traducen a cantidades relevantes para nuestra área del conocimiento. ¿Qué nos dice la pendiente \\(\\beta_2 = -118.95\\)? Definitivamente el decremento subsecuente no es de 118.95 cm por año. Los parámetros de una regresión polinomial son solo perillas para ajustar el modelo, sin un significado “real” que sea claro. Luego tenemos el problema de seleccionar el orden adecuado. Una alternativa es ir construyendo regresiones polinomiales incrementando el orden y parando cuando el último coeficiente deje de ser significativamente distinto de 0, o empezar del orden máximo e ir disminuyendo. De cualquier manera, creo que estarás de acuerdo conmigo en que es innecesario pero, ¿qué pasa si sé que la relación de mis datos no es lineal, pero no tengo el modelo teórico que la describa? En ese caso te recomendaría probar un modelo aditivo generalizado (GAM) o, de lo que vamos a hablar ahora, una regresión LOESS.",
    "crumbs": [
      "Técnicas no paramétricas y modelación no lineal",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Modelos no lineales</span>"
    ]
  },
  {
    "objectID": "c13_nolin.html#regresión-loess",
    "href": "c13_nolin.html#regresión-loess",
    "title": "13  Modelos no lineales",
    "section": "13.8 Regresión LOESS",
    "text": "13.8 Regresión LOESS\nLOESS: Locally Estimated Scatterplot Smoothing; es decir, es un suavizado estimado localmente. ¿A qué nos referimos con suavizado? A que vamos a tratar de encontrar una curva que “suavice” (minimice) los “movimientos” a través de nuestros datos. El suavizado LOESS se construye haciendo un gran número de lineas de regresión (de orden dos o uno) a través de una ventana que se mueve a través del eje \\(x\\). Es decir, va a partir el eje \\(x\\) en intervalos y ajustar una regresión para cada intervalo, luego el modelo final resulta de unirlas todas. Entre más grande sea la ventana, más “suavizada” estará la curva, de modo que si la ventana es del tamaño de todo el eje \\(x\\) tendremos una regresión lineal simple. Si es muy pequeña, tendremos una curva que pasará por todos los puntos.\nApliquemos ahora LOESS a los datos jaws. La salida aquí es más críptica que la de la regresión polinomial. No tenemos ninguna prueba de hipótesis. Es más, no tenemos ningún parámetro. En este sentido la regresión (o mejor dicho suavizado) LOESS es una forma de regresión no paramétrica (híper-paramétrica sería tal vez un mejor término), pero eso no quiere decir que no haya forma de controlar el ajuste. De hecho, podemos cambiar todos los elementos que aparecen en la zona de Control settings, e incluso podríamos optimizarlos con validación cruzada, pero más de esto en el Capítulo 17.\n\njaws_loess &lt;- loess(bone~age, data = jaws)\nsummary(jaws_loess)\n\nCall:\nloess(formula = bone ~ age, data = jaws)\n\nNumber of Observations: 54 \nEquivalent Number of Parameters: 4.35 \nResidual Standard Error: 13.41 \nTrace of smoother matrix: 4.75  (exact)\n\nControl settings:\n  span     :  0.75 \n  degree   :  2 \n  family   :  gaussian\n  surface  :  interpolate     cell = 0.2\n  normalize:  TRUE\n parametric:  FALSE\ndrop.square:  FALSE \n\n\n¿Qué podemos extraer de este resumen? Que el modelo es un modelo cuadrático (segundo orden, degree) con una ventana del 75% de los datos, y tuvo un RSE de 13.41. Se asumió una distribución normal (family: gaussian, más de esto en el Capítulo 19), los datos fueron “normalizados” (más de esto en el Capítulo 14). Tenemos un “número equivalente de parámetros”, que sería análogo al orden en una regresión polinomial, y algunos otros detalles en los que no vamos a entrar. Gráficamente se ve muy cercano al modelo VB, aunque un poco más “bronco”:\n\nloess_plot &lt;- jaws_plot + \n              geom_smooth(method = \"loess\",\n                          color = \"#ff7f0e\",\n                          size = 0.5) +\n               labs(title = \"LOESS\")\nlm_plot + vb_plot + loess_plot +\n  plot_annotation(title = \"Relación long. mandíbula ~ edad\",\n                  caption = \"Datos: JAWS\")\n\n\n\n\n\n\n\n\nMi sugerencia es que, si no hay una ecuación que describa la relación no lineal, pienses antes en LOESS que en una regresión polinomial, pero ten en cuenta que esto aplica solo para ciertos casos en los que tiene sentido asumir el error como normal. En el Capítulo 19 vamos a ver cómo podemos generalizar el modelo lineal para contender con otro tipo de variables dependientes, conteos, por ejemplo, y capturar tendencias no lineales en el proceso. Una vez tengas ese conocimiento, puedes volver a ver el argumento family dentro de loess().\nCon esto terminamos esta sesión. Espero que haya sido de tu agrado, y nos vemos en la siguiente, donde ya vamos a entrar al tema de técnicas multivariadas.",
    "crumbs": [
      "Técnicas no paramétricas y modelación no lineal",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Modelos no lineales</span>"
    ]
  },
  {
    "objectID": "c13_nolin.html#ejercicio",
    "href": "c13_nolin.html#ejercicio",
    "title": "13  Modelos no lineales",
    "section": "13.9 Ejercicio",
    "text": "13.9 Ejercicio\nEn este ejercicio tienes dos opciones:\n\nUtiliza los datos dolphin_age.csv para ajustar el modelo de Von Bertalanffy y un modelo de crecimiento más. Compáralos utilizando el AIC y reporta e interpreta los resultados del “mejor” modelo (AIC más bajo). Estos son datos simulados del crecimiento del delfín Franciscana (o delfín del Plata, Pontoporia blainvillei). Puedes revisar Botta et al. (2010) para darte una idea del problema. Los datos que se simularon fueron los de las hembras.\nUtiliza datos propios para ajustar un modelo NO lineal y reporta e interpreta los resultados.\n\n\n\n\n\nBotta S, Secchi ER, Muelbert MMC, Danilewicz D, Negri MF, Cappozzo HL, Hohn AA. 2010. Age and growth of franciscana dolphins, Pontoporia blainvillei (Cetacea: Pontoporiidae) incidentally caught off southern Brazil and northern Argentina. Journal of the Marine Biological Association of the United Kingdom 90:1492-1500. DOI: 10.1017/S0025315410001141.\n\n\nCailliet GM, Smith WD, Mollet HF, Goldman KJ. 2006. Age and growth studies of chondrichthyan fishes: the need for consistency in terminology, verification, validation, and growth function fitting. Environmental Biology of Fishes 77:211-228. DOI: 10.1007/s10641-006-9105-5.\n\n\nChen S, Watanabe S. 1989. Age Dependence of Natural Mortality Coefficient in Fish Population Dynamics. NIPPON SUISAN GAKKAISHI 55:205-208. DOI: 10.2331/suisan.55.205.\n\n\nGompertz B. 1832. On the Nature of the Function Expressive of the Law of Human Mortality, and on a New Mode of Determining the Value of Life Contingencies. Phylosophical Transcriptions of the Royal Society of London 123:513-585.\n\n\nRicker WE. 1979. Growth rates and models. In: Hoar WS, Randall DJ, Brett JR eds. Fish physiology. 677-747.",
    "crumbs": [
      "Técnicas no paramétricas y modelación no lineal",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Modelos no lineales</span>"
    ]
  },
  {
    "objectID": "s04_mv.html",
    "href": "s04_mv.html",
    "title": "Técnicas Multivariadas",
    "section": "",
    "text": "Objetivo de aprendizaje\nEn esta sección del curso llegarás al punto de máxima abstracción y te adentrarás en diversas técnicas que te permitirán obtener conclusiones a partir de datos multivariados. Comenzarás analizando las relaciones entre tus variables mediante matrices de varianzas/covarianzas, formarás agrupaciones, compararás las mediciones multivariadas entre grupos, realizarás clasificaciones y, por último, realizarás regresiones múltiples y verás cómo controlar la complejidad de tus modelos.",
    "crumbs": [
      "Técnicas Multivariadas"
    ]
  },
  {
    "objectID": "c14_intromv.html",
    "href": "c14_intromv.html",
    "title": "14  Técnicas multivariadas: Introducción",
    "section": "",
    "text": "14.1 Librerías\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(corrplot)\nlibrary(PerformanceAnalytics)\nlibrary(MVN)\nlibrary(vegan)\ntheme_set(see::theme_lucid() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()))",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Técnicas multivariadas: Introducción</span>"
    ]
  },
  {
    "objectID": "c14_intromv.html#introducción",
    "href": "c14_intromv.html#introducción",
    "title": "14  Técnicas multivariadas: Introducción",
    "section": "14.2 Introducción",
    "text": "14.2 Introducción\nEn esta sección final del curso vamos a abordar problemas que se tornan sumamente complejos, pues estaremos lidiando con más de dos variables a la vez. Esto usualmente deriva de que una variable sola no es suficientemene informativa, o de que el fenómeno analizado es el resultado de la “interacción” de distintas variables. De cualquier manera, esto nos lleva a tener múltiples mediciones de distintas instancias.\n\n\n\n\n\n\nNota\n\n\n\nEn esta sección nos vamos a referir a una instancia como un conjunto de mediciones de distintas variables de una misma unidad observacional. Pueden ser distintas morfometrías de un mismo cráneo, o mediciones satelitales de distintas variables ambientales en una coordenada dada (Figura 14.1).\n\n\n\n\n\n\nFigura 14.1: Ejemplos de problemas multivariados.\n\n\n\n\n\nAl momento de añadir nuevas variables vamos a modificar también el cómo trabajamos con nuestros datos. Mientras que con análisis uni o bivariados podíamos tener bases de datos en formato corto (una columna para cada grupo) o largo (una columna con la variable de respuesta y otra con la de agrupamiento), aquí vamos a tener matrices de datos, en donde cada renglón es una instancia (individuo, unidad muestral), y cada columna representa una variable o un atributo diferente:\n\n\n\n\n\n\nFigura 14.2: De columnas a matrices.\n\n\n\n\n14.2.1 Álgebra lineal\nEsto me lleva a un (tal vez) tedioso recordatorio o una (tal vez) tediosa explicación sobre el álgebra lineal. ¿La razón? El tratar con cada columna por separado no solo es extremadamente ineficiente, sino que también nos lleva a incrementar la probabilidad de cometer errores de tipo I si nos ponemos a hacer pruebas uni o bivariadas para cada par de columnas (hablaremos más de esto en el Capítulo 16), por lo que es mejor trabajar con todas las variables al mismo tiempo.\n\nVamos a ver muchas matrices, algunas muy extensas por incluir las operaciones aritméticas correspondientes. NO te preocupes, el objetivo es solo que añadas a tu breviario cultural de dónde salen las matrices de covarianza y correlación.\n\nSean dos matrices \\(A\\) y \\(B\\), cada una con \\(m = 3\\) renglones y \\(n = 3\\) columnas:\n\\[\\begin{align*}\nA = \\left[\n  \\begin{matrix}\n  a_{1,1} & a_{1,2} & a_{1,3}\\\\\n  a_{2,1} & a_{2,2} & a_{2,3}\\\\\n  a_{3,1} & a_{3,2} & a_{3,3}\\\\\n  \n  \\end{matrix}\n  \\right] \\\\\nB = \\left[\n  \\begin{matrix}\n  b_{1,1} & b_{1,2} & b_{1,3}\\\\\n  b_{2,1} & b_{2,2} & b_{2,3}\\\\\n  b_{3,1} & b_{3,2} & b_{3,3}\\\\\n  \n  \\end{matrix}\n  \\right]\n\\end{align*}\\]\n\n\n\n\n\n\nImportante\n\n\n\nEn una matriz cada columna y cada renglón es un vector.\n\n\nTal y como vimos en el Capítulo 3, una de las operaciones más comunes al tratar con matrices es la transposición (denotada como \\(^T\\)); es decir, que las columnas se vuelvan renglones, y que los renglones se vuelvan columnas, tal que \\(A_{(m,n)} \\Rightarrow A^T_{(n,m)}\\):\n\\[\\begin{align*}\nA^T =\n\\left[\n\n\\begin{matrix}\na_{1,1} & a_{2,1} & a_{3,1}\\\\\na_{1,2} & a_{2,2} & a_{3,2}\\\\\na_{1,3} & a_{2,3} & a_{3,3}\\\\\n\n\\end{matrix}\n\\right]\\\\\nB^T =\n\\left[\n\n\\begin{matrix}\nb_{1,1} & b_{2,1} & b_{3,1}\\\\\nb_{1,2} & b_{2,2} & b_{3,2}\\\\\nb_{1,3} & b_{2,3} & b_{3,3}\\\\\n\n\\end{matrix}\n\\right]\n\\end{align*}\\]\nTambién podemos querer hacer operaciones básicas. La suma y resta de matrices está dada por cada elemento, tal que \\(S_{m,n} = (A_{m,n} \\pm B_{m,n})\\):\n\\[\\begin{align*}\nS = \\left[\n\\begin{matrix}\n(a_{1,1} \\pm b_{1,1}) &\n(a_{1,2} \\pm b_{1,2}) &\n(a_{1,3} \\pm b_{1,3})\n\\\\\n(a_{2,1} \\pm b_{2,1}) &\n(a_{2,2} \\pm b_{2,2}) &\n(a_{2,3} \\pm b_{2,3})\n\\\\\n(a_{3,1} \\pm b_{3,1}) &\n(a_{3,2} \\pm b_{3,2}) &\n(a_{3,3} \\pm b_{3,3})\n\\\\\n\n\n\\end{matrix}\n\\right]\n\\end{align*}\\]\n\n\n\n\n\n\nImportante\n\n\n\nPara hacer una suma o resta de dos matrices es necesario que \\((m_A,n_A) = (m_B,n_B)\\)\n\n\nLa multiplicación, desafortunadamente, no es tan simple. Aquí tenemos dos tipos de productos: el producto escalar (interno; \\(E_{m,n} = (A_m \\cdot B_n)\\)) y el producto externo. Solo vamos a explicar el primero porque es el que nos interesa para los fines del curso. En este la operación está dada como \\(E_{m,n} = \\sum a_{m} \\times b_n\\); es decir, el valor de cada celda de la matriz es el resultado de sumar los productos de los valores de la columna \\(m\\) por los valores del renglón \\(n\\):\n\\[\\begin{align*}\nE = \\left[\n\\begin{matrix}\n(\n(a_{1,1} \\times b_{1,1}) +  \n(a_{1,2} \\times b_{2,1}) +\n(a_{1,3} \\times b_{3,1})\n)&\n(\n(a_{1,1} \\times b_{1,2}) +  \n(a_{1,2} \\times b_{2,2}) +\n(a_{1,3} \\times b_{3,2})\n)&\n(\n(a_{1,1} \\times b_{1,3}) +  \n(a_{1,2} \\times b_{2,3}) +\n(a_{1,3} \\times b_{3,3})\n)\\\\\n(\n(a_{2,1} \\times b_{1,1}) +  \n(a_{2,2} \\times b_{2,1}) +\n(a_{2,3} \\times b_{3,1})\n)&\n(\n(a_{2,1} \\times b_{1,2}) +  \n(a_{2,2} \\times b_{2,2}) +\n(a_{2,3} \\times b_{3,2})\n)&\n(\n(a_{2,1} \\times b_{1,3}) +  \n(a_{2,2} \\times b_{2,3}) +\n(a_{2,3} \\times b_{3,3})\n)\\\\\n(\n(a_{3,1} \\times b_{1,1}) +  \n(a_{3,2} \\times b_{2,1}) +\n(a_{3,3} \\times b_{3,1})\n)&\n(\n(a_{3,1} \\times b_{1,2}) +  \n(a_{3,2} \\times b_{2,2}) +\n(a_{3,3} \\times b_{3,2})\n)&\n(\n(a_{3,1} \\times b_{1,3}) +  \n(a_{3,2} \\times b_{2,3}) +\n(a_{3,3} \\times b_{3,3})\n)\\\\\n\\end{matrix}\n\\right]\n\\end{align*}\\]\n\n\n\n\n\n\nImportante\n\n\n\nDebido a que estamos multiplicando los renglones (\\(m\\)) de una matriz por las columnas (\\(n\\)) de la otra, es necesario que \\(m_A = n_B\\).\n\n\n¿Para qué puse todos esos chorizos de operaciones matriciales? Créeme que no fue para presumir que puedo escribir matrices en LaTeX/Markdown, sino porque son la base de (posiblemente) las estructuras más importantes para las técnicas multivariadas: las matrices de covarianzas y de correlación.",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Técnicas multivariadas: Introducción</span>"
    ]
  },
  {
    "objectID": "c14_intromv.html#de-sigma-a-sigma-la-matriz-de-covarianzas",
    "href": "c14_intromv.html#de-sigma-a-sigma-la-matriz-de-covarianzas",
    "title": "14  Técnicas multivariadas: Introducción",
    "section": "14.3 De \\(\\sigma\\) a \\(\\Sigma\\): La matriz de covarianzas",
    "text": "14.3 De \\(\\sigma\\) a \\(\\Sigma\\): La matriz de covarianzas\nEmpecemos por la covarianza pues, como recordarás del Capítulo 11, la correlación es solo un cociente de la covarianza y, de hecho, comencemos recordando qué es matemáticamente la covarianza entre dos variables: la esperanza matemática (\\(E\\); el promedio, vamos) del producto de la diferencia de cada valor de cada variable menos la media de la variable:\n\\[\nCov(x,y) = E[(x-\\mu_x)(y-\\mu_y)]\n\\]\nSi la covarianza de \\(x\\) y \\(y\\) es el producto de centrar (retirarle la media) a cada variable, podemos ver a la varianza como la covarianza de una variable con respecto a sí misma:\n\\[\nCov(y,y) = Var(y) = \\sigma^2(y) \\Rightarrow E[(y-\\mu_y)(y-\\mu_{y})] = E[(y-\\mu_y)^2]\n\\] Estos son las covarianzas y varianzas poblacionales, pero recordarás que nosotros trabajamos con las varianzas (y por extensión covarianzas) muestrales, lo que se vería como:\n\\[\\begin{align*}\ns^2 = cov(y,y) = \\frac{\\sum_i^n(y_i-\\bar{y})^2}{n-1} \\\\\ncov(x,y) = \\frac{\\sum_i^n(x_i-\\bar{x})(y_i-\\bar{y})}{n-1}\n\\end{align*}\\]\nAl ser esto así podemos entonces calcular, al mismo tiempo, todas las covarianzas y varianzas. ¿Cómo? Siguiendo tres pasos:\n\nCentrar la matriz (\\(a_{m,n}-\\bar{a}_n\\))\nMultiplicar su transpuesta por la matriz original (\\(C^T_A \\cdot C_A\\)):\n\n\n\n\n\n\n\nImportante\n\n\n\nRecuerda que para realizar el producto interno de dos matrices necesitamos que \\(m_a = n_b\\), por lo que el orden de la multiplicación debe de ser, forzosamente, \\(C^T_A \\cdot C_A\\); es decir, la transpuesta por la original\n\n\n\\[\\begin{align*}\nC^T_A =\n\\left[\n\n\\begin{matrix}\n(a_{1,1} - \\bar{a}_1) &\n(a_{2,1} - \\bar{a}_1)\n\\\\\n(a_{1,2} - \\bar{a}_2) &\n(a_{2,2} - \\bar{a}_2)\n\\\\\n\n\\end{matrix}\n\\right]\n\n\\cdot\n\nC_A =\n\n\\left[\n\\begin{matrix}\n(a_{1,1} - \\bar{a}_1) &\n(a_{1,2} - \\bar{a}_2)\n\\\\\n(a_{2,1} - \\bar{a}_1) &\n(a_{2,2} - \\bar{a}_2)\n\\\\\n\n\\end{matrix}\n\\right]\n\n=\n\\end{align*}\\]\n\\[\\begin{align*}\n=\n\\left[\n\n\\begin{matrix}\n\n[((a_{1,1} - \\bar{a}_1)\n\\times\n(a_{1,1} - \\bar{a}_1))\n+\n((a_{2,1} - \\bar{a}_1)\n\\times\n(a_{2,1} - \\bar{a}_1))]\n\n&\n\n[((a_{1,1} - \\bar{a}_1)\n\\times\n(a_{1,2} - \\bar{a}_2))\n+\n((a_{2,1} - \\bar{a}_1)\n\\times\n(a_{2,2} - \\bar{a}_2))]\n\n\\\\\n[((a_{1,2} - \\bar{a}_2)\n\\times\n(a_{1,1} - \\bar{a}_1))\n+\n((a_{2,2} - \\bar{a}_2)\n\\times\n(a_{2,1} - \\bar{a}_1))]  \n\n&\n\n[((a_{1,2} - \\bar{a}_2)\n\\times\n(a_{1,2} - \\bar{a}_2))\n+\n((a_{2,2} - \\bar{a}_2)\n\\times\n(a_{2,2} - \\bar{a}_2))]\n\\\\\n\n\\end{matrix}\n\\right]\n\\end{align*}\\]\n\n\n\n\n\n\nImportante\n\n\n\nSi pones atención a las celdas que están en la diagonal te darás cuenta de que los productos que conforman la suma son iguales, por lo que podemos simplificarlos como como productos cuadráticos.\n\n\n\nMultiplicar por \\(\\frac{1}{m-1}\\), para completar nuestra varianza muestral; es decir, vamos a dividir cada elemento de nuestra matriz sobre el número de observaciones (renglones) que tenemos menos uno (grados de libertad):\n\n\\[\\begin{align*}\n=\n\\left[\n\n\\begin{matrix}\n\n[(a_{1,1}-\\bar{a}_1)^2+(a_{2,1}-\\bar{a}_1)^2]\n\n&\n\n[((a_{1,1} - \\bar{a}_1)\n\\times\n(a_{1,2} - \\bar{a}_2))\n+\n((a_{2,1} - \\bar{a}_1)\n\\times\n(a_{2,2} - \\bar{a}_2))]\n\n\\\\\n[((a_{1,2} - \\bar{a}_2)\n\\times\n(a_{1,1} - \\bar{a}_1))\n+\n((a_{2,2} - \\bar{a}_2)\n\\times\n(a_{2,1} - \\bar{a}_1))]  \n\n&\n\n[(a_{1,2}-\\bar{a}_2)^2+(a_{2,2}-\\bar{a}_2)^2]\n\\\\\n\n\\end{matrix}\n\\right]\n\n\\cdot \\frac{1}{2-1}\n\\end{align*}\\]\nSi simplificamos todo esto, tenemos nuestra matriz de covarianzas:\n\\[\\begin{align*}\n\\Sigma = \\left[\n\\begin{matrix}\ns^2(x) & cov(x,y) \\\\\ncov(x,y) & s^2(y)\n\\end{matrix}\n\\right]\n\\end{align*}\\]",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Técnicas multivariadas: Introducción</span>"
    ]
  },
  {
    "objectID": "c14_intromv.html#matriz-de-correlación",
    "href": "c14_intromv.html#matriz-de-correlación",
    "title": "14  Técnicas multivariadas: Introducción",
    "section": "14.4 Matriz de correlación",
    "text": "14.4 Matriz de correlación\nAhora recordemos que la correlación entre dos variables es el resultado de dividir su covarianza entre el producto de sus desviaciones estándar (que ez la raíz cuadrada de la varianza), tal que:\n\\[\ncor(x,y) = \\frac{cov(x,y)}{\\sigma_x \\sigma_y} = \\frac{\\sum X_cY_c}{\\sqrt{\\sum X^2_c \\sum Y^2_c}}\n\\]\nPartiendo de esta definición, calcular la matriz de correlación también se reduce a tres pasos:\n\nCentrar y estandarizar la matriz; es decir, a cada valor le vamos a restar la media de su columna y lo vamos a dividir entre la desviación estándar de su columna \\(\\left(\\frac{a_{m,n}-\\bar{a}_n}{\\sigma_n} \\right)\\).\nMultiplicar la matriz transpuesta por la matriz original (\\(E_A^T \\cdot E_A\\)).\nMultiplicar por \\(\\frac{1}{m}\\)\n\n\\[\\begin{align*}\n\nE^T_A =\n\\left[\n\\begin{matrix}\n\\frac{a_{1,1} - \\bar{a}_1}{\\sigma_1} &\n\\frac{a_{2,1} - \\bar{a}_1}{\\sigma_1}\n\\\\\n\\frac{a_{1,2} - \\bar{a}_2}{\\sigma_2} &\n\\frac{a_{2,2} - \\bar{a}_1}{\\sigma_2}\n\\\\\n\n\\end{matrix}\n\\right]\n\n\\cdot\n\nE_A =\n\n\\left[\n\\begin{matrix}\n\\frac{a_{1,1} - \\bar{a}_1}{\\sigma_1} &\n\\frac{a_{1,2} - \\bar{a}_2}{\\sigma_2}\n\\\\\n\\frac{a_{2,1} - \\bar{a}_1}{\\sigma_1} &\n\\frac{a_{2,2} - \\bar{a}_2}{\\sigma_2}\n\\\\\n\n\\end{matrix}\n\\right]\n\n\\cdot \\frac{1}{2}\n\n= \\\\\n\nr =\n\\left[\n\\begin{matrix}\n1 & cor(x,y) \\\\\ncor(x,y) & 1\n\\end{matrix}\n\\right]\n\\end{align*}\\]\nY con esto tenemos nuestra matriz de correlaciones.\n\n\n\n\n\n\nImportante\n\n\n\nSi te das cuenta, el orden de los pasos permite que aprovechemos las características del producto matricial interno para poder estimar todas las covarianzas o correlaciones “juntas”. Si bien es cierto que a mano puede sonar a que no hay mucha diferencia, esto simplifica mucho las cosas cuando empezamos a escalar en los procedimientos.",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Técnicas multivariadas: Introducción</span>"
    ]
  },
  {
    "objectID": "c14_intromv.html#matrices-de-covarianza-y-corrlación-en-r",
    "href": "c14_intromv.html#matrices-de-covarianza-y-corrlación-en-r",
    "title": "14  Técnicas multivariadas: Introducción",
    "section": "14.5 Matrices de covarianza y corrlación en R",
    "text": "14.5 Matrices de covarianza y corrlación en R\nAfortunadamente para nosotros, para obtener estas matrices en R utilizaremos las mismas funciones que para el cálculo individual (y por consiguiente podremos calcular también la \\(\\rho\\) de Spearman). Utilicemos como ejemplo la base de datos mtcars:\n\nhead(mtcars)\n\n\n  \n\n\n\nAhora estimemos ambas matrices:\n\n# Matriz de covarianzas\ncov_mat &lt;- cov(mtcars)\n# Matriz de correlación\ncor_mat &lt;- cor(mtcars)\n# Las imprimimos en pantalla\ncov_mat\n\n             mpg         cyl        disp          hp         drat          wt\nmpg    36.324103  -9.1723790  -633.09721 -320.732056   2.19506351  -5.1166847\ncyl    -9.172379   3.1895161   199.66028  101.931452  -0.66836694   1.3673710\ndisp -633.097208 199.6602823 15360.79983 6721.158669 -47.06401915 107.6842040\nhp   -320.732056 101.9314516  6721.15867 4700.866935 -16.45110887  44.1926613\ndrat    2.195064  -0.6683669   -47.06402  -16.451109   0.28588135  -0.3727207\nwt     -5.116685   1.3673710   107.68420   44.192661  -0.37272073   0.9573790\nqsec    4.509149  -1.8868548   -96.05168  -86.770081   0.08714073  -0.3054816\nvs      2.017137  -0.7298387   -44.37762  -24.987903   0.11864919  -0.2736613\nam      1.803931  -0.4657258   -36.56401   -8.320565   0.19015121  -0.3381048\ngear    2.135685  -0.6491935   -50.80262   -6.358871   0.27598790  -0.4210806\ncarb   -5.363105   1.5201613    79.06875   83.036290  -0.07840726   0.6757903\n             qsec           vs           am        gear        carb\nmpg    4.50914919   2.01713710   1.80393145   2.1356855 -5.36310484\ncyl   -1.88685484  -0.72983871  -0.46572581  -0.6491935  1.52016129\ndisp -96.05168145 -44.37762097 -36.56401210 -50.8026210 79.06875000\nhp   -86.77008065 -24.98790323  -8.32056452  -6.3588710 83.03629032\ndrat   0.08714073   0.11864919   0.19015121   0.2759879 -0.07840726\nwt    -0.30548161  -0.27366129  -0.33810484  -0.4210806  0.67579032\nqsec   3.19316613   0.67056452  -0.20495968  -0.2804032 -1.89411290\nvs     0.67056452   0.25403226   0.04233871   0.0766129 -0.46370968\nam    -0.20495968   0.04233871   0.24899194   0.2923387  0.04637097\ngear  -0.28040323   0.07661290   0.29233871   0.5443548  0.32661290\ncarb  -1.89411290  -0.46370968   0.04637097   0.3266129  2.60887097\n\ncor_mat\n\n            mpg        cyl       disp         hp        drat         wt\nmpg   1.0000000 -0.8521620 -0.8475514 -0.7761684  0.68117191 -0.8676594\ncyl  -0.8521620  1.0000000  0.9020329  0.8324475 -0.69993811  0.7824958\ndisp -0.8475514  0.9020329  1.0000000  0.7909486 -0.71021393  0.8879799\nhp   -0.7761684  0.8324475  0.7909486  1.0000000 -0.44875912  0.6587479\ndrat  0.6811719 -0.6999381 -0.7102139 -0.4487591  1.00000000 -0.7124406\nwt   -0.8676594  0.7824958  0.8879799  0.6587479 -0.71244065  1.0000000\nqsec  0.4186840 -0.5912421 -0.4336979 -0.7082234  0.09120476 -0.1747159\nvs    0.6640389 -0.8108118 -0.7104159 -0.7230967  0.44027846 -0.5549157\nam    0.5998324 -0.5226070 -0.5912270 -0.2432043  0.71271113 -0.6924953\ngear  0.4802848 -0.4926866 -0.5555692 -0.1257043  0.69961013 -0.5832870\ncarb -0.5509251  0.5269883  0.3949769  0.7498125 -0.09078980  0.4276059\n            qsec         vs          am       gear        carb\nmpg   0.41868403  0.6640389  0.59983243  0.4802848 -0.55092507\ncyl  -0.59124207 -0.8108118 -0.52260705 -0.4926866  0.52698829\ndisp -0.43369788 -0.7104159 -0.59122704 -0.5555692  0.39497686\nhp   -0.70822339 -0.7230967 -0.24320426 -0.1257043  0.74981247\ndrat  0.09120476  0.4402785  0.71271113  0.6996101 -0.09078980\nwt   -0.17471588 -0.5549157 -0.69249526 -0.5832870  0.42760594\nqsec  1.00000000  0.7445354 -0.22986086 -0.2126822 -0.65624923\nvs    0.74453544  1.0000000  0.16834512  0.2060233 -0.56960714\nam   -0.22986086  0.1683451  1.00000000  0.7940588  0.05753435\ngear -0.21268223  0.2060233  0.79405876  1.0000000  0.27407284\ncarb -0.65624923 -0.5696071  0.05753435  0.2740728  1.00000000\n\n\nVeamos la matriz de correlaciones gráficamente:\n\ncorrplot::corrplot(cor_mat, method = \"ellipse\", type = \"upper\")\n\n\n\n\n\n\n\n\nAhora utilicemos una función que, en un solo paso, computará la matriz de correlación, realizará una prueba de significancia para cada una y además nos presentará la matriz de manera gráfica:\n\n# Descarga la función desde esa url y la carga en memoria\nsource(\"http://www.sthda.com/upload/rquery_cormat.r\")\n# Para fines prácticos se extrae un subconjunto de las columnas\nmydata &lt;- mtcars[, c(1,3,4,5,6,7)] \n# Se aplica la función\nrquery.cormat(mydata)\n\n\n\n\n\n\n\n\n$r\n        hp  disp    wt  qsec  mpg drat\nhp       1                            \ndisp  0.79     1                      \nwt    0.66  0.89     1                \nqsec -0.71 -0.43 -0.17     1          \nmpg  -0.78 -0.85 -0.87  0.42    1     \ndrat -0.45 -0.71 -0.71 0.091 0.68    1\n\n$p\n          hp    disp      wt  qsec     mpg drat\nhp         0                                   \ndisp 7.1e-08       0                           \nwt   4.1e-05 1.2e-11       0                   \nqsec 5.8e-06   0.013    0.34     0             \nmpg  1.8e-07 9.4e-10 1.3e-10 0.017       0     \ndrat    0.01 5.3e-06 4.8e-06  0.62 1.8e-05    0\n\n$sym\n     hp disp wt qsec mpg drat\nhp   1                       \ndisp ,  1                    \nwt   ,  +    1               \nqsec ,  .       1            \nmpg  ,  +    +  .    1       \ndrat .  ,    ,       ,   1   \nattr(,\"legend\")\n[1] 0 ' ' 0.3 '.' 0.6 ',' 0.8 '+' 0.9 '*' 0.95 'B' 1\n\n\nOtra alternativa es utilizar la función chart.Correlation(data, histogram) de la librería PerformanceAnalytics, en la cual se muestran todos resultados en una misma gráfica:\n\nPerformanceAnalytics::chart.Correlation(mydata, histogram = T, pch = 19)\n\n\n\n\n\n\n\n\n\nGGally::ggpairs(mydata, progress = F)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n\n\n\n\n\n\n\n\nUn comentario final al respecto de estas matrices es que, además de ser la base de las técnicas multivariadas, nos permiten evaluar la asociación entre nuestras variables sin comprometer un modelo predictivo, a la vez que nos permiten hacer un filtrado de nuestras variables para evitar autocorrelaciones o incluir variables poco informativas, aunque de esto hablaremos más adelante.",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Técnicas multivariadas: Introducción</span>"
    ]
  },
  {
    "objectID": "c14_intromv.html#normalidad-multivariada",
    "href": "c14_intromv.html#normalidad-multivariada",
    "title": "14  Técnicas multivariadas: Introducción",
    "section": "14.6 Normalidad multivariada",
    "text": "14.6 Normalidad multivariada\nPara analizar si nuestros datos multivariados se ajustan a una distribución normal multivariada utilizaremos la librería MVN, cuya función mvn(data, mvnTest, multivariatePlot) nos permite realizar una serie de pruebas tanto multi como univariadas:\n\n\n\n\n\n\nImportante\n\n\n\nAl igual que en ANOVA, si estamos haciendo una comparación entre grupos no interesa tanto la normalidad Mv de cada grupo, sino de los residuales del modelo subyacente.\n\n\n\n14.6.1 Prueba de Mardia (1970)\nConsiste en analizar si los momentos de la distribución Mv de los datos difieren de los esperados de una distribución Normal Mv (Mardia 1970), las hipótesis nulas son:\n\n\\(H_0: S_{obs} = S_{NMv_{µ, \\Sigma}}\\)\n\\(H_0: K_{obs} = K_{NMv_{µ, \\Sigma}}\\)\n\nDonde \\(\\mu\\) representa el vector de medias de cada variable y \\(\\Sigma\\) la matriz de covarianzas.\nAl realizar esta prueba vemos que la distribución Mv observada tiene una curtosis adecuada, aunque se encuentra fuertemente sesgada. Si analizamos los datos univariados vemos que en apariencia 1/6 variables no presenta normalidad univariada.\n\nmvntest &lt;- MVN::mvn(mydata, mvnTest = \"mardia\")\nmardia_test &lt;- mvntest$multivariateNormality\nunitest &lt;- mvntest$univariateNormality\nmardia_test\n\n\n  \n\n\nunitest\n\n\n  \n\n\n\n\n\n14.6.2 Prueba de Henze-Zirkler\nSi observamos con atención los valores de p, veremos que hay algunos que se encuentran cercanos al umbral de 0.05 por lo que un análisis a partir de los cuantiles de la distribución puede ser una alternativa más informativa. Para ello podemos utilizar la prueba de Henze-Zirkler. Esta prueba considera una hipótesis compuesta, en la cual la distribución de X es una distribución normal no degenerada, cuyos resultados son consistentes contra cualquier distribución alternativa no normal (por ello compuesta). La representación está dada en términos de \\(L^2\\) (Distancia de Mahalanobis, más adelante hablaremos sobre medidas de distancia). Al ser una prueba de bondad de ajuste (observado vs. esperado), el estadístico de prueba sigue una distribución \\(\\chi^2\\) (Henze y Zirkler 2007).\nAplicandola con mvn() vemos que también sugiere una falta de normalidad, lo cual podemos comprobar al ver el gráfico Cuantil-Cuantil\n\nhz_test &lt;- MVN::mvn(mydata, mvnTest = \"hz\",\n                    multivariatePlot = \"qq\")$multivariateNormality\n\n\n\n\n\n\n\nhz_test\n\n\n  \n\n\n\nEse gráfico, aunque informativo, puede trabajarse para hacerse más agradable a la vista utilizando ggplot2 utilizando la siguiente función personalizada creada a partir del código utilizado para la gráfica anterior:\n\n# Funciones personalizadas:\n# Extraida de la función mvn(multivariatePlot = \"qq\") para un gráfico QQ\n# para normalidad multivariada utilizando la distancia de mahalanobis\nji2_plot &lt;- function(df){\n  # Datos\n  n &lt;- dim(mydata)[1] # número de datos\n  p &lt;- dim(mydata)[2] # número de grupos\n  # Centramos los datos (a-µ(a)) sin escalarlos (sin dividir por su \\sigma)\n  dif &lt;- scale(mydata, scale = F)\n  # Cálculo de la distancia de mahalanobis^2\n  d &lt;- diag(dif %*% solve(cov(mydata), tol = 1e-25) %*% t(dif)) \n  r &lt;- rank(d) # Asignación de rangos a las distancias\n  # Cuantiles teóricos según la distribución ji^2\n  ji2 &lt;-  qchisq((r - 0.5)/n, p)\n  # Reunimos los objetos en un data.frame para graficar con ggplot2\n  ji2_plot_data &lt;- data.frame(d, ji2)\n  \n  # Graficado\n  library(ggplot2)\n  ji2_qq &lt;- ggplot(data = ji2_plot_data, aes(x = d, y = ji2)) +\n            geom_abline(slope = 1,\n                        colour = rgb(118,78,144,\n                                     maxColorValue = 255),\n                        linewidth = 1) +\n            geom_point(colour = \"deepskyblue4\", alpha = 0.5, \n                       size = 4) + \n            labs(title = \n                   bquote(\"Gráfico QQ de\" ~~ {chi^2} ~~ \", g.l = \"~ .(p)),\n                 subtitle = bquote(\"Distancia de Mahalanobis (\"~~{D^2}~~\") \n                                   vs. Cuantiles teóricos\"),\n                 x = element_blank(),\n                 y = element_blank(),\n                 caption = \"Basado en mvn(..., multivariatePlot == \\\"qq\\\")\"\n                 )\n  return(ji2_qq)\n}\n\n\nqqjiplot &lt;- ji2_plot(mydata)\nqqjiplot\n\n\n\n\n\n\n\n\n\n\n14.6.3 Prueba de Royston\nEsta prueba es una extensión multivariada de la prueba por excelencia para la normalidad univariada: la prueba de Shapiro-Wilk. Originalmente propuesta en 1983, aunque fue corregida/ampliada por el mismo autor en 1992 (Royston vs. otras). Funciona mejor para muestras pequeñas, aunque no se recomienda emplearla con menos de 3 observaciones, ni con más de 2000.\nSu implementación sigue la misma línea que los casos anteriores. Si analizamos el valor de p, veremos que se encuentra en el límite de la significancia a un \\(\\alpha = 0.05\\); sin embargo, si consideramos también el gráfico QQ que elaboramos anteriormente, no podemos asumir que esas desviaciones sean despreciables.\n\nroyston_test &lt;- mvn(mydata, mvnTest = \"royston\")$multivariateNormality\nroyston_test",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Técnicas multivariadas: Introducción</span>"
    ]
  },
  {
    "objectID": "c14_intromv.html#igualdad-de-dispersiones-multivariadas",
    "href": "c14_intromv.html#igualdad-de-dispersiones-multivariadas",
    "title": "14  Técnicas multivariadas: Introducción",
    "section": "14.7 Igualdad de dispersiones multivariadas",
    "text": "14.7 Igualdad de dispersiones multivariadas\nConforme vayamos avanzando en el curso veremos que, entre los supuestos de algunas pruebas, vamos a encontrar el de “igualdad de dispersiones multivariadas”; i.e., igualdad de matrices de covarianza. Una alternativa es utilizar la prueba de Anderson (2005), la cual es un análogo multivariado a la prueba de Levene para la homogeneidad de varianzas. La prueba de hipótesis está basada en distancias no euclidianas entre los grupos (i.e., no utiliza el teorema de Pitágoras :( ). Un dato curioso es que este método también se ha utilizado para evaluar la diversidad \\(\\beta\\) de una comunidad.\nPara su implementación en R utilizaremos la función betadisper de la librería vegan. Al ser un método basado en distancias, primero habrá que transformar los datos a una matriz de distancias utilizando alguna de las funciones dist, betadiver o vegdist. Necesitamos, además, establecer los grupos a utilizar.\n\nlibrary(vegan)\ndist_mat &lt;- vegan::vegdist(mydata, method = \"euclidean\", type = \"median\")\ngroups &lt;- as.character(mtcars$cyl)\n# Por fines prácticos no se muestra,\n# ya que calcula una distancia entre cada par de instancias o grupos,\n# resultando en matrices sumamente grandes\n# dist_mat\n\nAhora utilizaremos la función betadisper para comprobar la homogeneidad de dispersiones entre los distintos cilindros. Esta prueba únicamente genera el espacio multivariado para realizar la prueba, realizando un ACoP para reducir la dimensionalidad de la base de datos y estimar las distancias a la mediana de cada uno de los grupos establecidos.\n\n# Realizar PCoA\ndisp_mv &lt;- vegan::betadisper(dist_mat,\n                             group = groups, type = \"median\")\ndisp_mv\n\n\n    Homogeneity of multivariate dispersions\n\nCall: vegan::betadisper(d = dist_mat, group = groups, type = \"median\")\n\nNo. of Positive Eigenvalues: 6\nNo. of Negative Eigenvalues: 0\n\nAverage distance to median:\n    4     6     8 \n30.33 34.32 76.12 \n\nEigenvalues for PCoA axes:\n    PCoA1     PCoA2     PCoA3     PCoA4     PCoA5     PCoA6 \n5.778e+05 4.507e+04 2.868e+02 4.526e+01 3.590e+00 2.691e+00 \n\n\nSi realizamos la prueba de hipótesis vemos que, al parecer, las dispersiones multiviariadas NO son similares entre los 3 grupos.\n\n# Prueba de hipótesis\nanova(disp_mv)\n\n\n  \n\n\n\nAhora veamos las dispersiones gráficamente:\n\n# Análisis gráfico\nPCoA_data &lt;- data.frame(disp_mv$vectors, group = groups)\nggplot(data = PCoA_data,\n       aes(x = PCoA1, y = PCoA2,\n           color = groups)) +\n  geom_point() +\n  stat_ellipse(level = 0.4) +\n  labs(title = \"Dispersión Mv\",\n       color = \"Group\")\n\n\n\n\n\n\n\n\nAl analizar el gráfico vemos que la dispersión del grupo 6 es más pequeña que la de los grupos 4 y 8, entonces realicemos las comparaciones univariadas con la prueba Honesta de Diferencias Significativas de Tukey (TukeyHSD). Los resultados sugieren que la dispersión del grupo 8 es significativamente mayor a la de los otros dos, lo cual a su vez pudiera sugerir que hay un equilibrio entre las dispersiones en el eje x con respecto a las dispersiones en el eje y.\n\nmod_HSD &lt;- TukeyHSD(disp_mv)\nmod_HSD\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = distances ~ group, data = df)\n\n$group\n         diff        lwr      upr     p adj\n6-4  3.987244 -29.382669 37.35716 0.9532104\n8-4 45.791195  17.982934 73.59946 0.0009461\n8-6 41.803951   9.854692 73.75321 0.0083488",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Técnicas multivariadas: Introducción</span>"
    ]
  },
  {
    "objectID": "c14_intromv.html#implicaciones-analíticas-de-la-multidimensionalidad",
    "href": "c14_intromv.html#implicaciones-analíticas-de-la-multidimensionalidad",
    "title": "14  Técnicas multivariadas: Introducción",
    "section": "14.8 Implicaciones analíticas de la multidimensionalidad",
    "text": "14.8 Implicaciones analíticas de la multidimensionalidad\nEs importante mencionar que entre más incrementemos la dimensionalidad de nuestro problema, más difícil será resumir en un solo resultado las pruebas de nuestros análisis y, en consecuencia, deberemos de considerar distintas técnicas/estrategias que analicen nuestros datos desde distintas perspectivas antes de emitir un juicio o extraer conclusiones. Por esta razón, es sumamente importante que realicemos una selección de variables de manera rigurosa antes de comenzar nuestro análisis, ya que incluir variables innecesariamente únicamente incrementará la varianza de los datos sin aportarnos ninguna información adicional, causando desviaciones de la normalidad, modelos complejos sobre o infra ajustados y pérdidas de poder estadístico.\nEn cuanto a la normalidad, es importante mencionar que si nuestros datos en realidad no se ajustan, o se encuentran fuertemente desviados de la normalidad, las conclusiones que extraigamos serán únicamente sobre la tendencia más general de nuestros datos (Revisar: desigualdad/teorema de Chebyshev) y, en consecuencia, pueden no ser una representación completa de nuestras muestras. Si esto es importante o no, dependerá de nuesta pregunta de investigación y qué tan fino querramos que sea el análisis.",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Técnicas multivariadas: Introducción</span>"
    ]
  },
  {
    "objectID": "c14_intromv.html#transformaciones-o-deformaciones",
    "href": "c14_intromv.html#transformaciones-o-deformaciones",
    "title": "14  Técnicas multivariadas: Introducción",
    "section": "14.9 Transformaciones o deformaciones",
    "text": "14.9 Transformaciones o deformaciones\nHabiendo tocado el tema de la estandarización, hablemos también del resto de transformaciones. En general, podemos considerar que existen dos tipos de transformaciones:\n\nAquellas que afectan la distribución de los datos (e.g., logarítmica)\nAquellas que simplemente cambian los límites de la distribución original (e.g., MinMax)\n\n¿Cuál utilizar? Dependerá de nuestros objetivos para hacerla, lo cual me lleva al punto de que: NINGUNA transformación debe de ser aplicada sin cuidado. Hay que tener en cuenta que, aunque no se cambie la distribución de los datos, el análisis ya no se realiza sobre los datos originales, lo cual puede causar errores de interpretación. Con esto no quiero decir que las transformaciones sean malas, solo que hay que emplearlas con una justificación y asegurarnos de re-transformar los resultados antes de interpretarlos.\nVeamos algunas de las transformaciones más comunes y cuáles son sus consecuencias en los datos. Para ello, consideremos estos datos sin transformar, los cuales están altamente sesgados y, en consecuencia, bastante alejados de la normalidad:\n\n# Función de graficado\nkdeplots &lt;- function(data, aes){\n  kdeplots &lt;- ggplot(data, aes) + \n            geom_density(show.legend = F) + \n            theme_bw() +\n            labs(title = \"Conteo de peces\",\n                 subtitle =\n                   \"Gráfico de densidad con datos y distintas transformaciones\",\n                 caption = \"Datos: http://bit.ly/comm_transf\",\n                 x = element_blank(),\n                 y = element_blank()) +\n            scale_y_continuous(breaks = NULL)\n  return(kdeplots)\n}\n\ndf &lt;- data.frame(datos = c(38, 1, 13, 2, 13, 20, 150, 9, 28, 6, 4, 43),\n                 transf = \"originales\")\ndf$transf &lt;- factor(df$transf, levels = c(\"originales\", \"Z\",\n                                          \"log(x)\", \"sqrt(x)\"))\nkde_plots &lt;- kdeplots(df, aes(datos, color = transf))\nkde_plots\n\n\n\n\n\n\n\n\n\n14.9.1 Estandarización\nVeamos el efecto de estandarizar los datos; es decir, utilizar la distribución Z. La estandarización consiste en centrar la variable (restarle su media) y dividirla entre su desviación estándar. El resultado es la misma distribución, aunque los datos ahora se encuentran escalados e indican a cuantas SD de la media se encuentra cada punto.\n\ndf2 &lt;- rbind(df, data.frame(datos = (df$datos-mean(df$datos))/sd(df$datos),\n                            transf = \"Z\"))\nkde_plots &lt;- kdeplots(df2, aes(datos, color = transf)) + \n             facet_wrap(~transf, scales = \"free\")\nkde_plots\n\n\n\n\n\n\n\n\n\n\n14.9.2 Transformación logarítmica\nPosiblemente la transformación más conocida, utilizada y, en consecuencia, abusada. Se realiza aplicando la ecuación \\(X_{log} = log(X)\\). (OJO: si hay ceros será \\(X_{log} = log(X+c)\\), ya que el logaritmo de 0 no existe). Esta transformación cambia notablemente la distribución de los datos, acercándolos a una forma de campana:\n\ndf2 &lt;- rbind(df2, data.frame(datos = log(df$datos),\n                            transf = \"log(x)\"))\nkde_plots &lt;- kdeplots(df2, aes(datos, color = transf)) + \n             facet_wrap(~transf, scales = \"free\")\nkde_plots\n\n\n\n\n\n\n\n\n¿Cuándo aplicarla? Primero, los casos en los cuales puedes encontrarla aplicada:\n\nLinealizar un modelo. Esta ya la vimos en el Capítulo 13 cuando hablamos de la relación talla-peso.\nEnlazar la salida de un modelo lineal con una distribución del error de la familia de distribuciones exponenciales (Poisson, por ejemplo).\nSimilar al anterior, cuando la variable de respuesta sea el resultado de la interacción entre distintas variables, y querramos formar un modelo con términos multiplicativos (\\(Y \\sim x_1*x_2*x3*...*e\\)) más que aditivos (\\(Y \\sim x_1+x_2+x_3+...+e\\))\nForzar la distribución de los datos a una normal para cumplir con los supuestos de pruebas paramétricas.\nIntentar poner dos variables en la misma escala.\nAlguna otra aberración que se me esté quedando en el tintero.\n\nLos únicos escenarios donde es realmente válido aplicar una transformación logarítmica son los primeros tres, y en ninguno estamos transformando directamente los datos, sino que la transformación es una consecuencia del modelo que estamos considerando.\nEl aplicar una transformación logarítmica a los datos para forzar distribuciones normales es práctica “estándar” o, tal vez mejor dicho, común. Este caso tiene, evidentemente, defensores y detractores. Personalmente me encuentro en el segundo grupo. En un escenario de ANOVA, por ejemplo, vimos que la distribución de los datos crudos no importa y que, de hecho, pretender que la distribución global de los datos sea normal es un despropósito que va en contra de nuestro objetivo. ¿Qué pasa entonces con una comparación de dos grupos; es decir, en un escenario donde aplicamos una t de Student? Ahí dijimos que sí importa la distribución de cada grupo porque comparamos directamente la diferencia en medias en relación a la desviación estándar mancomunada. Hagamos ese ejercicio. Tomemos los datos de nuestros conteos de peces y formemos dos grupos, donde el grupo B sean los conteos del grupo 1 + 50:\n\nA &lt;- df$datos\nB &lt;- df$datos+50\n\ndf_test &lt;- data.frame(A = A, B = B, id = seq_along(A)) |&gt; \n           reshape2::melt(id.vars = \"id\",\n                          variable.name = \"group\",\n                          value.name = \"orig\") |&gt; \n           mutate(log = log(orig)) |&gt;\n           reshape2::melt(id.vars = c(\"id\", \"group\"),\n                          variable.name = \"transf\",\n                          value.name = \"value\")\ndf_test\n\n\n  \n\n\n\nAhora obtengamos los índices de los datos más cercanos a la media para cada grupo que, evidentemente, tienen que ser iguales (solo los desplazamos a la derecha):\n\ncls_A &lt;- DescTools::Closest(A, mean(A), which = T)\ncls_B &lt;- DescTools::Closest(B, mean(B), which = T)\ncls_A \n\n[1] 9\n\ncls_B\n\n[1] 9\n\n\nRepitamos el ejercicio, pero ahora con los datos en escala logarítmica:\n\ncls_logA &lt;- DescTools::Closest(log(A), mean(log(A)), which = T)\ncls_logB &lt;- DescTools::Closest(log(B), mean(log(B)), which = T)\ncls_logA\n\n[1] 3 5\n\ncls_logB\n\n[1] 6\n\n\nAquí ya no fueron iguales. ¿Qué implicaciones tiene esto? Veamos las medias de cada uno de nuestros grupos con los datos en escala original:\n\nt_orig &lt;- t.test(A, B)\nt_orig$estimate\n\nmean of x mean of y \n    27.25     77.25 \n\nunname(t_orig$estimate[2] - t_orig$estimate[1])\n\n[1] 50\n\n\nComo era de esperarse, la diferencia es exactamente 50, que fue lo que establecimos arriba. ¿Qué pasa si hacemos lo mismo con los datos en escala logarítmica?\n\nt_log &lt;- t.test(log(A), log(B))\nt_log$estimate\n\nmean of x mean of y \n 2.494640  4.264556 \n\nunname(t_log$estimate[2]-t_log$estimate[1])\n\n[1] 1.769916\n\n\nHay una diferencia de 1.76 unidades en escala logarítmica. ¿Qué tiene que ver eso con nuestros datos originales? Ahora veamos qué pasa si reconvertimos estos resultados a la escala original: no coinciden y, de hecho, la diferencia se vuelve más grande:\n\nexp(t_log$estimate)\n\nmean of x mean of y \n 12.11737  71.13332 \n\nunname(exp(t_log$estimate[2])-exp(t_log$estimate[1]))\n\n[1] 59.01594\n\n\nEsto es completamente diferente a lo que veíamos en el Capítulo 13 con la linealización del modelo potencial, pues ahí el modelo reconvertido quedaba perfectamente dentro de nuestros datos originales.\nEn estos momentos tu podrías decirme “Ok Arturo, pero habíamos dicho que una prueba t no es la mejor manera de comparar datos de conteos” y sí, pero solamente me darías otro argumento para orillarte hacia una prueba no paramétrica y, de hecho, la posición de la mediana sí que se mantiene constante:\n\nDescTools::Closest(A, median(A), which = T)\n\n[1] 3 5\n\nDescTools::Closest(log(A), median(log(A)), which = T)\n\n[1] 3 5\n\n\n\n\n14.9.3 Raíz cuadrada\nOtra transformación muy empleada, consiste en en obtener la raíz cuadrada de cada uno de los datos (\\(\\sqrt{X}\\), OJO: SOLO SE UTILIZA CON DATOS POSITIVOS, nada de cucharear antes de transformar). Veamos su efecto en la distribución:\n\ndf2 &lt;- rbind(df2, data.frame(datos = sqrt(df$datos), transf = \"sqrt(x)\"))\nkde_plots &lt;- kdeplots(df2, aes(datos, color = transf)) +\n             facet_wrap(~transf, scales = \"free\")\nkde_plots\n\n\n\n\n\n\n\n\nEn este caso el cambio en la forma no es tan agresivo, y la consecuencia es únicamente que las diferencias entre los valores más altos y los más pequeños se redujo. Su re-transformación es: \\(\\sqrt{x}^2\\). Su uso más común es con datos de conteo (abundancias, bacterias en una caja petri, etc.); sin embargo, al igual que con la transformación logarítmica, la decisión sobre su uso debe de ir más en función del proceso que se está modelando que de si los datos están sesgados o no.\n\n\n14.9.4 Transformaciones como estabilizadoras de la varianza\nCerremos el tema de las transformaciones aterrizando una de las funciones de las transformaciones: estabilizar la varianza de un modelo. ¿A qué me refiero con esto? En pocas palabras, a linealizarlo. En el modelo de relación talla-peso hicimos justamente eso: partimos de un modelo no lineal de la forma \\(W = aL^B\\) y aplicando logaritmos llegamos al modelo \\(log(W) = log(a) + B* log(L)\\). De manera más precisa, cuando hablamos de “estabilizar la varianza” hablamos de la varianza del modelo, y el objetivo de este tipo de transformaciones es poder aplicar una regresión lineal. Este tema es bastante complejo, tendríamos que hablar de las dichosas transformaciones de potencias (power transformations) y la prueba Box-Cox para determinar la transformación óptima. Por simplicidad no vamos a entrar en esos detalles, además de que veremos algunas técnicas que pueden ser más útiles, pero quiero que te lleves algo:\n\n\n\n\n\n\nImportante\n\n\n\nTodos los supuestos van alrededor de la distribución del error, no de los datos. En el Capítulo 19 vamos a hablar de, justamente, como extender el modelo lineal a errores no normales.\n\n\nCOROLARIO: No apliques transformaciones si no tienes claro el qué estás transformando, por qué lo estás transformando, y cómo debes de interpretar los resultados para obtener conclusiones válidas a partir de tus datos.",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Técnicas multivariadas: Introducción</span>"
    ]
  },
  {
    "objectID": "c14_intromv.html#aprendizaje-automatizado",
    "href": "c14_intromv.html#aprendizaje-automatizado",
    "title": "14  Técnicas multivariadas: Introducción",
    "section": "14.10 Aprendizaje Automatizado",
    "text": "14.10 Aprendizaje Automatizado\nAhora hablemos sobre la diferencia entre estadística multivariada y aprendizaje automatizado. Hasta este momento la mayor parte de nuestros problemas han consistido en tener unos datos, encasillarlos en un conjunto de reglas utilizando programación o pruebas de significancia estadística para obtener una respuesta (e.g., “las medias de los grupos fueron diferentes; Figura 14.3); sin embargo, vimos en los Capítulo 11 y Capítulo 13 que podemos decirle a la computadora que nos diga bajo qué reglas están jugando nuestros datos. Eso justamente es el aprendizaje automatizado Figura 14.4, Figura 14.5. Este Machine Learning es una parte de la estadística, en la cuál la computadora”aprende” a partir de ejemplos que nosotros le proporcionamos. En los temas de regresíon teníamos pares de datos \\(x\\) y \\(y\\), y le dijimos a la computadora que encontrara un conjunto de parámetros que permitieran transormar los valores de \\(x\\) a los valores de \\(y\\) con el menor error posible.\n\n\n\n\n\n\nFigura 14.3: Paradigma tradicional para resolver problemas.\n\n\n\nDentro del aprendizaje automatizado tenemos dos categorías generales:\n\nAprendizaje supervisado: Aquí se engloban las técnicas que, como nuestras regresiones, mapean los valores de nuestros predictores hacia los valores de la variable (o las variables) a predecir. Esta categoría se sub-divide en dos clases adicionales, que están definidas por las características de la variable dependiente:\n\nRegresión: En esta la variable dependiente es una variable numérica (puede ser continua u ordinal). Tanto los modelos lineales como no lineales forman parte de esta clase de técnicas.\nClasificación: Si la regresión trabaja con variables numéricas (ordinales o continuas), la clasificación trabaja con variables nominales (algunas pueden trabajar con variables ordinales); es decir, lo que vamos a predecir son etiquetas de clases o grupos (de ahí “clasificación”). Tenemos una gran cantidad de algoritmos de clasificación, desde los más simples que suponen diferencias lineales entre las clases hasta algunos altamente complejos con grandes cantidades de parámetros como las redes neuronales convolucionales.\n\n\n\n\n\n\n\n\nFigura 14.4: Aprendizaje supervisado.\n\n\n\n\nAprendizaje no supervisado: Aquí se engloban las técnicas o algoritmos que permiten extraer patrones ocultos en los datos. En el aprendizaje automatizado no tenemos una variable dependiente/de respuesta, solamente un conjunto de observaciones multivariadas de las cuales queremos extraer información que nos sea útil. Aquí también tenemos diversas clases, pero podemos resumirlas en dos:\n\nReducción de dimensionalidad: Aquí el objetivo es poder “resumir” nuestros datos multivariados en una menor cantidad de variables, conservando la mayor cantidad de la información original. Aquí encontramos técnicas como los Ánálisis de Componentes Principales (PCA), Análisis de Coordenadas Principales (PCoA), Escalamiento Multidimensional No Métrico (NMDS). Si ya has tenido un acercamiento previo a estas técnicas puede que esto no te cuadre del todo, pues el NMDS (por ejemplo) es una técnica de ordenación. Y no estarías mal, pero en el proceso estamos reduciendo la dimensional original de los datos para poder realizar la ordenación. Nuestra entrada en estas técnicas es un conjunto de observaciones multivariadas, y la salida otro conjunto de observaciones compuesto por un menor número de variables\nAgrupamientos: El nombre lo dice todo, su objetivo es el formar grupos. ¿Cómo? Igual a como lo haríamos nosostros: juntando las instancias que más se parecen entre sí en un solo grupo, y mandando a las que no se parecen a otro grupo. Nuestra entrada es un conjunto de observaciones multivariadas, y la salida la agrupación a la que pertenece cada una.\n\n\n\n\n\n\n\n\nFigura 14.5: Aprendizaje no supervisado.\n\n\n\n\n\n\n\n\n\nNota\n\n\n\n¿Por qué “supervisado” y “no supervisado”? Porque en el primero tenemos una función de pérdida (error) que podemos supervisar y optimizar para mejorar nuestras predicciones, mientras que en el segundo no.\n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nAunque en la clasificación y en los agrupamientos estemos lidiando con grupos o clases, no son lo mismo. La clasificación es aprendizaje supervisado, la agrupación es aprendizaje no supervisado.\n\n\nAdicional a estas dos tenemos otras dos: el aprendizaje por refuerzo y el aprendizaje semi-supervisado, pero esos quedan fuera del alcance de este curso.\nCon esto terminamos esta sesión. Sé que estuvo fuertemente cargada de teoría y matemática, pero es necesario que tengas al menos una noción de dónde surgen las cosas. No es necesario que te las aprendas de memoria, o que sepas calcular las matrices a mano, solo que entiendas la lógica detrás de los procedimientos para que no abuses de ellos. Como recompensa por llegar hasta aquí, no hay ejercicio para esta sesión ;).\n\n\n\n\nAnderson MJ. 2005. Distance-Based Tests for Homogeneity of Multivariate Dispersions. Biometrics 62:245-253. DOI: 10.1111/j.1541-0420.2005.00440.x.",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Técnicas multivariadas: Introducción</span>"
    ]
  },
  {
    "objectID": "c15_nosup.html",
    "href": "c15_nosup.html",
    "title": "15  Aprendizaje No Supervisado",
    "section": "",
    "text": "15.1 Librerías\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(vegan)\nlibrary(ggdendro)\nlibrary(dendextend)\nlibrary(FactoMineR)\nlibrary(factoextra)\nlibrary(ggrepel)\ntheme_set(see::theme_lucid() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()))",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Aprendizaje No Supervisado</span>"
    ]
  },
  {
    "objectID": "c15_nosup.html#introducción",
    "href": "c15_nosup.html#introducción",
    "title": "15  Aprendizaje No Supervisado",
    "section": "15.2 Introducción",
    "text": "15.2 Introducción\nEn el Capítulo 14 hablamos de que en el aprendizaje no supervisado vamos a extraer patrones o grupos contenidos en los datos, lo cual podemos hacer mediante la reducción de la dimensionalidad o formando agrupaciones. Bueno, hablemos a fondo de ambas y veamos algunas técnicas.",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Aprendizaje No Supervisado</span>"
    ]
  },
  {
    "objectID": "c15_nosup.html#agrupamientos",
    "href": "c15_nosup.html#agrupamientos",
    "title": "15  Aprendizaje No Supervisado",
    "section": "15.3 Agrupamientos",
    "text": "15.3 Agrupamientos\nEmpecemos por lo “sencillo”: formar agrupaciones. ¿Por qué las comillas? Para responder a esa pregunta pongamos un ejemplo. Observa la siguiente imagen y dime: ¿cuántos grupos formarías? ¿qué “bicho” entra en cuál grupo?\n\n\n\n\n\n\nFigura 15.1: Artrópodos a agrupar\n\n\n\nUna forma de hacerlo es poner a las mariposas/polillas en un grupo, los caracoles en otro, las arañas en otro y las abejas en otro:\n\n\n\n\n\n\nFigura 15.2: Artrópodos agrupados\n\n\n\nFue bastante sencillo, ¿no? A los seres humanos no nos resulta especialmente complicado categorizar, pero para la computadora hay un total de 115,975 combinaciones de estas imágenes. ¿Cómo saber cuáles son las agrupaciones relevantes? Hay una gran variedad de técnicas, que podemos partir en dos grandes grupos:\n\nMétodos basados en distancias: Agrupamiento jerárquico, métodos particionales (k-medias, k-medianas o k-medioides) y espectrales.\nMétodos basados en densidades: OPTICS, DBSCAN, HDBSCAN, entre otros.\n\nCada método tiene sus peculiaridades, ventajas y desventajas, dadas por la lógica con la que trabajan y, en consecuencia, con diversos niveles de “éxito” según los datos con los que estemos trabajando:\n\n\n\n\n\n\nFigura 15.3: Técnicas de agrupaciones en distintos juegos de datos sintéticos. Nota: los datos no tienen grupos “reales”, solo algunas formas interesantes (Tomado de la documentación de Scikit-Learn).\n\n\n\n\n15.3.1 Agrupamiento jerárquico\nDe toda esta diversidad de técnicas hay una que se ha destacado en el área de la biología: el agrupamiento jerárquico, mal llamado análisis clúster. Esta técnica tiene una lógica muy sencilla e intuitiva (lo cual es parte de la razón por la que sea tan popular):\n\nCalcula las distancias entre todos los pares de observaciones.\nforma un grupo con el par con la menor distancia.\nRepite hasta que termina con todos los datos.\n\nVeamos cómo es esto paso a paso con unos datos de ejemplo:\n\nLongitudes totales de algunas especies de cetáceos.\n\n\nEspecie\nLongitud total (m)\n\n\n\n\nTursiops truncatus (Tt)\n3\n\n\nGrampus griseus (Gg)\n3.6\n\n\nGlobicephala macrorhyncus (Gm)\n6.5\n\n\nOrcinus orca (Oo)\n7.5\n\n\nMegaptera novaeangliae (Mn)\n15\n\n\nBalaenoptera physalus (Bp)\n20\n\n\n\nEntonces los pasos con los que agruparíamos estas especies, según su longitud total, son:\n\nCalcular una matriz de distancias/disimilitudes Literalmente ver qué tanto se parecen nuestras especies entre sí. ¿Cómo hacemos esto? Utilizando alguna medida de distancia o disimilitud, pero dejemos el tema de la selección de la medida para después y utilicemos la distancia Euclidiana:\n\n\n\n\n\n\n\nNota\n\n\n\n¿Qué es la distancia Euclidiana? Es una medida basada en el famosísimo teorema de Pitágoras (¿creías que no servía para nada?):\n\\[D_{a,b} = \\sqrt{\\sum_{i = 1}^n (q_i - p_i)^2}\\]\n\n\n\n\n\n\nFigura 15.4: Teorema de Pitágoras\n\n\n\nEsta medida de distancia representa también la geometría que utilizamos en el día a día, por lo que la interpretación en casos univariados es directa, representando las unidades de diferencia que hay entre una observación y otra.\n\n\n\n\n\n\n\n\nFigura 15.5: Matriz de distancias. Se calcula la distancia entre cada par de observaciones y se disponen todas en forma matricial. Está solo la diagonal inferior; es decir, solo los datos de la diagonal hacia abajo, pues la diagonal superior es una imagen especular de lo que está abajo.\n\n\n\n\nIdentificar el par con la menor distancia y formar un grupo con ellos. Esto corresponde al método de unión (agrupamiento o amalgamiento) simple o sencillo, en el que encontramos la menor distancia en la matriz, que corresponde a Tt y Gg (\\(D = 0.6\\)), por lo que formamos un primer grupo [Tt, Gg], el cual estará representado por el promedio de Tt y Gg: 3.3.\nVolver a calcular la matriz de distancias/disimilitudes, sustituyendo las observaciones agrupadas por su grupo; es decir, ahora ya no vamos a incluir Tt y Gg en el cálculo de la matriz, sino que incluiremos [Tt, Gg] = 3.3:\n\n\n\n\n\n\n\nFigura 15.6: Matriz de distancias después de formar la primera agrupación entre [Tt, Gg]\n\n\n\n\nRepetir 2 y 3 hasta haber agrupado todas las observaciones.\n\n\n\n\n\n\n\nFigura 15.7: Agrupamiento jerárquico utilizando las distancias.\n\n\n\nAl final tenemos nuestro agrupamiento jerárquico [[[Tt, Gg], [Gm, Oo]], [Mn, Bp]]. ¿Por qué jerárquico? Porque tenemos grupos dentro de otros grupos. El grupo [Tt, Gg], por ejemplo, forma parte de un grupo más grande, donde se une con el grupo [Gm, Oo] y este grupo de grupos, a su vez, es parte de otro grupo más grande, donde se une con [Mn, Bp]. Ahora bien, esta notación es compacta, pero puede ser muy incómoda de leer si tenemos una gran cantidad de datos, por lo que en su lugar utilizamos una estructura conocida como dendrograma.\n\n\n15.3.2 El dendrograma\nUn dendrograma es una visualización de datos en la que se muestra al mismo tiempo las agrupaciones y las distancias a las que se unieron simulando el crecimiento de las ramas de un árbol por lo que podemos ver no solo quiénes forman qué grupos, sino que tan parecidos son los miembros de cada uno y qué tan diferentes son con los demás grupos. El dendrograma resultante de nuestro ejemplo es el siguiente:\n\n\n\n\n\n\nFigura 15.8: Dendrograma y cetáceos.\n\n\n\nComo ves en la figura Figura 15.8, nuestro dendrograma está compuesto por algunas estructuras:\n\nRaíz: Es la distancia a la que se unen todos nuestros grupos. En nuestro ejemplo hace falta porque no tenemos un “grupo hermano”; es decir, un grupo que sea cercano a nuestras especies, pero que no sea un cetáceo. Podríamos incluir un hipopótamo, por ejemplo. Si tenemos ese grupo hermano podemos graficar la raíz, y entonces nuestro dendrograma estará enraizado, de lo contrario tenemos un dendrograma no enraizado. El árbol comienza a “crecer” desde este punto (un solo grupo) hasta tener separados todos los puntos en el conjunto de datos.\nRamas: Cada una de las líneas verticales del árbol. Indican el “flujo” de las similaridades, en el sentido de que seguirán extendiéndose hasta la distancia en la que se forme una nueva agrupación.\nNodo: Es el punto donde se unen dos ramas; es decir, el punto en el que se forma una nueva agrupación.\nHojas: Son las “ramas terminales”, que se puede entender como un grupo de ramas que representan cada una de las instancias que conforman la base de datos. Las instancias más parecidas son las contiguas, y el parecido disminuye conforme nos alejamos de cada punto.\nClúster: Es un conjunto de hojas. ¿Cómo lo definimos? Esa pregunta merece que le dediquemos tiempo, pero primero hablemos de otros dos conceptos que dictan la forma final de nuestro dendrograma.\n\n\n\n15.3.3 Hablemos de distancias\nComo te darás cuenta, es un proceso iterativo y que puede llevar bastante tiempo, pero también es bastante sencillo. Ahora bien, ¿qué medidas de distancia/disimilitud hay y cuál utilizar? Resulta que la medida de distancia/disimilitud que seleccionemos determina la geometría del espacio en el que estamos haciendo el análisis. ¿La qué cosa de quien? Es más fácil entenderlo si te digo que un triángulo trazado en un papel tiene una sumatoria de sus ángulos diferente a un triángulo trazado en una esfera. Un ejercicio que puedes realizar para ver las consecuencias de moverse en diferentes espacios geométricos es, justamente, trazar un triángulo sobre una naranja, después retirarle la cáscara, ponerla sobre una superficie plana, y ver qué pasa con el triángulo que trazaste originalmente. Otra forma es una representación visual. En la Figura 15.9 tienes representadas tres distancias: la distancia Euclidiana (amarillo), Manhattan (rojo) y Chebyshev (azul). En todos los casos la distancia desde cualquier punto del perímetro hacia el centro es de 4 unidades, pero la forma del perímetro resultante es diferente porque el espacio geométrico es diferente:\n\n\n\n\n\n\nFigura 15.9: Formas de los espacios geométricos definidos por las distancias Euclidiana, Manhattan y Chebyshev. Las tres son casos especiales de la distancia Minkowski, con diferentes valores de \\(r\\)\n\n\n\n\n\n\n\n\n\nNota\n\n\n\n¿Es lo mismo una medida de distancia que una de disimilitud? Sí y no. Desde el punto de vista práctico ambas nos dan una idea de quién se junta con quien; sin embargo, formalmente no son lo mismo. Una distancia cumple con la desigualdad triangular, mientras que una disimilitud no. ¿Qué es la desigualdad triangular? Que la distancia más corta entre dos puntos es una linea recta. En una disimilitud esto no es el caso, pues el espacio geométrico no es Euclidiano. Pinta dos puntos sobre la superficie de una naranja y únelos con una línea. No es recta, ¿o sí?\n\n\nEsto, obviamente tiene consecuencias, y hace que algunas distancias o disimilitudes funcionen mejor en ciertos casos y peor en otros. Shirkhorshidi, Aghabozorgi & Wah (2015) compilaron la siguiente lista, con algunas distancias y detalles muy útiles sobre su aplicación:\n\n\n\n\n\n\nFigura 15.10: Medidas de distancia y sus peculiaridades.\n\n\n\n¿Cuál utilizar? Como ves en la Figura 15.10, depende de la naturaleza de tus datos pero, por lo general, la distancia Euclidiana es un buen punto de partida.\n\n\n15.3.4 Métodos de unión\nLa otra cosa que debemos de decidir es el criterio bajo el cuál vamos a unir nuestros grupos. En el ejercicio con nuestros cetáceos utilizamos el método simple/sencillo, pero no es el único. En todos los métodos se busca minimizar algo, la diferencia es el qué se minimiza. Tomemos el siguiente problema, donde queremos unir el punto solitario con algo de los otros clústers:\n\n\n\n\n\n\nFigura 15.11: ¿Con quién voy?\n\n\n\n\nSimple: También llamado sencillo o método del “vecino más cercano”, en el que se minimiza la separación mínima entre clústers; es decir, vamos a calcular la matriz de distancias y formar una agrupación con los grupos que tengan los puntos con la menor distancia:\n\n\n\n\n\n\n\nFigura 15.12: ¿Con mi vecino más cercano?\n\n\n\n\nCompleto: También lo puedes encontrar como el método del “vecino más lejano”, porque minimiza la separación máxima entre clústers; es decir, se calcula la matriz de distancias, se seleccionan los puntos de cada grupo entre los que se encuentra la distancia, y de esos se agrupan los que tengan la menor. ¿Rebuscado? Sin duda, pero gráficamente es más sencillo:\n\n\n\n\n\n\n\nFigura 15.13: ¿Con mi vecino más lejano?\n\n\n\n\nCentroide: Si en el método simple se minimiza la separación mínima entre clústers, y en el método completo la separación máxima, debe de haber un punto intermedio. El método del centroide minimiza la distancia entre los puntos centrales de los clústers:\n\n\n\n\n\n\n\nFigura 15.14: ¿Con mi vecino central?\n\n\n\n\nPromedio: En este se minimiza la distancia promedio entre clústers; es decir, calcula todas las distancias entre los puntos, obtiene el promedio de la distancia por cada par de grupos, y forma uno nuevo con los grupos/puntos que resultaron en la menor distancia promedio.\nWard: Este es un poco más abstracto, pues minimiza la suma de cuadrados del error (ESS) o, en otras palabras, la varianza de los grupos resultantes. Es decir que primero agrupa cada par de grupos disponible, calcula la varianza resultante y se queda con la agrupación que tenga la menor.\n\n¿Cómo decidir cuál utilizar? Este es un análisis no supervisado, por lo que no podemos utilizar algún criterio matemático para definir si nuestra selección de distancia/método de unión es la correcta. Lo que sí podemos hacer es ver si tienen sentido las agrupaciones resultantes, lo cual me lleva a la siguiente parte.\n\n\n15.3.5 ¿Cuántos grupos?\nAhora sí, ¿con qué magia obscura determinamos cuántos grupos tenemos y quién pertenece a cuál? La forma es muy sencilla desde el punto de vista práctico: seleccionar un nivel de corte en nuestro dendrograma; es decir, una distancia en la cual el árbol deja de exenderse, y el número de grupos está dado por el número de ramas inmediatamente debajo:\n\n\n\n\n\n\nFigura 15.15: Cortando el árbol. Las líneas horizontales representan algunos posibles niveles de corte, el número a su derecha el número de grupos resultantes. Los colores de las etiquetas de las hojas representan el resultado de formar 4 agrupaciones.\n\n\n\nNuevamente, este es un análisis no supervisado, por lo que no tenemos etiquetas de grupos para evaluar el agrupamiento; sin embargo, podemos utilizar distintas medidas (30, de hecho) para estimar el nivel de corte. ¿Cuántas? 30, por ejemplo, y luego formar un ensamble; es decir, tomar el voto mayoritario; es decir, quedarnos con el número de agrupaciones que tenga el mayor número de “votantes”. El último “sello de garantía” de un análisis clúster es el coeficiente de correlación cofenética, el cuál es una medida de lo bien o mal representadas que están las distancias reales en el dendrograma.\n\n\n15.3.6 Implementación\nRealizar un agrupamiento jerárquico en R es bastante sencillo, únicamente debemos de tener un data.frame cuyos nombres de renglones sean las etiquetas de las instancias a agrupar, calcular las distancias con la función dist(x, method) y unir los grupos con la función hclust(dist, method).\n\ndf1 &lt;- data.frame(long = c(3, 3.6, 6.5, 7.5, 15, 20),\n                  row.names = c(\"Tt\", \"Gg\", \"Gm\", \"Oo\", \"Mn\", \"Bp\"))\ndist_mat &lt;- dist(df1, method = \"euclidean\")\nhc_av &lt;- hclust(dist_mat, method = \"average\")\n\ndf1\n\n\n  \n\n\n\nEste objeto podemos graficarlo con ggplot, utilizando la librería ggdendro, lo cual nos da un dendrograma bastante sencillo:\n\nggdendrogram(hc_av, labels = T) +\n  labs(x = element_blank(),\n       y = \"Distancia Euclidiana\")\n\n\n\n\n\n\n\n\nSi bien es cierto que este dendrograma es “suficiente”, podemos personalizarlo utilizando la función dendextend::set(object, what, value):\n\n# Transformamos los agrupamientos a un dendrograma\ndend &lt;- as.dendrogram(hc_av)\n\n# Cambiamos el color a las ramas\ndend &lt;- set(dend, \"branches_k_color\",\n            value = \"deepskyblue4\",\n            # value = c(\"red\", \"blue\"),\n            k = 1)\n\n# Cambiamos el ancho de las ramas\ndend &lt;- set(dend, \"branches_lwd\", 1)\n\n# Cambiamos el color de las etiquetas asumiendo 4 grupos\ndend &lt;- set(dend, \"labels_col\",\n            value = c(rgb(0,118, 186, maxColorValue = 255),\n                      rgb(0, 123, 118, maxColorValue = 255),\n                      rgb(255, 147, 0, maxColorValue = 255),\n                      rgb(181, 23, 0, maxColorValue = 255)), \n            k = 4)\n\nCon estas líneas añadimos las características visuales que queremos que tenga el dendrograma, pero falta graficarlo utilizando ggplot:\n\ngg_dend &lt;- as.ggdend(dend)\ngg_dendro &lt;- ggplot(gg_dend,\n                    offset_labels = -1,\n                    theme = theme_bw()) +\n             ylim(-2.4, max(get_branches_heights(dend))) +\n             scale_x_continuous(breaks = NULL) +\n             labs(title = \"Dendrograma de especies de cetáceos\",\n                  subtitle = \"Agrupamientos por longitud (método: promedio)\",\n                  x = element_blank(),\n                  y = \"Distancia euclidiana\",\n                  caption = \"Datos de bit.ly/clust_medium\")\n\ngg_dendro\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNota\n\n\n\nSi te das cuenta, solo indicamos en set(dend, labels_col) que tenemos cuatro grupos y pasamos un vector con cuatro colores, los cuales se asignaron en nuestro dendrograma. ¿Cómo supo dendextend quién iba en qué grupo? Recuerda que lo que buscamos es un nivel de corte en el dendrograma, por lo que solo hay una forma de agrupar los datos en la que tenemos cuatro grupos resultantes.\n\n\nAhora bien, esta es una técnica bastante sencilla de implementar (la mayor parte del “engorro” la hicimos al graficar), pero eso no quiere decir que debamos utilizarla a diestra y siniestra sin tener cuidado de qué datos le damos a R. Es necesario que nuestras variables estén aproximadamente en la misma escala (unidades, decenas, milésimas, etc.), pues de lo contrario las variables con las magnitudes más grandes van a llevar un mayor peso durante la agrupación. Por otro lado, también hay que tener cuidado con variables que estén muy sesgadas, pues esa distribución va a modificar el cálculo de las distancias y, por lo tanto, los agrupamientos finales.\n\n\n15.3.7 Ejercicio multivariado\nYa sabemos cómo funciona el análisis de ahrupamientos y también cómo implementarlo en R. ¿Qué falta? Aplicarlo con un conjunto de datos con más observaciones y variables. Para esto utilizaremos los datos cluster.txt, en los cuales tenemos mediciones de variables ambientales en distitntos sitios de muestreo, y nuestro objetivo es agrupar aquellos que tengan caracterísiticas similares. Si vemos el resumen de los datos podemos ver que hay diferencias sumamente importantes en las escalas de las variables, por lo que será necesario escalar los datos:\n\nclust_df &lt;- read.table(\"datos/cluster.txt\",\n                       header = T, row.names = 1) # ¡OJO con row.names!\nsummary(clust_df)\n\n      Temp            NH4             NO3              OD             Prof    \n Min.   :16.90   Min.   :0.540   Min.   :0.400   Min.   :0.810   Min.   : 27  \n 1st Qu.:18.20   1st Qu.:0.900   1st Qu.:1.830   1st Qu.:2.650   1st Qu.: 76  \n Median :19.60   Median :0.980   Median :2.500   Median :3.800   Median :122  \n Mean   :20.31   Mean   :1.047   Mean   :2.394   Mean   :4.103   Mean   :109  \n 3rd Qu.:21.40   3rd Qu.:1.250   3rd Qu.:3.120   3rd Qu.:5.840   3rd Qu.:148  \n Max.   :25.80   Max.   :1.570   Max.   :4.210   Max.   :8.010   Max.   :199  \n     Trans            Caud             SST              STD       \n Min.   : 4.00   Min.   : 0.520   Min.   :   4.0   Min.   : 86.9  \n 1st Qu.: 6.00   1st Qu.: 1.960   1st Qu.:  50.0   1st Qu.:101.5  \n Median :10.00   Median : 5.580   Median : 134.0   Median :104.3  \n Mean   :17.62   Mean   : 5.877   Mean   : 254.3   Mean   :108.7  \n 3rd Qu.:18.00   3rd Qu.: 8.100   3rd Qu.: 298.7   3rd Qu.:113.7  \n Max.   :56.00   Max.   :15.440   Max.   :1163.3   Max.   :137.7  \n      PO4             DBO5            DQO       \n Min.   :0.000   Min.   :20.21   Min.   : 5.00  \n 1st Qu.:0.250   1st Qu.:21.29   1st Qu.:16.00  \n Median :0.400   Median :23.27   Median :32.00  \n Mean   :0.391   Mean   :27.45   Mean   :32.52  \n 3rd Qu.:0.600   3rd Qu.:32.62   3rd Qu.:50.00  \n Max.   :0.870   Max.   :59.82   Max.   :62.00  \n\n\nAdemás de las diferencias en órdenes de magnitud, tenemos otro problema no tan evidente: la distribución de SST se encuentra bastante sesgada, por lo que habrá que a) aplicar una transformación para “normalizar” los datos o b) retirarla del análisis. No sabemos a qué corresponde SST, por lo que no sabemos si las mediciones están bien o mal, por lo que es mejor errar por precavidos y retirarla del análisis.\n\nmask &lt;- colnames(clust_df)[colnames(clust_df) != \"SST\"]\nclust_filt &lt;- clust_df[, mask]\ncolnames(clust_filt)\n\n [1] \"Temp\"  \"NH4\"   \"NO3\"   \"OD\"    \"Prof\"  \"Trans\" \"Caud\"  \"STD\"   \"PO4\"  \n[10] \"DBO5\"  \"DQO\"  \n\n\nAhora sí, podemos escalar todos nuestros datos. Si observamos el resumen es evidente que hay algunas otras variables que también están sesgadas; sin embargo, podemos utilizar una medida de distancia que contienda con este tipo de distribuciones para evitar una transformación más agresiva:\n\nclust_scale &lt;- scale(clust_filt)\nsummary(clust_scale)\n\n      Temp             NH4               NO3                OD         \n Min.   :-1.315   Min.   :-1.8357   Min.   :-1.9515   Min.   :-1.4896  \n 1st Qu.:-0.814   1st Qu.:-0.5326   1st Qu.:-0.5518   1st Qu.:-0.6573  \n Median :-0.275   Median :-0.2430   Median : 0.1039   Median :-0.1370  \n Mean   : 0.000   Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  \n 3rd Qu.: 0.418   3rd Qu.: 0.7343   3rd Qu.: 0.7108   3rd Qu.: 0.7859  \n Max.   : 2.112   Max.   : 1.8926   Max.   : 1.7776   Max.   : 1.7675  \n      Prof             Trans              Caud               STD         \n Min.   :-1.5885   Min.   :-0.8294   Min.   :-1.19044   Min.   :-1.4307  \n 1st Qu.:-0.6393   1st Qu.:-0.7076   1st Qu.:-0.87045   1st Qu.:-0.4734  \n Median : 0.2518   Median :-0.4640   Median :-0.06603   Median :-0.2898  \n Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.00000   Mean   : 0.0000  \n 3rd Qu.: 0.7555   3rd Qu.: 0.0232   3rd Qu.: 0.49395   3rd Qu.: 0.3266  \n Max.   : 1.7435   Max.   : 2.3373   Max.   : 2.12502   Max.   : 1.9003  \n      PO4                DBO5              DQO          \n Min.   :-1.45097   Min.   :-0.7880   Min.   :-1.43661  \n 1st Qu.:-0.52313   1st Qu.:-0.6704   1st Qu.:-0.86246  \n Median : 0.03358   Median :-0.4548   Median :-0.02734  \n Mean   : 0.00000   Mean   : 0.0000   Mean   : 0.00000  \n 3rd Qu.: 0.77585   3rd Qu.: 0.5631   3rd Qu.: 0.91217  \n Max.   : 1.77792   Max.   : 3.5244   Max.   : 1.53851  \n\n\n\n\n\n\n\n\nNota\n\n\n\nRecuerda que si trabajamos con datos estandarizados estamos escalando nuestros datos a valores de una distribución Z, los cuáles representan a cuántas desviaciones estándar estamos de la media.\n\n\nAhora realicemos los agrupamientos utilizando la distancia Mahalanobis (que no es tan sensible a valores extremos y considera la correlación entre las variables) y el método de unión Ward.D2 (unión Ward con distancias cuadráticas) para minimizar la varianza intra-grupos:\n\ndist_mv1 &lt;- vegan::vegdist(clust_scale, method = \"mahalanobis\")\nhc_mv1 &lt;- hclust(dist_mv1, method = \"ward.D2\")\n\n# Transformamos a dendrograma\ndend_mv1 &lt;- as.dendrogram(hc_mv1)\n\n# Cambiamos el color de las ramas\ndend_mv1 &lt;- set(dend_mv1, \"branches_k_color\",\n            value = \"deepskyblue4\",\n            k = 1)\n\n# Cambiamos el ancho de las ramas\ndend.mv1 &lt;- set(dend_mv1, \"branches_lwd\", 0.7)\n\n# Graficado\nggd1 &lt;- as.ggdend(dend_mv1)\nggd1_plot &lt;- ggplot(ggd1, offset_labels = -1, theme = theme_bw()) + \n             expand_limits(y = -2.4) +\n             scale_x_continuous(breaks = NULL) +\n             scale_y_continuous(breaks =\n                                  seq(0,max(get_branches_heights(dend_mv1)),\n                                      2)) +\n             labs(title = \"Dendrograma de sitios de muestreo\",\n                  subtitle = \"Método de agrupamiento: Ward.D2\",\n                  x = element_blank(),\n                  y = \"Distancia Mahalanobis\",\n                  caption = \"Datos: clust.txt\")\n\nggd1_plot\n\n\n\n\n\n\n\n\n¿Puedes decirme cuántos grupos hay? Veamos qué pasa si utilizamos la distancia Euclidiana:\n\ndist_mv2 &lt;- vegdist(clust_scale, method = \"euclidean\")\nhc_mv2 &lt;- hclust(dist_mv2, method =\"ward.D2\")\n\n# Transformemos nuestro objeto a un dendrograma:\ndend_mv2 &lt;- as.dendrogram(hc_mv2)\n\n# Cambiemos el color a las ramas:\ndend_mv2 &lt;- set(dend_mv2, \"branches_k_color\",\n            value = \"deepskyblue4\",\n            k = 1)\n\ndend_mv2 &lt;- set(dend_mv2, \"branches_lwd\", 0.7)\n\nggd2 &lt;- as.ggdend(dend_mv2)\nggd2_plot &lt;- ggplot(ggd2, offset_labels = -1, theme = theme_bw()) + \n             expand_limits(y = -2.4) +\n             scale_x_continuous(breaks = NULL) +\n             scale_y_continuous(breaks = \n                                  seq(0,\n                                      max(get_branches_heights(dend_mv2)),\n                                      2)) +\n             labs(title = \"Dendrograma de sitios de muestreo\",\n                  subtitle = \"Método de agrupamiento: Ward.D2\",\n                  x = element_blank(),\n                  y = \"Distancia Euclidiana\",\n                  caption = \"Datos: clust.txt\")\n\nggd2_plot\n\n\n\n\n\n\n\n\n¿Y con la distancia Manhattan? Las agrupaciones resultantes son similares; sin embargo, las medidas de distancia son muy diferentes. Entonces, arbitrariamete, nos quedaremos con la distancia Euclidiana.\n\ndist_mv3 &lt;- vegdist(clust_scale, method = \"manhattan\")\nhc_mv3 &lt;- hclust(dist_mv3, method =\"ward.D2\")\n\n# Transformemos nuestro objeto a un dendrograma:\ndend_mv3 &lt;- as.dendrogram(hc_mv3)\n\n# Cambiemos el color a las ramas:\ndend_mv3 &lt;- set(dend_mv3, \"branches_k_color\",\n            value = \"deepskyblue4\",\n            k = 1)\n\ndend_mv3 &lt;- set(dend_mv3, \"branches_lwd\", 0.7)\n\nggd3 &lt;- as.ggdend(dend_mv3)\nggd3_plot &lt;- ggplot(ggd3, offset_labels = -1, theme = theme_bw()) + \n             expand_limits(y = -3.5) +\n             scale_x_continuous(breaks = NULL) +\n             scale_y_continuous(breaks =\n                                  seq(0,max(get_branches_heights(dend_mv3)),\n                                             5)) +\n             labs(title = \"Dendrograma de sitios de muestreo\",\n                  subtitle = \"Método de agrupamiento: Ward.D2)\",\n                  x = element_blank(),\n                  y = \"Distancia Manhattan\",\n                  caption = \"Datos: clust.txt\")\n\nggd3_plot\n\n\n\n\n\n\n\n\n\n\n15.3.8 ¿Cuántos grupos?\nPara calcular los 30 índices de los que podemos echar mano para estimar el número “óptimo” de agrupaciones utilizaremos la función NbClust de la librería con el mismo nombre. La salida de esta función nos dice el número de índices que proponen cada número de agrupamientos (4 propusieron 12, 11 propusieron 3, etc.) y una conclusión: “De acuerdo con la regla de la mayoría, el mejor número de clústers es 3”. Además de estos índices numéricos tenemos dos índices gráficos: Hubert y D. En ambos el número óptimo está dado por una “rodilla” (un punto de inflexión).\n\nn_gps &lt;- NbClust::NbClust(data = clust_scale,\n                          diss = dist_mv2,\n                          distance = NULL,\n                          method = \"ward.D2\",\n                          index = \"all\",\n                          max.nc = 8)\n\n\n\n\n\n\n\n\n*** : The Hubert index is a graphical method of determining the number of clusters.\n                In the plot of Hubert index, we seek a significant knee that corresponds to a \n                significant increase of the value of the measure i.e the significant peak in Hubert\n                index second differences plot. \n \n\n\n\n\n\n\n\n\n\n*** : The D index is a graphical method of determining the number of clusters. \n                In the plot of D index, we seek a significant knee (the significant peak in Dindex\n                second differences plot) that corresponds to a significant increase of the value of\n                the measure. \n \n******************************************************************* \n* Among all indices:                                                \n* 4 proposed 2 as the best number of clusters \n* 11 proposed 3 as the best number of clusters \n* 1 proposed 5 as the best number of clusters \n* 2 proposed 7 as the best number of clusters \n* 5 proposed 8 as the best number of clusters \n\n                   ***** Conclusion *****                            \n \n* According to the majority rule, the best number of clusters is  3 \n \n \n******************************************************************* \n\n\n\n\n\n\n\n\nNota\n\n\n\nAlgunas peculiaridades a la cuáles poner atención: la función a) recibe los datos a agrupar (después de cualquier procesado); b) puede recibir o la matriz de distancias (diss) o calcularla internamente (distance = \"method\"); c) podemos decir qué índice queremos utilizar con el argumento index (todos con index = \"all\"); y d) hay que definir un número máximo de clústers a evaluar. Este número debe de ser lo suficientemente grande para permitir al algoritmo probar distintas cantidades de agrupaciones y lo suficientemente pequeño para seguir siendo interpretable. En este cso tenemos 21 sitios, por lo que tener 10 o más grupos pudiera ya no tener caso.\n\n\nYa tenemos nuesro número de grupos (\\(k = 3\\)), pero ¿podemos confiar en nuestro dendrograma? Calculemos el coeficiente de correlación cofenético:\n\nccc &lt;- cophenetic(hc_mv2)\n# Correlación entre distancias reales y graficadas\nccofen &lt;- cor(dist_mv2, ccc, method = \"spearman\")\nccofen\n\n[1] 0.6868661\n\n\nFue de prácticamente el 70%, que para nuestros objetivos es suficiente. Ahora veamos nuestros grupos en el dendrograma:\n\n# Cambiamos el color de nuestr\ndend_mv2 &lt;- set(dend_mv2, \"labels_col\",\n                value = 1:3,\n                k = 3)\n\ndend_mv2 &lt;- set(dend_mv2, \"branches_k_color\",\n            value = 1:3,\n            k = 3)          \n          \ndend_mv2 &lt;- set(dend_mv2, \"branches_lwd\", 0.7)\n\nggd2 &lt;- as.ggdend(dend_mv2)\nggd2_plot &lt;- ggplot(ggd2, offset_labels = -1, theme = theme_bw()) + \n             expand_limits(y = -2.4) +\n             scale_x_continuous(breaks = NULL) +\n             scale_y_continuous(breaks = seq(0,max(get_branches_heights(dend_mv2)),2)) +\n             labs(title = \"Dendrograma de sitios de muestreo\",\n                  subtitle = \"Método de agrupamiento: Ward.D2\",\n                  x = element_blank(),\n                  y = \"Distancia Euclidiana\",\n                  caption = \"Datos: clust.txt\")\n\nggd2_plot\n\n\n\n\n\n\n\n\n¿Cómo podemos interpretar este dendrograma? Hay un montón de maneras, pero todas se reducen a ver cómo se relacionan estos grupos con las variables originales; es decir, no vamos a simplemente asumir que tenemos una zona alta, intermedia y baja del río solo porque en un grupo tengamos los primerios sitios, en otro los que le siguen, y en un último los faltantes. Esos son cuentos chinos. Una forma es revisar los centroides (promedios) de cada variable por cada grupo:\n\naggregate(clust_filt, by = list(n_gps$Best.partition), FUN = mean)\n\n\n  \n\n\n\nEsto evidencía gradientes en cada variable, algunos positivos (Temp, NH4, por ejemplo) y algunos negativos (e.g., NO3, OD). ¿Qué quiere decir esto? Bueno, esa interpretación ya depende del conocimiento que tengamos del área (tanto del conocimiento como del área geográfica).\n\n\n\n\n\n\nNota\n\n\n\n¿Es esta la única manera de estimar el número de grupos? No. Puedes seleccionar solo una de estas medidas (incluso puedes hacerlo dentro de NbClust con el argumento index), pero también puedes utilizar la librería pvclust. Su funcionamiento es demasiado rebuscado para describirlo en una nota, pero puedes ver la documentación correspondiente.",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Aprendizaje No Supervisado</span>"
    ]
  },
  {
    "objectID": "c15_nosup.html#reducción-de-la-dimensionalidad",
    "href": "c15_nosup.html#reducción-de-la-dimensionalidad",
    "title": "15  Aprendizaje No Supervisado",
    "section": "15.4 Reducción de la dimensionalidad",
    "text": "15.4 Reducción de la dimensionalidad\nHablemos ahora de la otra parte del aprendizaje no supervisado: la reducción de la dimensionalidad. Una manera “sencilla” de entenderla es lo que pasa cuando nosotros nos ponemos al sol (somos tridimensionales) y vemos nuestra sombra (un plano): perdemos los detalles más finos, pero nuestra estructura general se mantiene. Con nuestros datos multidimensionales haremos algo similar (Figura 15.16): proyectar datos multidimensionales a un espacio con menos dimensiones (usualmente una o dos).\n\n\n\n\n\n\nFigura 15.16: Proyectando a menos dimensiones. En tres dimensiones solo vemos una nube de puntos de colores, en dos dimensiones vemos que están “agrupados”, mientras que en una sola tenemos un gradiente continuo.\n\n\n\nInfortunadamente para nosotros, el proceso de reducir dimensiones no es tan sencillo como ir poniendo las coordenadas de dimensiones enteras en 0, pero antes de entrar a ver el cómo, veamos el por qué, utilizando una de las aplicaciones más famosas: el índice multivariado del ENSO.\n\n15.4.1 Índice Multivariado del ENSO\nSi eres parte de alguna carrera de ciencias marinas es muy posible que hayas escuchado del fenómeno del Niño, si no, aprenderás algo interesante. Su nombre completo es “El Niño Oscilación del Sur” (El Niño Southern Oscilation, ENSO; Figura 15.17), y hace referencia a un fenómeno de variabilidad ambiental interanual que tiene una fase cálida (“El Niño”) y una fase fría (“La Niña”).\nDurante la fase fría (condiciones “normales”) los vientos alisios soplan con fuerza del este al oeste en el ecuador, arrastrando consigo la capa superficial del océano y acumulando agua en las costas del Pacífico central occidental. Como sabrás, el ecuador recibe la mayor cantidad de calor solar, por lo que el agua arrastrada es agua caliente, haciendo que la termoclina sea más profunda en el occidente que en el oriente del Pacífico. Esto permite que haya una surgencia costera (emergimiento de agua profunda, rica en nutrientes, a la superficie) muy importante que sostiene pesquerías importantísimas en la corriente de Humboldt. Además, la radiación solar también hace que haya evaporación en el ecuador, por lo que el aire que circula vaya recogiendo humedad. Conforme este aire caliente va recogiendo humedad se va enfriando, perdiendo densidad, y comienza a ascender, lo cual se ve acelerado cuando choca con el continente en el Pacífico occidental. ¿Qué pasa cuando enfriamos aire húmedo rápidamente? Se forma condensación, lo cual se traduce en lluvias abundantes en esa zona. El aire entonces pierde toda su humedad, se enfría y la circulación en la atmósfera alta se invierte, por lo que hacia América viaja aire frío y seco (denso), lo cual ocasiona que descienda en América, reiniciando el ciclo. A este proceso se le conoce como celda de circulación de Walker, y como resultado tenemos una zona de baja presión en el Indo-Pacífico (sube aire) y una zona de alta presión (baja aire) cerca de América.\nDurante la fase cálida (“El Niño”) los vientos alisios se debilitan (en algunas zonas del Pacífico central el viento puede incluso soplar en dirección contraria), por lo que el agua caliente que se había acumulado en el occidente se regresa a América, hundiendo la termoclina, lo que causa que la surgencia costera “recicle” el agua caliente superficial, en vez de subir agua fría, y que llueva en América (se “invierte” la celda de Walker).\n\n\n\n\n\n\nFigura 15.17: Fases cálida (“El Niño”) y fría (“La Niña”) del ENSO.\n\n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nSimplifiqué la explicación del ENSO lo más que pude sin dejar demasiadas cosas del proceso en el tintero, pero el costo es que haya algunas imprecisiones en la explicación. Si te interesa este tema, te recomiendo que leas más al respecto. Entre tus lecturas te encontrarás el por qué de “El Niño”.\n\n\nComo te podrás imaginar, es un proceso sumamente complejo (de hecho comunica el Atlántico con el Pacífico, pero eso es otra historia) en el que se ven modificadas una gran cantidad de variables ambientales y cuyas consecuencias económicas son muy importantes (se modifica la pesca y la agricultura en prácticamente tres continentes). Podemos monitorear cada variable de manera individual, pero ¿no sería mejor poder resumirlas todas en un solo número? Pues eso fue lo que hicieron Wolter & Timlin (1998), aplicando un Análisis de Componentes Principales (Análisis de Funciones Empíricas Ortogonales, EOF) a seis variables (presión atmosférica a nivel del mar (P), componentes zonal (U) y meridional (V) del viento superficial, temperatura superficial del mar (S), temperatura superficial del aire (A), y la fracción total de nubes en el cielo (C)) en el Pacífico tropical, lo cual da lugar al Índice Multivariado del ENSO (Figura 15.18).\n\n\n\n\n\n\nFigura 15.18: Índice Multivariado del ENSO (MEI) y las variables que lo construyen.\n\n\n\n\n\n\n\n\n\nNota\n\n\n\nEl índice que se utiliza ahora es la versión 2 del MEI, el cual considera la radiación de onda larga (OLR) en vez de la la temperatura superficial del aire y la fracción total de nubes en el cielo.\n\n\n\n\n15.4.2 Análisis de componentes principales\nEspero que este ejemplo te haya motivado, pues el MEI es uno de los instrumentos más importantes en el monitoreo de las condiciones ambientales del Pacífico, así es que vamos a ver cómo funciona la técnica que lo origina: el análisis de componentes principales (PCA). Este análisis aproxima los datos utilizando una menor cantidad de variables (con \\(\\mu = 0\\) y \\(\\sigma = 1\\)), las cuales son ortogonales (independientes) entre sí. ¿Cómo hace esto? Respuesta corta: buscando la mejor proyección de los datos originales (Figura 15.19).\n\n\n\n\n\n\nFigura 15.19: Proyección de datos bivariados a una sola variable.\n\n\n\n\n\n\n\n\n\nNota\n\n\n\nVamos a ver un poco de los detalles matemáticos detrás del PCA. No te preocupes, solo es para expandir la noción detrás del PCA más allá de solo “buscar la mejor proyección de los datos originales”. Al igual que en el Capítulo 11, puedes quedarte para toda la explicación o saltar al corolario al final.\n\n\nEsa respuesta corta fue, tal vez, muy corta, así que entremos en más detalles. El PCA encuentra una de dos líneas:\n\nuna que minimice el error promedio, o\nuna que maximice la distancia al origen (suma de cuadrados, \\(SS\\)).\n\nCuriosamente, es más fácil encontrar la segunda línea utilizando un viejo conocido: el teorema de Pitágoras. Observa la Figura 15.20. Si hacemos la longitud de \\(c\\) lo suficientemente grande, el valor de \\(b\\) se hace más pequeño, ya que \\(a\\) no se mueve.\n\n\n\n\n\n\nImportante\n\n\n\nSi lo que estamos encontrando es una línea que maximice la distancia al origen, y esta línea representa una suma de cuadrados, esa línea está capturando una porción de la varianza de los datos.\n\n\n\n\n\n\n\n\nFigura 15.20: Teorema de Pitágoras, de nuevo. Dado que ya tenemos la longitud de \\(a\\), dada por el dato en sí mismo, podemos encontrar una línea teórica que minimice \\(b\\) (el error), o que maximice \\(c\\) (la distancia al origen). Por facilidad, esta línea \\(c\\) es la proyección de los datos.\n\n\n\nEsta nueva línea es nuestra proyección. En el espacio multivariado cada componente principal (CP) es una combinación lineal de nuestras variables originales, donde la pendiente de cada variable representa qué tanto contribuye esa variable a la varianza capturada por el CP. Después de haber encontrado esa primera línea vamos a girarla hasta que tenga una pendiente de 0º, el resultado es un nuevo eje (variable) llamado componente principal (CP).\n\n\n\n\n\n\nFigura 15.21: Variables ortogonales\n\n\n\nLo interesante es que si escalamos nuestra línea \\(c^2\\) a que tenga una longitud de 1 tenemos un eigenvector, en donde las pendientes se convierten en proporciones que indican la contribución (carga) de cada variable a cada CP. Por otra parte, si \\(c^2\\) es una suma de cuadrados es también el eigenvalor de cada CP, y la \\(\\sqrt{SS}\\) es su valor singular.\nEsto del valor singular es solo breviario cultural, lo que sí es importante es que tenemos una suma de cuadrados, tal y como tenemos en la definición de la varianza (Capítulo 14), por lo que si la dividimos entre los grados de libertad de la muestra podemos obtener la varianza capturada por ese CP. ¿Por qué por ese CP? ¿Recuerdas que mencioné en la definición de PCA que aproximamos los datos utilizando una menor cantidad de variables? Bueno, esta línea que encontramos es solo una de estas variables (llamadas CPs). El siguiente paso es trazar un nuevo eje (variable, CP) a exactamente 90º de la primera (Figura 15.21), y después repetir el proceso hasta haber capturado toda la varianza.\nYa que hicimos esto podemos estimar la porción de la varianza explicada por el CP dividiendo su varianza explicada entre la varianza total:\n\\[\n\\sigma^2_{PC_i} = \\frac{SS}{n-1} \\therefore \\sigma^2_{T} = \\sum_{i = 1}^k\\sigma^2_{PC_i} \\therefore \\sigma^2_{exp_i} = \\frac{\\sigma^2_{PC_i}}{\\sigma^2_T}\n\\]\n\n\n\n\n\n\nNota\n\n\n\n¿Qué son los eigenvalores y eigenvectores? Las eigencosas son de las cosas más abstractas en el álgebra lineal, pero podemos resumirlas como que los eigenvalores son una medida de dispersión, mientras que los eigenvectores son una medida de dirección.\n\n\n\n\n\n\n\n\nImportante\n\n\n\nEn resumen: el PCA es una técnica de reducción de dimensionalidad en la que los datos originales se aproximan mediante combinaciones lineales (componentes principales), las cuales son independientes (ortogonales) entre sí. Este procedimiento permite resumir los datos originales en una o dos variables que preservan la mayor cantidad de la varianza original posible.\n\n\nDespués de esta vuelta (para algunos innecesaria), apliquemos el PCA.\n\n15.4.2.1 Implementación\nUtilicemos una base de datos con una cantidad interesante de variables (31): Medidas.txt:\n\nbiom &lt;- read.table(\"datos/Medidas.txt\", header = TRUE)\nhead(biom)\n\n\n  \n\n\n\nLa primera columna tiene etiquetas de especies, las cuales no utilizaremos en este momento, por lo que podemos descartarla:\n\npca_data &lt;- biom[, 2:length(biom)]\nsummary(pca_data)\n\n     furcal            base             radio              alto        \n Min.   :0.2527   Min.   :0.07088   Min.   :0.04408   Min.   :0.05319  \n 1st Qu.:0.2716   1st Qu.:0.09108   1st Qu.:0.06748   1st Qu.:0.06709  \n Median :0.2753   Median :0.09679   Median :0.07090   Median :0.07140  \n Mean   :0.2757   Mean   :0.09645   Mean   :0.07048   Mean   :0.07116  \n 3rd Qu.:0.2801   3rd Qu.:0.10142   3rd Qu.:0.07430   3rd Qu.:0.07534  \n Max.   :0.3202   Max.   :0.11394   Max.   :0.08311   Max.   :0.09723  \n     gordo             angos              ojo             X1erarc       \n Min.   :0.02679   Min.   :0.02996   Min.   :0.02142   Min.   :0.03485  \n 1st Qu.:0.04139   1st Qu.:0.03465   1st Qu.:0.02451   1st Qu.:0.04332  \n Median :0.04402   Median :0.03577   Median :0.02673   Median :0.05129  \n Mean   :0.04405   Mean   :0.03616   Mean   :0.02740   Mean   :0.05158  \n 3rd Qu.:0.04680   3rd Qu.:0.03745   3rd Qu.:0.02875   3rd Qu.:0.05899  \n Max.   :0.05859   Max.   :0.04727   Max.   :0.08191   Max.   :0.07738  \n     dist.              largo              cabez             hueso        \n Min.   :0.007179   Min.   :0.004202   Min.   :0.07311   Min.   :0.02857  \n 1st Qu.:0.030485   1st Qu.:0.022402   1st Qu.:0.08476   1st Qu.:0.03236  \n Median :0.035368   Median :0.029488   Median :0.08951   Median :0.03415  \n Mean   :0.031366   Mean   :0.026533   Mean   :0.08860   Mean   :0.03488  \n 3rd Qu.:0.038824   3rd Qu.:0.034574   3rd Qu.:0.09284   3rd Qu.:0.03582  \n Max.   :0.048008   Max.   :0.051843   Max.   :0.09815   Max.   :0.10411  \n     mandib           hueso.1            arriba            cabeza       \n Min.   :0.03141   Min.   :0.02348   Min.   :0.01855   Min.   :0.07118  \n 1st Qu.:0.03598   1st Qu.:0.03917   1st Qu.:0.02546   1st Qu.:0.08309  \n Median :0.03980   Median :0.04413   Median :0.02952   Median :0.08796  \n Mean   :0.03923   Mean   :0.04498   Mean   :0.02946   Mean   :0.08890  \n 3rd Qu.:0.04247   3rd Qu.:0.04995   3rd Qu.:0.03391   3rd Qu.:0.09483  \n Max.   :0.04758   Max.   :0.08783   Max.   :0.03828   Max.   :0.10760  \n      boca             abiert           cachete           largo.1        \n Min.   :0.02828   Min.   :0.01810   Min.   :0.02938   Min.   :0.008724  \n 1st Qu.:0.03476   1st Qu.:0.02348   1st Qu.:0.03911   1st Qu.:0.012881  \n Median :0.03830   Median :0.02531   Median :0.04308   Median :0.014240  \n Mean   :0.03954   Mean   :0.02547   Mean   :0.04237   Mean   :0.014366  \n 3rd Qu.:0.04368   3rd Qu.:0.02741   3rd Qu.:0.04508   3rd Qu.:0.015936  \n Max.   :0.05552   Max.   :0.03248   Max.   :0.06553   Max.   :0.026475  \n     ancho            orificio           densi            hocic        \n Min.   :0.00274   Min.   :0.01968   Min.   :0.6990   Min.   :0.01546  \n 1st Qu.:0.02663   1st Qu.:0.03741   1st Qu.:0.7782   1st Qu.:0.02491  \n Median :0.03031   Median :0.04381   Median :0.8451   Median :0.02929  \n Mean   :0.03253   Mean   :0.04429   Mean   :0.8401   Mean   :0.02937  \n 3rd Qu.:0.03447   3rd Qu.:0.05085   3rd Qu.:0.9031   3rd Qu.:0.03294  \n Max.   :0.22628   Max.   :0.07390   Max.   :1.1139   Max.   :0.04922  \n    interbr            ojoatra            proye              base.1       \n Min.   :0.007687   Min.   :0.02685   Min.   :0.007313   Min.   :0.01546  \n 1st Qu.:0.014457   1st Qu.:0.03796   1st Qu.:0.013341   1st Qu.:0.02052  \n Median :0.015776   Median :0.04050   Median :0.014982   Median :0.02238  \n Mean   :0.016304   Mean   :0.04067   Mean   :0.016217   Mean   :0.02300  \n 3rd Qu.:0.017609   3rd Qu.:0.04256   3rd Qu.:0.016613   3rd Qu.:0.02474  \n Max.   :0.028804   Max.   :0.08124   Max.   :0.127873   Max.   :0.04476  \n     altura             area             Intesti      \n Min.   :0.02514   Min.   :0.001431   Min.   :0.1295  \n 1st Qu.:0.03815   1st Qu.:0.006723   1st Qu.:0.1774  \n Median :0.04422   Median :0.009716   Median :0.1960  \n Mean   :0.04474   Mean   :0.010638   Mean   :0.1980  \n 3rd Qu.:0.04928   3rd Qu.:0.014274   3rd Qu.:0.2150  \n Max.   :0.08398   Max.   :0.027268   Max.   :0.3246  \n\n\nApliquemos el Análisis de Componentes Principales:\n\n#Aplicamos el PCA con los datos escalados y centrados\nbiom_pca &lt;- FactoMineR::PCA(pca_data,\n                            graph = F,\n                            ncp = length(pca_data),\n                            scale.unit = T)\n\nRecordarás que el PCA está basado en el teorema de Pitágoras, por lo que es necesario que todas las variables estén en la misma escala. Podemos utilizar la función scale() como hicimos para el análisis clúster, o podemos indicarle a R que lo haga automáticamente si utilizamos la función FactoMineR::PCA() con el argumento scale.unit. Podemos ver los resultados de manera numérica:\n\nsummary(biom_pca)\n\n\nCall:\nFactoMineR::PCA(X = pca_data, scale.unit = T, ncp = length(pca_data),  \n     graph = F) \n\n\nEigenvalues\n                       Dim.1   Dim.2   Dim.3   Dim.4   Dim.5   Dim.6   Dim.7\nVariance               6.146   4.632   2.809   2.271   1.685   1.550   1.260\n% of var.             19.826  14.943   9.061   7.325   5.436   5.001   4.065\nCumulative % of var.  19.826  34.769  43.829  51.154  56.590  61.591  65.655\n                       Dim.8   Dim.9  Dim.10  Dim.11  Dim.12  Dim.13  Dim.14\nVariance               1.108   1.059   0.966   0.845   0.756   0.727   0.628\n% of var.              3.574   3.416   3.117   2.725   2.440   2.344   2.025\nCumulative % of var.  69.230  72.646  75.763  78.487  80.927  83.271  85.295\n                      Dim.15  Dim.16  Dim.17  Dim.18  Dim.19  Dim.20  Dim.21\nVariance               0.530   0.512   0.450   0.398   0.377   0.355   0.308\n% of var.              1.708   1.653   1.452   1.285   1.215   1.144   0.992\nCumulative % of var.  87.003  88.656  90.108  91.393  92.608  93.752  94.745\n                      Dim.22  Dim.23  Dim.24  Dim.25  Dim.26  Dim.27  Dim.28\nVariance               0.285   0.266   0.208   0.193   0.168   0.143   0.133\n% of var.              0.920   0.858   0.671   0.622   0.543   0.462   0.428\nCumulative % of var.  95.665  96.523  97.194  97.815  98.358  98.820  99.248\n                      Dim.29  Dim.30  Dim.31\nVariance               0.107   0.076   0.050\n% of var.              0.345   0.244   0.162\nCumulative % of var.  99.593  99.838 100.000\n\nIndividuals (the 10 first)\n             Dist    Dim.1    ctr   cos2    Dim.2    ctr   cos2    Dim.3    ctr\n1        |  6.196 | -1.768  0.489  0.081 |  2.830  1.663  0.209 |  2.494  2.129\n2        | 10.682 | -2.869  1.288  0.072 |  2.272  1.072  0.045 |  2.522  2.177\n3        |  4.520 | -2.199  0.757  0.237 |  1.517  0.478  0.113 |  0.853  0.249\n4        |  4.079 | -2.569  1.033  0.397 |  1.812  0.681  0.197 |  0.994  0.338\n5        |  4.642 | -2.728  1.165  0.345 |  2.026  0.852  0.190 |  1.343  0.618\n6        |  4.142 | -2.810  1.236  0.460 | -0.572  0.068  0.019 |  0.359  0.044\n7        |  3.740 | -1.366  0.292  0.133 |  1.066  0.236  0.081 |  0.765  0.200\n8        |  5.488 | -3.158  1.560  0.331 |  2.032  0.857  0.137 |  2.699  2.494\n9        |  4.753 | -2.983  1.392  0.394 |  1.637  0.556  0.119 |  1.324  0.600\n10       |  6.548 | -2.115  0.700  0.104 |  3.068  1.954  0.220 |  1.928  1.272\n           cos2  \n1         0.162 |\n2         0.056 |\n3         0.036 |\n4         0.059 |\n5         0.084 |\n6         0.008 |\n7         0.042 |\n8         0.242 |\n9         0.078 |\n10        0.087 |\n\nVariables (the 10 first)\n            Dim.1    ctr   cos2    Dim.2    ctr   cos2    Dim.3    ctr   cos2  \nfurcal   | -0.174  0.494  0.030 |  0.113  0.275  0.013 | -0.221  1.732  0.049 |\nbase     | -0.082  0.109  0.007 |  0.085  0.155  0.007 | -0.317  3.582  0.101 |\nradio    | -0.294  1.409  0.087 | -0.045  0.044  0.002 | -0.246  2.160  0.061 |\nalto     | -0.129  0.272  0.017 |  0.323  2.251  0.104 |  0.442  6.968  0.196 |\ngordo    |  0.243  0.959  0.059 |  0.256  1.414  0.065 |  0.496  8.744  0.246 |\nangos    | -0.193  0.608  0.037 |  0.422  3.838  0.178 |  0.162  0.930  0.026 |\nojo      | -0.159  0.411  0.025 | -0.209  0.942  0.044 |  0.060  0.129  0.004 |\nX1erarc  | -0.764  9.497  0.584 |  0.282  1.715  0.079 | -0.005  0.001  0.000 |\ndist.    | -0.436  3.098  0.190 |  0.285  1.758  0.081 |  0.155  0.854  0.024 |\nlargo    | -0.650  6.870  0.422 |  0.357  2.755  0.128 |  0.220  1.720  0.048 |\n\n\nLa salida es bastante extensa, pero lo que realmente nos interesa es lo que aparece en Eigenvalues: La varianza de cada componente, el porcentaje de la varianza total que representa, y cuánta varianza se ha acumulado hasta ese componente, pero siempre es mejor ver los resultados de manera gráfica con algunas funciones de la librería factoextra:\n\nfactoextra::fviz_pca_var(biom_pca,\n                         col.var = \"coord\",\n                         gradient.cols = c(\"#00AFBB\",\n                                           \"#E7B800\",\n                                           \"#FC4E07\"),\n                         legend.title = \"Carga\")\n\n\n\n\n\n\n\n\nEn este gráfico tenemos representada la importancia de variables (carga factorial) para cada componente principal. Entre más larga (y naranja) sea la flecha, mayor es su importancia. Entre más horizontal esté, más contribuye al componente principal representado en el eje \\(x\\), y entre más vertical esté, más contribuye al componente principal puesto en el eje \\(y\\), y la dirección de la relación con el CP está dada por el signo. En nuestro ejemplo, las variables abiert, boca, arriba, mandib y cabez están positivamente relacionadas con el CP1, pero negativamente relacionadas con el CP2; es decir, entre más grande sea la longitud de la mandíbula, por ejemplo, más grande será la coordenada en el CP1, pero también se hará más pequeña en el CP2. Otra forma de ver estos resultados es tratarlos como si fueran una matriz de correlaciones:\n\nbiom_pca_vars &lt;- factoextra::get_pca_var(biom_pca)\ncorrplot::corrplot(biom_pca_vars$coord)\n\n\n\n\n\n\n\n\n¿Por qué es esto importante? El PCA reduce las dimensiones tratando de preservar la mayor cantidad de varianza (covarianza) posible en cada PC. Una consecuencia es que los puntos más parecidos se acercarán más entre sí, mientras que los más alejados se alejarán más. A lo que esto nos lleva es a una forma de ordenación, entonces podemos graficar este nuevo espacio reducido y ver cómo quedan distribuidos nuestros datos:\n\nfviz_pca_ind(biom_pca,\n             geom.ind = \"point\", # Mostrar solo puntos (sin textos)\n             col.ind = biom[,1], # Variable de agrupamiento\n             palette = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), # Colores a utilizar\n             addEllipses = TRUE, # Elipses de concentración\n             legend.title = \"Especies\")\n\n\n\n\n\n\n\n\nAquí podemos ver que las especies jordani y labarcae se encuentran bien diferenciadas en el espacio multivariado, mientras que consocium tiene mediciones intermedias.\n\n\n\n\n\n\nAdvertencia\n\n\n\nAlgunas personas consideran el PCA como una técnica de ordenación; sin embargo, el objetivo no es ese, sino solo una consecuencia de reducir las dimensiones, tal y como vimos en la Figura 15.16.\n\n\n¿Cómo están dadas las diferencias? Puedes volver al gráfico de variables del PCA y ver cómo apuntan las flechas. Por ejemplo, labarcae tiene medidas más grandes en abiert, boca, arriba, mandib y cabez que las otras dos. Una forma más directa sería graficar ambas cosas a la vez:\n\nfviz_pca_biplot(biom_pca, \n                col.ind = biom$especie,\n                addEllipses = TRUE, label = \"var\",\n                palette = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n                col.var = \"black\", repel = TRUE,\n                legend.title = \"Especies\") \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNota\n\n\n\nEn el Capítulo 17 vamos a ver cómo estamos dejando de lado la información más valiosa de toda la base de datos: las etiquetas de los grupos. ¿La razón? Aquí las estamos utilizando solo como una ayuda visual para la interpretación, más que como datos que informen nuestros análisis.\n\n\nAhora bien, ya sabemos cómo interpretar nuestros componentes principales. La pregunta es, ¿capturan la suficiente información de la varianza?\n\n\n15.4.2.2 Evaluación de los componentes principales\nRegresemos a las varianzas explicadas por cada componente:\n\neig_val &lt;- get_eigenvalue(biom_pca)\neig_val\n\n       eigenvalue variance.percent cumulative.variance.percent\nDim.1  6.14608922       19.8260942                    19.82609\nDim.2  4.63221636       14.9426334                    34.76873\nDim.3  2.80882601        9.0607291                    43.82946\nDim.4  2.27068129        7.3247783                    51.15424\nDim.5  1.68514008        5.4359358                    56.59017\nDim.6  1.55020741        5.0006691                    61.59084\nDim.7  1.26002881        4.0646091                    65.65545\nDim.8  1.10797871        3.5741249                    69.22957\nDim.9  1.05894728        3.4159590                    72.64553\nDim.10 0.96627425        3.1170137                    75.76255\nDim.11 0.84470657        2.7248599                    78.48741\nDim.12 0.75624648        2.4395048                    80.92691\nDim.13 0.72656100        2.3437452                    83.27066\nDim.14 0.62761130        2.0245526                    85.29521\nDim.15 0.52955055        1.7082276                    87.00344\nDim.16 0.51240639        1.6529238                    88.65636\nDim.17 0.45010263        1.4519440                    90.10830\nDim.18 0.39821456        1.2845631                    91.39287\nDim.19 0.37664998        1.2149999                    92.60787\nDim.20 0.35475436        1.1443689                    93.75224\nDim.21 0.30760753        0.9922823                    94.74452\nDim.22 0.28534181        0.9204574                    95.66498\nDim.23 0.26608062        0.8583246                    96.52330\nDim.24 0.20787739        0.6705722                    97.19387\nDim.25 0.19269314        0.6215908                    97.81546\nDim.26 0.16829393        0.5428837                    98.35835\nDim.27 0.14322652        0.4620210                    98.82037\nDim.28 0.13259938        0.4277399                    99.24811\nDim.29 0.10706999        0.3453871                    99.59350\nDim.30 0.07572896        0.2442870                    99.83778\nDim.31 0.05028750        0.1622177                   100.00000\n\n\nLas primeras dos dimensiones (los primeros dos CPs) capturan menos del 50% de la varianza total; por lo tanto, es prudente preguntarse cuántos CPs debemos de considerar. Una alternativa es el análisis gráfico de la varianza con un scree-plot. En este gráfico buscamos un punto de inflexión, el cuál indicaría el límite de los PCs representativos:\n\nfviz_eig(biom_pca, addlabels = T)\n\n\n\n\n\n\n\n\n¿Dónde considerarías que está el primer punto de inflexión? Yo en el tercer o cuarto componente, pero veamos otros dos criterios. El primero es el criterio de Kaiser-Guttman, en el cual consideraremos aquellos CPs cuya varianza explicada sea superior al valor promedio. El segundo criterio es un modelo de repartición de recursos desarrollado en la ecología de comunidades: el modelo Broken-Stick (MacArthur, 1957), donde el recurso compartido es la varianza total; entonces, consideraremos aquellos CPs que tengan una varianza explicada superior a la esperada según el modelo. Para graficar ambos criterios a la vez utilizaremos una función personalizada (evggplot):\n\nevggplot &lt;- function(eigv, kind = \"both\"){\n  # Inspired by the `evplot` function by De La Cruz-Agüero (?)\n  #\n  \n  library(patchwork)\n  # Broken stick model (MacArthur 1957)\n  bsm &lt;- function(n){\n    p &lt;- 1:n\n    p[1] &lt;- 1/n\n    for (i in 2:n) {p[i] &lt;- p[i-1] + (1/(n+1-i))}\n    p &lt;- 100*p/n\n    return(rev(p))\n  }\n  \n  # Number of CPs\n  n &lt;- nrow(eigv)\n  # New df from the eigenvalue matrix\n  ev &lt;- as.data.frame(eigv) |&gt; \n        mutate(CP = factor(1:n,\n                           labels = paste0(\"CP \", 1:n),\n                           ordered = T),\n               p = bsm(n),\n               .before = 1) |&gt; \n        select(CP, eigenvalue, p) |&gt; \n        reshape2::melt(id.vars = \"CP\")\n  \n  # Broken-Stick data\n  ev_bsm &lt;- ev |&gt;\n            mutate(value = ifelse(variable == \"eigenvalue\",\n                                  unname(eigv[,2]),\n                                  value))\n  \n  # Kaiser-Gutmann data\n  ev_kg &lt;- ev |&gt; filter(variable == \"eigenvalue\")\n  m_eig &lt;- mean(ev_kg$value)\n  \n  # Plotting\n  \n  base_plot &lt;- ggplot() +\n               theme(axis.text.x = element_text(angle = 90)) +\n               labs(x = element_blank())\n  \n  bsm_plot &lt;- base_plot + \n              geom_bar(data = ev_bsm,\n                       aes(x = CP, y = value, fill = variable),\n                       stat = \"identity\",\n                       position = \"dodge\") +\n              labs(y = \"% Varianza\",\n                   fill = element_blank()) +\n              scale_fill_manual(values = c(\"bisque\", \"red\"),\n                                labels = c(\"Eigenvalor\", \"Broken-Stick\")) +\n              theme(legend.position = c(0.8, 0.8))\n  \n  \n  kg_plot &lt;- base_plot +\n             geom_bar(data = ev_kg,\n                      aes(x = CP, y = value),\n                      stat = \"identity\",\n                      fill = \"bisque\",\n                      color = NA) +\n             geom_hline(yintercept = m_eig,\n                        color = \"red\") +\n             labs(y = \"Eigenvalor\")\n  \n  ev_plot &lt;- switch (kind,\n                     bs = bsm_plot,\n                     kg = kg_plot,\n                     both = (kg_plot +\n                               theme(axis.text.x = element_blank(),\n                                     axis.title.x = element_blank())) /\n                          bsm_plot)\n              \n  return(ev_plot)\n}\n\n\nevggplot(eig_val) + plot_annotation(title = \"¿Cuántos CPs?\")\n\nWarning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n3.5.0.\nℹ Please use the `legend.position.inside` argument of `theme()` instead.\n\n\n\n\n\n\n\n\n\nBajo el criterio de Kaiser-Guttman consideraríamos 8 CPs, mientras que en el modelo de Broken-Stick consideraríamos únicamente 4. De cualquier modo, ambos criterios sugieren un número mayor al que yo consideré en el gráfico de la varianza. ¿Qué pasa entonces con nuestra interpretación original con respecto a la separación de las especies? Ya no es tan robusta, pero no significa que no sea válida. El MEI (v1), por ejemplo, es el primer componente principal y representa “solo” el 31% de la varianza total, pero aún así es útil para conocer en qué fase del fenómeno ENSO (y su intensidad) se encuentra un mes/año dado.\nAhora bien, si la ordenación es solo una consecuencia de la reducción de la dimensionalidad, ¿tenemos alguna técnica donde el objetivo sea la ordenación?",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Aprendizaje No Supervisado</span>"
    ]
  },
  {
    "objectID": "c15_nosup.html#ordenación",
    "href": "c15_nosup.html#ordenación",
    "title": "15  Aprendizaje No Supervisado",
    "section": "15.5 Ordenación",
    "text": "15.5 Ordenación\nEn una ordenación el objetivo es que las observaciones (instancias) que más se parezcan entre sí se mantengan lo más cercanas posible. Un ejemplo de esto, análogo al PCA, es el Escalamiento multidimensional no métrico (NMDS).\n\n15.5.1 Escalamiento Multidimensional No Métrico (NMDS)\nA diferencia del PCA en el cual es importante que las variables estén centradas y estandarizadas, el Escalamiento Multidimensional No Métrico (NMDS) es una técnica más flexible al estar basada en órdenes de rangos (distancias) para la ordenación. Como resultado, puede aceptar una variedad mayor de tipos de datos. Otra diferencia es que en el ACP el objetivo primordial no es la ordenación, sino la reducción de la dimensionalidad, mientras que en NMDS sí se busca una ordenación.\nEl NMDS ordena los objetos según sus similitudes, y en el proceso representa el espacio en menos dimensiones. Esta técnica minimiza la deformación de las distancias (representada por el “estrés”). Todo eso suena muy bien, ¿hay algún bemol? Sí. Es un proceso iterativo; es decir, requiere de múltiples vueltas para llegar a una solución estable. Veamos entonces cómo funciona utilizando los siguientes datos tridimensionales:\n\n\n\n\n\n\nFigura 15.22: ¿Cómo nos ordenamos?\n\n\n\n\nCalcular la matriz de distancias y asignarles un rango, donde a la menor distancia le corresponde el número 1.\n\n\n\n\n\n\n\nFigura 15.23: Matriz de distancias y sus rangos.\n\n\n\n\nProponer un acomodo “aleatorio” en el nuevo espacio con menos dimensiones. Digo “aleatorio” porque en realidad se siguen algunas heurísticas para comenzar desde un ordenamiento no tan alejado.\n\n\n\n\n\n\n\nFigura 15.24: Primer acomodo.\n\n\n\n\nVolver a calcular la matriz de distancias y sus rangos y evaluar las diferencias. En este caso, la distancia entre A y B es demasiado grande, por lo que los acercamos en un nuevo acomodo:\n\n\n\n\n\n\n\nFigura 15.25: Segundo acomodo.\n\n\n\n\nRepetir la operación hasta que la diferencia entre la matriz de distancias y rangos originales con la representación sea mínima; es decir, minimizar el estrés. Como regla general, valores de estrés de 0-0.1 son considerados buenos; de 0.1-0.2, aceptables; de 0.2-0.3, altos; y con más de 0.3 se considera que la representación mostrada es “aleatoria”.\n\nPara analizar este caso utilicemos los mismos datos del caso sitios (clúster):\n\nset.seed(0)\n# Matriz de distancias\ndist_nmds &lt;- dist_mv2\nmds &lt;- metaMDS(dist_nmds, distance = \"euclidean\", k = 2, trace = T)\n\nRun 0 stress 0.1033851 \nRun 1 stress 0.1188198 \nRun 2 stress 0.1033851 \n... Procrustes: rmse 5.544036e-05  max resid 0.0001722519 \n... Similar to previous best\nRun 3 stress 0.2137544 \nRun 4 stress 0.2207749 \nRun 5 stress 0.1642917 \nRun 6 stress 0.1033851 \n... Procrustes: rmse 4.677317e-06  max resid 1.339618e-05 \n... Similar to previous best\nRun 7 stress 0.1188198 \nRun 8 stress 0.1033851 \n... New best solution\n... Procrustes: rmse 1.651991e-05  max resid 5.167188e-05 \n... Similar to previous best\nRun 9 stress 0.1033851 \n... Procrustes: rmse 2.428306e-05  max resid 7.760812e-05 \n... Similar to previous best\nRun 10 stress 0.1033851 \n... New best solution\n... Procrustes: rmse 1.357454e-05  max resid 4.294826e-05 \n... Similar to previous best\nRun 11 stress 0.1188198 \nRun 12 stress 0.1033851 \n... Procrustes: rmse 1.236052e-06  max resid 3.575611e-06 \n... Similar to previous best\nRun 13 stress 0.1932816 \nRun 14 stress 0.1033851 \n... Procrustes: rmse 1.306206e-05  max resid 4.136513e-05 \n... Similar to previous best\nRun 15 stress 0.1642917 \nRun 16 stress 0.2027265 \nRun 17 stress 0.1033851 \n... Procrustes: rmse 2.953983e-05  max resid 9.352906e-05 \n... Similar to previous best\nRun 18 stress 0.1188198 \nRun 19 stress 0.2202381 \nRun 20 stress 0.1033851 \n... Procrustes: rmse 3.998356e-06  max resid 1.257446e-05 \n... Similar to previous best\n*** Best solution repeated 5 times\n\nmds_dims &lt;- data.frame(NMDS1 = mds$points[,1], NMDS2 = mds$points[,2])\n\nmds_plot_data &lt;- cbind(mds_dims, clust_scale)\n\nDado que pasamos metaMDS(..., trace = T), R nos mostró los resultados de cada iteración. La iteración (Run) 0 es el acomodo inicial, y tuvo un estrés de 0.10. A partir de ahí comienza a mover los puntos y evaluar si el acomodo es mejor o peor utilizando un análisis Procrustes (un análisis para comparar geometrías). Al final, realizó 20 iteraciones, de las cuales 8 fueron la mejor solución. Este estrés de 0.10 es aceptable, por lo que podemos seguir con la interpretación, que es similar a lo que hicimos con el PCA:\n\n# Extraemos los coef. de determinación de cada variable ~ NMDS (flechas)\nfit &lt;- envfit(mds, clust_scale)\narrow &lt;- with(fit$vectors,\n              data.frame(arrows, R = r, p = pvals))\narrow[\"Variable\"] &lt;- rownames(arrow)\n\n# Extraemos aquellas que tengan una corr. significativamente diferente de 0\narrow_p &lt;- subset(arrow, p &lt;= 0.05)\n\n#Ordenamos de manera descendente según su valor de R2\narrow_p &lt;- arrow_p[order(arrow_p$R, decreasing = T), ]\nhead(arrow_p)\n\n\n  \n\n\n\n¿Cuál fue la variable más importante? Ahora veamos el NMDS resultante:\n\nmds_plot_data[\"group\"] &lt;- LETTERS[n_gps$Best.partition]\nmds_plot &lt;- ggplot(mds_plot_data, aes(NMDS1, NMDS2)) +\n            geom_point(alpha = 0.7) +\n            geom_label_repel(aes(label = rownames(clust_scale))) +\n            stat_ellipse(aes(fill = group),\n                         type = \"t\", linewidth = 1, geom = \"polygon\",\n                         alpha = 0.2, show.legend = F) +\n            labs(title = \"Escalamiento Multidimensional no métrico (NMDS)\",\n                 subtitle = paste('Estrés =',round(mds$stress,3)),\n                 caption = \"Datos: cluster.txt\") +\n            theme_bw() +\n            scale_color_manual(name = \"Grupo\",\n                               values = c(\"firebrick\", \"forestgreen\", \"black\")) +\n            scale_fill_manual(name = \"Grupo\",\n                              values = c(\"firebrick\", \"forestgreen\", \"black\")) #+\n            # geom_segment(data = arrow_p,\n            #              aes(x=0, y=0,\n            #                  xend = NMDS1, yend = NMDS2, lty = Variable),\n            # # Flechas escaladas según su R^2\n            #              arrow = arrow(length = unit(.25, \"cm\")*arrow_p$R)\n            #             )\n\nmds_plot\n\n\n\n\n\n\n\n\nSi lo comparas con el dendrograma te darás cuenta de que, efectivamente, los puntos de cada grupo son más cercanos entre sí que con los demás grupos:\n\nggd2_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nEl objetivo del NMDS NO es la formación de grupos. Al igual que en los gráficos del PCA, las etiquetas de los grupos son solo con fines de visualización.\n\n\nCon esto llegamos al final de esta sesión. Espero que la encuentres de utilidad, o cuando menos interesante.",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Aprendizaje No Supervisado</span>"
    ]
  },
  {
    "objectID": "c15_nosup.html#ejercicio",
    "href": "c15_nosup.html#ejercicio",
    "title": "15  Aprendizaje No Supervisado",
    "section": "15.6 Ejercicio",
    "text": "15.6 Ejercicio\nAplica un ACP a esta misma base de datos.\n\n¿Los datos deben de centrarse y estandarizarse?\nRealiza el ACP. ¿Cuántos CPs considerarías?\n¿Cuál es la varianza explicada entre los dos primeros componentes principales?\n¿Cuáles son las variables más imporantes para esos dos componentes?\n¿Hay diferencias en la ordenación con respecto a los otros dos métodos?\nRealizar las tres técnicas con las biometrías, y comparar sus resultados. (Opcional)\n\n\n\n\n\nMacArthur RH. 1957. On the relative abundance of bird species. Proceedings of the National Academy of Sciences USA 43:293-295. DOI: 10.1073/pnas.43.3.293.\n\n\nShirkhorshidi AS, Aghabozorgi S, Wah TY. 2015. A comparison study on similarity and dissimilarity measusres in clustering continuous data. PLoS ONE 10:e0144059. DOI: 10.1371/journal.pone.0144059.\n\n\nWolter K, Timlin MS. 1998. Measuring the strength of ENSO events: How does 1997/98 rank. Weather 53:315-324.",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Aprendizaje No Supervisado</span>"
    ]
  },
  {
    "objectID": "c16_mvcomps.html",
    "href": "c16_mvcomps.html",
    "title": "16  Hipótesis Multivariadas",
    "section": "",
    "text": "16.1 Librerías\nlibrary(ggplot2)\nlibrary(palmerpenguins)\nlibrary(dplyr)\n# library(Hotelling)\nlibrary(GGally)\nlibrary(vegan)\n# devtools::install_github(\"pmartinezarbizu/pairwiseAdonis/pairwiseAdonis\")\nlibrary(pairwiseAdonis)\ntheme_set(see::theme_lucid() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()))",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Hipótesis Multivariadas</span>"
    ]
  },
  {
    "objectID": "c16_mvcomps.html#introducción",
    "href": "c16_mvcomps.html#introducción",
    "title": "16  Hipótesis Multivariadas",
    "section": "16.2 Introducción",
    "text": "16.2 Introducción\nVamos a dejar por un momento el tema del aprendizaje automatizado para hablar sobre pruebas de significancia multivariadas, pero es necesario que antes retomemos un tema que dejé pendiente en la Capítulo 10: el problema de las múltiples hipótesis.",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Hipótesis Multivariadas</span>"
    ]
  },
  {
    "objectID": "c16_mvcomps.html#controlando-el-error",
    "href": "c16_mvcomps.html#controlando-el-error",
    "title": "16  Hipótesis Multivariadas",
    "section": "16.3 Controlando el error",
    "text": "16.3 Controlando el error\nHasta antes de esta sección de multivariado habíamos abordado problemas en donde respondíamos preguntas individuales a nuestros datos, principalmente en términos de pruebas de significancia; sin embargo, en el Capítulo 10 vimos que había que utilizar un ANOVA seguido de una prueba post-hoc al comparar las medias una variable numérica entre más de dos dos grupos para evitar inflar nuestro valor de \\(\\alpha\\), pero no entramos en más detalles.\n¿Qué pasa cuando contrastamos simultáneamente más de una hipótesis? Conforme incrementamos la cantidad de preguntas incrementamos la probabilidad de obtener respuestas incorrectas. ¿Por qué? Porque tenemos un error por cada tipo de pregunta. Esto hace que sea necesario corregir de alguna manera para que la tasa de error a nivel de la familia (Family-wise Error Rate) se mantenga constante. es decir que nuestro valor de \\(\\alpha\\) represente la probabilidad de que tengamos al menos un error de tipo I.\nEsta probabilidad de encontrar al menos un falso positivo al realizar múltiples comparaciones independientes incrementa a una tasa de \\(1-(1-\\alpha)^m\\), donde \\(m\\) es el número de comparaciones que estamos realizando (King & Eckersley, 2019):\n\nerror &lt;- data.frame(m = 1:20)\nerror[\"P_alpha\"] &lt;- 1 - (1-0.05)^error$m\n\nerror_rate &lt;- ggplot(data = error, aes(m, P_alpha)) + \n              geom_line(color = rgb(118,78,144, maxColorValue = 255)) +\n              scale_y_continuous(breaks = NULL) +\n              scale_x_continuous(breaks = c(1, 2, 5, 10, 20)) +\n              expand_limits(y = c(0,1)) +\n              theme_bw() +\n              geom_hline(yintercept = 0.05,\n                         colour = \"deepskyblue4\", linetype = \"dashed\") +\n              geom_hline(yintercept = 0.5,\n                         colour = \"firebrick\", linetype = \"dashed\",\n                         alpha = 0.5) +\n              geom_hline(yintercept = 1,\n                         colour = \"firebrick\", linetype = \"dashed\",\n                         alpha = 0.5) +\n              annotate(\"text\", x = 0, y = 0.08,\n                       label = \"0.05\", colour = \"deepskyblue4\", \n                       alpha = 0.5) + \n              annotate(\"text\", x = 0, y = 0.53,\n                       label = \"0.5\", colour = \"firebrick\",\n                       alpha = 0.5) +\n              annotate(\"text\", x = 0, y = 0.97,\n                       label = \"1\", colour = \"firebrick\",\n                       alpha = 0.5) +\n              labs(title = \"P(error) al incrementar el número de pruebas (m)\",\n                   subtitle = bquote({1 - (1- alpha)^m}),\n                   x = element_blank(),\n                   y = element_blank(),\n                   caption = \"King & Eckersley (2019)\"\n                   )\n\nerror_rate\n\n\n\n\n\n\n\n\nNotarás que con \\(m = 10\\) ya nos acercamos peligrosamente a tener un 50% de probabilidad de encontrarnos al menos un falso positivo. Creo que estarás conmigo que eso es una apuesta bastante arriesgada cuando originalmente pensabas que la probabilidad era del 5%.\nImagina que estamos contratados en una farmacéutica como científicos de datos, y que nuestra jefa nos da la tarea de hacer los ensayos clínicos para un nuevo medicamento diseñado para incrementar la fuerza de voluntad de los estudiantes para aprender estadística. Nosotros vamos y llegamos a la conclusión de que el medicamento no tuvo un efecto sobre ese atributo en particular. Evidentemente ni a la jefa ni a sus superiores les agrada la noticia, por lo que deciden explotarnos laboralmente y hacer un ensayo clínico en el que probemos otras 1000 cosas (reducir la ansiedad, mejorar la memoria, etc.), y nos piden que utilicemos el mismo valor de \\(\\alpha\\) de antes: 0.05. Como empleados responsables que quieren evitar un despido nosotros hacemos el colosal ensayo, y encontramos la siguiente distribución de efectos:\n\nset.seed(45)\nn &lt;- 1000\nrobs &lt;- data.frame(z = rnorm(n))\nrobs[\"color\"] &lt;- ifelse(robs$z &lt; -1.96 | robs$z &gt; 1.96, \n                        \"firebrick\", \"deepskyblue4\")\n\nrobs_plot &lt;- ggplot(data = robs, aes(z)) + \n             geom_density(color = \"deepskyblue4\") + \n             theme_bw() + \n             labs(title = \n                    \"Observaciones aleatorias a un alpha de 0.05\",\n                  subtitle = \"Bandas indican el límite de \\\"significancia\\\"\",\n                  caption = paste(n, \" datos simulados\"),\n                  x = \"Z\",\n                  y = element_blank()\n                  ) +\n             geom_vline(xintercept = -1.96, \n                        colour = rgb(118,78,144, maxColorValue = 255), \n                        linetype = \"dashed\"\n                        ) +\n             geom_vline(xintercept = 1.96, \n                        colour = rgb(118,78,144, maxColorValue = 255), \n                        linetype = \"dashed\"\n                        ) +\n             geom_point(aes(x = z, y = 0), \n                        color = robs$color, alpha = 0.3) +\n             scale_y_continuous(breaks = NULL) +\n             scale_x_continuous(breaks = c(-3, -1.96, -1, 0, 1, 1.96, 3), \n                                labels = \n                                  as.character(c(-3, -1.96, -1, 0, 1, 1.96, 3))\n                                ) +\n             annotate(\"text\", x = 3, y = 0.5, \n                      label = paste(\"# sign = \", \n                                    length(robs$color[robs$color ==\n                                                        \"firebrick\"]),\n                                    \"/\", n),\n                      colour = \"firebrick\"\n                      )\n\nrobs_plot\n\n\n\n\n\n\n\n\n¡Tuvimos un efecto significativo en 58! Hacemos el reporte correspondiente a nuestra jefa e inmediatamente la empresa se dispone a vender el producto. ¿Te hace algún tipo de ruido ese valor de 58/1000? Es sospechosamente cercano a nuestra probabilidad de \\(\\alpha\\) de 0.05, ¿no? Eso quiere decir que cuando menos una parte de esos efectos significativos son productos del azar. ¿La consecuencia de haber hecho así el análisis? La empresa fue demandada por publicidad engañosa porque el medicamento no cumplió con lo prometido.\n¿Cómo podíamos haber evitado la avalancha de demandas? Hay una alternativa sumamente sencilla: la corrección de Bonferroni. Lo único que teníamos que hacer era dividir nuestro valor de \\(\\alpha\\) nominal (con el que estamos trabajando) entre el número de contrastes a realizar. El resultado es que la tasa de error se vuelve prácticamente constante:\n\nerror[\"P_bonf\"] &lt;- 1 - (1-(0.05/error$m))^error$m\n\nerror_corr &lt;- ggplot(data = error, aes(m, P_bonf)) + \n              geom_line(color = rgb(118,78,144, maxColorValue = 255)) +\n              scale_y_continuous(breaks = NULL) +\n              scale_x_continuous(breaks = c(1, 2, 5, 10, 20)) +\n              expand_limits(y = c(0.04, 0.06)) +\n              theme_bw() +\n              geom_hline(yintercept = 0.05,\n                         colour = \"deepskyblue4\", linetype = \"dashed\") +\n              geom_hline(yintercept = 0.04,\n                         colour = \"lightslategray\", linetype = \"dashed\",\n                         alpha = 0.5) +\n              geom_hline(yintercept = 0.06,\n                         colour = \"lightslategray\", linetype = \"dashed\",\n                         alpha = 0.5) +\n              annotate(\"text\", x = 0, y = 0.0505,\n                       label = \"0.05\", colour = \"deepskyblue4\",\n                       alpha = 0.5) + \n              annotate(\"text\", x = 0, y = 0.0405,\n                       label = \"0.04\", colour = \"lightslategray\",\n                       alpha = 0.5) +\n              annotate(\"text\", x = 0, y = 0.0595,\n                       label = \"0.06\",colour = \"lightslategray\",\n                       alpha = 0.5) +\n              labs(title = \"P(error) al corregir según el número de pruebas (m)\",\n                   subtitle = bquote({1 - (1- (alpha/m))^m}),\n                   x = element_blank(),\n                   y = element_blank(),\n                   caption = \"King & Eckersley (2019)\"\n                   )\n\nerror_corr\n\n\n\n\n\n\n\n\nApliquemos entonces esta simple corrección a nuestro ensayo clínico y veamos cómo se distribuyen nuestros efectos:\n\nn &lt;- 1000\na_corr &lt;- (0.05/n)/2\nsig_lev &lt;- abs(qnorm(a_corr))\nrobs &lt;- data.frame(z = rnorm(n))\nrobs[\"color\"] &lt;- ifelse(robs$z &lt; -sig_lev | robs$z &gt; sig_lev,\n                        \"firebrick\", \"deepskyblue4\")\n\nrcor_plot &lt;- ggplot(data = robs, aes(z)) + \n             geom_density(color = \"deepskyblue4\") + \n             theme_bw() + \n             labs(title = \"Observaciones aleatorias a un alpha de 0.05\",\n                  subtitle = \"Bandas indican límites del LS corregido\",\n                  caption = paste(n, \" datos simulados\"),\n                  x = \"Z\",\n                  y = element_blank()\n                  ) +\n             geom_vline(xintercept = -sig_lev, \n                        colour = rgb(118,78,144, maxColorValue = 255), \n                        linetype = \"dashed\"\n                        ) +\n             geom_vline(xintercept = sig_lev, \n                        colour = rgb(118,78,144, maxColorValue = 255), \n                        linetype = \"dashed\"\n                        ) +\n             geom_point(aes(x = z, y = 0), \n                        color = robs$color, alpha = 0.3) +\n             scale_y_continuous(breaks = NULL) +\n             scale_x_continuous(breaks = c(-3, -1.96, -1, 0, 1, 1.96, 3), \n                                labels =\n                                  as.character(c(-3, -1.96, -1, 0, 1, 1.96, 3))\n                                ) +\n             annotate(\"text\", x = 2.5, y = 0.5, \n                      label = paste(\"# sign = \", \n                                    length(robs$color[robs$color ==\n                                                        \"firebrick\"]),\n                                    \"/\", n),\n                      colour = \"firebrick\"\n                      )\n\nrcor_plot\n\n\n\n\n\n\n\n\nAhora no tuvimos ningún efecto significativo. Básicamente le dimos un placebo a todos nuestros sujetos experimentales. Vamos con nuestra jefa, le damos el reporte correspondiente, se pone furiosa porque la empresa perdió dinero y le va a tocar una llamada de atención, pero preferible eso a tener pérdidas millonarias derivadas de la fabricación en masa de un producto que no sirve, y la subsecuente lluvia de demandas.\n¿Eso quiere decir que siempre vamos a utilizar una corrección de Bonferroni? Como mucho en esta vida, depende. ¿De qué? De la respuesta que demos a la misma pregunta que deberíamos de hacernos al seleccionar un valor de \\(\\alpha\\): ¿Qué prefiero sea más probable, un falso positivo o un falso negativo? A final de cuentas, si disminuímos la probabilidad de tener falsos positivos, por lógica incrementamos la probabilidad de tener falsos negativos. ¿La razón? Nos estamos haciendo más conservadores y, de hecho, esa es una de las principales críticas (Perneger, 1998) a la corrección de Bonferroni, que es muy conservadora e incrementa \\(\\beta\\). Hay otra crítica un poco más filosófica que tiene que ver con la pregunta de si cambia la interpretación de UNA prueba porque se hicieron otras 999, pero no entraremos en esos detalles.\nAhora sí, podemos seguir con las pruebas multivariadas.",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Hipótesis Multivariadas</span>"
    ]
  },
  {
    "objectID": "c16_mvcomps.html#t2-de-hotelling",
    "href": "c16_mvcomps.html#t2-de-hotelling",
    "title": "16  Hipótesis Multivariadas",
    "section": "16.4 \\(T^2\\) de Hotelling",
    "text": "16.4 \\(T^2\\) de Hotelling\nHasta el momento hemos visto que los análisis multivariados son abstractos. Podemos, sin ningún problema visualizar una distribución normal bivariada, solo hay que graficar una variable contra la otra (Figura 16.1), ¿pero con veinte variables? Nuestra imaginación de seres tridimensionales no llega hasta allá.\n\n\n\n\n\n\nFigura 16.1: Distribuciones normales bivariadas\n\n\n\nLa solución es hacer una prueba multivariada (para mantener nuestro error controlado), y acompañarla de pruebas univariadas. La primera de estas prueba es la prueba \\(T^2\\) de Hotelling, que es una generalización multivariada de la prueba \\(t\\) de Student. Como tal, es una prueba paramétrica, por lo que existen los supuestos de normalidad multivariada y de igualdad de matrices de covarianza (en el Capítulo 14 ya revisamos esos temas, así que obviaré el procedimiento). Las ecuaciones son una adaptación de la prueba \\(t\\) hacia el espacio multivariado, pasando de trabajar con una o dos medias a trabajar con uno o dos vectores de medias:\n\nPara una muestra o muestras pareadas:\n\n\\[\\begin{align*}\nT^2 = n(\\bar{x}- \\mu)^TC^{-1}(\\bar{x}-\\mu) \\\\\nC = \\frac{1}{n-1}\\sum_{i=1}^n{(x_i-\\bar{x})(x_i-\\bar{x})^T} \\\\\n\\nu = p, n-p\n\\end{align*}\\]\n\nPara dos muestras independientes\n\n\\[\\begin{align*}\nT^2 = n(\\bar{x_1}- \\bar{x_2})^TC^{-1}(\\bar{x_1}- \\bar{x_2}) \\\\\nC =\n\\frac{(n_1 - 1)C_1 + (n_2-1)C_2}\n{n_1 + n_2 - 2} \\\\\n\n\\nu = p, n_1 + n_2 - p - 1\n\\end{align*}\\]\nEl estadístico de prueba sigue una distribución \\(F\\) de Fisher, como en el ANOVA:\n\\[\nF = \\frac{n - p}{p(n-1)}T^2\n\\]\n\n\n\n\n\n\nNota\n\n\n\nEl objetivo de mostrar las ecuaciones no es que te las aprendas, solo que veas la lógica detrás de las “generalizaciones” que vamos a ver más adelante.\n\n\n\n16.4.1 Implementación en R\nPara todas las pruebas que revisaremos el día de hoy vamos a utilizar los datos de palmerpenguins que utilizamos para los gráficos del Capítulo 8, por lo que vamos a filtrar la base penguins para quedarnos solo con las variables numéricas sobre peso o longitud. Tiene mucho que no vemos algo de tidyverse, así que pongámoslo en práctica:\n\nnum_data &lt;- penguins |&gt;\n            select(contains(c(\"mm\", \"g\", \"species\"))) |&gt; \n            na.omit()\nhead(num_data)\n\n\n  \n\n\n\nLa prueba la aplicaremos con la función Hoteling::hotelling.test(x, y), donde \\(x\\) y \\(y\\) son las matrices de datos de cada grupo.\n\nx &lt;- num_data |&gt; filter(species == \"Adelie\") |&gt; select(where(is.numeric))\ny &lt;- num_data |&gt; filter(species == \"Gentoo\") |&gt; select(where(is.numeric))\n\nhot_t2 &lt;- Hotelling::hotelling.test(x = x, y = y)\nhot_t2\n\nTest stat:  4586.7 \nNumerator df:  4 \nDenominator df:  269 \nP-value:  0 \n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nLa función hotelling.test puede recibir una fórmula, pero hay un error y tanto el estadístico de prueba como el valor de p resultan en NaN (“Not a Number).\n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nEl valor de p no es exactamente 0, solo es extremadamente pequeño (del orden \\(\\times10^{-16}\\) o aún más pequeño) y la función lo muestra como 0.\n\n\nSi la distribución del estadístico de prueba es la distribución \\(F\\), entonces los resultados se interpretan como los resultados de un ANOVA: Hay diferencias en al menos una de las medias (\\(F_{4,269} = 4586.7; p &lt; 0.0001\\)).\n\n16.4.1.1 Comparaciones univariadas\nRealizaremos 4 comparaciones univariadas. ¿tiene sentido aplicar una corrección de Bonferroni?\n\n# Formamos un objeto para la aplicación de la prueba t\nt2_data &lt;- num_data |&gt;\n           filter(species == \"Adelie\" | species == \"Gentoo\")\n\n# Prueba t para cada variable\nt.test(bill_length_mm~species, data = t2_data)\n\n\n    Welch Two Sample t-test\n\ndata:  bill_length_mm by species\nt = -24.725, df = 242.58, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group Adelie and group Gentoo is not equal to 0\n95 percent confidence interval:\n -9.407672 -8.019303\nsample estimates:\nmean in group Adelie mean in group Gentoo \n            38.79139             47.50488 \n\nt.test(bill_depth_mm~species, data = t2_data)\n\n\n    Welch Two Sample t-test\n\ndata:  bill_depth_mm by species\nt = 25.337, df = 271.98, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group Adelie and group Gentoo is not equal to 0\n95 percent confidence interval:\n 3.102837 3.625651\nsample estimates:\nmean in group Adelie mean in group Gentoo \n            18.34636             14.98211 \n\nt.test(flipper_length_mm~species, data = t2_data)\n\n\n    Welch Two Sample t-test\n\ndata:  flipper_length_mm by species\nt = -34.445, df = 261.75, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group Adelie and group Gentoo is not equal to 0\n95 percent confidence interval:\n -28.79018 -25.67652\nsample estimates:\nmean in group Adelie mean in group Gentoo \n            189.9536             217.1870 \n\nt.test(body_mass_g~species, data = t2_data)\n\n\n    Welch Two Sample t-test\n\ndata:  body_mass_g by species\nt = -23.386, df = 249.64, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group Adelie and group Gentoo is not equal to 0\n95 percent confidence interval:\n -1491.183 -1259.525\nsample estimates:\nmean in group Adelie mean in group Gentoo \n            3700.662             5076.016 \n\n\nEn todos los casos rechazamos la hipótesis de nulidad (\\(p &lt; 0.0001\\)), lo cual no es de sorprender: los pingüinos Adelie son más pequeños que los Gentoo (aunque sorprende el resultado de la profundidad del pico). Aquí normalmente haríamos un gráfico de acompañamiento, pero mejor escalemos al caso para más de dos grupos.\n\np.adjust(c(0.04, 0.05, 0.0001, 0.02), method = \"bonferroni\")\n\n[1] 0.1600 0.2000 0.0004 0.0800",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Hipótesis Multivariadas</span>"
    ]
  },
  {
    "objectID": "c16_mvcomps.html#análisis-multivariado-de-la-varianza",
    "href": "c16_mvcomps.html#análisis-multivariado-de-la-varianza",
    "title": "16  Hipótesis Multivariadas",
    "section": "16.5 Análisis Multivariado de la Varianza",
    "text": "16.5 Análisis Multivariado de la Varianza\nEs, como te imaginarás, una extensión multivariada del ANOVA factorial. Aquí ya no entraré en los detalles de la prueba, pues vimos los fundamentos del ANOVA en el Capítulo 10, y cómo comprobar los supuestos básicos en Capítulo 14. Solo es importante conocer que se puede realizar con diversos estadísticos de prueba, entre los que destacan\n\nLa traza de Pillai: Contenida en el intervalo \\([0,1]\\), donde valores cercanos a 1 sugieren diferencias en al menos uno de los grupos en el espacio multivariado. Este es el estadístico más robusto a la violación de los supuestos del MANOVA (normalidad multivariada del error y homogeneidad de las matrices de covarianza).\nLa \\(\\lambda\\) de Wilk. También contenida en el intervalo \\([0,1]\\), aunque aquí son los valores más cercanos a 0 los que sugieren las diferencias. Si se cumplen todos los supuestos, esta es la prueba más relacionada con el criterio de la razón de verosimilitud (más sobre esto en Capítulo 19).\n\nY que el procedimiento \\(post-hoc\\) tiene dos pasos:\n\nUn ANOVA por cada variable dependiente.\nUna prueba HSD de Tukey en los ANOVAs significativos.\n\n\n16.5.1 Implementación\nLa implementación de la prueba en R tiene un paso muy engorroso, especialmente en casos con muchísimas variables: hay que seleccionar manualmente las columnas con las variables dependientes y unirlas con la función cbind. Podemos aplicar la prueba con la función car::Manova, la cual recibe un modelo ajustado con la función lm:\n\nmanova_model &lt;- lm(cbind(bill_length_mm,\n                         bill_depth_mm,\n                         flipper_length_mm,\n                         body_mass_g)~species,\n                   data = penguins)\ncar::Manova(manova_model, test.statistic = \"Wilks\")\n\n\nType II MANOVA Tests: Wilks test statistic\n        Df test stat approx F num Df den Df    Pr(&gt;F)    \nspecies  2  0.018785   528.87      8    672 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nO también con la función manova de R base:\n\nres_manova &lt;- manova(cbind(bill_length_mm,\n                           bill_depth_mm,\n                           flipper_length_mm,\n                           body_mass_g)~species,\n                     data = penguins)\nsummary(res_manova)\n\n           Df Pillai approx F num Df den Df    Pr(&gt;F)    \nspecies     2 1.6366   379.49      8    674 &lt; 2.2e-16 ***\nResiduals 339                                            \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nImportante\n\n\n\nAl igual que el supuesto de normalidad del ANOVA, el supuesto de normalidad multivariada está en términos del error, por lo tanto, debemos de realizarla con los residuales del MANOVA. Otro supuesto es el de no multicolinealidad, pero hablaremos más adelante de cómo verificarlo y por qué es importante.\n\n\n\nMVN::mvn(manova_model$residuals, mvnTest = \"royston\")\n\n$multivariateNormality\n     Test        H     p value MVN\n1 Royston 16.59531 0.002387391  NO\n\n$univariateNormality\n              Test          Variable Statistic   p value Normality\n1 Anderson-Darling  bill_length_mm      0.7546    0.0491    NO    \n2 Anderson-Darling   bill_depth_mm      0.6678    0.0805    YES   \n3 Anderson-Darling flipper_length_mm    0.5647    0.1428    YES   \n4 Anderson-Darling    body_mass_g       0.7719    0.0445    NO    \n\n$Descriptives\n                    n          Mean    Std.Dev        Median          Min\nbill_length_mm    342  5.902118e-17   2.951161   0.008609272    -7.933824\nbill_depth_mm     342 -6.130085e-17   1.117532   0.017886179    -2.846358\nflipper_length_mm 342 -1.066487e-16   6.622023   0.046357616   -17.953642\nbody_mass_g       342 -5.810614e-16 460.916730 -33.088235294 -1126.016260\n                          Max         25th        75th      Skew    Kurtosis\nbill_length_mm      12.095122   -2.2048780   2.0661765 0.2867048  0.50987802\nbill_depth_mm        3.153642   -0.7821138   0.7178862 0.2577874 -0.28856549\nflipper_length_mm   20.046358   -4.8235294   4.8130081 0.1642883 -0.09165713\nbody_mass_g       1223.983740 -333.0882353 316.9117647 0.1808593 -0.50669384\n\n\nPosteriormente podemos utilizar este objeto para realizar un ANOVA por variable. Nuevamente, ¿es necesario corregir los valores de p?\n\nsummary.aov(res_manova)\n\n Response bill_length_mm :\n             Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nspecies       2 7194.3  3597.2   410.6 &lt; 2.2e-16 ***\nResiduals   339 2969.9     8.8                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response bill_depth_mm :\n             Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nspecies       2 903.97  451.98  359.79 &lt; 2.2e-16 ***\nResiduals   339 425.87    1.26                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response flipper_length_mm :\n             Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nspecies       2  52473 26236.6   594.8 &lt; 2.2e-16 ***\nResiduals   339  14953    44.1                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n Response body_mass_g :\n             Df    Sum Sq  Mean Sq F value    Pr(&gt;F)    \nspecies       2 146864214 73432107  343.63 &lt; 2.2e-16 ***\nResiduals   339  72443483   213698                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n2 observations deleted due to missingness\n\n\nY por último aplicamos una prueba post-hoc HSD de Tukey para cada variable (solo con la primera por simplicidad):\n\nTukeyHSD(aov(bill_length_mm~species, data = num_data))\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = bill_length_mm ~ species, data = num_data)\n\n$species\n                      diff       lwr        upr     p adj\nChinstrap-Adelie 10.042433  9.024859 11.0600064 0.0000000\nGentoo-Adelie     8.713487  7.867194  9.5597807 0.0000000\nGentoo-Chinstrap -1.328945 -2.381868 -0.2760231 0.0088993\n\n\nAhora sí, podemos construir un gráfico de acompañamiento. Podemos construir un gráfico de interacción para cada variable. Hacerlo a mano puede resultar extremadamente tedioso, así que hagámoslos con un ciclo for. Primero, creemos objetos con la información básica que vamos a necesitar: los nombres de las variables, una lista vacía para llenar, y un espacio gráfico con los elementos que se reutilizan en todos los casos para evitar repeticiones.\n\n# Nombres de las variables\nvar_names &lt;- colnames(num_data)[colnames(num_data)!=\"species\"]\n\n# Lista vacía\nplot_list &lt;- list()\n\n# Inicializamos el espacio gráfico base\nbase_plot &lt;- ggplot(data = num_data,\n                    aes(x = species,\n                        colour = species)) +\n             theme_bw() \n\nbase_plot\n\n\n\n\n\n\n\n\nLuego aplicamos el ciclo for. Como en este caso tenemos los nombres de columnas en un vector de cadenas de caracter necesitamos de dos pasos: convertir la cadena de caracteres a un símboo con la función sym() y luego quitarle las comillas con el operador !! dentro de aes:\n\n# Para cada variable:\n\nfor (variable in var_names){\n  # Asigna el nombre de la variable como símbolo (sin comillas)\n  plot_var &lt;- sym(variable)\n  # Genera el gráfico para la variable\n  plot_list[[variable]] &lt;- base_plot +\n    geom_violin(aes(y = !!plot_var),\n                show.legend = F) +\n    geom_boxplot(aes(y = !!plot_var),\n                 width = 0.1,\n                 show.legend = F) +\n    labs(x = element_blank(),\n         y = element_blank(),\n         title = variable)\n}\n\n\n\n\n\n\n\nTip\n\n\n\nEl operador !! es una ayuda de tidy para poder pasar los nombres de las variables de forma que sean reconocidas por las funciones del tidyverse. Forma parte de los “Tidy eval helpers”. Si quieres ver más información puedes revisar su documentación.\n\n\nLuego podemos juntarlas utilizando patchwork:\n\npatchwork::wrap_plots(plot_list, ncol = 2, nrow = 2)\n\n\n\n\n\n\n\n\nOtra forma sería con facet_wrap, tratando las variables como si fueran variables de agrupamiento (¿te animas a programarlo?). Una forma tal vez más simple de representar toda esta información en un solo panel es un gráfico de coordenadas paralelas:\n\nparcoord_plot &lt;- ggparcoord(data = num_data,\n                            columns = 1:4,\n                            groupColumn = 5,\n                            showPoints = TRUE,\n                            scale = \"std\",\n                            order = \"anyClass\",\n                            alphaLines = 0.5) +\n                 labs(title = \"Gráfico de coordenadas paralelas\",\n                      y = element_blank(),\n                      x = element_blank(),\n                      caption = \"Datos: Palmerpenguins\") +\n                 scale_color_discrete(name = \"Especie\") \nparcoord_plot",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Hipótesis Multivariadas</span>"
    ]
  },
  {
    "objectID": "c16_mvcomps.html#análisis-permutacional-multivariado-de-la-varianza",
    "href": "c16_mvcomps.html#análisis-permutacional-multivariado-de-la-varianza",
    "title": "16  Hipótesis Multivariadas",
    "section": "16.6 Análisis Permutacional Multivariado de la Varianza",
    "text": "16.6 Análisis Permutacional Multivariado de la Varianza\nVamos a cerrar este capítulo con la alternativa no paramétrica al MANOVA: el Análisis Permutacional Multivariado de la Varianza (MANOVA). En este análisis se hace una partición geométrica del espacio multivariado; es decir, es una prueba basada en distancias. Además, es una prueba basada en un análisis permutacional, donde “revuelve” las etiquetas de la matriz de distancias.\n\n\n\n\n\n\nNota\n\n\n\nTe recomiendo visitar este sitio web para que te familiarices con los análisis permutacionales de una manera visual.\n\n\nLa prueba trabaja en tres pasos:\n\nCalcula un valor de \\(F = \\frac{SS_A}{SS_R} \\cdot \\left[\\frac{N-g}{g-1} \\right]\\) para el acomodo original de los datos.\nPermuta las etiquetas de la matriz de distancias \\(k\\) veces, calculando un valor de \\(F\\) para cada permutación. Esto permite “crear” una distribución empírica con todas las \\(F\\).\nCalcula el valor de p comparando la F observada vs. la distribución “creada”.\n\nDebido a la naturaleza permutacional del análisis es muy importante que tengamos factores balanceados; i.e., que todos los factores tengan aproximadamente la misma cantidad de observaciones. La función que utilizaremos (adonis2) compensa esta parte, pero tampoco hay que abusar, y esto me lleva a la implementación.\n\n16.6.1 Implementación\nLo primero, evidentemente, es calcular la matriz de distancias pero, al igual que con el análisis clúster, es importante que todas nuestras variables estén en la misma escala:\n\n# Escalado de los datos\nscaled_data &lt;- as.data.frame(scale(num_data[,1:4]))\n\n# Objeto con los niveles de agrupamiento, anidados o no\nspecies &lt;- num_data$species\n\n# Cálculo de la matriz de distancias\ndist_mat &lt;- dist(scaled_data,\n                 method = \"euclidean\")\n\nLuego aplicamos el PERMANOVA\n\nperm_res &lt;- adonis2(dist_mat~species,\n                    data = scaled_data,\n                    permutations = 999)\nperm_res\n\n\n  \n\n\n\nRechazamos la hipótesis de nulidad, y aprendimos que hay diferencias entre las medias multivariadas de al menos un par de especies. El gráfico de acompañamiento para esta prueba es el resultado de un análisis de coordenadas principales (un PCA, pero con cualquier medida de distancia, no solo Euclidiana), o un NMDS. Dado que el análisis lo hicimos con una matriz de distancias euclidianas podemos también hacer un gráfico de PCA:\n\ndist_pca &lt;- FactoMineR::PCA(scaled_data,\n                            graph = F,\n                            ncp = length(num_data),\n                            scale.unit = F)\nfactoextra::fviz_pca_biplot(dist_pca,\n                         geom.ind = \"point\",\n                         col.ind = species,\n                         addEllipses = T,\n                         palette = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n                         legend.title = \"Especies\")\n\n\n\n\n\n\n\n\nComo era de esperarse hubo diferencias muy marcadas, principalmente en los pingüinos Gentoo pero ¿en qué variables? No existe una prueba post-hoc como tal para el PERMANOVA, pero podemos hacer un PERMANOVA para cada par de especies (nuevamente, ¿es necesario corregir los valores de p de estos PERMANOVAs?) con la función pairwiseAdonis::pairwise.adonis2:\n\npw_adonis &lt;- pairwise.adonis2(x = dist_mat~species,\n                              data = cbind(scaled_data, species))\npw_adonis\n\n$parent_call\n[1] \"dist_mat ~ species , strata = Null , permutations 999\"\n\n$Adelie_vs_Gentoo\n          Df SumOfSqs      R2      F Pr(&gt;F)    \nspecies    1   823.01 0.72176 705.58  0.001 ***\nResidual 272   317.27 0.27824                  \nTotal    273  1140.28 1.00000                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$Adelie_vs_Chinstrap\n          Df SumOfSqs      R2      F Pr(&gt;F)    \nspecies    1   166.95 0.39664 142.65  0.001 ***\nResidual 217   253.95 0.60336                  \nTotal    218   420.90 1.00000                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$Gentoo_vs_Chinstrap\n          Df SumOfSqs    R2      F Pr(&gt;F)    \nspecies    1   359.23 0.617 304.48  0.001 ***\nResidual 189   222.99 0.383                  \nTotal    190   582.21 1.000                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nattr(,\"class\")\n[1] \"pwadstrata\" \"list\"      \n\n\nY luego iríamos por cada par de especies con “diferencias significativas” viendo entre qué variables están las diferencias. ¿Engorroso? Lo que le sigue, pero hay que hacerlo si te interesa saber en qué variables están las diferencias, caso contrario puedes saltarte esta útlima parte y solo reportar los resultados de los PERMANOVAs para cada par de especies.\nEsto es todo para esta sesión. Espero que haya sido de tu agrado y que te sea útil en algún momento. En la siguiente sesión volveremos al aprendizaje automatizado, esta vez para hablar de algo que, dependiendo del problema, podemos aplicar en vez de los MANOVAs: la clasificación.\n\n\n\n\nKing AP, Eckersley RJ. 2019. Chapter 8 - Inferential Statistics V: Multiple and Multivariate Hypothesis Testing. In: King AP, Eckersley RJ eds. Statistics for Biomedical Engineers and Scientists. Academic Press, 173-199. DOI: https://doi.org/10.1016/B978-0-08-102939-8.00017-7.\n\n\nPerneger TV. 1998. What’s wrong with Bonferroni adjustments. BMJ 316:1236-1238. DOI: 10.1136/bmj.316.7139.1236.",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Hipótesis Multivariadas</span>"
    ]
  },
  {
    "objectID": "c17_clasif.html",
    "href": "c17_clasif.html",
    "title": "17  Aprendizaje supervisado: Clasificación",
    "section": "",
    "text": "17.1 Librerías\nlibrary(palmerpenguins)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(vip)\n# library(ranger)\ntheme_set(see::theme_lucid() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()))",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Aprendizaje supervisado: Clasificación</span>"
    ]
  },
  {
    "objectID": "c17_clasif.html#introducción",
    "href": "c17_clasif.html#introducción",
    "title": "17  Aprendizaje supervisado: Clasificación",
    "section": "17.2 Introducción",
    "text": "17.2 Introducción\nHasta el momento hemos visto dos formas de involucrar respuestas categóricas: crearlas con alguna técnica de agrupamientos, o utilizarlas para hacer comparaciones con pruebas de significancia. En este capítulo vamos a hablar de (en mi opinión) una mejor alternativa (dependiendo de tu objetivo): la clasificación. La clasificación es una técnica de aprendizaje supervisado en la que el objetivo es predecir clases (etiquetas) a partir de los valores de los predictores. Al ser aprendizaje supervisado podemos evaluar su desempeño; es decir, qué tan bien estamos prediciendo nuestras etiquetas. Si tenemos un buen desempeño, podemos utilizar alguna medida de importancia de variables para explicar el modelo. Si no, toca volear mesas, jalarnos los cabellos, y regresar a pensar qué es lo que está fallando: ¿el modelo seleccionado no es el óptimo? ¿Debíamos de utilizar algún preprocesamiento? ¿el modelo no está optimizado adecuadamente? Al terminar este capítulo deberías de ser capaz de responder adecuadamente a todas estas preguntas. En la Figura 17.1 tienes una pequeña comparación entre los agrupamientos, la comparación y la clasificación.\n\n\n\n\n\n\nFigura 17.1: Comparación entre agrupamientos, comparación y clasificación",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Aprendizaje supervisado: Clasificación</span>"
    ]
  },
  {
    "objectID": "c17_clasif.html#clasificación-y-clasificadores",
    "href": "c17_clasif.html#clasificación-y-clasificadores",
    "title": "17  Aprendizaje supervisado: Clasificación",
    "section": "17.3 Clasificación y clasificadores",
    "text": "17.3 Clasificación y clasificadores\nYa sabemos en qué consiste la clasificación, ahora veamos el fundamento detrás de todos los modelos de clasificación (clasificadores). Hoy en día uno de los temas que está muy de moda es la visión por computadora. Aunque se puede hacer un modelo no supervisado para la identificación de objetos, en muchos casos tenemos algo como lo siguiente: un conjunto de imágenes de un objeto (elefantes) y otro conjunto de imágenes de otro (jirafas). Le damos este par de etiquetas e imágenes a la computadora para entrenar un clasificador; es decir, para ajustar los parámetros del modelo. Posteriormente verificamos que el modelo funcione bien pasándole una imagen sin etiquetar y que sea el clasificador quien le asigne una etiqueta. Si el modelo aprendió lo que tenía que aprender debería de ser capaz de decirnos que una imagen de una jirafa es una jirafa, y no un elefante.\n\n\n\n\n\n\nFigura 17.2: Clasificación de imágenes de elefantes y jirafas.\n\n\n\n\n\n\n\n\n\nImportante\n\n\n\nEste proceso de entrenamiento y prueba debemos de aplicarlo con todos los modelos de aprendizaje supervisado, independientemente de si son regresiones o clasificaciones.\n\n\nYa tenemos una idea del qué hace la clasificación, pero nos falta el cómo. Para regresión hemos hablado de distintos modelos (lineales, Capítulo 11 o no lineales, Capítulo 13), y en clasificación es lo mismo, tenemos una gran cantidad de métodos, cada uno con sus supuestos, fortalezas, y debilidades. En los renglones de la Figura 17.3 tenemos tres conjuntos de datos bivariados con dos clases (roja y azul) cada uno, y en las columnas los márgenes de decisión de los clasificadores; es decir, el límite entre ambas clases que aprendió cada clasificador. Esta es la gran diferencia entre los clasificadores: la complejidad de los límites que son capaces de aprender. En la tercera columna tenemos una máquina de soporte vectorial con un kernel lineal (linear SVM); es decir, que solamente puede aprender un límite lineal entre las clases. La consecuencia es que tiene un desempeño muy pobre en el segundo conjunto de datos, pero uno bastante aceptable el tercero. A su derecha tenemos también una máquina de soporte vectorial, solo que esta vez con un kernel RBF (RBF SVM) que puede aprender un límite más complejo. Como resultado el desempeño es mejor en todos los casos que en su contraparte lineal. Y así podríamos ir analizando caso por caso, viendo quién funciona mejor en qué tipo de casos, pero verás una tendencia clara: los clasificadores con límites de decisión más flexibles son los que tienen un mejor desempeño, independientemente del problema.\n\n\n\n\n\n\nFigura 17.3: Comparación de clasificadores\n\n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nEso son excelentes noticias, ¿no? Simplemente seleccionemos el algoritmo más flexible de todos y quitémonos de problemas. NO, para nada. Usualmente, conforme incrementa la flexibilidad del modelo, incrementa su complejidad.",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Aprendizaje supervisado: Clasificación</span>"
    ]
  },
  {
    "objectID": "c17_clasif.html#consideraciones-al-construir-un-modelo-de-aprendizaje-automatizado",
    "href": "c17_clasif.html#consideraciones-al-construir-un-modelo-de-aprendizaje-automatizado",
    "title": "17  Aprendizaje supervisado: Clasificación",
    "section": "17.4 Consideraciones al construir un modelo de aprendizaje automatizado",
    "text": "17.4 Consideraciones al construir un modelo de aprendizaje automatizado\nEsto último me lleva a hablar sobre con qué cosas debemos de tener cuidado al generar un modelo de aprendizaje automatizado, pues va mucho más allá de solo utilizar lm() y reportar un \\(R^2\\).\n\n17.4.1 Ajuste (sub y sobre)\nEn los Capítulo 11 y Capítulo 13 vimos cómo entrenar y “evaluar” un modelo de regresión, pero no pusimos demasiada atención a la parte de la bondad de ajuste. ¿Por qué? Porque evaluar un modelo PREDICTIVO a partir de los datos que lo entrenaron es un SINSENTIDO. Es el equivalente a que yo te diga que al final del curso va a haber un examen, que te proporcione una guía de estudio, que tú como estudiante diligente la resuelvas y la estudies, y que el examen tenga exactamente las mismas preguntas. ¿Qué representaría tu calificación? ¿Lo que aprendiste sobre el curso o qué tan bien memorizaste la guía de estudio? Evidentemente lo segundo. En el caso de los modelos de aprendizaje supervisado pasa lo mismo: “aprenden” la información contenida en los datos, pero en el peor de los casos solo “memorizan” los datos que se les dieron (Figura 17.4).\n\n\n\n\n\n\nFigura 17.4: Probar un modelo de ML con datos de entrenamiento es ponerse una medalla a uno mismo por su buen desempeño: no es una evaluación objetiva.\n\n\n\n¿Cómo contendemos con esto? Hacemos una división de nuestros datos: la mayor parte de los datos (usualmente alrededor del 75%) se utiliza para entrenamiento, mientras que los datos restantes se utilizan para probar el modelo final:\n\n\n\n\n\n\nFigura 17.5: División entrenamiento/prueba.\n\n\n\nEsto lo que nos permite es evaluar objetivamente el desempeño de nuestro modelo, pues nunca ha visto los datos de prueba. Volviendo a la analogía del examen final y la guía de estudio, sería como si yo esta vez te diera un examen que abarque los mismos temas que en la guía, pero con problemas que te hagan pensar o abordarlos desde otra perspectiva para que realmente demuestres tu dominio sobre el temario, y no solo que resolviste la guía.\nEsta “memorización” tiene un nombre: sobreajuste, que se define formalmente como una poca capacidad de generalización del modelo; es decir, que vamos a tener medidas de bondad de ajuste (o medidas de evaluación) mucho mejores en los datos de entrenamiento que en los datos de prueba. En todo escenario de aprendizaje supervisado buscamos un modelo que sea capaz de extraer la mayor cantidad de información posible de los datos, sin llegar a memorizar cada dato (sobreajuste), pero tampoco pasarnos de simples (subajuste; Figura 17.6)\n\n\n\n\n\n\nFigura 17.6: Tres modelos ajustados a un conjunto de datos: a) un modelo cuadrático que captura adecuadamente la relación entre las variables, no predice exactamente ningún punto, pero tampoco hay casos que estén extremadamente lejos (ajuste óptimo); b) un modelo lineal que no es suficiente para capturar adecuadamente la relación entre las variables (subajustado); y c) un modelo polinomial de orden n-1 que memorizó a la perfección los datos de entrenamiento, pero al que si se le da un dato fuera de ellos no podrá predecir adecuadamente (sobreajustado).\n\n\n\n\n\n\n\n\n\nNota\n\n\n\n¿A qué medidas de bondad de ajuste me refiero? A aquellas que tienen que ver con la capacidad predictiva del modelo. En regresiones serían medidas como el \\(R^2\\) o el \\(MSE\\). Más adelante explicaremos a detalle cómo evaluar el desempeño con un clasificador, pero es con una matriz de confusión, la curva de Características del Operador Receptor (ROC) y el área bajo esta (AUC-ROC).\n\n\nAdemás de la parte práctica/filosófica tenemos otro problema que resolver: la compensación/balance de sesgo-varianza. El sesgo lo entendemos como la diferencia entre la predicción promedio de nuestro modelo y el valor real que queremos predecir (nuestro error), de manera que modelos con un alto sesgo no le prestan demasiada atención a los datos (error grande) y en consecuencia están sobre-simplificados. La varianza en este contexto la entendemos como la variabilidad del modelo de predicción para un punto dado, de modo que un modelo con una alta varianza le presta demasiada atención a los datos de entrenamiento y no generaliza bien a datos que nunca ha visto. ¿Por qué compensación? Porque, como ya había mencionado superficialmente antes, entre más complejo sea el modelo, mayor es la probabilidad del sobreajuste. ¿Qué tan severo puede ser? Tan severo como en la siguiente figura:\n\n\n\n\n\n\nFigura 17.7: Relación entre el desempeño de un modelo en realción a su complejidad, evaluado en datos de entrenamiento y en datos de prueba. A mayor complejidad incrementa el desempeño en los datos de entrenamiento, pero no es así con datos que no ha visto.\n\n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nCuando hablo de la complejidad del modelo no me refiero solo a seleccionar un modelo que sea más complejo que otro, sino también a la cantidad de variables. No porque podamos meter cientos o miles de variables a un modelo quiere decir que debemos hacerlo, hay que hacer una rigurosa selección de variables para reducir la diferencia entre los datos de entrenamiento y los de prueba.\n\n\n\n17.4.1.1 Validación cruzada y optimización\nCon esto llegamos a la validación cruzada y la optimización de hiper-parámetros. La validación cruzada consiste en partir los datos de entrenamiento en \\(k\\) conjuntos de entrenamiento/prueba, evaluar el desempeño para cada uno, y promediarlos (Figura 17.8)\n\n\n\n\n\n\nFigura 17.8: Validación cruzada y cómo repartir los datos para entrenar, optimizar y validar un modelo de aprendizaje supervisado.\n\n\n\nLo que conseguimos con esto es una evaluación más robusta del desempeño de nuestro modelo, pues ya estamos viendo qué pasa con distintas combinaciones de datos vistos/no vistos. Pudiera llegar a suceder que tenemos mucha suerte y que la partición original se preste para obtener buenas métricas, o tener muy mala suerte y que se preste para tener malas métricas. Y para acabar de “redondear” lo escépticos que somos con nuestro modelo, después de haber probado con distintos \\(k\\) sub-conjuntos (k-folds), lo volvemos a probar con los datos de prueba originales (hay quien los denomina en este contexto datos de validación).\n\n\n\n\n\n\nNota\n\n\n\nAsí como no muestreamos solo un individuo para hacer inferencias, tampoco consideramos una sola partición entrenamiento prueba. ¿Cuántas sub-divisiones debemos de hacer? Pueden ser tan pocas como 5, o tantas como n-1, pero un “estándar” (más bien una convención por comodidad) es que la validación cruzada sea de orden 10 (10 k-folds).\n\n\n\n\n\n\n\n\nImportante\n\n\n\nEl tamaño de la partición entrenamiento-prueba original y el orden de la validación cruzada dependen del número de datos que tengas. Si tu conjunto de prueba queda con un tamaño muy reducido (pensemos menos de diez observaciones), mejor solo quédate con la validación cruzada en todos tus datos.\n\n\n¿Y qué pinta la “optimización de hiperparámetros” aquí? Para empezar, ¿qué es la optimización de hiperparámetros? Pues bien, resulta que muchos modelos de aprendizaje supervisado tienen no solo los parámetros que queremos ajustar (por ejemplo las pendientes en la regresión lineal), sino que también tienen otros parámetros que controlan su complejidad. Esto puede ser el valor de \\(\\alpha\\) o \\(\\lambda\\) en una regresión regularizada como las que veremos en el Capítulo 18, o el número máximo de predictores que considera un árbol de decisión. Es importante optimizar esos hiperparámetros, para no terminar con un modelo innecesariamente complejo que no va a tener una buena capacidad predictiva, y la mejor manera de hacerlo es utilizando validación cruzada.\n\n\n\n17.4.2 Fuga de información\nHay otra cosa con la que debemos de tener muchísimo cuidado al construir cualquier modelo de aprendizaje automatizado: la fuga de información. Volviendo a nuestra analogía del examen sería el equivalente a que alguien te soplara las respuestas del examen, o a que sacaras un acordeón. ¿Cómo pasa esto? No prestando atención a la información que le damos al modelo. El conjunto de prueba debe de aislarse completamente del de entrenamiento. ¿Necesitas pre-procesar los datos de una manera (escalarlos, por ejemplo)? Escala los datos de entrenamiento y utiliza esos parámetros para escalar los datos de prueba. Si vas a centrar y estandarizar, separa tus datos, calcula las medias y desviaciones de cada variable con los datos de entrenamiento y con esas medias y desviaciones centra y estandariza ambos conjuntos de datos; es decir, no escales la base completa, de lo contrario le estás dando al modelo información sobre datos que no debería de ver en ningún momento antes de la evaluación. Tampoco las escales por separado, pues eso te introducirá un sesgo aleatorio derivado de la división.\nOtro caso de fuga de información, aún menos evidente que el anterior, es tener casos en los que tengamos algo tipo “la variable \\(A\\) lógicamente implica la respuesta \\(Y\\)”. ¿Cómo es esto? Imagina que una tienda departamental que vende únicamente joyas, películas y electrónica (raro, lo sé) nos contrata para saber cuánto va a gastar un cliente en joyería (el segmento con mayores utilidades) a partir de lo que gasta en las otras dos cosas, por lo que nos da los registros cliente a cliente en la siguiente tabla:\n\n\n\n\n\n\nFigura 17.9: Registro de ventas de una tienda departamental imaginaria. ¿Qué cliente no da información útil para predecir el gasto en joyería a partir de lo gastado en películas y electrónica?\n\n\n\n¿Qué cliente salta a la vista? Espero que me digas que el 11125, pues la única razón por la que el cliente está en el registro es porque compró algo de joyería. Nada más. Esto hace que nuestro modelo aprenda algo, pero ese algo es una peculiaridad y no algo relevante sobre lo modelado.\n\n\n\n\n\n\nNota\n\n\n\nEl tema de la fuga de información es sumamente extenso, pero una excelente lecura al respecto es Kaufman et al. (2012).\n\n\nAhora sí, hablemos de cómo evaluar un clasificador.",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Aprendizaje supervisado: Clasificación</span>"
    ]
  },
  {
    "objectID": "c17_clasif.html#evaluación-de-un-clasificador",
    "href": "c17_clasif.html#evaluación-de-un-clasificador",
    "title": "17  Aprendizaje supervisado: Clasificación",
    "section": "17.5 Evaluación de un clasificador",
    "text": "17.5 Evaluación de un clasificador\nHace un par de subtemas mencioné someramente las formas con las que evaluamos la calidad de las predicciones de un clasificador. Bien, ahora entremos a los detalles correspondientes.\n\n17.5.1 Matriz de confusión\nLa matriz de confusión no es otra cosa mas que una tabla de contingencia que relaciona las clases observadas y las predichas por el clasificador. En un caso de clasificación binaria (dos clases objetivo) tendríamos una estructura como la siguiente:\n\nMatriz de confusión para un clasificador binario. V: verdadero; F, falso; P: positivo; N: negativo; es decir, en la diagonal tenemos las clasificaciones correctas (verdaderos positivo y negativo), y encima y debajo los errores (falsos positivo y negativo).\n\n\n\nA\nB\n\n\n\n\n\\(\\hat{A}\\)\nVP\nFP\n\n\n\\(\\hat{B}\\)\nFN\nVN\n\n\n\nSi lo piensas con atención, esto es todo lo que necesitamos: en qué acertó y en qué se equivocó, aunque esto se vuelve muy impráctico cuando tenemos más de dos clases, y no nos permite evaluar rápidamente nuestro clasificador. Es aquí donde entran las medidas derivadas de esta matríz. Expliquémoslas todas con la siguiente matriz de confusión:\n\nEjemplo de matriz de confusión.\n\n\n\nA\nB\n\n\n\n\n\\(\\hat{A}\\)\n4\n1\n\n\n\\(\\hat{B}\\)\n2\n5\n\n\n\n\nExactitud (Accuracy): Representa el porcentaje de clasificaciones correctas, independientemente de si fueron verdaderos positivos o verdaderos negativos. Es decir, la suma de los elementos en la diagonal dividida entre el número total de observaciones. En nuestro ejemplo el clasificador tuvo una exactitud del 75%.\n\n\\[Exact = \\frac{VP+VN}{N} = \\frac{4+5}{12} = \\frac{3}{4} =0.75\\]\n\nPrecisión (Precision): Representa el porcentaje de clasificaciones positivas correctas; es decir, de todas las clasificaciones que fueron positivas, qué proporción era en realidad positiva. Nuestro clasificador de ejemplo tuvo una precisión del 80%:\n\n\\[Prec = \\frac{VP}{VP+FP} = \\frac{4}{4+1} = \\frac{4}{5} = 0.8 \\]\n\nTasa de verdaderos positivos (True Positive Rate, TPR). También conocida como sensibilidad. Esta indica qué porcentaje de los verdaderos positivos se clasificó correctamente; es decir, dividimos los verdaderos positivos entre la suma de estos y los falsos negativos, pues debieron de haber sido verdaderos positivos. Nuestro clasificador de ejemplo tuvo una sensibilidad del 66%\n\n\\[TPR = \\frac{VP}{VP+FN} = \\frac{4}{6} \\approx 0.66\\]\n\nTasa de verdaderos negativos (True Negative Rate, TNR). También conocida como especificidad. Es la proporción de los verdaderos negativos que se clasificó correctamente, siguiendo la misma lógica de la TPR. Nuestro clasificador de ejemplo tuvo una especificidad del 80%.\n\n\\[TNR = \\frac{VN}{VN+FP} = \\frac{5}{6} \\approx 0.8\\]\n\nF1. Esta es una medida que relaciona la precisión y la tasa de verdaderos positivos para dar una medida de la robustez del modelo. En nuestro modelo de ejemplo fue del 77%:\n\n\\[F1 = \\frac{2*Prec*TPR}{Prec + TPR} = 2*\\frac{0.75*0.8}{0.75+0.8} \\approx 0.77\\]\n\nTasa de No Información (No-Information Rate, NIR). Es el porcentaje que representa la clase mayoritaria del total, y representa la exactitud que tendría un modelo “bobo”; es decir, un modelo que predijera siempre esa clase mayoritaria. Nuestras clases están balanceadas, por lo que la NIR es del 50%:\n\n\\[NIR = max(n_i) = \\frac{6}{12} = 0.5\\]\nAdemás de estas R nos da otras dos que también son útiles:\n\n\\(p(Exact &gt; NIR)\\): el resultado de una prueba binomial para ver si la exactitud del modelo es superior a la de un modelo bobo.\nKappa de Cohen: es un índice de la confiabilidad interna del clasificador. En pocas palabras, es el porcentaje de clasificaciones correctas que no son atribuíbles al azar. Como buen índice, está contenido en el intervalo \\([-1,1]\\), pero el valor máximo alcanzable en nuestro problema depende de qué tan balanceadas (o no) estén nuestras clases. Entre más balanceadas estén, más fácil será alcanzar valores altos, y viceversa.\n\nSi bien todas estas medidas representan distintos aspectos de la clasificación, hay un par de formas de evaluación más que, para la mayor parte de los problemas, dan una mejor representación de la capacidad predictiva del modelo.\n\n\n17.5.2 Curva de Características del Operador Receptor (ROC)\nSi has escuchado algo sobre los modelos de distribución de especies (MAXENT o similares), seguramente también hayas escuchado sobre la curva ROC. Esto es porque esos modelos son, en realidad, clasificadores que predicen si una especie puede estar (positivo) o no (falso) en un lugar determinado, pero eso es salirnos del tema.\nLa curva ROC muestra la relación entre la TPR y la FPR (que es el complemento de la TPR: 1-TPR) a distintos niveles, lo cual da una evaluación más confiable de la capacidad predictiva del modelo (Meyer-Baese & Schmid, 2014) donde el mejor modelo llevará la “curva” hacia la esquina superior izquierda (una tasa de verdaderos positivos perfecta en todos los casos), y un modelo bobo estará cercano a una línea central de referencia con pendiente de 1, lo que representa una clasificación al azar:\n\n\n\n\n\n\nFigura 17.10: Curva ROC y sus partes. Entre mejor sea el modelo más se acercará a la esquina superior izquierda. Un modelo que solo haga predicciones al azar (bobo) estará en la línea amarilla. Si la curva se va debajo de esa línea solo quiere decir que el modelo está prediciendo la clase positiva como negativa.\n\n\n\n\n\n17.5.3 Área bajo la curva ROC (ROC-AUC)\nLa curva ROC es sumamente útil para ver qué tan bueno o malo es el modelo, pero siempre es más fácil lidiar con un solo número. Es ahí donde entra el área bajo la curva (AUC-ROC). Esta área bajo la curva está en el intervalo [0,1], donde 1 es una clasificación perfecta, 0.5 una clasificación aleatoria (no hubo clasificación), y 0 es que el modelo reciprocó (invirtió) la clase a predecir.\nEl AUC tiene dos bondades:\n\nEs invariable con respecto a la escala; es decir, mide qué tan bien se clasifican las predicciones, más que dar sus valores absolutos.\nEs invariable con respecto al umbral de clasificación. Cuando entrenamos un modelo de clasificación podemos seleccionar un umbral para que el modelo diga que algo es la clase positiva. Entre más bajo sea este umbral, más positivos vamos a tener. El AUC mide la calidad de las predicciones del modelo, independientemente de qué umbral se haya seleccionado.\n\nEstas dos bondades, sin embargo, tienen dos bemoles:\n\nLa invarianza de escala puede ser inconveniente. A veces necesitamos resultados de probabilidad bien calibrados (¿qué probabilidad hay de que una observación sea clasificada como A y no como B?), y el AUC no es eso.\nLa invarianza al umbral de clasificación puede ponernos el pie. En casos en los que tenemos grandes disparidades en el costo de falsos negativos con relación a falsos positivos es imperante minimizar uno de esos errores de clasificación, y el AUC no es la medida para eso.\n\nNo hay un criterio claro para decir “qué tanto es tantito” y definir si un modelo es “bueno” o “malo”, pero Hosmer, Lemeshow & Sturdivant (2013) dan los siguientes intervalos:\n\n0.5: no hay clasificación\n(0.5, 0.7]: clasificación pobre\n(0.7, 0.8]: clasificación aceptable\n(0.8, 0.9]: excelente clasificación\n(0.9, 1]: clasificación excepcional\n\nAhora sí, después de toda esta teoría podemos aprender a implementar, evaluar e interpretar clasificadores.",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Aprendizaje supervisado: Clasificación</span>"
    ]
  },
  {
    "objectID": "c17_clasif.html#árboles-de-decisión-y-ensambles",
    "href": "c17_clasif.html#árboles-de-decisión-y-ensambles",
    "title": "17  Aprendizaje supervisado: Clasificación",
    "section": "17.6 Árboles de decisión y ensambles",
    "text": "17.6 Árboles de decisión y ensambles\nEn la primera versión del curso hablé sobre clasificadores “tradicionales” que se han empleado en el área de la biología (regresión logística y análisis de funciones discriminantes lineales), pero creo que vale la pena hablar de algunas alternativas con márgenes de decisión más flexibles y que son casi igual de interpretables.\n\n\n\n\n\n\nNota\n\n\n\nLa regresión logística la vamos a retomar en el Capítulo 19 porque es un modelo lineal con un error binomial.\n\n\n¿Por qué “casi” igual de interpretables? Porque usualmente entre más flexible es un modelo, más complejo es su funcionamiento, menos simple es explicar cómo hace las predicciones y, por lo tanto, sacar conclusiones sobre cómo influyen las variables. Pero empecemos por lo sencillo.\n\n17.6.1 Árboles de decisión\nLos árboles de decisión son un modelo que “clasifica según el principio de partición recursiva, donde el espacio de atributos se parte recursivamente en regiones que contienen observaciones con valores de respuesta similares” (Strobl, Malley & Tutz, 2009). A veces me pregunto porque los matemáticos hacen definiciones tan agresivas, pero tiene mucho sentido. No explicaré qué es la recursividad porque sería desviarnos del tema, pero podemos simplificar la definición de un árbol de decisión como un modelo que clasifica a partir de decisiones binarias sobre nuestros datos, como si fuera un diagrama de flujo. Imagina que queremos decidir si vamos a surfear o no (asume que sabemos), entonces seguiríamos un diagrama de flujo como el siguiente:\n\n\n\n\n\n\nFigura 17.11: ¿Surfear o no? He ahí el dilema\n\n\n\n\n\n\n\n\n\nNota\n\n\n\nLas partes del árbol de decisión son básicamente las mismas que las del dendrograma (Capítulo 15). La diferencia es que no tenemos una raíz, cada nodo (punto de decisión) involucra una sola variable, y al final no tenemos hojas, sino nodos terminales.\n\n\nAplicado a nuestros datos la lógica es la misma: encontrar límites en cada variable que permitan llevar la decisión hacia una clase u otra. El algoritmo (Breiman, 2001) tiene un objetivo: que después de cada decisión incremente la “pureza” de los nodos que le siguen; es decir, que con cada decisión se abarque la mayor cantidad de datos posible hasta lograr una clasificación perfecta. Esto se logra minimizando la impureza Gini. Este índice tiene bastante teoría detrás, pero podemos simplificarlo como la tasa de error que obtendríamos si retiramos una variable, de modo que un buen predictor minimizará ese error y dará como resultado nodos más puros [ShogaRangswamy_2018]. En pocas palabras: vamos a seleccionar la variable con la que nos equivoquemos menos al hacer las decisiones.\nComo te habrás dado cuenta, es un algoritmo con una lógica que nos es fácil de entender, pues es muy similar a cómo tomamos decisiones (si A, entonces B) y, de hecho, esa es una de sus ventajas:\n\nLos árboles de decisión son fáciles de interpretar: su lógica binaria y la representación visual hace que interpretarlos sea sencillo. Además, la estructura jerárquica facilita saber qué atributos son los más importantes e incluye la interacción (por el momento entendámosla como la correlación) entre ellos.\nRequieren de poca o nula preparación de los datos: pueden lidiar con muchos tipos de variables, sean continuas o discretas, e incluso pueden lidiar con datos faltantes.\nSon flexibles: Aunque los estamos viendo en el tema de clasificación, podemos utilizarlos también para problemas de regresión. De hecho, si utilizas el algoritmo de Breiman (2001) que describimos antes, también se les conoce como Classification and Regression Trees (CART).\n\nComo todo lo demás en esta vida, si hay ventajas también hay desventajas:\n\nPropensos al sobreajuste. ¿Recuerdas que buscamos perfeccionar la clasificación lo más posible? Si dejamos que el árbol crezca indefinidamente va a, literalmente, memorizar los datos que le dimos para entrenarlo.\nSon estimadores con alta varianza, de modo que pequeñas variaciones dentro de los datos pueden producir árboles muy diferentes.\nAlto costo computacional, derivado de la aproximación “codiciosa” (greedy) tomada durante su construcción (entiendelo como evaluar todo al mismo tiempo). Entre más datos y clases con límites complejos tengamos, más tiempo o poder computacional (o ambos) vamos a necesitar para entrenar el modelo.\n\nLa tercer desventaja la podemos “obviar”, pues para la mayoría de los fines de investigación no vamos a necesitar entrenar y obtener el modelo en tiempo real, pero las primeras dos sí que son importantes, y nos llevan a hablar de los ensambles de árboles, particularmente los bosques aleatorios.\n\n\n17.6.2 Bosques aleatorios\nPara empezar, ¿qué es un ensamble? Es juntar múltiples modelos de aprendizaje automatizado para obtener un modelo con una mejor capacidad predictiva. Si nuestro modelo base son árboles de decisión y juntamos varios, es natural llamar a su ensamble un “bosque”. O bueno, solo a uno de sus ensambles, porque en realidad hay más modelos (AdaBoost, XGBoost, bagged trees) que son ensambles de árboles. ¿Por qué seleccionar a los bosques aleatorios? Realmente por practicidad, pues tanto AdaBoost como XGBoost tendrían razones similares:\n\nComparten las fortalezas de los árboles de decisión, con excepción de la interpretación (ya hablaremos de eso en el ejemplo).\nNulifican la principal desventaja de los árboles de decisión: al ser varios árboles se minimiza la probabilidad de sobreajuste.\nMinimizan también la alta varianza o, mejor dicho, se apropian de ella. La naturaleza aleatoria de los bosques aleatorio hace que, aún con los mismos datos, podamos obtener bosques completamente diferentes. ¿Importa? Para nada, porque no vamos a explicar cada bosque de manera individual.\n\nAhora bien, ¿cómo funcionan los bosques aleatorios? No puedo construir diferentes árboles de decisión si tengo los mismos datos de entrenamiento, entonces hay que cambiar eso. La forma en que lo hacemos deriva de dos conceptos (Pal, 2017):\n\nAgregación Bootstrap (“Bagging”): Se genera un conjunto de entrenamiento diferente para cada árbol, utilizando muestreos con reemplazo del conjunto original de datos de entrenamiento\nSelección aleatoria de atributos: Puedes entenderla como una agregación Bootstrap para los predictores en la que los atributos (variables) considerados en cada nodo son un subconjunto aleatorio de las variables originales.\n\nEs decir, cada árbol en el bosque va a trabajar con su propio subconjunto de observaciones, y en cada partición va a poder seleccionar solo de un subconjunto aleatorio de variables. El resultado es un modelo de aprendizaje supervisado que se considera como uno de los más eficientes (Carvajal, Maucec & Cullick, 2018), pero básta de cháchara y pongámonos manos a la obra.\n\n17.6.2.1 Entrenamiento\nUtilicemos esta vez los datos palmerpenguins::penguins_raw, y aprovechemos también para introducir el uso de tidymodels. Nuestro proceso consiste de básicamente 11 pasos (+1):\n\nGuardemos los datos en un objeto, manteniendo solo las variables numéricas (sin el identificador). El resultado es un conjunto de datos con 7 variables:\n\nLongitud del culmen\nProfundidad del culmen\nLongitud de la aleta\nMasa corporal\n\\(\\delta^{15}N\\) (‰)\n\\(\\delta^{13}C\\) (‰)\nSpecies, que contiene las etiquetas que queremos predecir\n\n\n\npeng_data &lt;- penguins_raw |&gt;\n             # Seleccionar solo las columnas numéricas y \"Species\"\n             select(where(is.numeric) |\n                    contains(\"Species\")) |&gt;\n             # Retirar el identificador de la muestra\n             select(-`Sample Number`) |&gt; \n             # Extraer la primera palabra de la columna `Species`\n             mutate(Species = str_extract(Species,\n                                          pattern = \"\\\\w*\")) |&gt;\n             # Descartar los datos faltantes\n             drop_na()\nhead(peng_data)\n\n\n  \n\n\n\nSolo hay que tener presente que hay un desbalance importante en el número de observaciones por especie:\n\npeng_data |&gt; count(Species)\n\n\n  \n\n\n\nAhora optimicemos e interpretemos el modelo de bosques aleatorios.\n\nDividir los datos en datos de entrenamiento y prueba. Para esto utilizaremos las funciones rsample::initial_split(data, strata) y rsample::training(split). Además, remuestrearemos los datos de entrenamiento para realizar la validación cruzada y optimizar los hiper-parámetros del modelo con la función rsample::vfold_cv()\n\n\nset.seed(0)\n# Dividimos los datos\npeng_split &lt;- rsample::initial_split(peng_data,\n                                     strata = Species)\n\n# Extraemos los datos de entrenamiento\npeng_train &lt;- rsample::training(peng_split)\n\n# Datos para validación cruzada\npeng_cv &lt;- vfold_cv(peng_train)\n\n\nPreprocesar los datos. Para esto haremos lo que tidymodels denomina como una receta. Recordarás que los árboles de decisión no requieren prácticamente ningún tipo de preprocesamiento, pero centremos las variables numéricas para ejemplificar.\n\nPasos:\n\nLe damos a la receta (recipe()) la fórmula y los datos de entrenamiento.\nAñadimos un paso adicional para centrar los datos numéricos.\n\nEl objeto peng_prep sigue los pasos que definimos para el procesamiento de los datos (por eso receta) y obtiene los parámetros que se van a utilizar, mientras que peng_juiced contiene los datos ya procesados.\n\n\n\n\n\n\n\n\nNota\n\n\n\nUna lista completa de todos los pasos de pre-procesamiento la puedes encontrar en la documentación correspondiente de tidymodels.\n\n\n\n# Formación de la receta\npeng_rec &lt;- recipe(Species~., data = peng_train) |&gt;\n            #update_role(`Sample Number`,\n            #            new_role = \"id vars\") |&gt;\n            #update_role_requirements(role = \"id vars\", bake = FALSE) |&gt; \n            update_role(Species, new_role = \"outcome\") |&gt; \n            step_center(all_numeric())\n            \n# Obtener parámetros para preprocesar\npeng_prep &lt;- peng_rec |&gt; prep()\n\n# Preprocesar los datos\npeng_juiced &lt;- peng_prep |&gt; juice()\n\n\nCrear el modelo. Ahora podemos especificar el modelo de bosques aleatorios.\n\nEvidentemente debemos de controlar la complejidad del modelo, lo cual haremos ajustando sus hiperparámetros:\n\nmtry (el número máximo de predictores por árbol)\nmin_n (el número de observaciones necesarias para seguir dividiendo los datos)\ntrees (el número de árboles en el ensemble).\n\nDespués especificamos que es un bosque para clasificación con set_mode, y por último le indicamos en set_engine que utilice la librería ranger para construir el bosque:\n\n\n\n# Especificar el modelo\n# Los hiperparámetros van a ser ajustados\ntune_spec &lt;- rand_forest(mtry = tune(),\n                         trees = tune(),\n                         min_n = tune()) |&gt; \n             # Es un problema de clasificación\n             set_mode(\"classification\") |&gt;\n             # Se va a resolver con la librería ranger\n             set_engine(\"ranger\")\n\n\n\n\n\n\n\nNota\n\n\n\nUna lista completa de modelos la puedes encontrar aquí\n\n\n\nLuego, y es aquí donde brilla tidymodels, formar un flujo de trabajo (workflow()) que contenga ambos pasos: la receta de preprocesamiento y el modelo.\n\n\n# Se inicia un flujo de trabajo\ntune_wf &lt;- workflow() |&gt; \n           # Sigue la receta para pre-procesar datos\n           add_recipe(peng_rec) |&gt; \n           # Entrena el modelo\n           add_model(tune_spec)\n\n\n\n\n\n\n\nNota\n\n\n\nEn este punto no hemos entrenado el modelo, solo le hemos dicho a tidymodels cómo queremos que se manejen nuestros datos y qué modelo queremos aplicar. ¿Por qué “brilla” entonces tidymodels? Porque separa los pasos de la declaración del modelo de la ejecución, por lo que puedes modificar la especificación del modelo o del preprocesamiento de los datos de manera independiente, y permite que re-entrenemos el modelo simplemente modificando los datos en el paso 1.\n\n\n\nAjustar los hiper-parámetros\n\nEstablecer el procesamiento en paralelo para que el modelo se entrene de manera más eficiente\nEntrenar el modelo, lo cual podemos hacer con fuerza bruta, proponiendo una malla gigantesca de posibles valores, o podemos hacerlo de manera más eficiente escogiendo 20 puntos aleatorios para guiar la búsqueda. Hagamos lo segundo:\n\n\n\nset.seed(123)\n# Asignar núcleos para la computación en paralelo\ndoParallel::registerDoParallel(cores = parallel::detectCores(logical = F))\n\n# Buscar los mejores hiperparámetros de entre 20 valores\ntune_res &lt;- tune_grid(tune_wf,\n                      resamples = peng_cv,\n                      grid = 20)\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\n\nEvaluar primer ajuste. En este punto solo propusimos 20 valores tentativos para los hiper-parámetros del bosque, con el objetivo de guiar la búsqueda y no abusar de la confianza de nuestras computadoras. Pues bien, ahora hay que ver alrededor de qué valores vamos a hacer la optimización. ¿Cómo? Graficando los valores de AUC que le corresponden a cada valor de cada hiperparámetro. En la figura de abajo pareciera que todo es un caos, pero en realidad podemos ver que el AUC está en sus puntos más altos con:\n\nmin_n entre 5 y 10\nmtry con menos de 3\ntrees con alrededor de 1000.\n\n\n\nplot_rf_cv &lt;- function(tune_res){\n  # Graficar los valores de AUC de validación cruzada\n  # Recuperar los valores\n  tune_res |&gt; collect_metrics() |&gt;\n              # Quedarnos solo con los AUC\n              filter(.metric == \"roc_auc\") |&gt;\n              # Extraer las columnas con las medidas\n              select(mean, min_n, mtry, trees) |&gt;\n              # Poner la malla en formato largo\n              pivot_longer(min_n:trees,\n                           values_to = \"value\",\n                           names_to = \"parameter\") |&gt;\n              # Graficar con `ggplot2`\n              ggplot(aes(value, mean, color = parameter)) +\n              geom_point(show.legend = F) +\n              facet_wrap(~parameter, scales = \"free_x\") +\n              labs(y = \"AUC\")\n}\n\nplot_rf_cv(tune_res)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNota\n\n\n\nNota que estamos encadenando el procesamiento de los resultados y su graficado. Eso no tiene nada de sorprendente, a final de cuentas ggplot recibe como primer argumento un data.frame, pero es importante que observes que el operador de encadenamiento pasa de |&gt; a +.\n\n\n\nAjustar el ajuste; es decir, ajustar mejor los hiperparámetros con valores alrededor de esos “óptimos” que encontramos arriba.\n\nEstablecemos una malla de búsqueda más fina con la función grid_regular.\nRe-entrenamos el modelo, pero esta vez de manera más dirigida con rf_grid. El resultdo es que ahora todos los AUCs resultantes están por encima de 0.995.\n\n\n\n# Establecer una malla con los nuevos valores de referencia\nrf_grid &lt;- grid_regular(mtry(range = c(1, 3)),\n                        min_n(range = c(5, 10)),\n                        trees(range = c(700, 1100)),\n                        levels = 5)\n\n# Optimizar los hiperparámetros con esa malla\ntune2_res &lt;- tune_grid(tune_wf,\n                       resamples = peng_cv,\n                       grid = rf_grid)\n\n# Obtener y graficar los resultados\nplot_rf_cv(tune2_res)\n\n\n\n\n\n\n\n\n\nSeleccionar el mejor modelo. Con el código anterior optimizamos lo más posible los hiper-parámetros, ahora toca seleccionar la mejor combinación de hiperparámetros.\n\nIdentificar el modelo con la función select_best()\nActualizar la especificación original (tune_spec)\n\n\n\n# Seleccionar el modelo con los mejores hiperparámetros\n# (a los que les corresponde el mayor AUC)\nbest_auc &lt;- select_best(tune2_res, \"roc_auc\")\n\n# Actualizar el modelo con los nuevos parámetros\nfinal_rf &lt;- finalize_model(x = tune_spec,\n                           parameters = best_auc)\n\n# Muestra los detalles del modelo\nfinal_rf\n\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  mtry = 1\n  trees = 700\n  min_n = 6\n\nComputational engine: ranger \n\n\n\nRevisar importancia de variables. ¿Qué podemos aprender de este modelo? Las variables más importantes para la clasificación son la longitud del culmen, seguida de la longitud de la aleta. ¿Tienen sentido biológico estos resultados? Esa es una discusión para otro momento, pues solo tomamos estos datos para ejemplificar la implementación de los bosques aleatorios.\n\n\n# Graficar la importancia de variables\n\n# Pasa el modelo final\nfinal_rf |&gt; \n  # Fija que se va a extraer la importancia de variables\n  set_engine(\"ranger\", importance = \"permutation\") |&gt;\n  # Estimada desde los datos de entrenamiento (- el identificador)\n  fit(Species ~.,\n      data = juice(peng_prep) |&gt; select(-contains(\"Sample\"))) |&gt; \n  # Calcula y grafica la importancia de variables\n  vip() +\n  # geom_point(color = \"deepskyblue3\") +\n  labs(title = \"Importancia de variables\",\n       y = element_blank())\n\n\n\n\n\n\n\nFigura 17.12: Importancia de variables del bosque aleatorio\n\n\n\n\n\n\n\n\n\n\n\nNota\n\n\n\nSi pones atención te darás cuenta de que estamos obteniendo la importancia de variables utilizando permutaciones, esto es porque es una manera más robusta de obtenerlas que solo utilizando Gini. ¿Cómo se calculan? Permutando los valores y viendo qué tanto incrementa el error después de esa permutación; i.e., entre más importante sea una variable, mayor va a ser el impacto si la permutamos.\nTenemos algunas alternativas como valores Shapely/explicaciones SHAP o LIME que son agnósticas al modelo, entre otras, cada una con sus ventajas y desventajas. Una de las ventajas de SHAP, por ejemplo, es que se puede obtener la importancia de cada variable para cada grupo.\n\n\n\nMuy importante, verificar que el modelo no esté sobreajustado. Para esto vamos a crear un último flujo de trabajo, y después ajustar una última vez. ¿Por qué? Porque nuestros modelos hasta ahora se entrenaron en 10 subconjuntos de nuestros datos de entrenamiento (validación cruzada). Para este último ajuste usaremos la función last_fit(), la cual ajusta el modelo final en todos los datos de entrenamiento y evalúa en los datos de prueba. Los valores de ROC-AUC y de exactitud son muy buenos, pero además en línea con lo que esperábamos de la optimización (AUC &gt; 0.995), lo cual quiere decir que el modelo no está sobre-ajustado, sino que es muy bueno.\n\n\n# Flujo de trabajo final\nfinal_wf &lt;- workflow() |&gt; \n            add_recipe(peng_rec) |&gt; \n            add_model(final_rf)\n\n# Último ajuste\nfinal_res &lt;- final_wf |&gt; \n             last_fit(peng_split)\n\n# Resultados de la evaluación en los datos de prueba\nfinal_res |&gt; \n  collect_metrics()\n\n\n  \n\n\n# Creación de las curvas ROC\nrf_auc &lt;- final_res |&gt;\n          collect_predictions() |&gt; \n          roc_curve(Species, c(.pred_Adelie,\n                               .pred_Chinstrap,\n                               .pred_Gentoo)) |&gt; \n          mutate(model = \"Random Forest\")\n\nautoplot(rf_auc) +\n  labs(title = \"Curvas ROC\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportante\n\n\n\nEn la función roc_curve del objeto rf_auc estamos pasando un vector con tres objetos ocultos: .pred_Adelie, .pred_Chinstrap, .pred_Gentoo. Estos contienen la probabilidad de pertenencia de cada observación a cada especie, y es con los que se obtiene la curva ROC que le corresponde a cada especie, pues la curva es solo para clasificaciones binarias. ¿Cómo funciona en este caso? La clase positiva es la clase predicha en cada curva, y la negativa es lo demás, básicamente representando un clasificador clase + vs. resto.\n\n\n\nElaborar el reporte ¿Cómo reportamos todo esto? Te recomiendo que le eches un ojo a la GitHub Pages de Enríquez-García et al. (2023) para ver cómo describir el método, pero los resultados podemos resumirlos como:\n\n\nEl bosque aleatorio tuvo un excelente desempeño sin sobreajustar (AUC-ROC = 1 en los datos de prueba). Las variables más importantes para la clasificación fueron la longitud del culmen (\\(\\approx 17\\%\\)), seguida de la longitud de la aleta (\\(\\approx 12.5\\%\\); Figura 17.12). El bosque aleatorio optimizado estuvo compuesto por 800 árboles, cada uno formado con un predictor (mtry), considerando al menos 5 observaciones para poder hacer una división (n_min).\n\n\n\n17.6.2.2 Predicción\nAntes vimos como podemos pasarle datos de validación a nuestro modelo para, valga la redundancia, validar su desempeño. Aunque aquí lo estamos haciendo para darnos una idea de qué tanto podemos confiar en nuestras inferencias, en realidad el objetivo es ver si podemos predecir adecuadamente nuevos datos, i.e., extrapolar. Si no generaliza bien, los resultados siguen siendo válidos para la muestra con la que entrenamos el modelo. Esto tiene sus bemoles y el qué tanto valga la pena interpretar los resultados de un modelo sobre ajustado es una decisión propia. Personalmente, no le veo mucho caso, pero lo cierto es que si no generaliza bien no podemos confiar, en absoluto, en las predicciones que nos de.\nCon esto lo que quiero decir es que, si tienes un modelo que generaliza bien y observaciones para las cuales no tienes su clase, puedes utilizar el modelo entrenado para obtener a qué clase es más probable que pertenezcan. Imagina que alguien curioso quiso hacer su buena acción y midió un pingüino, pero no le tomó una foto y no sabes a qué especie pertenece. Las medidas que este buen samaritano te dio son:\n\nvar_names &lt;- colnames(peng_train)[1:6]\nnew_peng &lt;- data.frame(v1 = 45.15, v2 = 17.30, v3 = 197.0,\n                       v4 = 4062, v5 = 8.657, v6 = -25.83) |&gt;\n            rename_at(vars(starts_with(\"v\")),\n                      ~ var_names)\n\nnew_peng\n\n\n  \n\n\n\nPuedes entonces asignar la especie con el modelo entrenado. Lo primero es, entonces, recuperar todo el flujo de trabajo:\n\nrf_parsnip &lt;- final_res |&gt; extract_workflow()\n\n\n\n\n\n\n\nTip\n\n\n\nAl extraer el flujo de trabajo nos ahorramos tener que pre-procesar los datos a mano, podemos simplemente pasar los datos crudos y se aplicará todo el procesamiento que hayamos definido en nuestra receta antes de pasar los datos al modelo per-se.\n\n\nLuego puedes utilizar la función predict() para hacer tus predicciones. En este caso, lo más probable es que sea Adelie:\n\nrf_parsnip |&gt; predict(new_peng, type = \"class\")\n\n\n  \n\n\n\n¿Qué tan probable? Solo cambia \"class\" por \"prob\":\n\nrf_parsnip |&gt;  predict(new_peng, type = \"prob\")\n\n\n  \n\n\n\nAhora te pregunto, ¿qué tanto confías en esta predicción?\nEsto fue todo para este (espero) motivante capítulo, donde entramos de lleno al mundo del aprendizaje supervisado. Me hubiera gustado darte un poco menos de teoría, pero aplicar modelos de aprendizaje automatizado es algo que requiere que pongamos muchísima atención a qué es lo que hacemos en cada paso y por qué, pues es muy fácil que resbalemos y que terminemos presumiendo un modelo sobreajustado. Voy a confirmar tus sospechas: también hay que verificar que nuestras regresiones no estén sobreajustadas, pero eso lo dejamos para la siguiente sesión.",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Aprendizaje supervisado: Clasificación</span>"
    ]
  },
  {
    "objectID": "c17_clasif.html#ejercicio",
    "href": "c17_clasif.html#ejercicio",
    "title": "17  Aprendizaje supervisado: Clasificación",
    "section": "17.7 Ejercicio",
    "text": "17.7 Ejercicio\nAplica un clasificador de bosques aleatorios a los datos Medidas.txt que utilizamos en el Capítulo 15. Compara la importancia de variables con las cargas factoriales del ACP y con las variables significativas para la ordenación del NMDS. ¿Hay diferencias?\n\n\n\n\nBreiman L. 2001. Random Forests. Machine Learning 45:5-32. DOI: 10.1023/A:1010933404324.\n\n\nCarvajal G, Maucec M, Cullick S. 2018. Components of artificial intelligence and data analytics. In: Intelligent Digital Oil and Gas Fields. Concepts, Collaboration, and Right-Time Decisions. Cambridge, Massachusetts, USA: Gulf Professional Publishing, 101-148. DOI: 10.1016/B978-0-12-804642-5.00004-9.\n\n\nEnríquez-García AB, Cruz-Escalona V, Carriquiry JD, Ehemann NR, Mejía-Falla PA, Marín-Enríquez E, Treinen-Crespo C, Vélez-Tacuri JR, Navia AF. 2023. Trophic assessment of three sympatric batoid species in the southern Gulf of California. PeerJ 11:e16117. DOI: 10.7717/peerj.16117.\n\n\nHosmer DWJr, Lemeshow S, Sturdivant RX. 2013. Applied Logistic Regression. John Wiley & Sons, Inc. DOI: 10.1002/9781118548387.\n\n\nKaufman S, Rosset S, Perlich C, Stitelman O. 2012. Leakage in Data Mining: Formulation, Detection, and Avoidance. ACM Transactions on Knowledge Discovery from Data 6. DOI: 10.1145/2382577.2382579.\n\n\nMeyer-Baese A, Schmid V. 2014. Chapter 7 - Foundations of neural networks. In: Meyer-Baese A, Schmid V eds. Pattern recognition and signal analysis in medical imaging. Oxford: Academic Press, 197-243. DOI: 10.1016/B978-0-12-409545-8.00007-8.\n\n\nPal R. 2017. Regression trees, Random Forests, Probabilistic trees, Stacked generalization, Probabilistic Random Forests, Weight optimization. In: Pal R ed. Predictive Modeling of Drug Sensitivity. Academic Press, 149-188. DOI: 10.1016/B978-0-12-805274-7.00007-5.\n\n\nStrobl C, Malley J, Tutz G. 2009. An Introduction to Recursive Partitioning: Rationale, Aplication and Characteristics of Classification and Regression Trees, Bagging and Random Forests. Physiological Methods 14:323-348. DOI: 10.1037/a0016973.",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Aprendizaje supervisado: Clasificación</span>"
    ]
  },
  {
    "objectID": "c18_mvregs.html",
    "href": "c18_mvregs.html",
    "title": "18  Regresiones múltiples",
    "section": "",
    "text": "18.1 Librerías\nlibrary(ggplot2)\nlibrary(GGally)\nlibrary(dplyr)\nlibrary(tidymodels)\nlibrary(performance)\nlibrary(vip)\n# library(glmnet)\ntheme_set(see::theme_lucid() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()))",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Regresiones múltiples</span>"
    ]
  },
  {
    "objectID": "c18_mvregs.html#introducción",
    "href": "c18_mvregs.html#introducción",
    "title": "18  Regresiones múltiples",
    "section": "18.2 Introducción",
    "text": "18.2 Introducción\nEn el Capítulo 11 vimos una bastante extensa introducción al modelo lineal, y en el Capítulo 13 vimos que la regresión polinomial era una extensión de este donde añadíamos “nuevas” pendientes que correspondían con potencias de nuestra variable independiente. Bueno, hoy vamos a introducir pendientes para otros predictores, y hablar de lo que eso implica. También veremos cómo podemos controlar la complejidad de nuestros modelos lineales, y ver cómo juzgar/evaluar adecuadamente un modelo de regresión múltiple, pues es más complejo que solo revisar sus supuestos.\n\n18.2.1 Regresión Lineal Múltiple\nLa regresión lineal múltiple (RLM) es, en escencia, lo mismo que la regresión lineal simple: un modelo de aprendizaje supervisado donde se predice una variable numérica siguiendo un modelo lineal, solo que esta vez añadimos una serie de variables independientes. El modelo lineal múltiple queda dado entonces por:\n\\[\nY = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_nX_n + \\epsilon\n\\]\nO podemos ponerlo en notación matricial, donde \\(X\\) es ahora una matriz que contiene \\(m\\) variables independientes y \\(n\\) observaciones (\\(m \\times n\\)), y \\(\\beta\\) es un vector de \\(m\\) coeficientes.\n\\[\nY = \\alpha + X\\cdot\\beta + \\epsilon\n\\]\n\n\n\n\n\n\nNota\n\n\n\nTanto en la RLS como en la RLM las variables independientes pueden ser categóricas, ordinales y (por supuesto) continuas, pero el cómo las integremos al modelo y su interpretación tienen bemoles que veremos más adelante.\n\n\nNo vamos a entrar en los detalles del álgebra lineal correspondientes porque los vimos en el Capítulo 14, lo que sí es “importante” saber es que, mientras en la RLS encontramos una línea que “mapea” los valores de \\(x\\) hacia \\(y\\), en la RLM encontramos un hiper-plano (una sábana) de \\(m\\) dimensiones. Nuestra intución como criaturas tridimensionales solo nos permite ver hasta tres dimensiones, donde tendríamos algo como esto:\n\n\n\n\n\n\nFigura 18.1: La regresión lineal múltiple genera un hiperplano que explica la relación \\(Z\\sim X+Y\\) (\\(Y \\sim X_1 + X_2\\)).\n\n\n\nDicho esto, vayamos a nuestros datos alojados en una dirección web. Estos datos cuentan con las siguientes variables:\n\nLongitud del organismo (L_o)\nDiámetro del organismo (D_o)\nAltura del organimso (A_o)\nPeso entero (P_e)\nPeso sin concha (P_sc)\nPeso de las vísceras (P_v)\nPeso de la cáscara (P_c)\nEdad (años) (Edad)\n\n\n# URL donde se alojan los datos, partida en dos segmentos para acortarla\nabulon_url &lt;- paste0(\"https://storage.googleapis.com/\",\n                     \"download.tensorflow.org/data/abalone_train.csv\")\n\n# Descargar los datos en un objeto\nabulon_dwn &lt;- RCurl::getURL(abulon_url)\n\n# Leer los datos descargados\nabulon_data &lt;- read.csv(text = abulon_dwn,\n                        header = F)\n\n# Nombres de columnas\nvar_names &lt;- c(\"L_o\", \"D_o\", \"A_o\", \"P_e\",\n               \"P_sc\",\"P_v\", \"P_c\", \"Edad\")\ncolnames(abulon_data) &lt;- var_names\n\nhead(abulon_data)\n\n\n  \n\n\n\nEl objetivo es construir un modelo de RLM que prediga la edad de los abulones a partir del resto de medidas. Podemos ver la relación entre cada par de variables. Para hacernos la vida más sencilla vamos a utilizar la función GGally::ggpairs():\n\nGGally::ggpairs(abulon_data,\n                progress = F,\n                upper = \"blank\")\n\n\n\n\n\n\n\n\nSi ponemos atención al último renglon es claro que hay una relación entre Edad y el resto de variables, pero también tenemos dos pequeños “problemas”:\n\nDos individuos que tienen valores de A_o que se alejan considerablemente del resto.\nComo era de esperarse dados los nombres de las variables, algunas están altamente correlacionadas.\n\nPor el momento ignoremos ambas cosas y ajustemos nuestro modelo.\n\n18.2.1.1 Implementación\nLos pasos a seguir son básicamente los mismos que seguimos en el Capítulo 17 con el bosque aleatorio, salvo peculiaridades propias de la regresión lineal:\n\nDividir los datos en datos de entrenamiento y prueba. Si pusiste atención al enlace te darás cuenta de que los datos que descargamos son datos de prueba, por lo que esta vez nos saltaremos este paso.\nPreprocesar los datos. En este caso no es “necesario” que nuestras variables estén en la misma escala, por lo que también vamos a saltarnos este paso; sin embargo, más adelate veremos que es recomendable y que cuando querramos controlar la complejidad del modelo se vuelve necesario. De cualquier manera (y para hacernos el hábito) vamos a especificar la receta, aunque no tenga ningún paso más que la fórmula:\n\n\n# Formación de la receta\nabulon_rec &lt;- recipe(Edad~., data = abulon_data)\n\n# Obtener parámetros para preprocesar\nabulon_prep &lt;- abulon_rec |&gt; prep()\n\n# Preprocesar los datos\nabulon_juiced &lt;- abulon_prep |&gt; juice()\n\n\nCrear el modelo. Ahora podemos especificar el modelo de regresión lineal múltiple\n\n\n# Especificación de la regresión lineal\nlm_spec &lt;- linear_reg() |&gt;\n           set_engine(\"lm\") |&gt;\n           set_mode(\"regression\")\n\n\n\n\n\n\n\nNota\n\n\n\nSi te das cuenta las únicas diferencias entre la especificación del bosque aleatorio que construimos en el Capítulo 17 y esta regresión múltiple es que como “motor” estamos utilizando lm, y que el “modo” del modelo es una regresión. En tidymodels solo hay que cambiar estos dos argumentos para aplicar cualquiera de los modelos que tienen una interface en parsnip, los cuales puedes ver aquí.\n\n\n\nFormar un flujo de trabajo que aplique la receta de preprocesamiento de los datos y se la pase a la especificación del modelo:\n\n\nlm_wf &lt;- workflow() |&gt;\n         add_recipe(abulon_rec) |&gt;\n         add_model(lm_spec)\n\n\nAjustar el modelo. Aquí no tenemos hiperparámetros, por lo que saltaremos este paso, pero más adelante lo vamos a retomar. Por el momento solo ajustemos el modelo con los datos de prueba:\n\n\nlm_fit &lt;- lm_wf |&gt; fit(data = abulon_data)\n\n\nVer el resumen. De nuevo, no tenemos hiperparámetros a optimizar, pero podemos ver la salida de la función lm en el atributo lm_fit$fit$fit$fit (sí, 3 veces fit, no me preguntes por qué lo enterraron hasta allá). Aquí podríamos hablar de qué coeficientes son significativamente diferentes de 0, de si sus relaciones son positivas o negativas, darnos una idea de lo mal ajustado que está el modelo con el \\(R^2\\) y todo lo demás que vimos en el Capítulo 11. Las únicas diferencias en la interpretación es que ahora tenemos más parámetros, que ahora Multiple R-squared tiene sentido y que también vale la pena ver el resultado del ANOVA. La hipótesis de nulidad de ese ANOVA es que todos los parámetros son iguales a 0 (modelo nulo), y la alternativa que al menos uno es diferente. ¿Cuál o cuáles? Eso lo dan las pruebas de \\(t\\) de cada parámetro.\n\n\n# Extraer la salida de la función lm\nlm_form &lt;- extract_fit_engine(lm_fit)\nsummary(lm_form)\n\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.4264  -1.3563  -0.3701   0.9374  12.2263 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.1004     0.2939  10.551  &lt; 2e-16 ***\nL_o          -1.7735     2.0821  -0.852    0.394    \nD_o          13.5051     2.5404   5.316 1.13e-07 ***\nA_o          11.1089     1.6075   6.911 5.77e-12 ***\nP_e           9.9224     0.8287  11.973  &lt; 2e-16 ***\nP_sc        -20.8791     0.9265 -22.536  &lt; 2e-16 ***\nP_v         -10.7353     1.4591  -7.357 2.35e-13 ***\nP_c           8.0065     1.2375   6.470 1.13e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.184 on 3312 degrees of freedom\nMultiple R-squared:  0.5369,    Adjusted R-squared:  0.5359 \nF-statistic: 548.4 on 7 and 3312 DF,  p-value: &lt; 2.2e-16\n\n\n\nComprobar los supuestos. Aquí llevaríamos el ajuste de los hiperparámetros a mejores valores, pero como no tenemos podemos aprovechar para comprobar los supuestos. Vemos que nuestro posterior predictive check no está del todo bien, el grafico de linealidad tampoco, el de homogeneidad de varianzas menos, tenemos una observación altamente influyente, el gráfico de normalidad está bastante mal, y tenemos un gráfico adicional: el gráfico de colinealidad, que tampoco se ve bien. ¿Qué es la colinealidad? El grado de correlación que hay entre nuestros predictores, pero ahorita vemos más de eso.\n\n\nperformance::check_model(lm_form)\n\n\n\n\n\n\n\n\n\nRevisar la importancia de variables: A diferencia de lo que vimos en el Capítulo 17 sobre la importancia de variables en bosques aleatorios, en modelos lineales qué variable es más importante usualmente se define por el valor absoluto del estadístico \\(t\\) (entre más grande sea, más importante es la variable):\n\n\nlm_spec |&gt;\n  set_engine(\"lm\") |&gt; \n  fit(Edad~.,\n      data = juice(abulon_prep)) |&gt; \n  vip(geom = \"col\") +\n  # geom_point(color = \"dodgerblue4\") +\n  labs(title = \"Importancia de variables\",\n       subtitle = \"Modelo: RLM\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nLa librería vip tiene más funciones que solo vip. Te recomiendo que revises su viñeta para darte una idea de qué puedes hacer.\n\n\n\nVerificar que el modelo no esté sobreajustado. Si bien es cierto que nuestro ajuste general no es muy bueno, ¿qué pasa con los datos de prueba? Como no hicimos la separación entrenamiento-prueba no podemos utilizar el mismo método que usamos con el bosque aleatorio, pero sí que podemos obtener nuestras medidas de ajuste manualmente con ayuda de la función predict. Primero, bajemos los datos de prueba, luego predigamos las edades observadas utilizando nuestro modelo, y luego calculemos el RMSE y el \\(R^2\\) con las funciones yardstick::rmse y yardstick::rsq.\n\n\n# Descargar datos de prueba\nabulon_testd &lt;- paste0(\"https://storage.googleapis.com/\",\n                       \"download.tensorflow.org/data/abalone_test.csv\")\ndownload &lt;- RCurl::getURL(abulon_testd)\nabulon_test &lt;- read.csv(text = download, header = F)\ncolnames(abulon_test) &lt;- var_names\n\n# Predecir edades en los datos\nabulon_test[\"pred\"] &lt;- predict(lm_form, abulon_test)\n\n# Calcular rmse y r2 y juntar los resultados\nrbind(\nabulon_test |&gt; yardstick::rmse(truth = Edad,\n                               estimate = pred),\nabulon_test |&gt; yardstick::rsq(truth = Edad,\n                               estimate = pred)\n)\n\n\n  \n\n\n\nEl RMSE es mayor en los datos de prueba que en los de entrenamiento (2.35 vs. 2.18), mientras que el \\(R^2\\) es mayor en los datos de entrenamiento (0.53 vs. 0.48). En cualquiera de los casos no tenemos una diferencia particularmente grande, por lo que no podemos asegurar que el modelo se encuentre sobreajustado (la diferencia es del 5-7% con respecto a los datos de entrenamiento) y podemos decir que el modelo generaliza “bien”. ¿Por qué “bien”? Porque el valor de \\(R^2\\) no es precisamente alentador, pero no hay una diferencia grande entre los datos entre los que fue entrenado y datos que nunca vio; es decir, el modelo está sub-ajustado y utilizarlo para predicciones sería arriesgado.\n\n\n\n\n\n\nNota\n\n\n\nPara variar, ¿qué tanto es tantito? Pues resulta que si hacemos una partición entrenamiento prueba, hacemos validación cruzada, etc., etc., nuestro valor para los datos de prueba depende de cómo se haya hecho esa partición. Si tenemos mala suerte podemos terminar con pésimas métricas de evaluación. Si tenemos buena suerte, pueden ser excelentes. Todo solo por un proceso aleatorio. Es por eso que es importante o realizar validación cruzada o tener datos que sean realmente de prueba. De cualquier manera, una diferencia hacia arriba o hacia abajo del 5-10% puede ser dada por “buena”. ¿Qué tan estricto tiene que ser el criterio? Depende, para variar, de tu problema, tus objetivos, y qué consecuencias tenga para ti el cometer un error.\n\n\n¿Y para interpretación? Pues si no podemos hacer buenas predicciones no tiene caso hacer interpretaciones; sin embargo, veamos por qué, aún si tuviera un buen poder predictivo, no podemos hacer una buena interpretación. Volvamos a ver los gráficos diagnósticos:\n\nperformance::check_model(lm_form)\n\n\n\n\n\n\n\n\nArriba mencionamos todos los problemas que tiene el modelo, evidenciados por todos los gráficos, pero no hemos hablado a detalle de la colinealidad. En la RLM tenemos un supuesto adicional a los de la RLS: No colinealidad de los predictores; es decir, que las variables independientes sean independientes entre sí (no pueden tendría caso que fueran independientes de la variable a predecir, ¿o sí?). La explicación tiene varios bemoles. Bueno, en realidad es uno, pero hay que ver algo de álgebra.\n\n\n18.2.1.2 Supuesto de no colinealidad\nLa multicolinealidad (i.e.,tener dos o más predictores correlacionados) puede ser entendida como darle al modelo información redundante; es decir, que aquellos predictores correlacionados están aportando “la misma” información para predecir la variable de interés. Vuelve al gráfico de pares y verás que hay relaciones aparentes no solo entre Edad y los predictores, sino también entre varios pares de predictores. Esto nos lleva a que cuando el coeficiente de uno de ellos aumente, el otro aumente o disminuye según si la correlación entre ellos es negativa o positiva, pero hagámos el ejercicio algebráico. Nuestro modelo está declarado de la siguiente manera, donde \\(L\\) es la longitud, \\(D\\) el diámetro, \\(P\\) el peso (\\(_E\\): entero, \\(_{SC}\\): sin concha):\n\\[\nEdad = \\alpha + \\beta_1 L_o + \\beta_2 D_o + \\beta_3 P_e + \\beta_4 P_{sc} + \\dots + \\epsilon\n\\]\nAsumamos que dos variables altamente correlacionadas, por ejemplo \\(P_e\\) y \\(P_{sc}\\). Es más, asumamos que no solo son equivalentes, sino matemáticamente idénticas. En ese escenario estas variables podemos resumirlas en una sola que se llame \\(Peso\\), y reescribir el modelo como:\n\\[\nEdad = \\alpha + \\dots + (\\beta_3 + \\beta_4)*Peso + \\dots\n\\]\nResulta entonces que es la suma de \\(\\beta_3\\) y \\(\\beta_4\\) la que afecta a \\(Edad\\), y no sus valores separados. Dicho de otra manera, podemos hacer una de ellas cada vez más pequeña, siempre que tengamos la otra. En la práctica, entonces, no tenemos dos variables y, por lo tanto, tampoco tenemos dos parámetros. Un caso extremo como este, con una correlación prefecta, nos lleva a un modelo indeterminado; es decir, \\(\\beta\\) podría tomar cualquier valor en el intervalo \\((-\\infty, \\infty)\\). Este no es nuestro caso, pero es la razón de ser del “variables independientes”.\n¿Qué implicaciones tiene esto? Depende de cuál sea nuestro interés. Si nos interesa solamente la predicción, ninguna, pues esa correlación entre los parámetros es solamente una consecuencia de nuestros datos y el modelo. La historia cambia si nos interesa explicar el modelo, pues en un modelo de RLM cada parámetro tiene sentido solo en el contexto de los otros. Como te imaginarás, y como vimos arriba, en el mundo real las correlaciones entre los predictores no son extrañas. ¿Qué tan fuerte debe de ser la correlación entre dos o más variables para ser un problema? \\(r = 0.85\\). Nah, mentira, para variar no hay un criterio o número mágico, pero podemos revisar el Factor de Inflación de la Varianza (VIF).\n\n\n\n\n\n\nAdvertencia\n\n\n\nNo importa la correlación “cruda” entre los predictores, sino qué es lo que pasa dentro el modelo. Recuerda, aquí estamos analizando la pertinencia del modelo y, por lo tanto, de los coeficientes que lo conforman. Si cada parámetro/coeficiente tiene sentido solo en el contexto de los otros, no tiene sentido hacer un análisis a priori. Siempre buscamos la colinealidad entre predictores en el contexto del modelo.\n\n\n¿Por qué es esto así? Si el caso extremo de una correlación perfecta entre los predictores es que \\(\\beta\\) pueda estar entre \\((-\\infty, \\infty)\\) podemos inferir que, entre mayor sea la correlación entre los parámetros, mayor es la varianza en su estimación, lo cual se traduce en tamaños de efectos (valores de pendientes, vamos) pequeños y errores estándares grandes. El VIF, entonces, es una medida de qué tanto incrementa la varianza del modelo por tener un parámetro dado; es decir, vamos a tener un VIF por parámetro/coeficiente. Este VIF es un factor, por lo que es un término multiplicativo. Si los predictores son perfectamente independientes entre sí vamos a tener VIFs exactamente iguales a 1, e irán aumentando conforme cada coeficiente esté correlacionado con otro. Aquí sí tenemos una guía de qué tanto es tantito (James et al., 2013): Valores de menos de 5 son correlaciones bajas entre ese predictor y los demás, 5-10 indican correlaciones moderadas, y &gt;10 son correlaciones muy altas e inaceptables. Regresemos a nuestro ejemplo:\n\nrlm_colin &lt;- performance::check_collinearity(lm_form)\nplot(rlm_colin)\n\nVariable `Component` is not in your data frame :/\n\n\n\n\n\n\n\n\n\nVaya, estamos en una situación bastante complicada, pues solamente el coeficiente de la altura carece de una correlación importante con otro(s). Los demás están entre 20 para P_v y hasta más de 100 para P_e, lo cuál nos lleva a la pregunta ¿qué hacemos? Tenemos algunas alternativas:\n\nLa más simple y, por lo tanto, mi primera opción: simplemente eliminar una (o más) variables del análisis. ¿Cuál? No importa, la información es redundante. Puede ser por simple conveniencia, y eliminar aquella(s) de la(s) que sepamos menos, o que sea(n) más difícil(es) de interpretar o medir.\nPosiblemente la que menos me gusta: Crear una nueva variable. Esto puede ser tan simple como obtener el promedio de las variables redundantes (ojo con las escalas), o incluso aplicar una técnica de reducción de dimensionalidad como el ACP. ¿El problema? Que ahora tenemos combinaciones de nuestras variables originales, por lo que la interpretación, que es lo que estábamos buscando, se va por la borda.\nIncluir términos de interacción. Sí, como la interacción que vimos en el ANOVA factorial, pero entraremos en más detalles más adelante.\nAplicar un modelo de regresión regularizado. Estos modelos, que veremos más adelante, permiten penalizar los coeficientes en función de su “importancia” para el modelo, de modo que variables menos informativas van a tener coeficientes más pequeños (regresión Ridge) o incluso hacerse cero y ser eliminadas del modelo (regresión Lasso).\n\nComprobemos lo primero. Por fines de practicidad (y didácticos) utilizando la función lm, y quedémonos únicamente con la altura, el diámetro, y el peso:\n\nlm_nocol &lt;- lm(Edad~ A_o + D_o + P_v, data = abulon_data)\nsummary(lm_nocol)\n\n\nCall:\nlm(formula = Edad ~ A_o + D_o + P_v, data = abulon_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.7115  -1.5972  -0.6611   0.8557  15.4888 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.5117     0.2814   5.372 8.34e-08 ***\nA_o          19.5388     1.8630  10.488  &lt; 2e-16 ***\nD_o          15.8126     1.1295  14.000  &lt; 2e-16 ***\nP_v          -4.2458     0.9490  -4.474 7.93e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.577 on 3316 degrees of freedom\nMultiple R-squared:  0.3544,    Adjusted R-squared:  0.3538 \nF-statistic: 606.7 on 3 and 3316 DF,  p-value: &lt; 2.2e-16\n\n\nDe entrada, los errores estándar de D_o y P_v se hicieron más pequeños, y nuestro RSE (2.57) no fue mucho mayor que en el caso anterior (2.18); es decir, ahora nuestro modelo se equivoca (en promedio) 0.39 años más que antes. Esto, en el gran esquema de las cosas puede no ser “tan” grave, o al menos no tanto como podría hacerlo parecer el \\(R^2\\) múltiple de 0.35. ¿Por qué no confiar ciegamente/únicamente en el \\(R^2\\)? Porque tiene una peculiaridad: sigue creciendo conforme se le agregan variables al modelo, independientemente de si estas son informativas o no.\nSi revisamos nuestros VIF tenemos ahora valores con los que podemos vivir:\n\nrlm_nocolin &lt;- performance::check_collinearity(lm_nocol)\nplot(rlm_nocolin)\n\nVariable `Component` is not in your data frame :/\n\n\n\n\n\n\n\n\n\nAhora sí podemos interpretar adecuadamente nuestro modelo, el problema es la capacidad de predicción. Si esta es “suficientemente buena” o no es algo que depende enormemente de nuestro conocimiento del problema. ¿Es un error de 2.57 años grande? ¿Es aceptable? Eso es algo que tú tienes que decidir en función de las consecuencias. Imagina que hay una regulación por edad sobre este recurso, y que organismos que tengan menos de 5 años no pueden ser extraídos. ¿Pesa ese error promedio de 2.57 años? ¿Qué pasa si el límite se va hasta los 11 años? No hay respuestas universales. Decidir si un modelo es útil o no depende de nosotros, y no solo del valor de \\(R^2\\). Dicho esto, vayamos a la siguiente alternativa: incluir interacciones en el modelo.\n\n\n18.2.1.3 Interacciones\nPero antes, ¿qué diablos representa una interacción? En el Capítulo 10 hablábamos que indicaban que un factor dependía de otro, y aquí es exactamente eso: una interacción representa una relación de dependencia/correlación/moderación entre los predictores. En una RLM sin interacciones un cambio unitario en \\(X_i\\) resulta en un cambio constante (\\(\\beta\\)) en \\(Y\\) al mantener fijos los valores del resto de variables. En casos con multicolinealidad vimos que esto no es el caso, y que puede ser que los cambios en en \\(X_1\\) que afectan a \\(y\\) están siendo modulados por \\(X_2\\). Un ejemplo cotidiano es la ingesta de medicamentos y el consumo de bebidas alcohólicas (no lo hagas). El medicamento puede no tener reaccciones adversas tomado solo, pero tomar bebidas alcoholicas puede hacer que su efecto se reduzca dramáticamente o, incluso, comprometer la salud del paciente.\nHasta el momento en nuestro ejemplo de los abulontes todos nuestros predictores contribuyen de forma aditiva (independiente) a la predicción de la variable dependiente. Si queremos reflejar casos como el ejemplo de los medicamentos y las bebidas alcohólicas es necesario, entonces, incluir términos que no sean aditivos. Usualmente la interacción se incluye con términos multiplicativos, que en un caso con dos variables se vería de la siguiente manera:\n\\[\nY = \\alpha + \\beta_1*x_1 + \\beta_2*x_2 + \\beta_3*(x_1*x_2) + \\epsilon\n\\]\nEs decir, el nuevo coeficiente (\\(\\beta_3\\)) multiplica a una nueva variable que es el producto de dos (o más) variables que interactúan. Aunque incluir estos términos puede ayudar con la capacidad predictiva del modelo, introducen un desafío para su interpretación. Re-escribamos la ecuación de arriba:\n\nPara la pendiente de \\(x_1\\):\n\n\\[\nY = \\alpha + (\\beta_1 + \\beta_3*x2)*x1 + \\beta_2*x_2\n\\]\n\nPara la pendiente de \\(x_2\\):\n\n\\[\nY = \\alpha + \\beta_1*x_1 + (\\beta_2 + \\beta_3*x1)*x_2\n\\]\n\n\n\n\n\n\nNota\n\n\n\n¿De dónde salieron estas ecuaciones? De expandir el término \\(\\beta_3*(x_1*x_2)\\) y luego simplificar para cada variable.\n\n\nEsto nos muestra que: - El término de interacción puede ser entendido como un modelo lineal, por lo que tenemos un modelo lineal dentro de otro. - La interacción es simétrica, de modo que podemos pensar en la pendiente de \\(x_1\\) como una función de \\(x_2\\) y, al mismo tiempo, la pendiente de \\(x_2\\) como una función de \\(x_1\\)\nEstas interacciones tienen otro par de peculiaridades: - En una RLM sin interacciones obtenemos un hiperplano como el que vimos en la Figura 18.1; sin embargo, las interacciones permiten curvar ese espacio, pues las pendientes ya no son constantes, sino funciones de otra variable. - El coeficiente \\(\\beta_1\\) describe la influencia del predictor \\(x_1\\) solo cuando \\(x_2 = 0\\). Esto es porque, en ese caso, \\(\\beta_3*x_2 = 0\\), y el término de \\(x_1\\) se reduce a \\(\\beta_1*x_1\\). Por simetría, ese razonamiento aplica a \\(\\beta_2\\). ¿En Español? No podemos interpretar un efecto de primer orden (sin interacción) si esa variable está interactuando con otra, pues ese efecto en sí mismo solo es verdad cuando la otra variable es 0.\nVeamos qué pasa si incluimos un modelo con todas las interacciones (más adelante veremos cómo incluirlas en tidymodels:\n\nlm_inter &lt;- lm(Edad~L_o*D_o*A_o*P_e*P_sc*P_v*P_c, data = abulon_data)\nsummary(lm_inter)\n\n\nCall:\nlm(formula = Edad ~ L_o * D_o * A_o * P_e * P_sc * P_v * P_c, \n    data = abulon_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.7066  -1.1922  -0.2471   0.8796  11.6235 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                   2.002e+00  4.746e+00   0.422 0.673140    \nL_o                           6.259e+00  5.006e+01   0.125 0.900518    \nD_o                           4.961e+00  7.566e+01   0.066 0.947729    \nA_o                          -3.186e+01  9.720e+01  -0.328 0.743075    \nP_e                          -1.749e+02  1.925e+02  -0.909 0.363549    \nP_sc                          2.291e+02  3.177e+02   0.721 0.470868    \nP_v                           6.777e+02  4.532e+02   1.495 0.134918    \nP_c                          -5.851e+02  3.287e+02  -1.780 0.075180 .  \nL_o:D_o                       3.656e+01  2.132e+02   0.172 0.863823    \nL_o:A_o                       3.183e+01  7.987e+02   0.040 0.968211    \nD_o:A_o                       5.674e+02  1.211e+03   0.469 0.639276    \nL_o:P_e                       2.592e+03  1.167e+03   2.222 0.026336 *  \nD_o:P_e                      -3.299e+03  1.676e+03  -1.969 0.049062 *  \nA_o:P_e                       4.118e+03  2.479e+03   1.661 0.096757 .  \nL_o:P_sc                     -9.840e+02  1.373e+03  -0.717 0.473635    \nD_o:P_sc                      1.054e+03  2.412e+03   0.437 0.662293    \nA_o:P_sc                     -4.279e+03  4.159e+03  -1.029 0.303658    \nP_e:P_sc                      7.105e+02  1.223e+03   0.581 0.561381    \nL_o:P_v                      -5.725e+03  2.343e+03  -2.443 0.014613 *  \nD_o:P_v                       3.355e+03  3.476e+03   0.965 0.334488    \nA_o:P_v                      -1.443e+04  5.663e+03  -2.548 0.010879 *  \nP_e:P_v                       2.118e+03  2.426e+03   0.873 0.382722    \nP_sc:P_v                     -3.144e+03  5.171e+03  -0.608 0.543194    \nL_o:P_c                      -2.114e+03  1.887e+03  -1.120 0.262762    \nD_o:P_c                       8.248e+03  2.488e+03   3.316 0.000925 ***\nA_o:P_c                       5.782e+03  2.368e+03   2.441 0.014684 *  \nP_e:P_c                      -5.773e+03  2.448e+03  -2.358 0.018416 *  \nP_sc:P_c                     -4.737e+02  5.639e+03  -0.084 0.933053    \nP_v:P_c                       1.683e+04  7.571e+03   2.223 0.026257 *  \nL_o:D_o:A_o                  -2.409e+03  2.265e+03  -1.064 0.287583    \nL_o:D_o:P_e                   2.481e+01  1.654e+03   0.015 0.988032    \nL_o:A_o:P_e                  -2.213e+04  8.755e+03  -2.527 0.011545 *  \nD_o:A_o:P_e                   1.819e+04  1.111e+04   1.637 0.101694    \nL_o:D_o:P_sc                 -3.465e+02  2.744e+03  -0.126 0.899513    \nL_o:A_o:P_sc                  1.121e+04  1.238e+04   0.906 0.365124    \nD_o:A_o:P_sc                 -5.928e+03  1.842e+04  -0.322 0.747664    \nL_o:P_e:P_sc                 -3.411e+03  2.320e+03  -1.470 0.141631    \nD_o:P_e:P_sc                  3.812e+03  3.480e+03   1.096 0.273371    \nA_o:P_e:P_sc                 -1.707e+03  8.897e+03  -0.192 0.847848    \nL_o:D_o:P_v                   6.229e+03  4.714e+03   1.321 0.186515    \nL_o:A_o:P_v                   5.777e+04  1.899e+04   3.043 0.002362 ** \nD_o:A_o:P_v                   1.674e+03  2.660e+04   0.063 0.949829    \nL_o:P_e:P_v                  -7.235e+03  6.306e+03  -1.147 0.251330    \nD_o:P_e:P_v                  -8.467e+02  6.236e+03  -0.136 0.891996    \nA_o:P_e:P_v                  -8.950e+03  1.625e+04  -0.551 0.581831    \nL_o:P_sc:P_v                  1.359e+04  1.265e+04   1.074 0.282801    \nD_o:P_sc:P_v                 -8.907e+03  1.474e+04  -0.604 0.545736    \nA_o:P_sc:P_v                  6.483e+03  3.954e+04   0.164 0.869786    \nP_e:P_sc:P_v                 -4.559e+03  3.824e+03  -1.192 0.233322    \nL_o:D_o:P_c                  -7.834e+03  3.172e+03  -2.470 0.013566 *  \nL_o:A_o:P_c                   1.383e+04  1.279e+04   1.081 0.279776    \nD_o:A_o:P_c                  -6.052e+04  1.628e+04  -3.717 0.000205 ***\nL_o:P_e:P_c                   1.186e+04  4.436e+03   2.673 0.007561 ** \nD_o:P_e:P_c                   1.310e+04  6.151e+03   2.130 0.033243 *  \nA_o:P_e:P_c                   1.963e+04  1.428e+04   1.375 0.169352    \nL_o:P_sc:P_c                 -3.123e+03  1.062e+04  -0.294 0.768758    \nD_o:P_sc:P_c                  4.724e+02  1.782e+04   0.027 0.978853    \nA_o:P_sc:P_c                  1.404e+04  3.529e+04   0.398 0.690787    \nP_e:P_sc:P_c                 -9.506e+03  5.105e+03  -1.862 0.062672 .  \nL_o:P_v:P_c                  -1.970e+04  1.775e+04  -1.110 0.267102    \nD_o:P_v:P_c                  -6.260e+04  2.193e+04  -2.854 0.004342 ** \nA_o:P_v:P_c                  -9.779e+04  4.492e+04  -2.177 0.029548 *  \nP_e:P_v:P_c                   8.728e+03  1.055e+04   0.827 0.408347    \nP_sc:P_v:P_c                  5.037e+04  3.579e+04   1.407 0.159463    \nL_o:D_o:A_o:P_e               6.729e+03  1.211e+04   0.556 0.578427    \nL_o:D_o:A_o:P_sc              7.377e+02  2.364e+04   0.031 0.975108    \nL_o:D_o:P_e:P_sc             -5.170e+02  3.764e+03  -0.137 0.890738    \nL_o:A_o:P_e:P_sc              1.599e+04  1.497e+04   1.068 0.285801    \nD_o:A_o:P_e:P_sc             -2.611e+04  2.197e+04  -1.188 0.234740    \nL_o:D_o:A_o:P_v              -8.063e+04  3.351e+04  -2.406 0.016167 *  \nL_o:D_o:P_e:P_v               8.693e+03  7.877e+03   1.104 0.269852    \nL_o:A_o:P_e:P_v               3.979e+04  3.652e+04   1.090 0.275981    \nD_o:A_o:P_e:P_v              -2.009e+04  3.522e+04  -0.570 0.568469    \nL_o:D_o:P_sc:P_v             -6.862e+03  1.847e+04  -0.371 0.710304    \nL_o:A_o:P_sc:P_v             -6.635e+04  8.184e+04  -0.811 0.417548    \nD_o:A_o:P_sc:P_v              7.556e+04  9.739e+04   0.776 0.437867    \nL_o:P_e:P_sc:P_v              8.697e+03  6.391e+03   1.361 0.173682    \nD_o:P_e:P_sc:P_v              7.938e+03  9.316e+03   0.852 0.394202    \nA_o:P_e:P_sc:P_v              2.758e+04  2.407e+04   1.146 0.251952    \nL_o:D_o:A_o:P_c               5.957e+04  1.933e+04   3.082 0.002075 ** \nL_o:D_o:P_e:P_c              -2.399e+04  7.611e+03  -3.151 0.001640 ** \nL_o:A_o:P_e:P_c              -4.321e+04  2.509e+04  -1.722 0.085079 .  \nD_o:A_o:P_e:P_c              -5.657e+04  3.241e+04  -1.745 0.081025 .  \nL_o:D_o:P_sc:P_c              9.188e+03  1.945e+04   0.472 0.636745    \nL_o:A_o:P_sc:P_c             -8.657e+03  6.332e+04  -0.137 0.891261    \nD_o:A_o:P_sc:P_c             -4.986e+03  1.035e+05  -0.048 0.961585    \nL_o:P_e:P_sc:P_c              1.407e+04  6.495e+03   2.166 0.030357 *  \nD_o:P_e:P_sc:P_c              4.604e+03  9.346e+03   0.493 0.622280    \nA_o:P_e:P_sc:P_c              5.181e+04  2.738e+04   1.892 0.058520 .  \nL_o:D_o:P_v:P_c               8.002e+04  2.671e+04   2.996 0.002758 ** \nL_o:A_o:P_v:P_c               6.861e+04  9.871e+04   0.695 0.487053    \nD_o:A_o:P_v:P_c               3.788e+05  1.200e+05   3.158 0.001603 ** \nL_o:P_e:P_v:P_c              -2.347e+04  1.636e+04  -1.434 0.151612    \nD_o:P_e:P_v:P_c               1.123e+03  2.139e+04   0.052 0.958138    \nA_o:P_e:P_v:P_c              -6.974e+03  5.261e+04  -0.133 0.894539    \nL_o:P_sc:P_v:P_c             -5.108e+04  5.640e+04  -0.906 0.365200    \nD_o:P_sc:P_v:P_c             -8.373e+04  8.418e+04  -0.995 0.319948    \nA_o:P_sc:P_v:P_c             -2.833e+05  1.995e+05  -1.421 0.155551    \nP_e:P_sc:P_v:P_c             -5.070e+03  8.550e+03  -0.593 0.553275    \nL_o:D_o:A_o:P_e:P_sc          8.803e+03  2.362e+04   0.373 0.709337    \nL_o:D_o:A_o:P_e:P_v          -2.117e+04  4.495e+04  -0.471 0.637793    \nL_o:D_o:A_o:P_sc:P_v          1.069e+04  1.193e+05   0.090 0.928610    \nL_o:D_o:P_e:P_sc:P_v         -1.512e+04  9.927e+03  -1.524 0.127719    \nL_o:A_o:P_e:P_sc:P_v         -5.207e+04  3.556e+04  -1.464 0.143291    \nD_o:A_o:P_e:P_sc:P_v         -3.552e+04  5.282e+04  -0.672 0.501334    \nL_o:D_o:A_o:P_e:P_c           9.841e+04  4.048e+04   2.431 0.015104 *  \nL_o:D_o:A_o:P_sc:P_c         -3.662e+04  1.119e+05  -0.327 0.743482    \nL_o:D_o:P_e:P_sc:P_c         -9.837e+03  1.029e+04  -0.956 0.339257    \nL_o:A_o:P_e:P_sc:P_c         -7.408e+04  3.453e+04  -2.145 0.032017 *  \nD_o:A_o:P_e:P_sc:P_c         -2.627e+04  4.702e+04  -0.559 0.576452    \nL_o:D_o:A_o:P_v:P_c          -4.137e+05  1.390e+05  -2.976 0.002938 ** \nL_o:D_o:P_e:P_v:P_c           1.777e+04  2.521e+04   0.705 0.480953    \nL_o:A_o:P_e:P_v:P_c           7.169e+04  8.083e+04   0.887 0.375226    \nD_o:A_o:P_e:P_v:P_c          -5.016e+04  1.050e+05  -0.478 0.632954    \nL_o:D_o:P_sc:P_v:P_c          8.043e+04  9.540e+04   0.843 0.399242    \nL_o:A_o:P_sc:P_v:P_c          3.311e+05  3.024e+05   1.095 0.273569    \nD_o:A_o:P_sc:P_v:P_c          4.302e+05  4.464e+05   0.964 0.335307    \nL_o:P_e:P_sc:P_v:P_c          6.111e+03  1.134e+04   0.539 0.590088    \nD_o:P_e:P_sc:P_v:P_c          1.318e+04  1.496e+04   0.881 0.378434    \nA_o:P_e:P_sc:P_v:P_c         -5.125e+03  3.930e+04  -0.130 0.896237    \nL_o:D_o:A_o:P_e:P_sc:P_v      7.409e+04  5.592e+04   1.325 0.185284    \nL_o:D_o:A_o:P_e:P_sc:P_c      5.409e+04  5.251e+04   1.030 0.303035    \nL_o:D_o:A_o:P_e:P_v:P_c      -3.105e+04  1.220e+05  -0.255 0.799030    \nL_o:D_o:A_o:P_sc:P_v:P_c     -4.621e+05  5.015e+05  -0.922 0.356847    \nL_o:D_o:P_e:P_sc:P_v:P_c     -1.408e+04  1.584e+04  -0.889 0.373862    \nL_o:A_o:P_e:P_sc:P_v:P_c      2.837e+03  5.194e+04   0.055 0.956450    \nD_o:A_o:P_e:P_sc:P_v:P_c     -2.743e+04  6.648e+04  -0.413 0.679946    \nL_o:D_o:A_o:P_e:P_sc:P_v:P_c  2.591e+04  7.167e+04   0.362 0.717700    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.013 on 3192 degrees of freedom\nMultiple R-squared:  0.6209,    Adjusted R-squared:  0.6059 \nF-statistic: 41.17 on 127 and 3192 DF,  p-value: &lt; 2.2e-16\n\n\n¡Tenemos un ch…orro de parámetros! No solo eso, sino que coeficientes que antes eran significativos ya no lo son. Pensemos por un momento en lo que hicimos. Si estos términos de interacción corresponden con los productos de las variables involucradas son, entonces, altamente colineales con las variables originales. ¿Importa esto para la predicción? Para nada. Si pones atención, tanto el \\(R^2\\) como el RSE son mejores que en el modelo de efectos de primer orden (principales).\n\n\n\n\n\n\nTip\n\n\n\nTe recomiendo incluir en tu modelo solo interacciones relevantes y no interpretar los efectos de primer orden de las que sean significativas. Si la interacción es significativamente diferente de 0 no podemos interpretar los coeficientes “individuales”. Las razones teóricas de esto las vimos arriba.\n\n\nEsto último me lleva a hablar de algo sumamente importante para cualquier modelo de aprendizaje automatizado: la selección de variables:\n\n\n\n\n\n\nImportante\n\n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nNo porque “podamos” incluir tantas variables querramos (y sus interacciones) quiere decir que debamos hacerlo. Es importante que en nuestros modelos incluyamos variables e interacciones que tengan una justificación teórica y evitar ir a “pescar” efectos significativos; es decir, “a ver qué sale significativo”.\n\n\n\n\nEn nuestro ejemplo tenemos P_e, P_sc, P_v, P_c, que todas indican lo mismo: la masa del individuo, solo que vista de distintas materas. Con la longitud y el diámetro tenemos una historia similar, y tenemos además la correlación inherente entre todas por que representan la talla de un organismo: un abulón más largo seguramente tendrá valores más altos en casi todas las demás variables.\n\n\n\n\n\n\nImportante\n\n\n\nMenos usualmente es más en modelos multivariados (como en muchas otras cosas); es decir, buscamos modelos parsimoniosos. OJO: cuando hablamos de modelos parsimoniosos no hablamos necesariamente de los modelos matemáticamente más simples, sino de aquellos en los que no incluimos información “basura”.\n\n\nEn el Capítulo 19 vamos a hablar más a profundidad de cómo encontrar el modelo más parsimonioso de entre un conjunto de posibilidades, pero antes de eso veamos cómo podemos penalizar nuestros coeficientes según qué tan importantes sean.\n\n\n\n18.2.2 Regresiones penalizadas\nArriba vimos un modelo lineal que tiene todo lo que no debería de tener, especialmente en el departamento de variables correlacionadas y no informativas. Imaginemos que nuestra teoría justifica que las incluyamos todas, ¿cómo podemos asignarle menos peso a las que no sean tan relevantes? Penalizando nuestros modelos. ¿Cómo hacemos esto? Ajustamos nuestro modelo y luego penalizamos los coeficientes que sean pequeños utilizando un término que puedes encontrar como \\(\\alpha\\) o \\(\\lambda\\), cuya función es minimizar (para variar):\n\nLa suma de cuadrados de nuestras pendientes. Esto hace que los coeficientes se vuelvan más cercanos hacia 0 y hacia los demás. A este tipo de regresión se le conoce como Regresión Ridge, y a su penalización como penalización L2 (L de loss; es decir, pérdida y el 2 hace referencia al cuadrado de la suma). Esta regresión podemos aplicarla cuando tengamos muchos efectos pequeños o medianos, o cuando debamos de forzar la inclusión de todos los términos. Matemáticamente:\n\n\\[\nRSS_{Ridge}(\\beta_j, \\beta_0) = \\sum_{i=1}^n(y_i-(\\beta_j*x_i+\\beta_0))^2+\\alpha*\\sum_{j=1}^p\\beta_j^2    \n\\]\n\nLa suma de absolutos de las pendientes, dando lugar a una penalización L1 y una regresión Lasso. El resultado es que los coeficientes pequeños se hagan 0 y, por lo tanto, sean retirados del modelo. Esta la podemos aplicar cuando tengamos solo unos pocos efectos medianos/grandes, o cuando querramos hacer una selección rigurosa de las variables. Matemáticamente:\n\n\\[\nRSS_{Lasso}(\\beta_j, \\beta_0) = \\sum_{i=1}^n(y_i-(\\beta_j*x_i+\\beta_0))^2+\\alpha*\\sum_{j=1}^p|\\beta_j|\n\\]\n\n\n\n\n\n\nAdvertencia\n\n\n\nEs extremadamente importante que, si vamos a aplicar alguno de estos modelos, escalemos los datos adecuadamente. De no hacerlo, la diferencia en escalas hará que cambien las escalas de los coeficientes y que la regularización no se aplique homogéneamente.\n\n\nEvidentemente, aquí añadimos un híper-parámetro a nuestro modelo de RLM, el cual hay que optimizar. En R esto está implementado en la librería glmnet, (aunque podemos seguir utilizando tidymodels) pero hablemos de cómo podemos escalar nuestras variables y qué efectos tienen esas transformaciones sobre la interpretación de nuestros coeficientes.\n\n18.2.2.1 Re-visitando las transformaciones\nEsta necesidad de escalar los datos me lleva a re-visitar las transformaciones y cómo impactan nuestra interpretación. Me voy a limitar a las dos más comunes en escenarios de RLM: el centrado y la estandarización.\nYa sabemos que centrar una variable consiste en quitarle a cada valor \\(x_i\\) la media de la variable \\(\\overline{x}\\): \\(x' = x- \\overline{x}\\). Como resultado \\(x'\\) estará centrada en 0. Esto tiene dos ventajas, una con nuestras variables directamente y la otra con nuestro intercepto y las pendientes. Si nuestras variables están centradas en 0, entonces todas están en la misma escala, y podemos aplicar modelos penalizados sin preocuparnos por nada. La otra es que en todo modelo lineal hay una correlación implícita entre el intercepto y la pendiente. ¿Por qué? Porque no importa qué línea ajustemos a nuestros datos, todos deben de pasar por un punto: la media de \\(x\\) y la media de \\(y\\); es decir, nuestro ajuste se reduce a girar un lápiz (una línea) alrededor de ese punto, cual rehilete. Si incrementamos la pendiente, debemos forzosamente reducir el intercepto. Si centramos \\(x\\) entonces el intercepto siempre será el valor de \\(y_i\\) que corresponde a la media de \\(x\\), que el modelo ve como 0; es decir, ahora nuestra pendiente puede cambiar libremente sin que se modifique el intercepto. ¿Y si necesitamos reportar los resultados en la escala original? Podemos hacer la siguiente corrección:\n\\[\n\\alpha = \\alpha' - \\beta'x\n\\]\nLa cual se deriva del siguiente razonamiento algebráico:\n\\[\\begin{align*}\ny = \\alpha' + \\beta' x' + \\epsilon \\\\\ny = \\alpha' + \\beta' (x - \\overline{x}) + \\epsilon \\\\\ny = \\alpha' - \\beta' \\overline{x} + \\beta'x + \\epsilon\n\\end{align*}\\]\nEsto también implica que \\(\\beta = \\beta'\\), lo cual tiene todo el sentido del mundo, solo estamos desplazando la distribución de \\(x\\) a tener media 0, no estamos modificando su escala de ninguna manera.\nEn la estandarización la historia es un poco diferente, pues aquí sí que cambiamos la escala tanto de \\(x\\) como de \\(y\\), que ahora tendrán valores \\(Z\\) que representan a cuántas desviaciones estándar está cada valor de la media. La ventaja es que el intercepto siempre está alrededor de 0, y las pendientes en el intervalo \\([-1, 1]\\). La interpretación de la pendiente aquí sí cambia. Si tienes una pendiente de 0.9 en escala \\(Z\\), autamáticamente sé que me voy a mover 0.9 desviaciones estándares en \\(y\\), independientemente de su media o de su desviación estándar. Como ves, la interpretación no es tan simple como en el caso anterior, pero la reconversión tampoco es tan engorrosa: solamente hay que multiplicar la pendiente por la desviación estándar de \\(y\\). El intercepto es otra historia.\nAhora sí, podemos ir a aplicar los modelos de regresión penalizados.\n\n\n18.2.2.2 Regresión Ridge\nEmpecemos con la regresión Ridge (penalización L2). A diferencia del modelo sin penalizar, esta vez sí necesitamos cerciorarnos de que nuestros predictores estén en la misma escala, por lo que hay que añadir un paso de centrado a nuestra receta:\n\nDividir datos en entrenamiento-prueba. No es necesario porque ya tenemos nuestros datos de prueba, pero sí es necesario que establezcamos las divisiones de validación cruzada:\n\n\nabulon_cv &lt;- vfold_cv(abulon_data)\n\n\nPreprocesar los datos\n\n\n# Receta para centrar los predictores\nabulon_rec &lt;- recipe(Edad ~., abulon_data) |&gt;\n              step_center(all_predictors())\n\n# Obtener parámetros para preprocesar\nabulon_prep &lt;- abulon_rec |&gt; prep() \n\n# Preprocesar los datos\nabulon_juiced &lt;- abulon_prep |&gt; juice()\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nEstamos utilizando la función all_predictors() dentro de step_center() para centrar todos los predictores. Si tenemos variables que están codificadas, por ejemplo el sexo como 0 y 1, es importante que las excluyamos pasando los nombres a step_center, por ejemplo: step_center(all_predictors(), -Sexo). Con esto evitamos que nuestra codificación se mueva y que obtengamos resultados erróneos. Más adelante hablaremos de cómo incluir estas variables categóricas en el modelo y cómo interpretar los resultados.\n\n\n\nEspecificar el modelo. Seguimos tratando con una regresión lineal, por lo que también vamos a utilizar linear_reg(); sin embargo, aquí tenemos dos hiper-parámetros: penalty, que es el término de penalización y mixture que define la penalización a aplicar. En nuestro caso estamos interesados en una regresión Ridge, por lo que mixture = 0. Otra diferencia es que el “motor” de la regresión va a cambiar a \"glmnet\":\n\n\nridge_spec &lt;- linear_reg(penalty = tune(),\n                         mixture = 0) |&gt;\n              set_engine(\"glmnet\")\n\n\nFormar el flujo de trabajo. Nuevamente, formamos un flujo donde la receta de preprocesamiento se le pasa al modelo que especificamos:\n\n\ntune_wf &lt;- workflow() |&gt;\n           add_recipe(abulon_rec) |&gt;\n           add_model(ridge_spec)\n\n\nAjustar los hiper-parámetros: Tal y como en el Capítulo 17, establecemos el procesamiento en paralelo y entrenamos un primer modelo con 20 puntos aleatorios para “guiar” nuestra optimización:\n\n\nset.seed(123)\n# Asignar núcleos para la computación en paralelo\ndoParallel::registerDoParallel(cores = parallel::detectCores(logical = F))\n\n# Buscar los mejores hiperparámetros\ntune_ridge &lt;- tune_grid(tune_wf,\n                        resamples = abulon_cv,\n                        grid = 50)\n\n\nEvaluar el primer ajuste. Por practicidad hagamos una función que reciba nuestro objeto tune_res y el orden de la validación cruzada, y que nos devuelva un gráfico donde veamos la relación del RMSE y el \\(R^2\\) con la penalizacón. Al ver el gráfico de abajo podemos concluir que valores de penalización muy pequeños son los que a) minimizan el RMSE y b) maximizan el \\(R^2\\), y que tampoco hay mucha diferencia con el ajuste sin penalización. Esto quiere decir que este problema no se beneficia mucho de una regresión ridge; de hecho, entre más queremos penalizar la pendiente “peor” es el ajuste del modelo:\n\n\nplot_optim_res &lt;- function(tune_res, cv_order){\n  tune_res |&gt; collect_metrics() |&gt;\n              select(mean, std_err, penalty, .metric) |&gt; \n              ggplot(aes(penalty, mean, color = .metric)) +\n              geom_line(linewidth = 1.5) +\n              geom_errorbar(aes(ymin = mean - std_err,\n                                ymax = mean + std_err),\n                            alpha = 0.5) +\n              scale_x_log10() +\n              facet_wrap(~.metric,\n                         scales = \"free_y\",\n                         nrow = 2) +\n              labs(x = \"Penalización\",\n                   y = element_blank(),\n                   title = \"Medias y errores estándar de validación cruzada\",\n                   subtitle = paste0(\"Orden: \", cv_order)) +\n              theme_bw() +\n              theme(legend.position = \"none\")\n}\n\nplot_optim_res(tune_ridge, cv_order = 10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNota\n\n\n\nSi pones atención a la función plot_optim_res notarás que estamos añadiendo la capa scale_x_log10(). Esto es porque el valor de penalización que arroja tune_grid debe transformarse a escala logarítmica por el funcionamiento interno de tune_grid y cómo arroja los resultados.\n\n\n\nFinalizar el ajuste: No hubo mucho efecto de la penalización, por lo que podemos directamente seleccionar el mejor modelo:\n\n\nbest_rmse &lt;- select_best(tune_ridge, \"rmse\")\nfinal_ridge &lt;- finalize_workflow(tune_wf,\n                                 best_rmse)\n\n\nRevisar la importancia de variables. Ya sabemos que este modelo lineal no es precisamente bueno, pero igual revisemos la importancia de variables. Vu\n\n\nfinal_fit &lt;- final_ridge |&gt;\n             fit(abulon_data)\n\nfinal_res &lt;- final_fit |&gt; extract_fit_parsnip()\n\nfinal_res |&gt; vip(geom = \"col\") +\n             theme_bw() +\n             labs(title = \"Importancia de variables\",\n                  y = element_blank())\n\n\n\n\n\n\n\n\n\nRevisar la capacidad de generalización: Al igual que en la RLM no penalizada/regularizada, obtengamos las medidas de bondad de ajuste con los datos de prueba. En este caso no fueron muy diferentes de la RLM no penalizada, lo cual tiene todo el sentido del mundo: no tenemos una regularización muy fuerte.\n\n\n# Predecir edades en los datos\nabulon_test[\"pred_ridge\"] &lt;- predict(final_res, abulon_test)\n\n# Calcular rmse y r2 y juntar los resultados\nrbind(\nabulon_test |&gt; yardstick::rmse(truth = Edad,\n                               estimate = pred),\nabulon_test |&gt; yardstick::rsq(truth = Edad,\n                               estimate = pred)\n)\n\n\n  \n\n\n\n\nPor curiosidad, ¿cómo quedaron nuestros coeficientes?\n\n\nfinal_res |&gt; tidy()\n\n\n  \n\n\n\n\n\n18.2.2.3 Regresión Lasso\nVeamos ahora qué pasa con la regresión Lasso. Los pasos son los mismos, por lo que resumiré el código en bloques que lleguen hasta salidas de interés. Vamos a trabajar con los mismos datos, entonces podemos reutilizar tanto nuestras divisiones de validación cruzada como la receta para preprocesar los datos. La especificación del modelo es sumamente sencilla, solamente hay que cambiar el argumento mixture = 1 en linear_reg:\n\nset.seed(123)\n# Especificación del modelo Lasso\n# Optimizar el término de penalización\nlasso_spec &lt;- linear_reg(penalty = tune(),\n                         mixture = 1) |&gt; \n              set_engine(\"glmnet\")\n\n# Flujo de trabajo para el modelo Lasso\ntune_wf &lt;- workflow() |&gt;\n           add_recipe(abulon_rec) |&gt;\n           add_model(lasso_spec)\n\n# Ajuste de hiperparámetros\ndoParallel::registerDoParallel(cores = parallel::detectCores(logical = F))\ntune_lasso &lt;- tune_grid(tune_wf,\n                        resamples = abulon_cv,\n                        grid = 50)\n\n# Resultados de validación cruzada\nplot_optim_res(tune_lasso, cv_order = 10)\n\n\n\n\n\n\n\n\nCuriosamente los resultados de la validación cruzada son sumamente similares a los de la regresión Ridge, en el sentido de que las penalizaciones que maximizan la bondad de ajuste son bastante pequeñas. ¿Qué sucede? Que este problema es demasiado complejo para un modelo lineal. Si vuelves a los gráficos diagnósticos de la regresión, especialmente al de residuales/linealidad, es más que obvio que el supuesto que tiene el mismo nombre que el modelo no se cumple, entonces buscar mejorar ese modelo solo quitando o metiendo variables es un sinsentido. Hagamos algo más productivo y utilicemos el modelo con todas las interacciones. Aquí sí que hay que regresar a modificar la receta original o, mejor dicho, añadirle un paso adicional con step_interact(terms = ~RHS), donde RHS es el lado derecho de la fórmula que incluye los términos de interaccón:\n\n\n\n\n\n\nAdvertencia\n\n\n\nNota el uso del operador ~ en el argumento terms.\n\n\n\n# Receta para centrar los predictores\nabulon_inter_rec &lt;- abulon_rec |&gt;\n                    step_interact(terms = ~L_o*D_o*A_o*P_e*P_sc*P_v*P_c)\n\n# Obtener parámetros para preprocesar\nabulon_prep &lt;- abulon_inter_rec |&gt; prep() \n\n# Preprocesar los datos\nabulon_juiced &lt;- abulon_prep |&gt; juice()\n\nAhora sí, ajustemos el flujo de trabajo para incluir esta receta, y optimicemos el modelo Lasso:\n\nset.seed(123)\n\n# Flujo de trabajo para el modelo Lasso\ntune_wf &lt;- workflow() |&gt;\n           add_recipe(abulon_inter_rec) |&gt;\n           add_model(lasso_spec)\n\n# Ajuste de hiperparámetros\ndoParallel::registerDoParallel(cores = parallel::detectCores(logical = F))\ntune_lasso &lt;- tune_grid(tune_wf,\n                        resamples = abulon_cv,\n                        grid = 50)\n\n# Resultados de validación cruzada\nplot_optim_res(tune_lasso, cv_order = 10)\n\n\n\n\n\n\n\n\nComo ya habíamos mencionado, un modelo lineal no es suficiente para describir adecuadamente estos datos, incluso en un modelo Lasso con interacciones. ¿Cómo sabemos que funcionó como debía? Podemos buscar los coeficientes que son EXACTAMENTE 0. Por practicidad solo veamos el número de casos en los que se cumple esta igualdad, que fue de 62 de 128 coeficientes del modelo con todas las interacciones sin penalización.\n\nbest_rmse &lt;- select_best(tune_lasso, \"rmse\")\nfinal_lasso &lt;- finalize_workflow(tune_wf,\n                                 best_rmse)\n\nfinal_fit &lt;- final_lasso |&gt;\n             fit(abulon_data)\n\nfit_res &lt;- final_fit |&gt; extract_fit_parsnip()\n\nfit_res |&gt; tidy() |&gt; filter(estimate == 0) |&gt; select(term)\n\n\n  \n\n\n\n\n\n18.2.2.4 Consideraciones\n\nEsta propiedad de que algunos términos se hagan exactamente 0 y, por lo tanto, sean eliminados del modelo es lo que hace que la regresión Lasso sea utilizada como un método de selección de variables, mientras que la regresión Ridge es solo una forma de controlar el grado de influencia de todas las variables en el modelo. En este sentido, la regresión Lasso brilla en problemas altamente dimensionales como nuestro modelo con interacciones, mientras que la regresión Ridge tiene más sentido cuando tenemos problemas con menos variables.\nHabrás notado que en estas regresiones penalizadas no hemos hablado de intervalos de confianza o de valores de p. Esto no es porque me de flojera hablar de ellos, no. En realidad, aunque podemos calcular los errores estándares para los coeficientes de manera muy sencilla con Bootstrap, por ejemplo, estos no son muy informativos para estimaciones altamente sesgadas como las que resultan de la implementación de estos modelos penalizados. ¿Por qué? Porque da la casualdidad de que la estimación penalizada reduce la varianza de los estimadores introduciendo un sesgo sustancial, de modo que este se vuelve una parte muy importante de su error cuadrático medio, mientras que la varianza puede contribuir solo en una pequeña parte. ¿La qué cosa de quién? En pocas palabras, el error estándar de cada coeficiente ya no representa solo qué tan seguros estamos de la estimación, sino también incluye el cómo fue penalizado; por lo tanto, no podemos confiar ciegamente en lo que nos dicen.\n¿Esto quiere decir que no debo de aplicar estos modelos de regresión? En absoluto, simplemente sé consciente de que no puedes reportar valores de p, que a algunos revisores eso puede no agradarles, y que entonces la recomendación es utilizar una regresión Ridge solo cuando estemos interesados en temas de predicción, y la regresión Lasso solo como un paso previo para seleccionar qué variables eliminar del análisis cuando tengamos demasiadas variables jugando al mismo tiempo.\n\n\n\n\n18.2.3 Predictores categóricos\nLo sé, lo sé, ya fue mucho en esta sesión, pero esto es muy importante. Usualmente nos enseñan que en una regresión lineal múltiple utilizamos predictores numéricos o, cuando menos, ordinales. Y esto es verdad, pero no quiere decir que no podamos hacerle manita de puerco al modelo lineal para incluir predictores categóricos o, tal vez mejor dicho, engañarlo. El proceso es sumamente sencillo, lo único que tenemos que hacer es codificar nuestras variables categóricas en 0 y 1. Nuestros datos de abulones no tienen esta información, así que retomemos los datos de pingüinos de Palmer:\n\nstr(penguins)\n\ntibble [344 × 7] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n\n\nAquí tenemos dos variables que son categóricas con tres niveles (species, island), y una binaria (sex).\n\n18.2.3.1 Predictores binarios\nImaginemos que queremos construir un modelo de regresión lineal simple, donde queremos predecir la masa corporal a partir del sexo. Por el momento olvida la confusión que puede estarte dando esa idea, y mejor escribamos el modelo:\n\\[\nmasa = \\alpha + \\beta*Sexo + \\epsilon\n\\]\nPero el sexo es “hembra” o “macho”. ¿Podemos multiplicar palabras por números? No, pero podemos vernos inteligentes y asignar ARBITRARIAMENTE valores a cada nivel; es decir, no importa a quién le toque qué numero, siempre y cuando sepamos la correspondencia:\n\nmacho: 1\nhembra: 0\n\n¿Qué ganamos con esto? Hagamos rápidamente el ejercicio de regresión y veamos los resultados:\n\nlm_m_sex &lt;- lm(body_mass_g~sex, data = penguins)\nsummary(lm_m_sex)\n\n\nCall:\nlm(formula = body_mass_g ~ sex, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1295.7  -595.7  -237.3   737.7  1754.3 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3862.27      56.83  67.963  &lt; 2e-16 ***\nsexmale       683.41      80.01   8.542  4.9e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 730 on 331 degrees of freedom\n  (11 observations deleted due to missingness)\nMultiple R-squared:  0.1806,    Adjusted R-squared:  0.1781 \nF-statistic: 72.96 on 1 and 331 DF,  p-value: 4.897e-16\n\n\nTenemos nuestro intercepto, que sabemos que es la ordenada al origen, y tenemos también una pendiente, que ahora no es solo sex, sino sexmale. ¿Te das alguna idea de qué representan en términos de nuestros datos? Regresa a cómo es que codificamos las variables, y piensa en por qué la pendiente está como sexmale. Tal vez un gráfico de dispersión te de una pista:\n\nggplot(data = na.omit(penguins), aes(x = sex, y = body_mass_g)) +\n  geom_point(color = \"dodgerblue4\", alpha = 0.5)\n\n\n\n\n\n\n\n\nSi imaginas una línea que conecte el centro de ambas columnas de puntos salta que la pendiente representa la diferencia en la variable dependiente entre ambas categorías. ¿Recuerdas que dije que era arbitrario el cómo asignamos la codificación? En este caso R la realiza de forma alfabética: female va antes que male, por lo tanto son 0 y 1. ¿Y el intercepto? Este no es tan evidente, pero representa la media del grupo 0. ¿Por qué representan eso? Recuerda las definiciones que dimos en el Capítulo 11: - La pendiente es la tasa de cambio de \\(x\\) a \\(y\\); es decir, cuántas unidades nos movemos en el eje \\(y\\) por cambio unitario en \\(x\\). En nuestros pingüinos nos movemos, en promedio, 683 g al “pasar” de hembras a machos. - El intercepto es el punto donde la recta corta al eje \\(y\\); es decir, el punto donde \\(x\\) = 0, por lo que si uno de nuestros grupos es 0, el intercepto representa su promedio. En nuestros pingüinos, entonces, la masa promedio de las hembras es de 3862 g.\n¿No me crees? Comprobemos ambas cosas. Primero calculemos la masa promedio de las hembras:\n\nfemale_mass &lt;- mean(penguins$body_mass_g[penguins$sex == \"female\"],\n                    na.rm = T)\nfemale_mass\n\n[1] 3862.273\n\n\nAhora la masa promedio de los machos, y obtengamos la diferencia. La pendiente es positiva, por lo que a la masa de los machos le restaremos la masa de las hembras:\n\nmale_mass &lt;- mean(penguins$body_mass_g[penguins$sex == \"male\"],\n                  na.rm = T)\nmale_mass - female_mass\n\n[1] 683.4118\n\n\n¡MAGIA NEGRA! En absoluto, solo es una consecuencia de cómo introdujimos la variable sex al modelo.\nAlgo importante a notar es que la pendiente es independiente de nuestros grupos, en el sentido de que no importa si les asignamos 0 y 1, 15 y 16, o 100 y 101, siempre que la diferencia entre los valores codificados sea de una unidad. El intercepto, por el contrario, sí que depende del valor de nuestros grupos, pues la recta debe de estirarse para cortar al eje \\(y\\). Es por esto el hincapié en codificar con 0 y 1. Este proceso se denomina en el área de ingenieria de variables como codificación de variables, particularmente como crear dummy variables. Ahora que ya tienes la prueba empírica déjame ser feliz con una demostración matemática muy sencilla, sustituyendo los valores de nuestros grupos en nuestro modelo lineal:\n\\[\\begin{align*}\nmasa = \\alpha + \\beta * sexo + \\epsilon \\\\\n\\therefore \\\\\nmasa = \\left\\{\n  \\begin{array}{lr}\n    \\text{Si } sexo = 0: \\alpha + \\beta*0 + \\epsilon \\Rightarrow Y_0 = \\alpha + \\epsilon \\\\\n    \\text{Si } sexo = 1: \\alpha + \\beta*1 + \\epsilon \\Rightarrow Y_1 = \\alpha + \\beta + \\epsilon\n  \\end{array}\n\\right\\}\n\\end{align*}\\]\nLuego restamos ambas ecuaciones:\n\\[\\begin{align*}\n\\Delta Y = Y_1 - Y_0 = \\alpha - \\alpha + \\beta + \\epsilon - \\epsilon \\\\\n\\therefore \\\\\n\\Delta Y = \\beta\n\\end{align*}\\]\nAhora realiza una prueba \\(t\\) para comparar las medias de masas de machos y hembras, ¿son consistentes los resultados? Bueno, ahora tienes tu respuesta de por qué te decía que la prueba \\(t\\), ANOVA y demás son solo aplicaciones del modelo lineal.\n¿Qué pasa con variables con más de una categoría? Aaaaaah, no desesperres, Pérrez (¿entiendes la referencia?), para allá vamos.\n\n\n18.2.3.2 Predictores con más de dos categorías\nSiguiendo la lógica anterior pensaríamos en un modelo como el siguiente:\n\\[\nmasa = \\alpha + \\beta*Especie + \\epsilon\n\\]\nLa variable “Especie” es categórica nominal con tres niveles. Nuevamente, siguiendo la lógica anterior podemos codificarla de la siguiente manera, ¿no?\n\nAdelie\nChinstrap\nGentoo\n\n¡NO! Si hacemos eso estamos indicando que la variable Especie es categórica ordinal, de modo que Adelie &lt; Chinstrap &lt; Gentoo. Aunque en el caso binario en cierta forma hicimos lo mismo, no importa, pues ya vimos que solamente estamos describiendo la diferencia entre ambos niveles y en realidad se mantiene que ambas categorías tienen la misma importancia (por ello es una variable nominal). ¿Cómo lo hacemos entonces cuando tenemos más de dos predictores? Al igual que en el caso anterior ajustemos el modelo lineal:\n\nlm_m_sp &lt;- lm(body_mass_g~species, data = penguins)\nsummary(lm_m_sp)\n\n\nCall:\nlm(formula = body_mass_g ~ species, data = penguins)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1126.02  -333.09   -33.09   316.91  1223.98 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       3700.66      37.62   98.37   &lt;2e-16 ***\nspeciesChinstrap    32.43      67.51    0.48    0.631    \nspeciesGentoo     1375.35      56.15   24.50   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 462.3 on 339 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.6697,    Adjusted R-squared:  0.6677 \nF-statistic: 343.6 on 2 and 339 DF,  p-value: &lt; 2.2e-16\n\n\nComo era de esperarse, ahora no tenemos solo una pendiente, sino dos. Espera, ¿dos? ¿Qué no son tres especies? Pues resulta que sí formamos tres nuevas variables, una para cada especie, tal que:\n\nspeciesAdelie: 1 si es Adelie, 0 en caso contrario.\nspeciesChinstrap: 1 si es Chinstrap, 0 en caso contrario.\nspeciesGentoo: 1 si es Gentoo, 0 en caso contrario.\n\nAl igual que en el caso anterior, una de ellas sirve de “referencia”. Como en el caso anterior, R toma como referencia la primera en orden alfabético. Es decir, el intercepto es el promedio de Adelie:\n\nmean_Adelie &lt;- mean(penguins$body_mass_g[penguins$species == \"Adelie\"],\n                    na.rm = T)\nmean_Adelie\n\n[1] 3700.662\n\n\nY entonces sí, siguiendo la lógica del caso binario, las pendientes son la diferencia entre el nivel de diferencia y el nivel de la pendiente correspondiente. Para Chinstrap:\n\nmean_Chinstrap &lt;- mean(penguins$body_mass_g[penguins$species == \"Chinstrap\"],\n                    na.rm = T)\nmean_Chinstrap - mean_Adelie\n\n[1] 32.42598\n\n\nPara Gentoo:\n\nmean_Gentoo &lt;- mean(penguins$body_mass_g[penguins$species == \"Gentoo\"],\n                    na.rm = T)\nmean_Gentoo - mean_Adelie\n\n[1] 1375.354\n\n\n\n\n\n\n\n\nTip\n\n\n\nEn tidymodels puedes especificar el uso de dummy variables añadiendo el paso step_dummy(var1, var2, ...) a tu receta, donde las var son las variables categóricas.",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Regresiones múltiples</span>"
    ]
  },
  {
    "objectID": "c18_mvregs.html#corolario",
    "href": "c18_mvregs.html#corolario",
    "title": "18  Regresiones múltiples",
    "section": "18.3 Corolario",
    "text": "18.3 Corolario\nEn esta sesión vimos un montón de detalles y formas de aplicar un modelo lineal. Comenzamos viendo cómo introducir múltiples predictores, el problema de la colinealidad, cómo introducir interacciones (dependencias parciales) entre los predictores, cómo penalizar estos modelos de regresión múltiple, y terminamos con el cómo podemos introducir variables categóricas a nuestros modelos lineales mediante el uso de dummy variables, y qué representan los coeficientes resultantes. Afortunadamente (o tal vez desafortunadamente), las aplicaciones del modelo lineal no terminan aquí. En el Capítulo 19 vamos a hablar de cómo utilizarlo para predecir otro tipo de variables, no solo numéricas.",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Regresiones múltiples</span>"
    ]
  },
  {
    "objectID": "c18_mvregs.html#ejercicio",
    "href": "c18_mvregs.html#ejercicio",
    "title": "18  Regresiones múltiples",
    "section": "18.4 Ejercicio",
    "text": "18.4 Ejercicio\nMe encantaría no dejarte un ejercicio porque este tema fue maratónico, pero justo por eso es necesario. Para compensarte te voy a dar dos alternativas:\n\nUtilizando los datos de pingüinos de Palmer aplica, utilizando tidymodels, un modelo de regresión lineal múltiple que prediga la masa corporal a partir del resto de variables. Tu objetivo es encontrar el modelo que minimice el RMSE y que maximice el \\(R^2\\). ¿Cuál fue el modelo final? ¿Es un modelo penalizado? ¿Es un modelo con interacciones? Elabora el reporte correspondiente con todo tu procedimiento.\nUtilizando los datos de abulones realiza una regresión por bosques aleatorios. ¿Mejora el RMSE? (OJO: bosques aleatorios es un modelo no paramétrico y no lineal, entonces aplicar un \\(R^2\\) es pasarse tres pueblos).\n\n\n\n\n\nJames G, Witten D, Hastie T, Tibshirani R (eds.). 2013. New York: Springer.",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Regresiones múltiples</span>"
    ]
  },
  {
    "objectID": "c19_glm.html",
    "href": "c19_glm.html",
    "title": "19  Modelos Lineales Generalizados",
    "section": "",
    "text": "19.1 Librerías\nlibrary(ggplot2)\nlibrary(stats4)\nlibrary(brms)\nlibrary(visreg)\nlibrary(pscl)\nlibrary(MASS)\nlibrary(MuMIn)\nlibrary(tidymodels)\ntheme_set(see::theme_lucid() +\n  theme(panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank()))",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Modelos Lineales Generalizados</span>"
    ]
  },
  {
    "objectID": "c19_glm.html#problemas-familiares",
    "href": "c19_glm.html#problemas-familiares",
    "title": "19  Modelos Lineales Generalizados",
    "section": "19.2 Problemas familiares",
    "text": "19.2 Problemas familiares\nEn el Capítulo 18 mencionamos cómo utilizar combinaciones lineales de variables para predecir una variable continua, pero también recordarás que en el Capítulo 11 hablamos de modificar el supuesto de la distribución de nuestros errores para obtener una mejor estimación, lo cual forma una parte fundamental de los modelos lineales generalizados (GLMs).\nLa modificación puede ser tan “simple” (al menos en términos prácticos) como “relajar” el supuesto de normalidad, pero podemos también utilizar distribuciones que nos permitan modelar otro tipo de información. Bueno, hoy aterrizaremos esa última idea: La estructura principal de un GLM sigue siendo un modelo lineal, aunque nuestro supuesto de la distribución de errores no será exclusivamente normal, sino que puede tomar alguna otra familia de distribuciones, enlazada a nuestros datos con alguna función. En la sesión de hoy revisaremos:\n\nRegresiones robustas, expandiendo un poco la distribución t como distribución de errores y revisando una alternativa.\nFunciones de enlace y enlace inverso.\nRegresiones para conteos: Poisson, Binomial Negativa y sus variantes infladas en zeros.\nSelección de modelos, i.e., cómo seleccionar el mejor de un conjunto de modelos candidatos.\nRegresiones para clases: Regresión logística binomial y multinomial.\n\n\n\n\n\n\n\nImportante\n\n\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nOJO: Por practicidad vamos a aplicar los GLMs con R base. Puedes también aplicar GLMs penalizados utilizando básicamente la misma estructura que vimos en el Capítulo 18. ¿Por qué no utilizar tidymodels? Podría verme elegante y decirte que porque ya conocemos el flujo básico de trabajo de tidymodels y las ventajas que nos ofrece, pero la realidad es otra: la implementación de GLMs en tidymodels es bastante limitada aún (aunque se puede hacer utilizando glmnet con mixture = 0 y penalty = 0). Prefiero que entonces nos enfoquemos hoy en la intuición de los GLMs y cómo interpretarlos adecuadamente, más que enfocarnos en la parte técnica. Si tienes dudas sobre el uso de tidymodels te recomiendo que revises Kuhn & Silge (2022). Es un libro web que habla justo sobre los detalles del uso de tidymodels. Recuerda también revisar su documentación.",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Modelos Lineales Generalizados</span>"
    ]
  },
  {
    "objectID": "c19_glm.html#regresiones-robustas",
    "href": "c19_glm.html#regresiones-robustas",
    "title": "19  Modelos Lineales Generalizados",
    "section": "19.3 Regresiones robustas",
    "text": "19.3 Regresiones robustas\nLa idea de una regresión robusta la revisamos en el Capítulo 11; es decir, utilizar una distribución con colas más altas que una distribución normal para poder contender con el efecto de puntos extremos, pero expandamos esa idea. ¿Qué significa el “peso” de las colas de una distribución? Qué tanta densidad (o masa, para distribuciones discretas) de probabilidad está acumulada lejos de la tendencia central. En palabras más sencillas, una distribución con colas ligeras como la normal piensa que la probabilidad de tener valores alejados de la tendencia central es muy baja; por lo tanto, considera “todos” los datos como igual de importantes y reacciona moviendo la estimación. ¿No me crees? Veamos un caso extremo, utilizando el tercer conjunto de datos del cuarteto de Anscombe:\n\n# Almacenados en R como anscombe\nansc &lt;- read.csv(\"datos/anscombe.csv\")\nansc$x &lt;- scale(ansc$x, center = TRUE, scale = FALSE)\nggplot(data = ansc, aes(x = x, y = y, color = conjunto)) +\n  geom_point(show.legend = F) +\n  facet_wrap(~conjunto) +\n  labs(title = \"Cuarteto de Anscombe\") +\n  scale_color_manual(values = c(\"gray70\", \"gray70\", \"#1f77b4\", \"gray70\"))\n\n\n\n\n\n\n\n\nSi vemos la distribución de y notaremos que no es exactamente normal, debido a ese punto extremo en 12.5:\n\nansc_iii = ansc[ansc$conjunto == \"III\",]\nggplot(data = ansc_iii, aes(x = y)) +\n  geom_density(color = \"#1f77b4\", fill = NA) +\n  labs(title = \"Densidad de y del conjunto III de Anscombe\")\n\n\n\n\n\n\n\n\nAjustemos entonces nuestra regresión lineal simple:\n\nansciii_lm &lt;- lm(y~x, data = ansc_iii)\nsummary(ansciii_lm)\n\n\nCall:\nlm(formula = y ~ x, data = ansc_iii)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   7.5000     0.3728  20.120 8.61e-09 ***\nx             0.4997     0.1179   4.239  0.00218 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\n\nGráficamente:\n\nggplot(data = ansc_iii,\n       aes(x = x, y = y)) +\n  geom_point(color = \"#1f77b4\") +\n  geom_smooth(method = \"lm\",\n              color = \"gray50\",\n              fill = NA) +\n  labs(title = \"RLS con supuesto de normalidad\")\n\n\n\n\n\n\n\n\nNo se ve mal; sin embargo, es claro que el punto extremo está influenciando la estimación. Podemos aplicar algún criterio de detección de valores extremos (o deformar nuestros datos) para cumplir con el supuesto de normalidad; sin embargo, más que parchar nuestros datos, es preferible modificar nuestro modelo. Cambiemos entonces a una regresión con una verosimilitud t de Student:\n\nLLt &lt;- function(b0, b1, df, sigma){\n  # Encontrar los residuales. Modelo a ajustar (lineal)\n  R = ansc_iii$y - ansc_iii$x*b1 - b0\n  \n  # Calcular la verosimilitud. Residuales con distribución t de student\n  \n  R = suppressWarnings(brms::dstudent_t(R, df = df,\n                                        mu = 0, sigma = sigma))\n  \n  # Sumar el logaritmo de las verosimilitudes\n  # para todos los puntos de datos.\n  -sum(R, log = TRUE)\n}\n\nmlet_fit &lt;- mle(LLt, \n                start = list(b0 = 0, b1 = 0, df = 2, sigma = 1),\n                nobs = length(ansc_iii$y),\n                lower = list(b0 = -20, b1 = -12, df = 1, sigma = 0.1),\n                upper = list(b0 = 20, b1 = 12, df = 30, sigma = 10))\nsummary(mlet_fit)\n\nWarning in sqrt(diag(object@vcov)): NaNs produced\n\n\nMaximum likelihood estimation\n\nCall:\nmle(minuslogl = LLt, start = list(b0 = 0, b1 = 0, df = 2, sigma = 1), \n    nobs = length(ansc_iii$y), lower = list(b0 = -20, b1 = -12, \n        df = 1, sigma = 0.1), upper = list(b0 = 20, b1 = 12, \n        df = 30, sigma = 10))\n\nCoefficients:\n        Estimate   Std. Error\nb0     7.1141555  0.015785311\nb1     0.3453896  0.005152567\ndf    30.0000000 36.983300378\nsigma  0.1000000          NaN\n\n-2 log L: -81.09539 \n\n\nAhora empatemos ambas regresiones en un mismo gráfico:\n\ncoefs_t &lt;- coef(mlet_fit)\nfitted &lt;- coefs_t[1] + coefs_t[2]*ansc_iii$x\nggplot(data = ansc_iii,\n       aes(x = x, y = y)) +\n  geom_line(aes(x = x, y = fitted),\n            color = \"#ff7f0e\",\n            linewidth = 1, alpha = 0.7) +\n  geom_smooth(method = \"lm\",\n              color = \"gray50\",\n              fill = NA) +\n  geom_point(color = \"#1f77b4\") +\n  labs(title = \"Regresión robusta\",\n       subtitle = \"OLS (gris) vs. t de Student (naranja)\")\n\n\n\n\n\n\n\n\nEn este caso el punto extremo ya no influenció la estimación de la regresión, lo cual en la mayoría de los casos es algo deseable. Aunque esta es una forma de realizar una regresión robusta, existen otras. Una de ellas es modificar la función de pérdida, como por ejemplo con la Regresión con pérdida Huber. Los detalles matemáticos los dejaré para tu investigación, lo realmente importante es entender que los errores (residuales) son ponderados según su magnitud; es decir, se resta importancia a aquellos residuales que sean grandes y, de hecho, si están por encima de cierto límite, son descartados por completo. Su implementación es sumamente sencilla, pues lo único que tenemos que hacer es modificar lm por la función rlm de la librería MASS:\n\nrr_huber &lt;- MASS::rlm(y~x, data = ansc_iii)\nsummary(rr_huber)\n\n\nCall: rlm(formula = y ~ x, data = ansc_iii)\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0049962 -0.0028591 -0.0007219  0.0028667  4.2421008 \n\nCoefficients:\n            Value     Std. Error t value  \n(Intercept)    7.1150    0.0013  5309.3547\nx              0.3457    0.0004   815.8284\n\nResidual standard error: 0.005248 on 9 degrees of freedom\n\n\n¿Notas algo interesante? Los resultados son los mismos que en la regresión t de Student, aunque aquí podemos ver la ponderación dada a cada punto:\n\nh_weights &lt;- data.frame(x = ansc_iii$x,\n                        resid = rr_huber$residuals,\n                        weight = rr_huber$w)\nh_weights\n\n\n  \n\n\n\nGráficamente:\n\nggplot(data = ansc_iii,\n       aes(x = x, y = y)) +\n  geom_smooth(method = MASS::rlm,\n             color = \"#ff7f0e\",\n             size = 1, alpha = 0.7,\n             fill = \"blue\") +\n  geom_smooth(method = \"lm\",\n              color = \"gray50\",\n              fill = NA) +\n  geom_point(aes(color = h_weights$weight)) +\n  labs(title = \"Regresión robusta\",\n       subtitle = \"OLS (gris) vs. Huber (naranja)\") +\n  scale_color_gradient(name = \"Peso\",\n                       low = \"firebrick\",\n                       high = \"#1f77b4\",\n                       limits = c(0,1))\n\n\n\n\n\n\n\n\nComo era de esperarse, los resultados son los mismos que los de la regresión t de Student. ¿Qué tiene que ver esto con los modelos lineales generalizados? Veamos primero los tres elementos clave que definen un GLM:\n\nEl modelo lineal, que es el mismo en ambos casos.\nUna familia para el error, que no modificamos en la regresión Huber; sin embargo la familia sería Gaussiana (Normal).\nUna función de enlace (o enlace inverso), que en ambos casos sería una función de identidad; es decir, ninguna modificación para pasar de nuestros datos a la distribución del error.\n\nEs decir, en un sentido amplio, ambas aproximaciones son GLMs, aunque usualmente nos referimos a GLMs cuando la distribución del error es diferente a una distribución normal (hablar de un glm con error Gaussiano es volver a una regresión lineal ajustada por mínimos cuadrados). ¿Cuál aplicar? Ya que el resultado es el mismo, puedes escoger una u otra, solo ten en cuenta que la implementación por máxima verosimilitud de un modelo t de Student puede tener muchos bemoles al momento de optimizarse, además de que su salida es incompatible con algunas otras funciones, incluyendo el cálculo de los intervalos de confianza (para los coeficientes de la regresión Huber puedes utilizar la función confint.default(rr.huber)).",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Modelos Lineales Generalizados</span>"
    ]
  },
  {
    "objectID": "c19_glm.html#funciones-de-enlace",
    "href": "c19_glm.html#funciones-de-enlace",
    "title": "19  Modelos Lineales Generalizados",
    "section": "19.4 Funciones de enlace",
    "text": "19.4 Funciones de enlace\nEste es un buen momento para hablar de un tema que a veces causa bastante confusión: las funciones de enlace o las funciones de enlace inverso. Estas son funciones “arbitrarias” (ojo a las comillas) que tienen una sola función (valga la redundancia): poner la salida de nuestro modelo lineal en los “requerimientos” de la familia de nuestro error. En el caso anterior, la distribución t es una distribución continua de probabilidad que está centrada en 0, como esperaríamos de nuestros residuales, por lo que la función de enlace es una función de identidad; es decir, no hacemos nada a la salida del modelo para poder obtener residuales continuos centrados en 0. Pero este no siempre es el caso; de hecho, las aplicaciones más comunes de GLM siempre requieren de algún enlace. ¿Y los enlaces inversos? Son simple y sencillamente el inverso de la función de enlace aplicados al lado contrario de la igualdad. Matemáticamente es más claro:\nEn un GLM con una función \\(f\\) de enlace tendríamos la siguiente estructura para nuestro modelo lineal:\n\\[\nf(y) = \\beta_0 + \\beta_1*x\n\\]\nEs decir, modificamos la salida (\\(y\\)) de nuestro modelo lineal utilizando una función \\(f\\). Si es una función logarítmica, por ejemplo, se vería de la siguiente manera:\n\\[\nlog(y) = \\beta_0 + \\beta_1*x\n\\]\nPero obtendríamos exactamente lo mismo si utilizamos un poco de álgebra y resolvemos para \\(y\\), aplicando un exponencial a ambos lados de la igualdad:\n\\[\\begin{align*}\ne^{log(y))} = e^{(\\beta_0 + \\beta_1*x)} \\\\\n\\therefore \\\\\ny = e^{(\\beta_0 + \\beta_1 *x)}\n\\end{align*}\\]\nEl apelativo “inversa” es simplemente para indicar el lado dónde se está aplicando el enlace. Habiendo dicho esto, vayamos a una de las aplicaciones más comunes para GLM: la regresión para conteos.",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Modelos Lineales Generalizados</span>"
    ]
  },
  {
    "objectID": "c19_glm.html#regresiones-para-conteos",
    "href": "c19_glm.html#regresiones-para-conteos",
    "title": "19  Modelos Lineales Generalizados",
    "section": "19.5 Regresiones para conteos",
    "text": "19.5 Regresiones para conteos\nTe preguntarás qué tienen de especial los conteos, y la respuesta es muy simple: son valores enteros mayores o iguales a 0. Esto quiere decir que una distribución continua en el intervalo \\((-\\infty, \\infty)\\) NO es adecuada para modelar los datos. ¿Qué hacemos? Utilizamos alguna distribución discreta que nos permita tratar con el número de veces en que algo sucede.\n\n19.5.1 Regresión Poisson\nMuy posiblemente esto te suene a ensayos de Bernoulli o ejercicios con distribuciones Poisson (¿cuántos autos rojos pasan en una hora por un punto determinado?, por ejemplo). Pues justamente podemos utilizar esa misma distribución (Poisson). Esta distribución tiene un par de peculiaridades. La primera es que asume que los eventos ocurren de manera independiente entre sí, a un intervalo fijo de espacio o tiempo. La segunda es que su único parámetro (\\(\\lambda\\)) representa tanto la media como la varianza de la distribución (más adelante hablaremos de las implicaciones de esto), por lo que DEBE ser positivo. ¿El problema? Nuestros residuales pueden ser negativos. ¿Qué podemos hacer? Aplicar una función de enlace inverso que nos permita restringir nuestro predictor a valores positivos, justo como la función exponencial, por lo que nuestro modelo se expresaría de la siguiente forma:\n\\[\\begin{align*}\n\\lambda = e^{(\\beta_0 + \\beta_1*x)} \\\\\ny \\sim Poisson(\\lambda)\n\\end{align*}\\]\nPara aplicarlo resolvamos un problema en el cuál trataremos de predecir el número de peces capturados en un lago por un pescador, considerando el número de hijos y si llevan o no un camper:\n\ncsvurl &lt;- \"https://stats.idre.ucla.edu/stat/data/fish.csv\"\nfish &lt;- read.csv(csvurl)[,c(\"child\", \"camper\", \"count\")]\nhead(fish)\n\n\n  \n\n\n\nExploremos nuestros datos. Al tratarse de una variable discreta podemos, sin ningún problema, utilizar un gráfico de frecuencias (¿cuál otro utilizarías?):\n\nggplot(aes(x = count), data = fish) +\n  geom_bar(stat = \"count\", color = NA, fill = \"dodgerblue4\") +\n  labs(title = \"Frecuencia de peces capturados en un lago\",\n       x = element_blank(),\n       y = element_blank())\n\n\n\n\n\n\n\n\nPor otra parte, habrás notado un par de cosas: a) hay una gran cantidad de ceros y b) tenemos algunos puntos “extremos”; i.e., algunos pescadores que tuvieron demasiada suerte y que capturaron demasiados peces en comparación con el resto. Este tipo de distribuciones no son extrañas en la naturaleza, y tienen un par de bemoles de los cuales hablaremos después. Por lo pronto, construyamos nuestro GLM. Podemos construirlo con la función glm de R base, cuyo uso es sumamente similar al de la función lm, salvo que indicaremos la familia como un argumento adicional:\n\npoiss &lt;- glm(count~., data = fish, family = \"poisson\")\nsummary(poiss)\n\n\nCall:\nglm(formula = count ~ ., family = \"poisson\", data = fish)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  0.91026    0.08119   11.21   &lt;2e-16 ***\nchild       -1.23476    0.08029  -15.38   &lt;2e-16 ***\ncamper       1.05267    0.08871   11.87   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2958.4  on 249  degrees of freedom\nResidual deviance: 2380.1  on 247  degrees of freedom\nAIC: 2723.2\n\nNumber of Fisher Scoring iterations: 6\n\n\nLa salida es muy similar a otras que hemos visto. Cómo llamamos a la función glm, un descriptor de los residuales, los coeficientes con sus respectivas pruebas de nulidad (después hablaremos de su interpretación), seguidas de algunos elementos propios de la función GLM. Primero tenemos una nota sobre un parámetro de dispersión, que se asumió como 1. Esto quiere decir que estamos asumiendo que la media es igual a la varianza, lo cual podemos tomar solo como un recordatorio para que revisemos dicho supuesto. Después tenemos información sobre la devianza del modelo. Podemos utilizar la devianza residual para realizar una prueba de bondad de ajuste para el modelo global. Esta es la diferencia entre la devianza del modelo y la máxima devianza de un modelo ideal, donde los valores predichos son idénticos a los observados (devianza nula). Por lo tanto, buscamos valores pequeños de la devianza residual. Dicha prueba podemos realizarla de la siguiente manera:\n\nwith(data = poiss,\n     expr = cbind(res.deviance = deviance,\n                  df = df.residual,\n                  p = pchisq(deviance, df.residual,\n                             lower.tail = F)))\n\n     res.deviance  df p\n[1,]      2380.12 247 0\n\n\n\n\n\n\n\n\nNota\n\n\n\nLa función with(data, expr) evalúa una expresión de R en un ambiente construido desde los datos. En el bloque de arriba nos ahorra el tener que extraeer manualmente poiss$deviance y poiss$df.residual.\n\n\nTenemos un valor de p sumamente pequeño, lo cual sugiere que el modelo no se encuentra bien ajustado. ¿Alguna idea de por qué? Como te imaginarás, tiene que ver con la distribución de nuestros datos, eso que mencionamos sobre muchos ceros y algunos pescadores con mucha suerte. De hecho, cada una de estas características es un problema en sí mismo, así que abordemoslos uno por uno. Una pregunta que puedes estarte haciendo es ¿y la función de enlace? Va implícita en la familia. En este caso, es una función de enlace logarítmica, que es el equivalente a la función de enlace inverso que revisamos antes.\n\nstr(poisson()[1:5])\n\nList of 5\n $ family  : chr \"poisson\"\n $ link    : chr \"log\"\n $ linkfun :function (mu)  \n $ linkinv :function (eta)  \n $ variance:function (mu)  \n\n\n\n\n19.5.2 Exceso de ceros: Regresión Poisson Inflada en Cero\nLa primera peculiaridad de nuestros datos es que hay una cantidad enorme de ceros. Aunque esto puede suceder de manera natural, la distribución Poisson no es capaz de contender adecuadamente con estos casos. Afortunadamente, hay una manera de extender el modelo Poisson para permitirnos arreglar esto. En su forma más fundamental, asumiremos que tenemos dos procesos:\n\nUno modelado con una distribución Poisson.\nUno generando ceros adicionales.\n\nEs decir, cuando hablemos de modelos “inflados en cero” estamos hablando de una situación en la que tenemos ceros “falsos” o, mejor dicho, extras a los ceros verdaderos que nos podemos encontrar. Veamos qué pasa al ajustar este modelo a nuestros datos. Para este modelo necesitaremos de la función zeroinfl() de la librería pscl:\n\nzi_poiss &lt;- pscl::zeroinfl(count~child+camper, data = fish)\nsummary(zi_poiss)\n\n\nCall:\npscl::zeroinfl(formula = count ~ child + camper, data = fish)\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-1.2395 -0.8340 -0.4694 -0.1764 24.1051 \n\nCount model coefficients (poisson with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.64535    0.08278  19.877   &lt;2e-16 ***\nchild       -0.77272    0.09103  -8.489   &lt;2e-16 ***\ncamper       0.75526    0.09112   8.289   &lt;2e-16 ***\n\nZero-inflation model coefficients (binomial with logit link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   0.0424     0.2426   0.175   0.8613    \nchild         1.0244     0.2200   4.656 3.22e-06 ***\ncamper       -0.7085     0.2926  -2.422   0.0155 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 10 \nLog-likelihood: -1025 on 6 Df\n\n\nLa salida es similar al caso anterior, solo tenemos coeficientes para la regresión logística para clasificar ceros verdaderos de falsos y los coeficientes del modelo Poisson sin el exceso de ceros; sin embargo, notarás que no hay ningún indicativo sobre si este modelo es mejor a nuestro modelo Poisson, por lo que podemos compararlos. Para ello podemos utilizar distintas alternativas: una prueba de Vuong (función pscl::vuong(mod_1, mod_2)) o utilizar una aproximación multi-modelo. Optaremos por esa última vía, la cual exploraremos a detalle más adelante. Por lo pronto, es suficiente que sepas que utilizaremos una medida llamada Criterio de Información de Akaike (AIC), y el mejor modelo será aquel que tenga el menor valor de AIC:\n\nAIC(poiss, zi_poiss)\n\n\n  \n\n\n\nComo era de esperarse, el modelo inflado en cero es un mejor candidato; sin embargo, tenemos un problema pendiente: nuestros pescadores muy suertudos.\n\n\n19.5.3 Sobre-dispersión: Regresión Binomial Negativa\nEsos pescadores muy suertudos pueden hacer lo mismo que nuestro punto extremo en el ejemplo de regresión robusta; es decir, jalar nuestras estimaciones hacia ellas y alejarlas de la estimación “real”, solo que aquí es un tanto diferente y tiene que ver con el supuesto de nuestra distribución Poisson: la media y la varianza son iguales. Este supuesto, evidentemente, no se sostiene cuando tenemos una dispersión muy grande de nuestros datos (varianza &gt; media), lo cual genera el problema de la sobre dispersión de nuestro modelo (no de los datos). Una estrategia es cambiar nuestra verosimilitud a una distribución Binomial Negativa, la cual tiene un (híper) parámetro adicional a la distribución Poisson. Este parámetro controla, justamente, la dispersión del modelo. Apliquemos entonces nuestra regresión binomial negativa:\n\nbineg &lt;- MASS::glm.nb(count~., data = fish, trace = F)\nsummary(bineg)\n\n\nCall:\nMASS::glm.nb(formula = count ~ ., data = fish, trace = F, init.theta = 0.2552931119, \n    link = log)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   1.0727     0.2425   4.424 9.69e-06 ***\nchild        -1.3753     0.1958  -7.025 2.14e-12 ***\ncamper        0.9094     0.2836   3.206  0.00135 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(0.2553) family taken to be 1)\n\n    Null deviance: 258.93  on 249  degrees of freedom\nResidual deviance: 201.89  on 247  degrees of freedom\nAIC: 887.42\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  0.2553 \n          Std. Err.:  0.0329 \n\n 2 x log-likelihood:  -879.4210 \n\n\nNotarás que esta salida es prácticamente la misma que la que tuvimos en nuestro GLM Poisson, salvo que ahora nos da el valor del parámetro de sobredispersión. Te estarás preguntando: ¿Cómo sé si, en efecto, mis modelos están sobre-dispersos? Para eso podemos utilizar una prueba de razón de verosimilitud, en la cual compararemos la verosimilitud de ambos modelos (binomial negativa y Poisson) y veremos si se ajustan a la misma distribución; es decir, la prueba de razón de verosimilitud es una prueba de bondad de ajuste, con distribución \\(\\chi^2\\). ¿Qué es lo que estamos comparando? Si el parámetro adicional ayuda a que el ajuste del modelo mejore significativamente. Para aplicarla podemos utilizar la función odTest(mod_bn) de la librería pscl:\n\npscl::odTest(bineg)\n\nLikelihood ratio test of H0: Poisson, as restricted NB model:\nn.b., the distribution of the test-statistic under H0 is non-standard\ne.g., see help(odTest) for details/references\n\nCritical value of test statistic at the alpha= 0.05 level: 2.7055 \nChi-Square Test Statistic =  1837.7652 p-value = &lt; 2.2e-16 \n\n\nOtra alternativa es una que ya conocemos (performance):\n\nod_bineg &lt;- performance::check_overdispersion(bineg)\nplot(od_bineg)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nA ojo de buen cubero era más que evidente que nuestro modelo iba a estar sobre disperso, por lo que estos resultados no son sorprendentes. Algo que puedes pensar es “si estoy comparando qué modelo está mejor ajustado, ¿puedo entonces utilizar el AIC?” Y la respuesta es, claro que sí:\n\nAIC(poiss, zi_poiss, bineg)\n\n\n  \n\n\n\nY los resultados son, como debe de ser, consistentes. Llegados a este punto podrías preguntarme: “Ok, Arturo, ya corregimos para el exceso de ceros y para la sobre dispersión pero lo hicimos de manera independiente. ¿Hay alguna manera de hacer ambas cosas al mismo tiempo?” En efecto, y es justo lo siguiente que vamos a revisar.\n\n\n19.5.4 Exceso de ceros y sobre dispersión: Regresión Binomial Negativa Inflada en Cero\nLa respuesta es una combinación de ambas; es decir, utilizaremos una distribución de error binomial negativa inflada en cero. La lógica es, entonces, modelar a los ceros verdaderos y luego construir el modelo de regresión binomial negativa. Para hacerlo utilizaremos la función zeroinfl() que vimos antes, solo que cambiaremos la familia a negbin:\n\nzi_bineg &lt;- pscl::zeroinfl(count~., data = fish, dist = \"negbin\")\nsummary(zi_bineg)\n\n\nCall:\npscl::zeroinfl(formula = count ~ ., data = fish, dist = \"negbin\")\n\nPearson residuals:\n      Min        1Q    Median        3Q       Max \n-0.512334 -0.497274 -0.325409 -0.004872 13.993013 \n\nCount model coefficients (negbin with log link):\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   1.0525     0.2700   3.899 9.67e-05 ***\nchild        -0.9110     0.2851  -3.195  0.00140 ** \ncamper        0.7967     0.3053   2.609  0.00907 ** \nLog(theta)   -1.2954     0.1315  -9.847  &lt; 2e-16 ***\n\nZero-inflation model coefficients (binomial with logit link):\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)  -10.726     38.510  -0.279    0.781\nchild          9.728     38.455   0.253    0.800\ncamper        -8.770     38.461  -0.228    0.820\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nTheta = 0.2738 \nNumber of iterations in BFGS optimization: 31 \nLog-likelihood: -434.9 on 7 Df\n\n\nFinalmente, podemos comparar nuestros cuatro modelos candidatos para encontrar el más adecuado:\n\nAIC(poiss, zi_poiss, bineg, zi_bineg)\n\n\n  \n\n\n\nVemos que los AIC de los modelos con distribución de error binomial negativa tienen los menores valores; por lo tanto seleccionaremos a alguno de los dos. ¿Cuál? En la siguiente sección hablaremos de las peculiaridades. Por lo pronto, sigamos con nuestro criterio de seleccionar el que tenga el menor AIC, que corresponde a la distribución binomial negativa inflada en cero. En este punto pasaríamos a interpretar nuestros coeficientes, pero antes es necesario que hablemos de los no supuestos de normalidad y homogeneidad de varianzas (homocedasticidad):\n\n\n\n\n\n\nImportante\n\n\n\nNotarás que hasta este momento no hemos ni siquiera mencionado los supuestos de normalidad y homocedasticidad/homogeneidad de varianzas.Esto no es casualidad: si la familia del error que estamos utilizando no es normal, ¿por qué deberíamos de buscar normalidad? Recuerda: el supuesto de normalidad del modelo lineal gira en torno a la distribución del error. Si en un GLM estamos cambiando explícitamente la distribución de ese error, buscar normalidad es un sinsentido. Un caso similar ocurre con el supuesto de homogeneidad de varianzas/homocedasticidad: en una distribución Poisson, por ejemplo, la media es igual a la varianza y, si en una regresión estimamos el valor promedio de \\(y\\) para cada valor de \\(x\\), no tiene ningún sentido que el error sea homogéneo: entre más grande sea \\(y\\), más grande es la varianza en ese punto. ¿Qué supuestos sí son importantes? Los que probamos indirectamente al crear nuestros modelos candidatos: sobredispersión e inflación en ceros.\n\n\nAhora sí, ¿qué nos dicen nuestros coeficientes?\n\n\n19.5.5 Interpretación\nDesafortunadamente, la interpretación no es tan simple como en la RLM o RLS debido a la función de enlace que utilizamos. Para facilitarnos la existencia, pensemos en un modelo Poisson con un solo predictor y una función de enlace logarítmica:\n\\[\nY \\sim Poisson(\\theta) \\\\\nlog(\\theta) = \\alpha + \\beta x \\\\\n\\therefore \\\\\n\\theta = e^{\\alpha + \\beta x}\n\\]\nPero, por las leyes de los exponentes, podemos reescribir la última ecuación como:\n\\[\n\\theta = e^{a}e^{\\beta x}\n\\]\nEsto quiere decir que los coeficientes no son aditivos, sino multiplicativos:\n\nIntercepto: \\(e^\\alpha\\), valor de \\(\\theta\\) cuando \\(x = 0\\). Si este parámetro es o no de interés depende totalmente del problema.\nPendiente(s): \\(e^\\beta\\).\n\n\nSi \\(\\beta = 0\\), entonces \\(e^\\beta = 1\\); es decir, no hay un efecto del predictor.\nSi \\(\\beta &gt; 0\\), entonces \\(e^\\beta &gt; 1\\); es decir, el predictor incrementa el valor de \\(\\theta\\) a una tasa de \\(e^\\beta\\) por cada incremento unitario en \\(x\\)\nSi \\(\\beta &lt; 0\\), entonces \\(0 \\leq e^\\beta &lt; 1\\); es decir, el predictor disminuye el valor de \\(\\theta\\) a una tasa de \\(e^\\beta\\) por cada incremento unitario en \\(x\\).\n\nRecuperemos los coeficientes de nuestra regresión binomial negativa inflada en cero y exponenciémoslos:\n\nexp(coef(zi_bineg)[1:3])\n\ncount_(Intercept)       count_child      count_camper \n        2.8649075         0.4021278         2.2181080 \n\n\nPongamos atención solo a aquellos coeficientes con count_, pues son los que realmente nos interesan. La interpretación entonces sería:\n\nIntercepto: Cuando el pescador no tiene hijos y no lleva un camper, el promedio de peces capturados es de 2.86\nPendientes:\n\n\nChild: El promedio de peces capturados disminuye un 60% (1-&gt;0.4) veces por cada hijo adicional.\nCamper: Si el pescador tiene un camper, el promedio de peces capturados incrementa 2.2 veces.\n\n\n\n\n\n\n\nImportante\n\n\n\nCorolario: La interpretación depende totalmente de la función de enlace que utilicemos, y siempre es necesario considerarla para poder interpretarlos.\n\n\nPara construir un gráfico podemos generar una línea con valores predichos o, mejor dicho, dos líneas: una para cada nivel de camper:\n\nfish$pred &lt;- predict(zi_bineg, type = \"response\")\n\nggplot(data = fish, aes(x = child, y = count,\n                        color = factor(camper))) +\n  geom_jitter(width = 0.1) +\n  geom_line(aes(y = pred)) +\n  labs(title = \"GLM Binomial negativo\",\n       x = \"Número de hijos\",\n       y = \"Peces capturados\") +\n  scale_color_manual(name = \"Camper\",\n                     labels = c(\"NO\", \"SI\"),\n                     values = c(\"#1f77b4\", \"#ff7f0e\")) +\n  scale_y_continuous(limits = c(0, 12),\n                     label = scales::label_comma(accuracy = 1))\n\n\n\n\n\n\n\n\nO utilizando visreg:\n\npartial_plots &lt;- visreg::visreg(bineg,\n                                scale = \"response\",\n                                ylab = \"Capturas\",\n                                gg = TRUE)\n\npatchwork::wrap_plots(partial_plots) & \n  scale_y_continuous(limits = c(0, 12),\n                     label = scales::label_comma(accuracy = 1)) &\n  # OJO: Los \"breaks\" del eje x se definieron así porque son pocos valores discretos!\n  scale_x_continuous(breaks = function(x){unique(floor(pretty(seq(min(x), (max(x))))))})\n\n\n\n\n\n\n\n\nY ahora podemos hablar de la selección de modelos.",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Modelos Lineales Generalizados</span>"
    ]
  },
  {
    "objectID": "c19_glm.html#selección-de-modelos",
    "href": "c19_glm.html#selección-de-modelos",
    "title": "19  Modelos Lineales Generalizados",
    "section": "19.6 Selección de modelos",
    "text": "19.6 Selección de modelos\nEl tener varios varios modelos candidatos no es algo extraño, entonces es necesario tener algún tipo de criterio que nos permita comparar entre ellos. Una aproximación es la que hemos utilizado hasta el momento; es decir, utilizar el Criterio de Información de Akaike (AIC) y seleccionar el menor. ¿Qué es el AIC y con qué se come?\n\n19.6.1 Criterio de Información de Akaike\nEl AIC es un criterio basado en teoría de la información, particularmente en la divergencia Kullback-Leibler. Ese detalle matemático va más allá del alcance de este curso; sin embargo, podemos entender que está basado en la verosimilitud de un modelo dado; es decir, qué tan verosímil es que ese modelo haya generado los datos. Un detalle es que, como hemos visto en sesiones anteriores, el “ajuste” de un modelo es directamente proporcional a su complejidad (al menos vs. los datos de entrenamiento). Es, entonces, necesario penalizar de alguna manera el número de parámetros en el modelo, para no comernos un “gol” con un modelo excesivamente complejo. Puesto en una ecuación, el AIC queda de la siguiente manera:\n\\[\nAIC = -2ln(L) + 2k\n\\]\nDonde \\(L\\) es la verosimilitud del modelo y \\(k\\) el número de parámetros. Si nuestro modelo tiene un gran número de parámetros, el valor de AIC se hará más grande, mientras que, si tiene un menor número, se hará más pequeño. ¿Qué nos dice un AIC en sí mismo? NADA, absolutamente nada. Si yo te digo que un modelo tiene un AIC de 800 no puedes saber si es bueno o malo, pues no hay una referencia. Esto nos lleva a hablar sobre algunas consideraciones que debemos de tener al utilizar el AIC:\n\nValores más bajos indican modelos más parsimoniosos.\nEs una medida relativa de la parsimonia de un modelo, por lo que solo tiene sentido cuando comparamos AIC para hipótesis (modelos) alternativas.\nPodemos comparar modelos no anidados. De hecho, podríamos comparar un modelo lineal con uno no lineal.\nLas comparaciones son válidas SOLO para modelos ajustados con los mismos valores de respuesta; i.e., mismos valores de \\(y\\).\nComparar muchos modelos con AIC es una mala idea, pues caemos en el mismo problema de las comparaciones múltiples, donde podemos encontrar por azar un modelo con el valor más bajo de AIC, cuando en realidad no es el modelo más apropiado.\nPara variar, cuando tratamos con tamaños de muestra pequeños (n/k &lt; 40) el AIC pierde confiabilidad, por lo que hay que aplicar una corrección:\n\n\\(AIC_c = AIC + \\frac{2k(k+1)}{n-k-1}\\)\nDado que conforme incrementa n, el \\(AIC_c\\) se aproxima al \\(AIC\\), es una buena idea utilizar \\(AIC_c\\).\n\nPodemos encontrar múltiples modelos que tengan AICs similares, esto solo sugiere que estas hipótesis alternativas tienen soportes similares. ¿Qué tanto es tantito? Esa respuesta es un poco más compleja, y requiere que presentemos el \\(\\Delta{AIC}\\) (también lo puedes encontrar como \\(\\Delta_i\\)):\n\n\\(\\Delta AIC = AIC_i - AIC_{min}\\); es decir, la diferencia de cada AIC respecto al valor mínimo de AIC entre los modelos candidatos. Esta transformación forza al “mejor” modelo a tener un \\(\\Delta AIC = 0\\), y representa la pérdida de información si utilizamos un modelo candidato \\(m_i\\) en vez de \\(m_{min}\\).\nModelos con un \\(\\Delta_i \\leq 2\\) tienen soporte substancial (evidencia),\nModelos con un \\(4 \\leq \\Delta_i \\leq 7\\) tienen considerablemente menos soporte y\nModelos con \\(\\Delta_i &gt; 10\\) carecen, escencialmente, de soporte.\n\n\nRecuperemos nuestra comparación anterior, calculemos los \\(AIC_c\\) (con la función AICc de la librería MuMIn) y calculemos los \\(\\Delta_i\\). Como mencionábamos antes, los modelos con distribuciones binomial negativas son los que tienen el mayor soporte, mientras que los Poisson carecen de cualquier soporte (dados estos datos y modelos candidatos). También podemos ver que pesa más la sobredispersión que el exceso de ceros, pues el modelo inflado en cero es marginalmente mejor que aquel no inflado.\n\nAICs &lt;- MuMIn::AICc(poiss, zi_poiss, bineg, zi_bineg)\nAICs$Delta &lt;- AICs$AIC - min(AICs$AIC)\nAICs",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Modelos Lineales Generalizados</span>"
    ]
  },
  {
    "objectID": "c19_glm.html#regresiones-para-clases",
    "href": "c19_glm.html#regresiones-para-clases",
    "title": "19  Modelos Lineales Generalizados",
    "section": "19.7 Regresiones para clases",
    "text": "19.7 Regresiones para clases\nEn el Capítulo 17 hablamos sobre los problemas de clasificación, y aplicamos un bosque aleatorio, pero mencionamos que había otro clasificador llamado regresión logística, sin dar más detalles. Aprovechemos, entonces, para revisar un poco más a profundidad la intuición detrás de ella.\n\n19.7.1 Regresión logística binaria\nLa regresión logística es un modelo lineal al cual aplicamos una función logística, lo que nos permite restringir nuestra salida al intervalo \\([0,1]\\) y, por lo tanto, predecir la probabilidad de pertenencia a una clase dada. Esa definición es correcta para resumir lo más posible la técnica; sin embargo, los detalles son un poco más complejos.\nLa regresión logística forma parte de los GLMs; por lo tanto, consta de un predictor lineal, una familia de distribución del error y una función de enlace. La familia de distribución del error es binomial; es decir, está en términos de la probabilidad de éxitos vs. la probabilidad de fracasos. Es por esto que la regresión logística tradicional solo nos permite clasificar entre dos clases. Su función de enlace es la función logit:\n\\[\nlogit(z) = \\frac{1}{1 + e^{-z}}\n\\]\nEsta función tiene la peculiaridad de que, independientemente de los valores de \\(z\\) (el predictor lineal), el resultado siempre estará contenido entre 0 y 1, el cual es, convenientemente, el mismo que el dominio del parámetro \\(p\\) de la distribución binomial (la probabilidad de éxito). Expresado en notación probabilística:\n\\[\\begin{align*}\n\\theta = logistic(\\alpha + \\beta x) \\\\\ny \\sim Binom(\\theta)\n\\end{align*}\\]\nApliquemos entonces una regresión logística para clasificar entre Biscoe y Dream de los pingüinos de Palmer, solo para ilustrar cómo interpretar los coeficientes. Primero, filtremos los datos:\n\n# Copia de los datos originales\npeng_dat &lt;- na.omit(palmerpenguins::penguins)\n# Mantener solo las especies de interés\npeng_dat &lt;- subset(peng_dat,\n                   island == \"Biscoe\" | island == \"Dream\")\n\npeng_dat &lt;- peng_dat |&gt;\n            # Centrar las variables numéricas\n            mutate(across(where(is.numeric), ~c(scale(., scale = F)))) |&gt;\n            # Eliminar covariables inútiles para la clasificación\n            dplyr::select(!c(species, sex, year))\n\nAhora ajustemos el modelo. No te olvides de dividir en entrenamiento-prueba (o, mejor aún, realizar validación cruzada), considerar si es necesario escalar los datos, y todos los demás pasos que vimos en el Capítulo 17:\n\nlogit_reg &lt;- glm(island~., data = peng_dat, family = \"binomial\")\nsummary(logit_reg)\n\n\nCall:\nglm(formula = island ~ ., family = \"binomial\", data = peng_dat)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       -0.7876798  0.2465237  -3.195   0.0014 ** \nbill_length_mm     0.2285915  0.0512441   4.461 8.16e-06 ***\nbill_depth_mm      0.6530103  0.1368932   4.770 1.84e-06 ***\nflipper_length_mm -0.0122859  0.0312904  -0.393   0.6946    \nbody_mass_g       -0.0026338  0.0005288  -4.981 6.33e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 390.87  on 285  degrees of freedom\nResidual deviance: 187.62  on 281  degrees of freedom\nAIC: 197.62\n\nNumber of Fisher Scoring iterations: 6\n\n\nY obtengámos los gráficos parciales, utilizando la función visreg(mod, scale = \"response\"), donde mod es el modelo aujustado:\n\npartial_plots &lt;- visreg::visreg(logit_reg, scale = \"response\",\n                                ylab = \"P(Isla)\",\n                                line.par = c(col = \"dodgerblue4\"),\n                                fill.par = c(fill = \"gray90\"),\n                                gg = TRUE)\n\npatchwork::wrap_plots(partial_plots) & \n  scale_y_continuous(breaks = c(0, 0.5, 1),\n                     limits = c(0,1),\n                     labels = c(\"Biscoe\", 0.5, \"Dream\"))\n\n\n\n\n\n\n\n\n¿Cómo interpretamos los coeficientes? Antes de pasar a eso, es necesario que pongamos atención al argumento scale de visreg, el cual indicamos como response. Esto lo que hizo fue poner nuestra salida en lo que nos interesa: la probabilidad de pertenencia a una especie, dadas las medidas de cada variable. Veamos qué pasa si retiramos ese argumento:\n\npartial_plots &lt;- visreg::visreg(logit_reg,\n                                ylab = \"log-odds(Isla)\",\n                                gg = TRUE,\n                                line.par = c(col = \"dodgerblue4\"))\n\npatchwork::wrap_plots(partial_plots)\n\n\n\n\n\n\n\n\nAhora nuestros gráficos están en la escala de nuestro modelo lineal; es decir, con response estamos introduciendo la función de enlace y graficando el GLM completo; sin embargo, esto no está presente en los coeficientes “crudos” arrojados por la función GLM, por lo que toca aplicar el álgebra correspondiente.\nEl modelo básico es:\n\\[\n\\theta = logistic(\\alpha + \\beta x)\n\\]\nEl inverso de la función logística es la función logit, dada por:\n\\[\nlogit(z) = log \\left( \\frac{z}{1-z} \\right)\n\\]\nPor lo que si tomamos la primera ecuación y aplicamos la función logit a ambos términos, obtenemos esta ecuación:\n\\[\nlogit(\\theta) = \\alpha + \\beta x\n\\]\nO, de manera equivalente:\n\\[\nlog \\left( \\frac{\\theta}{1-\\theta} \\right) = \\alpha + \\beta x\n\\]\nAhora, recordemos que \\(\\theta\\) en nuestro modelo es \\(p(y = 1)\\) (la probabilidad de “éxito”, o de ser Dream):\n\\[\nlog \\left(\\frac{p(y = 1)}{1 - p(y = 1)} \\right) = \\alpha + \\beta x\n\\]\nLa cantidad \\(\\frac{p(y = 1)}{1 - p(y = 1)}\\) se conoce como los odds o razones de probabilidad, que representan la probabilidad de éxito sobre la probabilidad de fracaso. Mientras que la probabilidad de obtener 2 al lanzar un dado es de 1/6, los odds para el mismo evento son \\(\\frac{1/6}{5/6} \\approx 0.2\\), o un éxito a cinco fracasos. En una regresión logística, \\(\\beta\\) representa el incremento en log-odds por incremento unitario en \\(x\\), no en la probabilidad de pertenencia a una clase, aunque la relación entre odds y probabilidad es monótona; es decir, conforme incrementa una, la otra también.\n\n\n\n\n\n\nTip\n\n\n\nSi te interesa ver directamente el cambio en probabilidad por incremento unitario de las variables, puedes utilizar una pariente cercana: la regresión probit.\n\n\n\nexp(coef(logit_reg))\n\n      (Intercept)    bill_length_mm     bill_depth_mm flipper_length_mm \n        0.4548990         1.2568285         1.9213158         0.9877893 \n      body_mass_g \n        0.9973696 \n\n\n\n\n19.7.2 Regresión multinomial\nAl igual que en el caso anterior, simplemente extenderemos aquellos detalles que no se aterrizaron por completo, particularmente el utilizar una red neuronal como análogo a una regresión logística multinomial. Como acabamos de ver, una regresión logística binaria nos permite predecir la probabilidad de éxito; i.e., de pertenecer a una sola clase. ¿Cómo lo extendemos a más de dos clases? Podemos construir modelos una clase vs. las demás, podemos utilizar una regresión softmax, o podemos utilizar una red neuronal. Una red neuronal está formada por capas, las cuales están conectadas entre sí tal cual neuronas:\n\n\n\nRed neuronal\n\n\nTenemos una capa de entrada, correspondiente a nuestros valores, seguida de una o más capas ocultas, compuestas por neuronas (perceptrones) que tienen funciones de activación, las cuales están conectadas por constantes multiplicadoras (pesos o weights) a las cuales se les añade una constante (sesgo o bias), cuyo resultado, finalmente, se envía a la capa de salida \\(y\\) (nuestras clases objetivo), resultando en la siguiente forma:\n\\[\ny = f(bias + \\sum(weight*input))\n\\]\nGráficamente:\n\n\n\nNeurona\n\n\n¿Suena familiar? Con solo una capa oculta y una función (\\(f\\)) de identidad tendríamos un modelo lineal cualquiera, solo que se ajusta mediante descenso estocástico de gradiente (fuera de esta discusión) en vez de mínimos cuadrados o máxima verosimilitud. Si esa función \\(f\\) la hacemos una función sigmoide (logística), tenemos entonces una regresión logística para más de dos clases. En un sentido estricto, esta aproximación no es un GLM (no tenemos una familia de distribución del error per-se), pero se puede considerar una generalización a más de dos clases. Desafortunadamente aquí tendré que romper la homogeneidad de la sesión e introducir tidymodels, porque hay un par de hiperparámetros a ajustar, pero no te preocupes, voy a asignar valores fijos para saltarnos el proceso de validación cruzada y optimización que ya conoces. Primero, hagamos una receta para preprocesar los datos:\n\npenguins_na &lt;- na.omit(palmerpenguins::penguins)\n# Formación de la receta\npeng_rec &lt;- recipe(island~.,\n                   data = penguins_na) |&gt;\n            #update_role(island, role = \"outcome\") |&gt; \n            step_select(!c(species, sex, year)) |&gt; \n            step_normalize(all_numeric_predictors()) |&gt; \n            step_dummy(all_nominal_predictors())\n            \n# Obtener parámetros para preprocesar\npeng_prep &lt;- peng_rec |&gt; prep()\n\n# Preprocesar los datos\npeng_juiced &lt;- peng_prep |&gt; juice()\n\nAhora especifiquemos el modelo. Aquí el argumento penalty especifica el grado de penalización del modelo, y mixture el tipo de penalización (Ridge o Lasso), tal y como vimos en las regresiones penalizadas en el Capítulo 18. De hecho, también utilizaremos la librería glmnet. Algo que no mencioné en ese capítulo fue que mixture puede tomar cualquier valor entre 0 y 1, y que cuando 0 &lt; mixture &lt; 1 tenemos un modelo de “red elástica”. ¿Qué es eso? Simplemente una mezcla entre ambas penalizaciones donde, evidentemente, entre más nos acerquemos a 1 más “Lasso” será el modelo. Obviamente debemos de optimizar al menos uno de los dos, pero eso ya vimos como hacerlo ;).\n\nmnom_reg &lt;- multinom_reg(penalty = 1,\n                         mixture = 0) |&gt;\n            set_engine(\"glmnet\")\n\nmnom_wf &lt;- workflow() |&gt;\n           add_recipe(peng_rec) |&gt;\n           add_model(mnom_reg)\n\nmnom_fit &lt;- mnom_wf |&gt; fit(data = penguins_na)\n\nmnom_res &lt;- mnom_fit |&gt; extract_fit_parsnip()\nmnom_res |&gt;\n  tidy() |&gt;\n  pivot_wider(names_from = term,\n              values_from = estimate)\n\n\n  \n\n\n\nAlgo importante a tener en cuenta es que al ajustar este modelo tomamos una clase como referencia (igual que con un predictor categórico como vimos en el Capítulo 18) y entonces esa no tiene coeficientes. ¿Es un problema? Sí y no. Usualmente solo nos interesa saber qué variables son más importantes para la clasificación; es decir, qué variables son “más diferentes” entre nuestras clases, para lo cual podemos utilizar vip como igual que en el Capítulo 17:\n\nmnom_res |&gt; vip::vip(geom = \"col\") +\n            labs(title = \"Importancia de variables\",\n                 y = element_blank())\n\n\n\n\n\n\n\n\nEsto sería todo para esta clase de GLM, aunque no quiere decir que sean los únicos. Si te interesa modelar el tiempo entre eventos puedes utilizar un modelo Gamma, puedes cambiar la relación entre la media y la varianza de la regresión para conteos utilizando un modelo Quasi-Poisson en vez de un modelo con distribución binomial negativa (ver lecturas recomendadas), entre otros. Puedes también, como vimos en el último ejemplo, aplicar GLMs penalizados utilizando glmnet, solo hay que especificar la familia correspondiente.",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Modelos Lineales Generalizados</span>"
    ]
  },
  {
    "objectID": "c19_glm.html#ejercicio",
    "href": "c19_glm.html#ejercicio",
    "title": "19  Modelos Lineales Generalizados",
    "section": "19.8 Ejercicio",
    "text": "19.8 Ejercicio\nNo podíamos terminar sin un ejercicio, entonces esta vez considera utilizar un GLM para predecir las edades de los abulones que utilizamos en el Capítulo 18 y responde:\n\n¿Qué familia del error utilizarías? ¿Hay opciones? ¿Considerarías un modelo penalizado/regularizado? Justifica tu respuesta.\nConstruye el/los GLM(s) que propusiste. ¿Cuál es el mejor modelo?\nCompara el ajuste del modelo final con la RLM que construimos en el Capítulo 18. ¿Mejoró?\n\n\n\n\n\nKuhn M, Silge J. 2022. Tidy Modeling with R. O’Reilly.",
    "crumbs": [
      "Técnicas Multivariadas",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Modelos Lineales Generalizados</span>"
    ]
  },
  {
    "objectID": "s06_despedida.html",
    "href": "s06_despedida.html",
    "title": "20  Despedida",
    "section": "",
    "text": "Con esto llegamos al final del curso. ¡Muchas felicidades! Acabas de pasar por un camino bastante escabroso, con algunas cosas bastante digeribles, y otras que son un dolor de cabeza, pero espero que el contenido haya sido de tu agrado, que te hayas llevado algo y, sobre todo, que lo aprendido te sea útil. En este momento cuentas con bases sólidas tanto para adentrarte más en cualquiera de los temas aquí vistos como para incursionar en otros temas. Una recomendación personal es revisar un paradigma de inferencia diferente: la Inferencia Bayesiana. Puedes también adentrarte más en el área del aprendizaje automatizado, pues es un mundo con un potencial enorme de aplicación a problemas biológicos.\nPor último, recuerda que no porque podamos hacer algo quiere decir que debamos de hacerlo. Nada me daría más gusto que saber que el curso te motivó a aplicar en tus análisis alguna de las técnicas que aquí vimos, pero siempre pregúntate si es pertinente para responder la pregunta que quieres responder.\nSin más que agregar, te deseo lo mejor, hoy y siempre.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Despedida</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Referencias",
    "section": "",
    "text": "Anderson MJ. 2005. Distance-based tests for homogeneity of multivariate\ndispersions. Biometrics 62:245–253. DOI: 10.1111/j.1541-0420.2005.00440.x.\n\n\nArmhein V, Greenland S, McShane B. 2019. Scientists rise up against\nstatistical significance. Nature 567:305–307. DOI: 10.1038/d41586-019-00857-9.\n\n\nBaak M, Koopman R, Snoek H, Klous S. 2019. A new correlation coefficient\nbetween categorical, ordinal and interval variables with\nPearson characteristics. ArXiV. DOI: 10.48550/arxiv.1811.11440.\n\n\nBotta S, Secchi ER, Muelbert MMC, Danilewicz D, Negri MF, Cappozzo HL,\nHohn AA. 2010. Age and growth of franciscana dolphins, pontoporia\nblainvillei (cetacea: Pontoporiidae) incidentally caught off\nsouthern brazil and northern argentina. Journal of the Marine\nBiological Association of the United Kingdom 90:1492–1500. DOI: 10.1017/S0025315410001141.\n\n\nBox GE. 1976. Science and statistics. Journal of the American\nStatistical Association 71:791–799. DOI: 10.1080/01621459.1976.10480949.\n\n\nBox GE. 1979. Science and statistics. ournal of the American\nStatistical Association 71:791–799. DOI: 10.1080/01621459.1976.10480949.\n\n\nBreiman L. 2001. Random forests. Machine Learning 45:5–32. DOI:\n10.1023/A:1010933404324.\n\n\nBreusch TS, Pagan AR. 1979. A simple test for\nheteroscedasticity and random coefficient variation.\nEconometrica 47:1287–1294.\n\n\nCailliet GM, Smith WD, Mollet HF, Goldman KJ. 2006. Age and growth\nstudies of chondrichthyan fishes: The need for consistency in\nterminology, verification, validation, and growth function fitting.\nEnvironmental Biology of Fishes 77:211–228. DOI: 10.1007/s10641-006-9105-5.\n\n\nCairo A. 2012. The functional art. Berkeley, USA: New Riders,\nPearson Education.\n\n\nCarvajal G, Maucec M, Cullick S. 2018. Components\nof artificial intelligence and data analytics. In:\nIntelligent digital oil and gas fields. Concepts, collaboration, and\nright-time decisions. Cambridge, Massachusetts, USA: Gulf\nProfessional Publishing, 101–148. DOI: 10.1016/B978-0-12-804642-5.00004-9.\n\n\nChen S, Watanabe S. 1989. Age dependence of natural mortality\ncoefficient in fish population dynamics. NIPPON SUISAN\nGAKKAISHI 55:205–208. DOI: 10.2331/suisan.55.205.\n\n\nEnríquez-García AB, Cruz-Escalona V, Carriquiry JD, Ehemann NR,\nMejía-Falla PA, Marín-Enríquez E, Treinen-Crespo C, Vélez-Tacuri JR,\nNavia AF. 2023. Trophic assessment of three sympatric batoid species in\nthe southern Gulf of California.\nPeerJ 11:e16117. DOI: 10.7717/peerj.16117.\n\n\nEnríquez-García AB, Villegas-Zurita F, Tripp-Valdez A, Moreno-Sánchez\nXG, Galván-Magaña F, Elorriaga-Verplancken FR. 2022. Foraging\nsegregation between spotted (stenella attenuata) and spinner\n(stenella longirostris) dolphins in the mexican south pacific.\nMarine Mammal Science 38:1070–1087. DOI: 10.1111/mms.12912.\n\n\nGerrodette T. 2011. Inference without significance: Measuring support\nfor hypotheses rather than rejecting them. Marine Ecology\n32:404–418. DOI: 10.1111/j.1439-0485.2011.00466.x.\n\n\nGompertz B. 1832. On the nature of the function expressive of\nthe law of human mortality, and on a new mode of determining the value\nof life contingencies. Phylosophical Transcriptions of the Royal\nSociety of London 123:513–585.\n\n\nHaig BD. 2010. What is a spurious corelation? Understanding\nStatistics 2:125–132. DOI: 10.1207/S15328031US0202_03.\n\n\nHöfer T, Przyrembel H, Verleger S. 2004. New evidence for the\ntheory of the stork. Paediatric and Perinatal Epidemiology\n18:88–92.\n\n\nHosmer DWJr, Lemeshow S, Sturdivant RX. 2013. Applied logistic\nregression. John Wiley & Sons, Inc. DOI: 10.1002/9781118548387.\n\n\nJames G, Witten D, Hastie T, Tibshirani R (eds.). 2013. New York:\nSpringer.\n\n\nKaufman S, Rosset S, Perlich C, Stitelman O. 2012. Leakage in data\nmining: Formulation, detection, and avoidance. ACM Transactions on\nKnowledge Discovery from Data 6. DOI: 10.1145/2382577.2382579.\n\n\nKing AP, Eckersley RJ. 2019. Chapter 8 - inferential statistics v:\nMultiple and multivariate hypothesis testing. In: King AP, Eckersley RJ\neds. Statistics for biomedical engineers and scientists.\nAcademic Press, 173–199. DOI: https://doi.org/10.1016/B978-0-08-102939-8.00017-7.\n\n\nKnuth DE. 1984. Literate programming. The Computer Journal\n27:97–111. DOI: 10.1093/comjnl/27.2.97.\n\n\nKuhn M, Silge J. 2022. Tidy modeling\nwith r. O’Reilly.\n\n\nMacArthur RH. 1957. On the relative abundance of bird species.\nProceedings of the National Academy of Sciences USA 43:293–295.\nDOI: 10.1073/pnas.43.3.293.\n\n\nMatloff N. 2020. Teaching\nR in a kinder, gentler, more effective manner: Teach\nbase-R, not just the tidyverse.\n\n\nMeyer-Baese A, Schmid V. 2014. Chapter 7 - Foundations of\nneural networks. In: Meyer-Baese A, Schmid V eds. Pattern\nrecognition and signal analysis in medical imaging. Oxford:\nAcademic Press, 197–243. DOI: 10.1016/B978-0-12-409545-8.00007-8.\n\n\nPal R. 2017. Regression trees, random forests, probabilistic trees,\nstacked generalization, probabilistic random forests, weight\noptimization. In: Pal R ed. Predictive modeling of drug\nsensitivity. Academic Press, 149–188. DOI: 10.1016/B978-0-12-805274-7.00007-5.\n\n\nPerneger TV. 1998. What’s wrong with bonferroni adjustments.\nBMJ 316:1236–1238. DOI: 10.1136/bmj.316.7139.1236.\n\n\nR Core Team. 2022. R: A\nlanguage and environment for statistical computing. Vienna,\nAustria: R Foundation for Statistical Computing.\n\n\nReshef DN, Reshef YA, Finucane HK, Grossman SR, McVean G, Turnbaugh PJ,\nLander ES, Mitzenmacher M, Sabeti PC. 2011. Detecting novel associations\nin large datasets. Science 334:1518–1524. DOI: 10.1126/science.1205438.\n\n\nRicker WE. 1979. Growth rates and models. In: Hoar WS, Randall DJ, Brett\nJR eds. Fish physiology. 677–747.\n\n\nRougier NP, Droettboom M, Bourne PE. 2014. Ten simple rules for better\nfigures. PLoS computational biology 10:e1003833–7. DOI: 10.1371/journal.pcbi.1003833.\n\n\nSavage LJ. 1954. The foundations of statistics. John Wiley\n& Sons.\n\n\nSawyer SF. 2013. Analysis of Variance: The\nFundamental Concepts. Journal of Manual & Manipulative\nTherapy 17:27E–38E. DOI: 10.1179/jmt.2009.17.2.27e.\n\n\nShirkhorshidi AS, Aghabozorgi S, Wah TY. 2015. A comparison study on\nsimilarity and dissimilarity measusres in clustering continuous data.\nPLoS ONE 10:e0144059. DOI: 10.1371/journal.pone.0144059.\n\n\nSies H. 1988. A new parameter for sex education.\nNature 332:495.\n\n\nStrobl C, Malley J, Tutz G. 2009. An introduction to recursive\npartitioning: Rationale, aplication and characteristics of\nclassification and regression trees, bagging and random forests.\nPhysiological Methods 14:323–348. DOI: 10.1037/a0016973.\n\n\nTufte E. 1983. The visual display of quantitative information.\nCheshire, Connecticut: Graphics Press.\n\n\nWilkinson L. 2005. The grammar of graphics. USA: Springer.\n\n\nWolter K, Timlin MS. 1998. Measuring the strength of ENSO\nevents: How does 1997/98 rank. Weather 53:315–324.\n\n\nZar JH. 2010. Biostatistical Analysis. Prentice\nHall.\n\n\nZuur AF, Ieno EN, Walker N, Saveliev AA, Smith GM. 2009. Mixed effects models and extensions in ecology with\nR. Springer. DOI: 10.1007/978-0-387-87458-6.",
    "crumbs": [
      "Referencias"
    ]
  }
]