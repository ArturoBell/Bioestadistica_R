[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bioestadística Aplicada con R y RStudio",
    "section": "",
    "text": "Prefacio\n¡Hola! Te doy la bienvenida al curso Bioestadística Aplicada con R y RStudio, ofertado por Dr. Plancton.\nEste es el libro de acompañamiento del curso. Aquí encontrarás tanto la teoría como el código que se aborda en el curso, dispuestos en una manera que facilita su lectura, y cuyo objetivo es simplemente proveer una versión lista para ser revisada en cualquier momento, sin necesidad de iniciar RStudio. Cada sección del libro tiene enlaces a los videos correspondientes, en caso de que prefieras ver y escuchar la explicación.\nTen en cuenta que puede existir un desfase entre el material del libro y los videos. La razón es que el material se actualizará para mejorar el contenido, la entrega o la explicación siempre que sea posible, lo cual es fácil de hacer en el libro y las libretas con el código, pero no en los videos; sin embargo, en el momento en el que el desfase sea lo suficientemente grande, también se actualizarán los videos de las secciones correspondientes."
  },
  {
    "objectID": "index.html#objetivos-de-aprendizaje",
    "href": "index.html#objetivos-de-aprendizaje",
    "title": "Bioestadística Aplicada con R y RStudio",
    "section": "Objetivos de aprendizaje",
    "text": "Objetivos de aprendizaje\nEl objetivo de este curso es que seas capaz no solo de implementar distintas técnicas de análisis de datos utilizando R, sino que también puedas ser crítico con tus resultados y que minimices, en la medida de lo posible, el sesgo algorítmico. En este curso aprenderás los fundamentos detrás de las pruebas vistas, en donde abordaremos la teoría desde un punto de vista práctico, buscando que puedas formarte una intuición propia. También trataremos de desmitificar el valor de p y, sobre todo, cómo no interpretarlo. En las partes más abstractas te adentrarás en el aprendizaje automatizado, y verás que hay vida más allá del \\(R^2\\) (regresiones) y la exactitud (clasificaciones). Adicionalmente, y no por ello menos importante, te adentrarás en la visualización de datos y aprenderás a realizar reportes que faciliten compartir y leer tu trabajo.\nUno de los errores más comunes al enseñar estadística con R es tratar de enseñar ambas cosas al mismo tiempo. En este curso, R es solo un medio y no un fin; es decir, no es un curso de programación en R tanto como es un curso de ciencia de datos aplicada; sin embargo, hay una amplia introducción al lenguaje al inicio, y espero que el explicar línea por línea el código te permita familiarizarte con el lenguaje. Dicho esto, el curso fue diseñado para que personas con nulo o muy poco conocimiento de programación en general puedan seguirlo, y siempre podrás contactarnos en caso de que no hayamos explicado algo adecuadamente."
  },
  {
    "objectID": "index.html#discord-y-acceso-al-material",
    "href": "index.html#discord-y-acceso-al-material",
    "title": "Bioestadística Aplicada con R y RStudio",
    "section": "Discord y acceso al material",
    "text": "Discord y acceso al material\nSi bien es cierto que puedes utilizar y acceder a todo el material del curso desde esta página, te recomiendo encarecidamente unirte al servidor de Discord utilizando tu enlace único (enviado a tu correo al registrarte), pues ahí podrás interactuar no solamente con tus compañeros y compañeras, sino también con tu profesor. Un último comentario, NO es necesario que instales el cliente de Discord en tu computadora o dispositivo móvil, aunque si lo haces podrás recibir las notificaciones sobre modificaciones que se hagan al material."
  },
  {
    "objectID": "index.html#estructura-del-libro",
    "href": "index.html#estructura-del-libro",
    "title": "Bioestadística Aplicada con R y RStudio",
    "section": "Estructura del libro",
    "text": "Estructura del libro\nEste libro es, como ya mencioné, el material utilizado en el curso y, por lo tanto, está dividido en distintas secciones, cada una formada por distintos capítulos. Esta división trata de seguir un órden lógico, desde los conceptos y temas más fundamentales hasta los procedimientos más abstractos y, por lo tanto, todos los temas están conectados: cada capítulo asume que ya no tienes ningún problema con los anteriores.\nEn cuanto a su uso, en la esquina superior izquierda hay una barra de búsqueda, por si deseas realizar alguna consulta rápida. Debajo tienes todas las secciones y capítulos, en la porción central el contenido y a la derecha la tabla de contenidos del tema que estás leyendo. Referente al contenido, al inicio de cada capítulo tienes el video al tema correspondiete (ojo al desfase), mientras que en el texto encontrarás algunas anotaciones con información relevante al tema:\n\n\n\n\n\n\nNote\n\n\n\nAquí habrá información adicional al tema que se está tratando.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAquí habrá puntos/conceptos/procedimientos con los cuales hay que tener mucho cuidado y la razón.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAquí habrá información que es sumamente importante que tengas presente.\n\n\nTambién encontrarás resaltados especiales para diferenciar claramente si estoy haciendo referencia a código en texto, funciones, librerías o software o a algunos conceptos clave. Todo el texto que veas con formato de hipervínculo te llevará a la referencia correspondiente: una figura, un capítulo, un enlace externo o una referencia bibliográfica."
  },
  {
    "objectID": "index.html#compartir-o-no-compartir-he-ahí-el-dilema",
    "href": "index.html#compartir-o-no-compartir-he-ahí-el-dilema",
    "title": "Bioestadística Aplicada con R y RStudio",
    "section": "¿Compartir o no compartir? He ahí el dilema",
    "text": "¿Compartir o no compartir? He ahí el dilema\nNo hay nada que te impida compartir este libro y con ello todo el curso. De hecho, nada me daría más gusto que saber que el curso te fue lo suficientemente agradable y útil como para dirigirlo a alguien más, pero te pido que lo hagas lo menos posible. Tu acceso al curso es vitalicio, pero ni el contenido ni el material son estáticos, por lo que dependemos de las nuevas ventas para poder seguir mejorándolo y mantenerlo en línea. Si quieres compartirlo puedes utilizar tu código de referido, el cuál da un descuento adicional a cualquier otro descuento o promoción.\nDicho esto, te doy nuevamente la bienvenida y espero que el curso cubra con tus expectativas (y un poco más)."
  },
  {
    "objectID": "s0_preparacion.html#r",
    "href": "s0_preparacion.html#r",
    "title": "Preparación",
    "section": "R",
    "text": "R\nEvidentemente, lo primero que deberás instalar es R. Puedes encontrar el instalador de la versión más reciente para tu sistema operativo en CRAN (The Comprehensive R Archive Network).\nSimplemente da click en el enlace al servidor más cercano a tu ubicación, por ejemplo https://cran.itam.mx/.\nDespués simplemente descarga la versión que corresponde a tu sistema operativo.\nIndependientemente del sistema operativo que utilices, la instalación consiste en ejecutar el instalador y seguir los pasos indicados en su ventana, dando click en aceptar y continuar según sea necesario."
  },
  {
    "objectID": "s0_preparacion.html#rstudio",
    "href": "s0_preparacion.html#rstudio",
    "title": "Preparación",
    "section": "RStudio",
    "text": "RStudio\nUna vez instalado R podemos instalar RStudio. Más adelante veremos cuál es la diferencia entre ambos, pero por el momento piensa en RStudio como una ventana para R. Primero, dirígete a https://posit.co/download/rstudio-desktop/.\n\n\n\n\n\n\nNote\n\n\n\n¿Por qué el enlace para descargar RStudio es de posit? En octubre del 2022 la empresa se volvió posit. Aquí puedes leer más al respecto, pero el resumen es que el nombre de la empresa cambió, pero el nombre de su ambiente integrado de desarrollo (IDE, RStudio) se mantendrá igual.\n\n\nEn la sección All Installers ubica tu sistema operativo y descarga el instalador correspondiente\nUna vez descargado, ejecuta el instalador y sigue los pasos que aparecen en pantalla."
  },
  {
    "objectID": "s0_preparacion.html#quarto-y-tinytex",
    "href": "s0_preparacion.html#quarto-y-tinytex",
    "title": "Preparación",
    "section": "Quarto y tinytex",
    "text": "Quarto y tinytex\nCon estas dos instalaciones sería más que suficiente para empezar a trabajar, sin embargo, podemos ir más allá y utilizar RStudio para crear reportes, artículos científicos, libros (incluyendo tesis), páginas web, presentaciones, y más. Esto solía (y puede) hacerse con librerías como rmarkdown, distilled o bookdown; sin embargo, tenemos la siguiente generación: Quarto. En el curso veremos una introducción para hacer reportes y, de hecho, este material fue escrito en documentos Quarto. Al igual que en los casos anteriores, dirígete a la página de descargas y descarga e instala la versión correspondiente a tu sistema operativo.\n\n\n\n\n\n\nImportant\n\n\n\nNO verás un ejecutable (acceso directo) después de que se haya realizado la instalación. Para utilizar Quarto necesitaremos de a) la línea de comandos o b) una interface, que en nuestro caso es RStudio. Debajo de los enlaces de descarga hay más información sobre cómo utilizarlo, incluyendo detalles sobre su uso en VS Code, Jupyter y en un editor de textos (línea de comandos). Puedes explorarlos, aunque aquí verás los detalles correspondientes para RStudio.\n\n\nAdicional a Quarto, y si deseas exportar tus documentos a archivos PDF, es necesario instalar una distribución de LaTeX. Si ya cuentas con alguna, puedes saltarte este paso, de lo contrario puedes o realizar la instalación completa o simplemente instalar TinyTeX desde aquí. Si optas por instalar TinyTeX y no tienes nada de experiencia con R, te recomiendo seguir los pasos de su página una vez que hayas pasado por el tema Bases de R. No te preocupes, hay un recordatorio al final de la sesión."
  },
  {
    "objectID": "s0_preparacion.html#instalaciones-opcionales",
    "href": "s0_preparacion.html#instalaciones-opcionales",
    "title": "Preparación",
    "section": "Instalaciones “opcionales”",
    "text": "Instalaciones “opcionales”\nAdicionalmente, te recomiendo encarecidamente (por no decir te solicito) que instales, dependiendo de tu sistema operativo, algunas herramientas. Tienes la descripción de cada una, así como desde donde realizar su instalación:\n\nWindows: Rtools. Esta es una serie de herramientas que nunca vas a ver cuando se utilicen, pero que son un dolor de cabeza si no cuentas con ellas y tienes la pésima fortuna de necesitar compilar un paquete desde su código fuente, o la dicha de querer publicar un paquete. A final de cuentas permiten justamente eso, administrar esas ejecuciones. Solía ser parte de la instalación de R en Windows, pero ya no es así, por lo que recomiendo la instales siguiendo las instrucciones oficiales (selecciona la versión más nueva de Rtools disponible).\nmacOS: XQuartz. R es un lenguaje con un enfoque muy fuerte hacia la generación de gráficos, lo cual permite hacer visualizaciones de muy alta calidad. Desafortunadamente, algunas cosas se apoyan del sistema de ventanas X.Org X Window System y que, de no estar disponible, pueden dar algunos dolores de cabeza. Apple solía incluir una implementación en la aplicación X11 en las versiones de OS X 10.5 a 10.7, pero ya no en sistemas más modernos. Es ahí donde entra el proyecto XQuartz. El instalador está disponible en la página del proyecto.\nmacOS: Xcode Command Line Tools. Si bien es cierto que macOS juega un papel importante en algunos círculos de desarrollo de software, no incluye algunas herramientas necesarias para el mismo objetivo que Rtools en Windows: compilar desde fuente. En este caso, hay dos formas de instalar lo que necesitamos, ambas provistas por Apple: a) instalar el entorno de desarrollo Xcode (40 GB) o, mi recomendación si no desarrollas aplicaciones para SOs de Apple, b) instalar las herramientas de línea de comando. Xcode lo puedes instalar directamente desde tu App Store, mientras que las herramientas de línea de comando desde el Terminal con el comando xcode-select --install. Puedes abrir el terminal utilizando Spotlight (CMD + espacio), o ejecutando su aplicación, ubicada en Aplicaciones -> Utilidades.\n\nUna vez instalado todo esto estás más que listo para avanzar con el programa del curso. Recuerda que si tuviste algún problema siempre puedes contactarme en el servidor de Discord del curso."
  },
  {
    "objectID": "s01_biolcdatos.html#objetivo-de-aprendizaje",
    "href": "s01_biolcdatos.html#objetivo-de-aprendizaje",
    "title": "Biología, Ciencia de Datos y R",
    "section": "Objetivo de aprendizaje",
    "text": "Objetivo de aprendizaje\nEn esta sección del curso comenzarás reflexionando sobre el cómo se han aplicado tradicionalmente las técnicas estadísticas a los problemas biológicos. Después, te introducirás a RStudio, la elaboración de reportes con Quarto y posteriormente a las bases del lenguaje R y su dialecto tidy."
  },
  {
    "objectID": "c01_biolcdatos.html",
    "href": "c01_biolcdatos.html",
    "title": "1  Ciencia de datos y biología",
    "section": "",
    "text": "¡Hola! En esta primera clase del curso Bioestadística Aplicada con R y RStudio abriremos con la relación emergente entre la biología y la ciencia de datos.\nPara comenzar, hablaremos un poco sobre lo que conocemos como estadística. El origen de esta disciplina se encuentra en la necesidad de los gobiernos de conocer cuál es el estado de su población, de ahí su raíz etimológica “relativo al Estado”. Sin embargo, en la actualidad, existen diversas definiciones y opiniones sobre lo que representa. Hay quienes mencionan que es la primera de las ciencias inexactas, mientras otras personas la consideran como la ciencia que nos permite cambiar nuestras ideas ante la incertidumbre. Si bien es cierto que estas visiones son en apariencia muy diferentes, ambas son perspectivas bastante válidas: la primera hace referencia a la relativa facilidad con la que se pueden manipular los datos o las pruebas (intencionalmente o no) para llegar al resultado que nosotros deseemos y la segunda a que nos permite tomar decisiones aún sin conocer en su totalidad un fenómeno. Si deseas leer sobre algunos mitos y realidades de la estadística, sigue este enlace.\nUna definición más formal es la de la Real Academia de la Lengua Española: “Rama de la matemática que utiliza grandes conjuntos de datos numéricos para obtener inferencias basadas en el cálculo de probabilidades”. De esta definición podemos retomar algunas ideas claves: la primera es que es una rama de la matemática, por lo cual nuestra resolución de problemas debe basarse en un razonamiento lógico bajo la cobertura de las matemáticas, por lo que los procedimientos y resultados deben de ser expresados sin ambigüedades. La siguiente es que requiere de grandes conjuntos de datos, lo cual hace referencia a un tema que será abordado más adelante en el curso: la representatividad. Por último, podemos también reconocer su objetivo, el cual es permitirnos obtener conclusiones a partir de este conjunto de datos. A partir de esta definición podemos considerar a la bioestadística como la aplicación de la estadística a problemas biológicos, desde la estimación del tamaño poblacional de una especie, comparaciones de tallas entre sitios de muestreo, modelar el crecimiento corporal, entre muchas otras.\n\n\n\n\n\n\nNote\n\n\n\nDebido a que la bioestadística está formada por una gran cantidad de procedimientos, muchos de ellos específicos a áreas particulares del conocimiento, en este curso abordaremos los fundamentos básicos de la estadística y técnicas de uso general.\n\n\nLlegados a este punto, quiero introducir otro concepto: la ciencia de datos. En pocas palabras, esta estudia los métodos para extraer información sobre los datos y, en última instancia, facilitar la toma de decisiones. Sus objetivos son i) describir los datos, comparar entre grupos/poblaciones/clases, ordenar o clasificar observaciones y, en última instancia, predecir un resultado futuro a partir de los datos con las que se cuenta.\nAl igual que con la definición de estadística, existen distintas definiciones y visiones, pero por el momento analicemos este diagrama que nos habla sobre las habilidades de un individuo realizando un procedimiento a sus datos y cuál sería el resultado:\n\n\n\nFigure 1.1: Ciencia de datos\n\n\n\nConsideremos las habilidades de hackeo como habilidades de programación y el conocimiento técnico necesario para la aplicación de distintas técnicas de análisis de datos, a la experiencia como el conocimiento que el individuo tenga sobre el fenómeno que está analizando y al conocimiento sobre matemáticas y estadística como el conocimiento teórico sobre las técnicas que está aplicando.\nEn la intersección de las habilidades de hackeo y la experiencia se encuentra una “zona de peligro”, la cual también podemos definir como el área de la “caja negra”, es decir, el área en la que se aplican pruebas y métodos sin conocer sus fundamentos y las conclusiones o inferencias están en función de la experiencia y los prejuicios de quien las realice, sin considerar su pertinencia.\nPor otra parte, en la intersección de la experiencia y el conocimiento de estadística se encuentra la investigación tradicional; es decir, existe un conocimiento teórico sobre las pruebas que se están aplicando y la experiencia para poder realizar inferencias sobre los datos considerando las limitaciones, fortalezas y debilidades de las técnicas; sin embargo, la visión sobre el problema se encuentra normalmente limitada a las pruebas tradicionales, lo cual a su vez limita el tipo de análisis y preguntas que se puedan resolver.\nEn la intersección de las habilidades de hackeo y el conocimiento sobre estadística se encuentra la disciplina del “aprendizaje automatizado”, algo que discutiremos más profundamente en la sección de Técnicas Multivariadas, pero que hace referencia al extraer relaciones entre los datos de manera eficiente, independientemente de si estas relaciones son causales o no.\nFinalmente, en el centro, recibiendo entradas de las tres áres encontramos a la ciencia de datos.\n\nTomando esto en consideración, te propongo hacer un ejercicio de reflexión sobre la pertinencia de aproximarnos a los problemas biológicos de una manera más flexible, eficiente, informada y que considere también la experiencia del investigador, más allá del modo tradicional e inamovible que nos ha llevado a pensar de manera incorrecta que la falta de significancia estadística es falta de significancia biológica o viceversa. ¿Es esto un problema muy grave? Esa pregunta la dejaremos para un tema donde es más adecuado abordarla: pruebas de hipótesis, en especial al hablar sobre los usos y abusos del famosísimo (¿infame?) valor de p.\nCon esta idea terminamos la primera clase del curso. Nos vemos en la siguiente para familiarizarnos con RStudio y cómo elaborar reportes utilizando Quarto."
  },
  {
    "objectID": "c02_intro_rs.html",
    "href": "c02_intro_rs.html",
    "title": "2  Introducción a RStudio y Quarto",
    "section": "",
    "text": "3 Entra a Quarto\nAhora sí, podemos dedicarle nuestra atención a Quarto. Antes mencioné que era un sistema para la creación de documentos científicos y técnicos, que se instala aparte de R y RStudio (lo hicimos ya en la sesión anterior), y que es sumamente flexible. Vimos también un ejemplo de cómo crear un documento Quarto y un poco de cómo interactuar con él. Ahora entremos a todos los detalles. Retomemos nuestro documento de ejemplo (Figure 2.11)."
  },
  {
    "objectID": "c02_intro_rs.html#r-vs.-rstudio",
    "href": "c02_intro_rs.html#r-vs.-rstudio",
    "title": "2  Introducción a RStudio y Quarto",
    "section": "2.1 R vs. RStudio",
    "text": "2.1 R vs. RStudio\nAntes de volar y pretender formar un sitio web o una tesis, comencemos hablando de RStudio. En la sección de preparación instalamos tres cosas diferentes: R, RStudio, y Quarto. Olvidemos a este último por un momento y centrémonos en los dos primeros. R es un lenguaje de programación. Como tal, se ejecuta en consola, ya sea el terminal (macOS/Linux) o el interpretador de comandos cmd en Windows. Lo único que veremos si abrimos/ejecutamos R per-se es una ventana como la siguiente:\n\n\n\nFigure 2.1: Consola de R (R GUI)\n\n\nEs decir, solamente veremos nuestra consola, compuesta por una descripción de la versión de R que estamos utilizando y un prompt (el símbolo >) que nos presiona a darle a la computadora una instrucción. Más allá de ser una interfaz extremadamente simple, no está pensada para el desarrollo de reportes como los que nosotros realizamos. No podemos escribir texto libre, ni tampoco podemos guardar nuestro progreso. Para eso habría que abrir un script, pero hay una mejor alternativa que nos permite hacer eso y mucho más: RStudio. Por el momento hasta aquí vamos a llegar con R, pero no te preocupes, le vamos a dedicar mucho más tiempo posteriormente."
  },
  {
    "objectID": "c02_intro_rs.html#el-ide-rstudio",
    "href": "c02_intro_rs.html#el-ide-rstudio",
    "title": "2  Introducción a RStudio y Quarto",
    "section": "2.2 El IDE RStudio",
    "text": "2.2 El IDE RStudio\nMientras que R es un lenguaje de programación ejecutable en consola, RStudio es un ambiente gráfico de desarrollo (IDE). ¿Qué significa? Que es una interfaz gráfica que nos permite no solo ejecutar nuestro código línea a línea, sino que también incluye otros páneles que nos facilitan enormemente la existencia y, además, abre otras puertas para la creación de documentos como este libro. Vayamos por partes.\n\n2.2.1 La ventana de RStudio\nAl abrir RStudio por primera vez te vas a topar con una ventana como la siguiente:\n\n\n\nFigure 2.2: Ventana de RStudio\n\n\nYa sé, ya sé, no se ve mucho más amable que la ventana de R. Es más, se ve mucho más intimidante porque ahora tenemos la consola y otros espacios. Al ser un IDE, RStudio incluye elementos gráficos para todo lo que pudiéramos llegar a necesitar mientras desarrollamos nuestros análisis, entonces vamos a descomponer esta ventana panel por panel, de arriba a abajo y de izquierda a derecha.\n\n\n\n\n\n\nImportant\n\n\n\nSi no te aparecen los cuatro páneles no te preocupes, simplemente ve a File -> New File -> R script. Con eso se abrirá el panel faltante.\n\n\n\n2.2.1.1 El editor\nEl primer panel es el editor:\n\n\n\nFigure 2.3: Editor en RStudio\n\n\nEste es, como el nombre sugiere, un editor de textos, que no debemos de confundir con procesador de palabras (i.e., Word o similares). En él vamos a poder escribir sripts o libretas que contengan la serie de pasos que realizamos durante nuestro análisis. Cada una de las pestañas en este panel es siempre un documento de texto simple, independientemente de si es un script o una libreta. Esto tiene varias ventajas, pero la más importante es que nos podemos llevar esos archivos a cualquier computadora y estar bastante seguros de que podremos, cuando menos, ver su contenido y editarlo sin preocuparnos por problemas de compatibilidad entre versiones del software o, peor aún, sistemas operativos (*ejem* Word *ejem*). Estos archivos de texto simple pueden, dependiendo del tipo de archivo, enviar instrucciones a R.\n\n\n2.2.1.2 La consola\nEl siguiente panel es la consola:\n\n\n\nFigure 2.4: Consola en RStudio\n\n\nEste panel es, literalmente, lo que veíamos al abrir R por sí solo; es decir, un espacio donde tenemos nuestro prompt y donde se ejecutarán nuestras instrucciones o líneas de comandos. Notarás que hay otras tres pestañas: una llamada Terminal, y otra llamada Background Jobs. Estas son interfaces a la terminal del sistema y a los trabajos que estemos ejecutando en segundo plano. Nuestra interacción con estas dos pestañas tiende a ser limitada (o incluso nula), salvo que realicemos algo muy especializado.\n\n\n2.2.1.3 Scripts: qué, cómo, cuándo y por qué.\nAntes de pasar al siguiente panel es importante hablar de los scripts, las libretas, e intentar hacer un poco de labor de convencimiento. Si has tenido un acercamiento previo a R/RStudio, es bastante probable que trabajar con scripts te sea familiar. Si no es así, un script de R es un archivo de texto con extensión .R en el que ponemos nuestro código línea a línea. Pensemos en un ejercicio en el que queremos primero sumar 1 y 1, y luego 2 y 2. Nuestro script en RStudio se vería así:\n\n\n\nFigure 2.5: Script en el editor\n\n\nSolo tenemos el código, no tenemos los resultados. ¿La razón? Aún no le hemos dicho a la computadora que queremos que las ejecute. ¿Cómo le decimos? Tenemos dos formas:\n\nEjecutar el script línea a línea, para lo que debemos posicionar nuestro cursor (dar click) sobre la línea a ejecutar, utilizar el atajo de teclado CMD + R en macOS/Linux o CTRL + R en Windows, o dar click sobre el botón Run que está cerca de la esquina superior derecha del panel.\nEjecutar el script completo, para lo que seleccionaríamos todo su contenido y utilizaríamos el mismo atajo de teclado o botón que antes.\n\nSea cual sea la opción que hayas escogido, la salida (el resultado) aparecerá en la consola:\n\n\n\nFigure 2.6: Script ejecutado en consola\n\n\n¿Cuál es el problema? El primero es que en el momento en el que cerremos RStudio esos resultados se van a perder, salvo que los hayamos guardado manualmente en algún lugar. El segundo tiene que ver con una falta de unificación: el código (las sumas) están por un lado, mientras que los resultados están en otro que, además, es volátil. El tercero se deriva de los dos anteriores: falta de legibilidad y reproducibilidad. No podemos hacer el reporte al mismo tiempo en que analizamos los datos y, si en algún momento volvemos al script, debemos de ejecutarlo todo nuevamente para ver los resultados. Suena engorroso, ¿no? Un cuarto problema es que no tenemos descripciones de nuestros resultados. Si ya has trabajado con estos archivos me vas a decir “para eso existen los comentarios”, a lo que yo te respondería que no, los comentarios no son para eso. Si no has trabajado con R ni ningún otro lenguaje de programación te preguntarás qué es un comentario. Bien, un comentario es un fragmento de texto no ejecutable; es decir, es algo que podemos escribir y pasarle a la consola pero que no se va a ejecutar. En R estos están dados por el operador #. Agreguemos un comentario a nuestro script con la palabra “Sumas” y ejecutémoslo todo nuevamente:\n\n\n\nFigure 2.7: Script con comentarios ejecutado en consola\n\n\nComo esperábamos, en la consola no hay una salida asociada a la instrucción # Sumas, por lo tanto puedo usar esos comentarios para describir mis resultados, ¿no? La respuesta es, como en muchas otras cosas, depende. O, mejor dicho, de que se puede, se puede, que debamos hacerlo, es otra historia. Los comentarios tienen la función de describir muy brevemente qué intención tiene el código, no escribir párrafos completos con el reporte de los resultados. Comentarios válidos son agregar al inicio del script quién lo escribió, qué hace el código contenido en él, un medio de contacto, y breves descripciones de qué se hace en cada línea, sin repetir el código en texto simple (no decir # Suma 1 y 1 si el código es 1+1, por ejemplo). Existe otro gran problema el cuál no es obvio en este ejercicio, pero que tiene que ver con la carga de datos en archivos dentro de nuestra computadora (archivos .csv o .xlsx, por ejemplo), pero eso lo veremos en un tema posterior. Por el momento veamos una alternativa que resuelve todos estos problemas.\n\n\n2.2.1.4 Libretas y reportes: Quarto\nAquí es donde entran Quarto y las libretas. Al instalar Quarto no instalamos un programa per-se, sino que instalamos una extensión a RStudio que es, y cito textualmente, “un sistema de publicación científica y técnica de código abierto construido sobre Pandoc”, que permite, citando nuevamente: i) crear contenido dinámico no solo con R sino con otros lenguajes de programación; ii) escribir documentos como texto plano; iii) publicar artículos, reportes, presentaciones, sitios web, blogs y libros de alta calidad en formatos HTML, PDF, MS Word, ePUB; y iv) escribir con markdown científico, incluyendo ecuaciones, citas, referencias cruzadas, páneles de figuras, anotaciones, diseños avanzados y más. ¿A que ya suena mejor que los scripts? Sin ir más lejos, todo el material que utilizaremos en este curso fue escrito en RStudio utilizando Quarto, y puedes ver la versión final en el sitio web de acompañamiento. Debido a que explicar Quarto es un tema que merece le dediquemos tiempo y estar más arriba que un subtema de IDE RStudio, vamos a dejarlo de lado por el momento, solo revisemos cómo crear un nuevo documento y las diferencias fundamentales con los scritps. Para crear un documento podemos ir a la barra de herramientas -> File -> New file -> Quarto document, o utilizar el botón correspondiente en la ventana de RStudio:\n\n\n\nFigure 2.8: Nuevo documento Quarto\n\n\nAl darle click nos va a aparecer la siguiente ventana para personalizar el documento:\n\n\n\nFigure 2.9: Opciones documento Quarto\n\n\nAquí añadirás el título de tu documento y opcionalmente el autor. Por el momento dejaremos todo lo demás tal y como está y daremos click en Create, lo que abrirá una pestaña nueva en el editor:\n\n\n\nFigure 2.10: Opciones documento Quarto\n\n\nLa pestaña se parece al contenido de un script, la única diferencia es un texto contenido entre ---. A esta parte la podemos identificar como el preámbulo del documento, y es un fragmento de nuestro documento escrito en formato YAML ¿Qué es eso y con qué se come? No te preocupes ahorita por eso, solo necesitas saber ahorita que vamos a desarrollar nuestro trabajo debajo del preámbulo. En la Figure 2.11 puedes ver un ejemplo básico de un documento Quarto con el código de nuestras sumas.\n\n\n\nFigure 2.11: Un documento Quarto básico\n\n\nUn ejemplo más claro del potencial de Quarto es, nuevamente, el material de este curso. Pero sigamos explorando la ventana de RStudio\n\n\n\n2.2.2 El ambiente de trabajo\nEl siguiente panel es lo que se conoce como el ambiente de trabajo (workspace), que nos da un listado de las cosas que le hemos dado a R para que recuerde (objetos). En la siguiente sesión hablaremos largo y tendido de esto, pero veamos un ejemplo donde, utilizando la consola, le digamos a R que recuerde el resultado de la suma 1 + 1, asignándolo a una referencia que arbitráriamente llamaré suma:\n\n\n\nFigure 2.12: Guardar un resultado en la consola\n\n\nAquí hay dos cosas a tener en cuenta: 1. La asignación se hizo con el operador <-. Esto es sumamente importante: en R guardamos cosas utilizando el operador <- y no =. 2. Al ejecutar la línea no obtuvimos el resultado. Esto es porque solo le dijimos que lo recordara, que lo anotara en un post-it, si quieres, no que nos lo mostrara. Si queremos que nos lo muestre solo tenemos que llamarlo por su nombre (ejecutar en la consola):\n\n\n\nFigure 2.13: Imprimir el resultado\n\n\n¿Qué tiene que ver esto con el ambiente de trabajo? Pues ahora ya no está vacío, ya tenemos una entrada en la lista, en donde se muestra el nombre del objeto y su valor. Conforme vayamos creando más objetos, más entradas tendrá esa lista. Prueba a crear un objeto que contenga el texto \"Hola mundo\" (ojo a las comillas) y dar click en su nombre en el ambiente de trabajo. ¿Qué ocurre?\n\n\n2.2.3 El ambiente gráfico\nEl último panel corresponde al ambiente gráfico. Este panel junta varios elementos a los que es más fácil acceder visualmente. La primera pestaña es un explorador de archivos. puedes dar click a cada carpeta para ver sus elementos, crear nuevas carpetas, y mucho más. En este curso no lo utilizaremos más que como una referencia visual de dónde están ubicados nuestros archivos.\n\n\n\nFigure 2.14: Explorador de archivos\n\n\nLa siguiente pestaña nos muestra la serie de gráficos que hemos ido generando. Podemos ejecutar en la consola el comando plot(cars) y verás que el gráfico se muestra en esta pestaña.\n\n\n\nFigure 2.15: Gráfico en el ambiente gráfico\n\n\nEn la siguiente pestaña encontraremos un listado de las librerías/paqueterías que tenemos instaladas, sus versiones, una breve descripción de para qué son, así como botones para instalarlas o actualizarlas.\n\n\n\nFigure 2.16: Paquetes\n\n\nDespués tenemos la pestaña de ayuda. Aquí podemos ver, valga la redundancia, ayuda sobre R, pero también sobre funciones en las que estemos interesados. Si queremos ver la ayuda de una función FUN podemos o utilizar el comando ?FUN Figure 2.17 o utilizar la barra de búsqueda de la pestaña\n\n\n\nFigure 2.17: ¡Ayuda!\n\n\nEn la siguiente pestaña tenemos un visor (Viewer), donde tendremos vistas previas de los documentos RMarkdown o Quarto con los que estemos trabajando. Tomando como ejemplo el documento Quarto que generamos antes, da click al botón Render con la flecha azul. ¿Qué obtienes?\nLa última pestaña es un visor de presentaciones, el cual no utilizaremos en este curso.\n\n\n2.2.4 Personalizando RStudio\nComo todo IDE, podemos personalizar la apariencia de RStudio. Para hacerlo simplemente ve a la barra de herramientas, luego en Tools -> Global Options. Te aparecerá la siguiente ventana\n\n\n\nFigure 2.18: Ventana de ajustes\n\n\nVamos a revisar algunas de las opciones que te recomiendo tener presentes. Puedes hacer los cambios y aplicarlos todos juntos al final con el botón Apply:\n\nGeneral\n\nBasic\n\nWorkspace: Restore .RData into workspace at startup & Save workspace to .RData on exit. ¿Quieres que cada que cierres RStudio todos los objetos de tu espacio de trabajo se guarden en un archivo, y que esos mismos se carguen la siguiente vez que abras RStudio? Personalmente no es algo con lo que esté de acuerdo, porque más frecuentemente que no vas a querer un ambiente limpio, por lo que estas opciones están desmarcada y en Never, respectivamente.\nHistory: La misma historia (je) que en el caso anterior. ¿Quieres que tu historial de comandos ejecutados se guarde, aún si no guardas el .RData? ¿Quieres que se remuevan los duplicados? Por las mismas razones que antes también las tengo desmarcadas.\n\nGraphics\n\nBackend a Cairo. Simplemente es con qué se están graficando las cosas. ¡Es INDISPENSABLE que hayas instalado XQuartz si estás en macOS!\n\n\nAppearance\n\nZoom: ¿Qué tan grandes quieres todos los elementos de la ventana? Para mi trabajo personal, y dependiendo de la resolución del monitor donde se encuentre la ventana, esta oscila entre 100% y 125%, para este curso está en 175-200%\nEditor font y Editor font size: Tipo y tamaño de letra. Personalmente recomiendo no cambiar el tipo de letra.\nEditor theme: Cambia el color de fondo y los colores de realce de la sintaxis. Usualmente trabajo por las noches, por lo que prefiero un tema con fondo obscuro como Tomorrow Night Bright, pero puedes buscar el que tú quieras.\n\nPane Layout\n\nAquí se muestran los cuatro paneles de la ventana de RStudio. Personalmente prefiero tener los dos elementos que más utilizo lado a lado y no uno encima del otro. ¿La razón? El código crece hacia abajo, entonces el espacio vertical tiende a ser más importante que el espacio horizontal. Es decir, que en el panel superior derecho pongo la consola, y en el panel inferior izquierdo el ambiente de trabajo.\n\n\nEl resto de opciones son más específicas, por lo que recomiendo no tocarlas salvo que sepas qué estás moviendo y con qué objetivo. Todo se puede revertir, pero no hay necesidad de buscar dolores de cabeza."
  },
  {
    "objectID": "c02_intro_rs.html#preámbulo",
    "href": "c02_intro_rs.html#preámbulo",
    "title": "2  Introducción a RStudio y Quarto",
    "section": "3.1 Preámbulo",
    "text": "3.1 Preámbulo\nEl primer elemento es el preámbulo, que mencionamos está en formato YAML y que está contenido entre ---. YAML es un formato de serialización de datos legible por humanos. ¿En castellano? Una lista con niveles de pares de claves:valores que definen los metadatos de nuestro documento. En nuestro ejemplo tenemos:\n---\ntitle: \"Untitled\"\n---\nEs decir, el título que aparecerá en el reporte es \"Untitled\", tal y como vimos en la vista previa. Esto no tiene mucho sentido, así que cambiémoslo por \"Mi primerQuarto\" y añadamos una nueva entrada para el autor, tal que:\n---\ntitle: \"Mi primer `Quarto`\"\nauthor: \"Tu nombre\"\n---\nOtro elemento que usualmente se agrega en el preámbulo es el formato de salida del documento renderizado; es decir, ya presentado para compartir/imprimir. Mi recomendación es exportar a archivos HTML, salvo que vayas a imprimir el documento (PDF), necesites paginación (PDF de nuevo) o que por alguna desafortunada razón necesites un archivo MS Word. Un HTML lo declaramos tal que:\n---\ntitle: \"Mi primer `Quarto`\"\nauthor: \"Tu nombre\"\nformat:\n  html:\n    code-fold: true\n---\nNotarás algunas cosas. La primera es que html está indentado; es decir, no comienza en la misma posición que format. Esto es para indicar que html pertenece a format, al igual que code-fold pertenece a html. La siguiente es, justamente, que agregamos a la lista la entrada code-fold. Esta es una opción que indica si queremos que el código sea colapsable mediante un botón en el archivo final. En este caso, la indicamos como true, por lo que así será. Si no lo quisiéramos así indicaríamos false. Si renderizamos nuestro documento ahora tendremos:\n\n\n\nFigure 3.1: Primer Quarto renderizado\n\n\nSi tienes curiosidad por saber qué características YAML dieron lugar al libro de acompañamiento, puedes revisar el archivo _Quarto.yml."
  },
  {
    "objectID": "c02_intro_rs.html#markdown",
    "href": "c02_intro_rs.html#markdown",
    "title": "2  Introducción a RStudio y Quarto",
    "section": "3.2 Markdown",
    "text": "3.2 Markdown\nPasando el preámbulo tenemos una sección de texto libre, con algunas anotaciones para el formato del texto. Estas anotaciones están hechas en lenguaje markdown. Markdown es un “lenguaje de programación para textos” y permite hacer cosas bastante interesantes. Las anotaciones más básicas son:\n\nEncabezados y secciones: #, ##, ###, ####, etc.\n*Itálicas* : Itálicas\n**Itálicas**: Negritas\n`Código`: Código\nHipervínculos: [Google](https://www.google.com): Google\n$y_i = \\alpha + \\beta*x_i + \\epsilon$: \\(y_i = \\alpha + \\beta*x_i + \\epsilon\\)\n\nPuedes hacer listas numeradas, como la anterior, o listas sin numerar:\n\nElemento\nOtro elemento\n\nE, incluso, puedes hacer listas anidadas añadiendo una indentación de doble tabulación a los elementos anidados:\n\nIntroducción a RStudio y Quarto\n\nR vs. RStudio\nIDE RStudio\n\n\nEn el archivo .qmd este capítulo puedes ver cómo añadí las capturas de la ventana de RStudio. Añadir imágenes es básicamente el mismo procedimiento que con un enlace, solo añadiendo el operador ! antes. Por ejemplo, la siguiente línea añade el logo de R desde su dirección oficial, le asigna el pie de foto “Logo R” y una etiqueta interna que se puede utilizar para referencias cruzadas (Figure 3.2) con @fig-logoR :\n![Logo `R`](https://www.r-project.org/logo/Rlogo.png){#fig-logoR}\n\n\n\nFigure 3.2: Logo R"
  },
  {
    "objectID": "c02_intro_rs.html#referencias-y-citas",
    "href": "c02_intro_rs.html#referencias-y-citas",
    "title": "2  Introducción a RStudio y Quarto",
    "section": "3.3 Referencias y citas",
    "text": "3.3 Referencias y citas\nPodemos agregar referencias, siempre y cuando estas estén contenidas en un archivo .bib como el archivo references.bib que está en este directorio y, por supuesto, referenciarlas en el texto (e.g. Knuth, 1984). Si estás viendo la página web con este material, pasa tu cursor sobre la cita y notarás como aparece la información bibliográfica completa. Este archivo .bib está compuesto por entradas en formato bibTeX, heredado del hermano mayor de Markdown: LaTeX. La sintaxis básica es la siguiente:\n@TIPO{CLAVE,\n      author = {},\n      year = {},\n      title = {}}\nLos campos adicionales dependerán del TIPO de referencia que se esté añadiendo. Si quieres ver algunas de las opciones más comunes, te recomiendo revisar esta página. Para incluir una referencia cruzada lo único que tienes que hacer es: @CLAVE. Si tomamos como ejemplo el artículo de Knuth de 1984 sobre la programación literal sería @Knuth_1984 para una referencia en el texto, Knuth (1984), o [@Knuth_1984] para una referencia dentro de paréntesis (Knuth, 1984). En cualquiera de los dos casos, si estás viendo el material renderizado, asegúrate de pasar el cursor sobre las citas, y verás que aparece la referencia bibliográfica completa. Para agregar la lista de referencias al final del texto, debes de agregar el div #refs y tener un encabezado de referencias al final del documento, tal que:\n# Referencias{.unnumbered}\n::: {#ref}\n:::\n\n\n\n\n\n\nImportant\n\n\n\nLa clave del divisor en el bloque mostrado es #ref, pero debería de ser #refs. Esto es para evitar que Quarto se confunda y ponga aquí las referencias en vez de en la sección correspondiente. También puedes cambiar el nombre del encabezado, si así lo deseas, o retirar la anotación {.unnumbered} si quieres que la sección esté numerada.\n\n\n\n\n\n\n\n\nNote\n\n\n\nYo construyo mis archivos .bib a mano, pero no es necesario. Si quieres pasar de un listado de referencias que ya tienes en Word puedes considerar text2bib o Edifix (1. OJO con las opciones; 2. Edifix es de pago). Si quieres una interfaz gráfica para manejar tus archivos .bib, puedes considerar JabRef. Finalmente, gestores de referencias como Mendeley o ReadCube Papers permiten exportar las referencias en formato bib."
  },
  {
    "objectID": "c02_intro_rs.html#consideraciones-sobre-quarto",
    "href": "c02_intro_rs.html#consideraciones-sobre-quarto",
    "title": "2  Introducción a RStudio y Quarto",
    "section": "3.4 Consideraciones sobre Quarto",
    "text": "3.4 Consideraciones sobre Quarto\nAunque Quarto es extremadamente potente y flexible, es importante tener presente algunas cosas. La primera es que NO hay manera de que en una sola sesión yo pueda explicarte con lujo de detalle todas sus funciones y posibilidades, para eso prefiero dirigirte a la (bastante completa) guía de Quarto. Otra consideración es que, aunque puedes exportar tus documentos como PDF, Word o ePUB u otros, mi recomendación es que siempre que tengas la libertad exportes a un HTML, que es un poco más permisivo con líneas de código muy largas, o al mostrar tablas con muchas columnas. Si NECESITAS de un PDF, asegúrate de tener instalada alguna distribución de LaTeX o, si no quieres la instalación completa, cuando menos asegurarte de haber instalado TinyTeX como sugerimos en la sesión de preparación, de lo contrario NO podrás exportar tus reportes a PDF."
  },
  {
    "objectID": "c02_intro_rs.html#ejercicio",
    "href": "c02_intro_rs.html#ejercicio",
    "title": "2  Introducción a RStudio y Quarto",
    "section": "3.5 Ejercicio",
    "text": "3.5 Ejercicio\nUtilizando Quarto genera un documento HTML con tabla de contenidos en el que te presentes. Las características son:\n\nTítulo: tu nombre\nIncluye tu correo electrónico en el preámbulo\nIncluye las secciones:\n\nGrado académico e institución de procedencia\nMotivación para tomar el curso\nExpectativas sobre el curso (¿hay alguna técnica particular que quieras aprender/revisar?)\nLibro(s) favorito(s) (como cita(s) en el texto, incluyendo la(s) referencia(s) completa(s) al final del documento)\nUna captura de pantalla con tu ventana de RStudio. No importa si es con los ajustes por defecto, si la pusiste igual a la mía, o si pusiste un tema con colores estridentes."
  },
  {
    "objectID": "c02_intro_rs.html#qué-sigue",
    "href": "c02_intro_rs.html#qué-sigue",
    "title": "2  Introducción a RStudio y Quarto",
    "section": "3.6 ¿Qué sigue?",
    "text": "3.6 ¿Qué sigue?\nEn esta sesión revisamos muy someramente el poder de RStudio y Quarto, limitándonos a las funciones más indispensables y que se utilizan de manera cotidiana, pero hay mucho más por ver. Proyectos dentro de RStudio, elaborar libros/páginas web/tesis o incluso artículos científicos con formato editorial según los requerimientos de algunas revistas, solo por mencionar algunos.\nPor otra parte, una de las ventajas poco conocidas de trabajar con archivos de texto simple (.R, .Rmd o .qmd), al menos en el área de ciencias biológicas, es el poder aprovechar al máximo sistemas de gestión de versiones, incluyendo por supuesto al más famoso: git. Muy posiblemente te hayas encontrado en algún momento con GitHub, que no era otra cosa mas que un almacén público de repositorios git. Ahora es un servidor capaz de mantener servicios simples en ejecución o, incluso, alojar páginas web (la página web de acompañamiento de este curso es un ejemplo), por lo que te recomiendo que le eches un ojo al funcionamiento de ambos (git y GitHub), y que revises la integración de git en RStudio. Personalmente utilizo solo la línea de comandos, pero es una herramienta más dentro de nuestro IDE y que puede resultarte más intuitiva.\n\n\n\n\nKnuth DE. 1984. Literate programming. The Computer Journal 27:97–111. DOI: 10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "c03_bases_r.html#el-lenguaje-de-programación-r",
    "href": "c03_bases_r.html#el-lenguaje-de-programación-r",
    "title": "3  Bases de R",
    "section": "3.1 El lenguaje de programación R",
    "text": "3.1 El lenguaje de programación R\nEn la sesión anterior hablamos de RStudio como una interfaz gráfica a R, pero no fuimos más allá de decir que R es un lenguaje de programación que se ejecuta en consola. Pues bien, R es, y cito textualmente, “un lenguaje y ambiente para la computación estadística y la creación de gráficos” (R Core Team, 2022). ¿En castellano? Es un lenguaje creado especialmente para procedimientos estadísticos y el graficado de datos. Es un software libre, por lo que además de ser gratuito es auditable (i.e., cualquiera puede revisar el código fuente), “cualquiera” puede contribuir (ojo a las comillas). En su código fuente base (R base) incluye diversos algoritmos, modelos y distribuciones de probabilidad, aunque también es fácilmente extensible por medio de paquetes o librerías.\nEn este punto me dirás “Ok, Arturo, de acuerdo con lo que dices, pero ¿por qué se ha vuelto tan popular en análisis bioestadísticos?” Pues tiene que ver con varios factores. El primero es que es un lenguaje de alto nivel; es decir, está hecho para ser leído por humanos y no por computadoras. El segundo es que es interactivo, por lo que podemos ir modificando el código y ver su salida en tiempo real sin necesidad de compilar primero el código. El tercero tiene que ver su extensibilidad. Hay una gran variedad de librerías que agregan funcionalidades particulares, que van desde funcionalidades muy generales hasta la implementaciones muy particulares dependientes del área de investigación. ¿Algunos ejemplos? En tidyverse tenemos un dialecto enfocado a la ciencia de datos y la programación funcional. En ggplot2 tenemos un potente graficador. En vegan tenemos una gran cantidad de funciones para problemas de ecología de comunidades. En SIBER tenemos una forma de estimar tamaños de nichos isotópicos. En STAN (rstan, para ser más preciso) tenemos un ambiente general para inferencia Bayesiana. En fin, hay tantas librerías como problemas a resolver, y continuamente se añaden nuevas, pero antes de saltar a utilizarlas necesitamos entender las bases de R base (je)."
  },
  {
    "objectID": "c03_bases_r.html#objetos",
    "href": "c03_bases_r.html#objetos",
    "title": "3  Bases de R",
    "section": "3.2 Objetos",
    "text": "3.2 Objetos\nR es un lenguaje en el cual domina el paradigma de la programación orientada a objetos; i.e., en R todo es un objeto. El método de creación es el mismo para todos los casos: utilizar el símbolo de asignación <- (atajo de teclado alt -). Como ejemplo, creemos un objeto que contenga el texto “¡Hola Mundo!”:\n\ntexto <- \"¡Hola mundo!\"\n\nNotarás que no hubo salida ni en la libreta ni en la consola. Esto quiere decir que el objeto fue creado satisfactoriamente, y ahora podemos acceder o utilizar ese texto llamando al objeto texto:\n\ntexto\n\n[1] \"¡Hola mundo!\"\n\n\n\n3.2.1 Consideraciones\nAunque podemos poner virtualmente cualquier nombre a nuestros objetos, es necesario que tengamos algunas cosas en cuenta:\n\nNo empezar nombres de objetos con números.\nNo utilizar nombres de funciones u otros objetos creados anteriormente: enmascaramiento (funciones) o sobre-escritura (variables)\nEvitar empezar con punto (.), pues el objeto queda oculto del ambiente de trabajo\nUtilizar nombres cortos, pero lo más descriptivos posibles (amundsen_plot >> plot)\nSe pueden utilizar _ o . dentro del nombre (como separadores, por ejemplo). La guía de estilo de R sugiere el uso de _, aunque la guía de estilo de Google para R sugiere el uso de CamelCase (AmundsenPlot). Lo más importante para uso personal/interno es ser consistente y evitar mezclar estilos."
  },
  {
    "objectID": "c03_bases_r.html#librerías-y-funciones",
    "href": "c03_bases_r.html#librerías-y-funciones",
    "title": "3  Bases de R",
    "section": "3.3 Librerías y funciones",
    "text": "3.3 Librerías y funciones\nAunque en R predomina el paradigma de la programación orientada a objetos, también podemos hacer uso del paradigma funcional de la programación; es decir, podemos construir nuestros programas mediante la aplicación y construcción de funciones. Aunque en este momento te suene poco intuitivo o como si estuviéramos comenzando por el final, vamos a comenzar con las funciones, para después hablar de los tipos de objetos que tenemos, pues resulta que para crear cierto tipo de objetos necesitamos utilizar funciones.\n\n3.3.1 Funciones\nLas funciones representan una serie de métodos para obtener un resultado, para utilizarlas emplearemos la estructura fun(arg1, arg2, ..., argn), donde arg*representa un argumento; es decir, un elemento “pasado” a la función para regular sus procesos. El ejemplo más simple, y con el cuál ya hemos estado en contacto, es la función print():\n\nprint(\"¡Hola Mundo!\")\n\n[1] \"¡Hola Mundo!\"\n\n\nEsta función “imprime” un resultado en pantalla, pero puede ser utilizada para mucho más que para imprimir texto. A final de cuentas, este mismo resultado lo podemos obtener simplemente poniendo el texto en la consola. ¿Qué puede hacer la función? Eso lo podemos ver consultando la ayuda de la función, utilizando el operador ? antes del nombre, tal que:\n\n?print\n\nTe darás cuenta que no tenemos una salida en la libreta, lo cuál es normal, pues la ayuda tiene su propio espacio en el ambiente gráfico. Si estuviéramos trabajando solo con la línea de comandos en el terminal, ahí sí veríamos la salida en la consola. Una de las partes más importantes de la documentación de ayuda es que tenemos los argumentos de la función; es decir, qué necesita la función para realizar su trabajo, así como el papel de cada uno de estos elementos.\nPor otra parte, cuando declaramos una función los llamaremos parámetros. Para declarar una función generaremos una variable cuyo nombre será el nombre “llamable” de la función, a la cual asignaremos el cuerpo de la función utilizando function(parámetros){cuerpo}. Para ejemplificar, creemos una función para calcular la media aritmética de un conjunto de números x:\n\nmedia.arit <- function(x){\n  # Mejora: Trabajo con NAs\n  suma <- sum(x)\n  n <- length(x)\n  return(suma/n)\n}\n\nmedia.arit(1:5)\n\n[1] 3\n\n\n¿Cuándo declarar una función? Cuando tengamos un flujo de trabajo que consista de exactamente los mismos pasos con posibles pequeñas variaciones, el cuál aplicaremos de manera continua. De hecho, uno de los productos que obtendremos de estas reuniones será un script con las funciones relacionadas con los análisis tróficos IIR, PSIRI, gráficos de Amundsen, etc. para que puedan ser utilizados de manera sencilla, repetitiva, y consistente.\nPor último, debido a que un gran número de funciones son altamente regulables (cuentan con un gran número de parámetros), te recomiendo hacer uso extensivo (excesivo) de la ayuda (?fun) para que obtengas de primera mano el conocimiento sobre su objetivo, sus entradas, su salida y, en consecuencia, ayudarte a prevenir o solucionar errores.\n\n\n3.3.2 Librerías\nAfortunadamente para nostros, muchas de las técnicas o procedimientos que realizamos ya fueron programados por alguien con más experiencia, y usualmente compilados en una librería de R. Hay una gran cantidad de librerías disponibles, algunas instalables directamente desde CRAN (R), mientras que otras son instalables desde repositorios públicos como GitHub. Por lo general, la gran mayoría las instalaremos desde R, utilizando la función install.packages(\"package\", dependencies = T). Esta función buscará la versión más reciente del paquete solicitado y la descargará e instalará para que pueda ser utilizada por nosotros. Un ejemplo:\ninstall.packages(\"tidyverse\", dependencies = T)\nEsta línea descargará el paquete tidyverse, que es un “súper” paquete formado por muchos otros paquetes que forman un “dialecto” dentro de R. Por el momento no te preocupes por eso, solo lo instalamos para evitarnos muchas descargas independientes de paquetes que pueden llegar a serte extremadamente útiles, tal como ggplot2. Es importante mencionar que una cosa es instalar la librería y otra cosa es utilizar la librería, para lo cual necesitamos de la función library(package), tal que:\n\nlibrary(ggplot2)\n\nUna vez que hicimos esto ya podemos utilizar TODAS las funciones que forman parte de la librería. ¿Y si solo quiero utilizar una función particular? En ese caso puedes utilizar el operador ::. Probemos viendo la ayuda de la función str_extract de la librería stringr que forma parte del tidyverse:\n\n?stringr::str_extract\n\nAhora que sabemos qué es una función, qué es una librería y cómo utilizarlos, vayamos a explorar el otro tipo de objetos: las variables.\n\n\n\n\n\n\nNote\n\n\n\nSi te interesa ver la referencia de una librería puedes hacerlo con la función citation(\"librería\")."
  },
  {
    "objectID": "c03_bases_r.html#variables",
    "href": "c03_bases_r.html#variables",
    "title": "3  Bases de R",
    "section": "3.4 Variables",
    "text": "3.4 Variables\nA diferencia de una función en la cual almacenamos series de pasos para obtener un resultado, una variable nos permite almacenar todo lo demás: resultados, números, texto, tablas, e incluso otras variables. Su declaración la vimos arriba: con el símbolo de asignación (<-). Para imprimir el resultado en pantalla podemos llamar a la variable o utilizar la función print(var):\n\nvar <- 1:5\nprint(var)\n\n[1] 1 2 3 4 5"
  },
  {
    "objectID": "c03_bases_r.html#tipos-de-variables",
    "href": "c03_bases_r.html#tipos-de-variables",
    "title": "3  Bases de R",
    "section": "3.5 Tipos de variables",
    "text": "3.5 Tipos de variables\nExisten dos tipos de variables, los cuales a su vez se subdividen en otros tipos. Para conocer el tipo de una variable utilizamos la función typeof(var), mientras que las funciones is.*() nos permiten probar si una variable es de un tipo en específico (e.g. is.character(var)).\n\n3.5.1 Datos\nLas variables que contienen un solo elemento se conocen como datos:\n\nCharacter: Cadena de caracteres, indicadas por comillas dobles o sencillas:\n\n\nchar <- \"a\"\ntypeof(char)\n\n[1] \"character\"\n\n\n\nInteger: Números enteros, indicados por la letra “L” después del número:\n\n\ninteger <- 5L\ntypeof(integer)\n\n[1] \"integer\"\n\n\n\nDouble: Fracciones, también conocidos como floating points:\n\n\ndbl <- 7/5\ntypeof(dbl)\n\n[1] \"double\"\n\n\n\nLogical: Valor lógico o booleano. Solo puede tomar dos valores: TRUE o FALSE o sus abreviaturas T o F\n\n\nbool <- is.double(dbl)\nprint(paste('valor: ', bool))\n\n[1] \"valor:  TRUE\"\n\nprint(paste('tipo: ', typeof(bool)))\n\n[1] \"tipo:  logical\"\n\n\n\nComplex: Números complejos, con una parte real y una imaginaria:\n\n\ncomp.n <- 8+3i\ntypeof(comp.n)\n\n[1] \"complex\"\n\n\n\n\n3.5.2 Estructuras/arreglos\nLas estructuras son colecciones de valores, cada una con sus propiedades y sus métodos de acceso a los valores que las conforman (indexación/indización):\n\n3.5.2.1 Vector\nLa estructura más básica. Una colección unidimensional de elementos. Las funciones para crearlos son: c(), la cual combina una serie de elementos en un vector (mismo tipo) o una lista (diferentes tipos); vector(mode, length): genera un vector “vacío” con longitud (número de elementos) length y tipo de datos 'mode'.\n\nvect_1 <- c(1:5)\nvect_2 <- vector(mode = 'double', length = 5)\nprint(vect_1)\n\n[1] 1 2 3 4 5\n\nprint(vect_2)\n\n[1] 0 0 0 0 0\n\n\nPara indexar un vector utilizamos: var[i], donde i representa la posición del (los) elemento(s) de interés:\n\nprint(vect_1[4])\n\n[1] 4\n\nprint(vect_1[2:3])\n\n[1] 2 3\n\n\n\n\n3.5.2.2 Factor\nRepresentan variables categoricas. Contienen los valores de la variable así como los valores posibles que puede tomar (niveles). Se crean con la función factor(x, levels, labels), donde x representa los valores de la variable, levels representa los posibles niveles y labels (opcional) representan etiquetas de cada nivel:\n\nfact.1 <- factor(x = c('a', 'b', 'c'), \n                 levels = c('a', 'b', 'c', 'd', 'e'))\nfact.1\n\n[1] a b c\nLevels: a b c d e\n\n\n\n\n3.5.2.3 Matrix\nUna estructura bidimensional (columnas/renglones). Se generan utilizando la función matrix(data, nrow, ncol, byrow), donde data representa la colección de objetos que formarán la matriz, nrow y ncol el número de renglones y columnas, respectivamente, y byrow si se llenará por renglones (FALSE) o por columnas (TRUE, por defecto)\n\nmat_1 <- matrix(c(T, F, F, T), nrow = 2, ncol = 2)\nmat_1\n\n      [,1]  [,2]\n[1,]  TRUE FALSE\n[2,] FALSE  TRUE\n\n\nPara indexar una matriz utilizaremos también corchetes; sin embargo, indicaremos el par renglón,columna donde se ubica el elemento:\n\nprint(mat_1[1,1])\n\n[1] TRUE\n\nprint(mat_1[2,1])\n\n[1] FALSE\n\n\nSi quisieramos indexar toda una dimensión (renglón o columna), utilizaríamos el mismo método, dejando en blanco la dimensión contraria; es decir, si nos interesa una columna, dejaremos en blanco el número de renglón y si nos interesa un renglón dejaremos en blanco el número de columna:\n\nprint(mat_1[,2])\n\n[1] FALSE  TRUE\n\nprint(mat_1[1,])\n\n[1]  TRUE FALSE\n\n\nOJO: print(mat_1[c(1,1)]) NO da la diagonal de la matriz, esa la obtenemos con diag(mat1), sino que repite 2 veces el primer elemento de la matriz\n\nmat_1[c(2,1),1]\n\n[1] FALSE  TRUE\n\n\n\n\n3.5.2.4 DataFrame\nEl DataFrame es la estructura con la que más comúnmente estaremos en contacto. Es una tabla completa que, a diferencia de la matriz, contiene nombres de columnas. Tiene dos particularidades que hay que considerar: 1) todos los elementos que forman a cada columna deberán ser del mismo tipo y 2) El número de renglones de todas las columnas debe de ser el mismo. Se crean utilizando la función data.frame(col_name = data, ...):\n\ndf_1 <- data.frame(col_a = c(0:5), \n                   col_b = c(20:25), \n                   col_c = c(15:20))\n# Nota: Si no se indica el nombre de las columnas este será asignado automáticamente\ndf_1\n\n\n\n  \n\n\n\nExisten distintos modos de indexar un DataFrame. El primero de ellos var$col_name:\n\ndf_1$col_a\n\n[1] 0 1 2 3 4 5\n\n\nEjercicio renglones 1 y 4 de la columna c:\n\ndf_1$col_c[c(1,4)]\n\n[1] 15 18\n\n\nComo vemos, este modo de indexación extrae la columna completa en forma de un vector, por lo que si queremos accesar un valor en particular solo habrá que utilizar ese método de indexación:\n\ndf_1$col_b[4]\n\n[1] 23\n\n\nFinalmente, también podemos utilizar el método de indexación de matrices, recordando que se especifica el par renglón, columna:\n\ndf_1[4,2]\n\n[1] 23\n\n\nEsta es la estructura con la que más debemos de familiarizarnos, pues la mayor parte de nuestros datos los representamos en ella. ¿Siempre debemos de ingresar los datos manualmente? Para nada, tenemos todo un abanico de funciones que nos permiten cargar datos directamente de archivos, pero eso lo veremos más adelante.\n\n\n3.5.2.5 List\nLas listas son una colección de cualquier combinación de datos o estructuras, incluyendo otras listas:\n\nl_1 <- list(df_1, mat_1, vect_1)\nprint(l_1)\n\n[[1]]\n  col_a col_b col_c\n1     0    20    15\n2     1    21    16\n3     2    22    17\n4     3    23    18\n5     4    24    19\n6     5    25    20\n\n[[2]]\n      [,1]  [,2]\n[1,]  TRUE FALSE\n[2,] FALSE  TRUE\n\n[[3]]\n[1] 1 2 3 4 5\n\n\nEn la salida de arriba vemos el método de indexación: var[[i]][j,k], donde i representa el número de objeto en la lista y j,k el par renglón,columna (de aplicar). En el caso de DataFrames podemos seguir utilizando el operador $ para utilizar los noombres de columnas:\n\nl_1[[1]]$col_a[6]\n\n[1] 5\n\n\nAhora que hemos hablado de todos los tipos de estructuras, y antes de encaminarnos hacia los procesos de automatización, hablemos de cómo cargar nuestros datos en R.\nEjercicios:\n\nl_1[[2]][2,2]\n\n[1] TRUE\n\nl_1[[3]][5]\n\n[1] 5\n\nl_1[[1]]$col_b[3:4]\n\n[1] 22 23\n\nl_1[[1]]$col_a[c(6,1)]\n\n[1] 5 0"
  },
  {
    "objectID": "c03_bases_r.html#carga-de-datos",
    "href": "c03_bases_r.html#carga-de-datos",
    "title": "3  Bases de R",
    "section": "3.6 Carga de datos",
    "text": "3.6 Carga de datos\nEl cómo carguemos nuestros datos depende de varios factores: a) el formato del archivo en el que estén archivados, b) el cómo esté acomodada la información, c) qué necesitemos para hacer los análisis posteriores. El ejemplo más simple es cargar un archivo de texto separado por comas, en el cuál las comas separan las columnas y los saltos de línea los renglones. Tomemos como ejemplo el archivo \"datos1.csv\":\n\ndatos1 <- read.table(\"datos/datos1.csv\", sep = \",\", header = T)\n\nPodemos verificar la información obteniendo el encabezado del data.frame:\n\nhead(datos1)\n\n\n\n  \n\n\n\nLos archivos separados por comas son uno de los formatos más comunes, por lo que R cuenta con una función dedicada (la función read.table() con valores predefinidos):\n\ndatos1 <- read.csv(\"datos/datos1.csv\")\nhead(datos1)\n\n\n\n  \n\n\n\nAquí todo se cargó sin ningún problema porque el archivo estaba listo para ser leído, pero esto no siempre es el caso. Por ejemplo, los datos pueden estar en la segunda hoja de un archivo Excel, la cuál tiene 5 renglones de encabezado dando una descripción de los datos y en el renglón 6 están dispuestos los nombres de las variables. Además, sabemos que vamos a realizar un análisis de agrupamientos jerárquicos (clúster), el cuál requiere que los nombres de los individuos estén marcados en los nombres de los renglones (Ver archivo datos2.xlsx):\n\ndatos2 <- read.table(\"datos/datos2.xlsx\")\n\nEsto, evidentemente, da un error, pues le dimos a la función read.table() un archivo que no es de texto simple, sino un Excel.\nVamos entonces por partes:\n\nFormato: es un archivo Excel, por lo que hay que utilizar una función que permita leer ese tipo de archivo. En nuestro caso utilizaremos la función readxl::read_xlsx(). Aquí no obtendremos ningún error, pues el tipo de archivo es el correcto. Lo único que obtenemos es un mensaje (New names:) que nos diría a qué columnas se les asignó nombres nuevos (y cuáles).\n\n\ndatos2 <- readxl::read_xlsx(\"datos/datos2.xlsx\")\n\nNew names:\n• `Lp14-C` -> `Lp14-C...94`\n• `Lp14-C` -> `Lp14-C...109`\n\n\nPero, ¿qué pasa si leemos el encabezado? Resulta que la función cargó la primera hoja del excel, cuando en realidad nosotros queríamos la segunda\n\nhead(datos2)\n\n\n\n  \n\n\n\nNecesitamos entonces indicar explícitamente que queremos se cargue la segunda hoja:\n\ndatos2 <- readxl::read_xlsx(\"datos/datos2.xlsx\", sheet = 2)\n\nNew names:\n• `` -> `...2`\n• `` -> `...3`\n• `` -> `...4`\n• `` -> `...5`\n• `` -> `...6`\n• `` -> `...7`\n• `` -> `...8`\n• `` -> `...9`\n• `` -> `...10`\n• `` -> `...11`\n• `` -> `...12`\n• `` -> `...13`\n• `` -> `...14`\n• `` -> `...15`\n• `` -> `...16`\n• `` -> `...17`\n• `` -> `...18`\n• `` -> `...19`\n• `` -> `...20`\n• `` -> `...21`\n• `` -> `...22`\n• `` -> `...23`\n• `` -> `...24`\n• `` -> `...25`\n• `` -> `...26`\n• `` -> `...27`\n• `` -> `...28`\n• `` -> `...29`\n• `` -> `...30`\n• `` -> `...31`\n• `` -> `...32`\n• `` -> `...33`\n• `` -> `...34`\n• `` -> `...35`\n• `` -> `...36`\n• `` -> `...37`\n• `` -> `...38`\n• `` -> `...39`\n• `` -> `...40`\n• `` -> `...41`\n• `` -> `...42`\n• `` -> `...43`\n• `` -> `...44`\n• `` -> `...45`\n• `` -> `...46`\n• `` -> `...47`\n• `` -> `...48`\n• `` -> `...49`\n• `` -> `...50`\n\nhead(datos2)\n\n\n\n  \n\n\n\n\nSaltar renglones: Ya tenemos la hoja que nos interesa, el problema es que cargó el encabezado como renglones con observaciones, por lo que hay que saltarlos:\n\n\ndatos2 <- readxl::read_xlsx(\"datos/datos2.xlsx\",\n                            sheet = 2,\n                            skip = 5)\nhead(datos2)"
  },
  {
    "objectID": "c03_bases_r.html#operaciones-comunes",
    "href": "c03_bases_r.html#operaciones-comunes",
    "title": "3  Bases de R",
    "section": "3.7 Operaciones comunes",
    "text": "3.7 Operaciones comunes\nComo ya vimos, no siempre vamos a obtener la información en el formato que necesitamos. Aunque podemos solventar algunas de estas carencias durante la carga de los archivos, a veces necesitamos “masajear” los datos o “manipularlos” para llevarlos a lo que las funciones que nos interesan nos piden. Tomemos como ejemplo los datos de la hoja número 1 del archivo datos2.xlsx:\n\ndatos3 <- readxl::read_xlsx(\"datos/datos2.xlsx\", sheet = 1)\n\nNew names:\n• `Lp14-C` -> `Lp14-C...94`\n• `Lp14-C` -> `Lp14-C...109`\n\nhead(datos3)\n\n\n\n  \n\n\n\n\n3.7.1 Transposición\nEn estos datos las presas están en los renglones, y los individuos de los depredadores en las columnas. Aunque esta disposición no tiene fundamentalmente nada de malo, normalmente las instancias (observaciones individuales/réplicas) están en los renglones, y las variables (presas) en las columnas. Es necesario entonces tranponer los datos. Esto lo podemos hacer de manera sencilla con la función t():\n\nhead(t(datos3))\n\n      [,1]              [,2]          [,3]           [,4]       \nPrey  \"Alpheus_lottini\" \"Alpheus_spp\" \"Alpheus_umbo\" \"Amphipods\"\nCu1-C \"0\"               \"0\"           \"0\"            \"0\"        \nCu2-C \"0\"               \"4\"           \"0\"            \"0\"        \nCu3-C \"0\"               \"0\"           \"0\"            \"0\"        \nCz1-C \" 0\"              \" 0\"          \" 0\"           \" 6\"       \nAr1-C \"0\"               \"0\"           \"0\"            \"0\"        \n      [,5]                [,6]               [,7]                   \nPrey  \"Apogon_retrosella\" \"Appendicularians\" \"Axoclinus_nigricaudis\"\nCu1-C \"0\"                 \"0\"                \"0\"                    \nCu2-C \"0\"                 \"0\"                \"0\"                    \nCu3-C \"0\"                 \"0\"                \"0\"                    \nCz1-C \" 0\"                \" 0\"               \" 0\"                   \nAr1-C \"0\"                 \"0\"                \"0\"                    \n      [,8]                   [,9]           [,10]                            \nPrey  \"Bittium_cerralvoense\" \"Chaetognaths\" \"Cirripedia_Chthamalus_anisopoma\"\nCu1-C \"0\"                    \"0\"            \"0\"                              \nCu2-C \"0\"                    \"0\"            \"0\"                              \nCu3-C \"0\"                    \"0\"            \"0\"                              \nCz1-C \" 0\"                   \" 0\"           \" 0\"                             \nAr1-C \"0\"                    \"0\"            \"0\"                              \n      [,11]                           [,12]                               \nPrey  \"Cladocerans_Penila_avirostris\" \"Cladocerans_Pseudovadne_tergestina\"\nCu1-C \"0\"                             \"0\"                                 \nCu2-C \"0\"                             \"0\"                                 \nCu3-C \"0\"                             \"0\"                                 \nCz1-C \" 0\"                            \" 0\"                                \nAr1-C \"0\"                             \"0\"                                 \n      [,13]                     [,14]                        [,15]            \nPrey  \"Copepods_Acartia_clausi\" \"Copepods_Calanus_pacificus\" \"Epitonium_canna\"\nCu1-C \"9\"                       \"0\"                          \"0\"              \nCu2-C \"0\"                       \"0\"                          \"0\"              \nCu3-C \"0\"                       \"0\"                          \"0\"              \nCz1-C \" 8\"                      \" 0\"                         \" 0\"             \nAr1-C \"0\"                       \"0\"                          \"0\"              \n      [,16]       [,17]          [,18]                     [,19]     \nPrey  \"Fish_eggs\" \"foraminifera\" \"Gnathophyllum_panamense\" \"Hidrozoa\"\nCu1-C \"0\"         \"0\"            \"0\"                       \"0\"       \nCu2-C \"0\"         \"0\"            \"2\"                       \"0\"       \nCu3-C \"0\"         \"0\"            \"0\"                       \"0\"       \nCz1-C \"13\"        \" 0\"           \" 0\"                      \" 0\"      \nAr1-C \"0\"         \"0\"            \"0\"                       \"0\"       \n      [,20]            [,21]                         [,22]                    \nPrey  \"Ichtyoplankton\" \"Larvae_crustaceans_megalopa\" \"larvae_crustaceans_zoea\"\nCu1-C \"6\"              \"0\"                           \"0\"                      \nCu2-C \"0\"              \"0\"                           \"0\"                      \nCu3-C \"0\"              \"0\"                           \"0\"                      \nCz1-C \" 0\"             \" 0\"                          \" 0\"                     \nAr1-C \"0\"              \"0\"                           \"0\"                      \n      [,23]                [,24]                 [,25]   [,26]               \nPrey  \"Liomera_cinctimana\" \"Litiopa_melanostoma\" \"Mysid\" \"Mytella_arciformis\"\nCu1-C \"0\"                  \"0\"                   \"0\"     \"0\"                 \nCu2-C \"0\"                  \"0\"                   \"0\"     \"0\"                 \nCu3-C \"0\"                  \"0\"                   \"0\"     \"0\"                 \nCz1-C \" 0\"                 \" 0\"                  \" 0\"    \" 0\"                \nAr1-C \"0\"                  \"0\"                   \"0\"     \"0\"                 \n      [,27]                 [,28]                 [,29]                \nPrey  \"Mytella_tumbezensis\" \"Nanocassiope_polita\" \"Nyctiphanes_simplex\"\nCu1-C \"0\"                   \"0\"                   \"0\"                  \nCu2-C \"0\"                   \"0\"                   \"0\"                  \nCu3-C \"0\"                   \"0\"                   \"0\"                  \nCz1-C \" 0\"                  \" 0\"                  \" 0\"                 \nAr1-C \"0\"                   \"0\"                   \"0\"                  \n      [,30]       [,31]      [,32]             [,33]               \nPrey  \"Ostracods\" \"Otoliths\" \"Palaemon_ritter\" \"Panopeus_purpureus\"\nCu1-C \"0\"         \"0\"        \"0\"               \"0\"                 \nCu2-C \"0\"         \"0\"        \"0\"               \"0\"                 \nCu3-C \"0\"         \"0\"        \"2\"               \"0\"                 \nCz1-C \" 0\"        \" 0\"       \" 0\"              \" 0\"                \nAr1-C \"0\"         \"0\"        \"0\"               \"0\"                 \n      [,34]                      [,35]              [,36]           \nPrey  \"Parviturbo_acuticostatus\" \"Parviturbo_erici\" \"Parviturbo_spp\"\nCu1-C \"0\"                        \"0\"                \"0\"             \nCu2-C \"0\"                        \"0\"                \"0\"             \nCu3-C \"0\"                        \"0\"                \"0\"             \nCz1-C \" 0\"                       \" 0\"               \" 0\"            \nAr1-C \"0\"                        \"0\"                \"0\"             \n      [,37]       [,38]              [,39]                   [,40]            \nPrey  \"Pteropods\" \"Quadrella_nitida\" \"Tagelus_californianus\" \"Tegula_globulus\"\nCu1-C \"0\"         \"0\"                \"0\"                     \"0\"              \nCu2-C \"0\"         \"0\"                \"0\"                     \"0\"              \nCu3-C \"0\"         \"0\"                \"0\"                     \"0\"              \nCz1-C \" 0\"        \" 0\"               \" 0\"                    \" 0\"             \nAr1-C \"0\"         \"0\"                \"0\"                     \"0\"              \n      [,41]            [,42]           [,43]                [,44]             \nPrey  \"Tegula_mariana\" \"Tellina_coani\" \"Trapezia_bidentata\" \"Trapezia_formosa\"\nCu1-C \"0\"              \"0\"             \"0\"                  \"0\"               \nCu2-C \"0\"              \"0\"             \"0\"                  \"0\"               \nCu3-C \"0\"              \"0\"             \"0\"                  \"0\"               \nCz1-C \" 0\"             \" 0\"            \" 0\"                 \" 0\"              \nAr1-C \"2\"              \"0\"             \"0\"                  \"0\"               \n      [,45]          [,46]              [,47]          [,48]     \nPrey  \"Trapezia_spp\" \"Ulva_dactylifera\" \"Ulva_lactuca\" \"Ulva_spp\"\nCu1-C \"0\"            \"0\"                \"0\"            \"0\"       \nCu2-C \"0\"            \"0\"                \"0\"            \"0\"       \nCu3-C \"0\"            \"0\"                \"0\"            \"0\"       \nCz1-C \" 0\"           \" 0\"               \" 0\"           \" 0\"      \nAr1-C \"0\"            \"0\"                \"0\"            \"0\"       \n      [,49]                                \nPrey  \"UOM (Unidentified Organic Material)\"\nCu1-C \"0\"                                  \nCu2-C \"0\"                                  \nCu3-C \"0\"                                  \nCz1-C \" 0\"                                 \nAr1-C \"0\"                                  \n\n\nEsto logró nuestro objetivo, aunque con un pequeño gran problema: toda la información es texto. ¿Por qué? Resulta que las columnas solo pueden contener datos de un solo tipo, por lo que al tener el texto de las especies presa todas las columnas son transformadas a cadenas de caracter. ¿Qué podemos hacer? Transponer los datos en tres pasos.\n\n\n3.7.2 “Rebanadas” (slices)\nEl primer paso es separar los nombres de las presas de los datos de los depredadores:\n\nprey <- datos3$Prey\nhead(prey)\n\n[1] \"Alpheus_lottini\"   \"Alpheus_spp\"       \"Alpheus_umbo\"     \n[4] \"Amphipods\"         \"Apogon_retrosella\" \"Appendicularians\" \n\ncounts <- datos3[,2:ncol(datos3)]\nhead(counts)\n\n\n\n  \n\n\n\nTransponer la matriz de conteos:\n\ncountst <- t(counts)\nhead(countst)\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]\nCu1-C    0    0    0    0    0    0    0    0    0     0     0     0     9\nCu2-C    0    4    0    0    0    0    0    0    0     0     0     0     0\nCu3-C    0    0    0    0    0    0    0    0    0     0     0     0     0\nCz1-C    0    0    0    6    0    0    0    0    0     0     0     0     8\nAr1-C    0    0    0    0    0    0    0    0    0     0     0     0     0\nPs1-C    0    0    0    0    0    0    0    0    0     0     0     0     0\n      [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25]\nCu1-C     0     0     0     0     0     0     6     0     0     0     0     0\nCu2-C     0     0     0     0     2     0     0     0     0     0     0     0\nCu3-C     0     0     0     0     0     0     0     0     0     0     0     0\nCz1-C     0     0    13     0     0     0     0     0     0     0     0     0\nAr1-C     0     0     0     0     0     0     0     0     0     0     0     0\nPs1-C     0     0     0     0     0     0     0     0     0     0     0     0\n      [,26] [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37]\nCu1-C     0     0     0     0     0     0     0     0     0     0     0     0\nCu2-C     0     0     0     0     0     0     0     0     0     0     0     0\nCu3-C     0     0     0     0     0     0     2     0     0     0     0     0\nCz1-C     0     0     0     0     0     0     0     0     0     0     0     0\nAr1-C     0     0     0     0     0     0     0     0     0     0     0     0\nPs1-C     0     0     0    15     0     0     0     0     0     0     0     0\n      [,38] [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49]\nCu1-C     0     0     0     0     0     0     0     0     0     0     0     0\nCu2-C     0     0     0     0     0     0     0     0     0     0     0     0\nCu3-C     0     0     0     0     0     0     0     0     0     0     0     0\nCz1-C     0     0     0     0     0     0     0     0     0     0     0     0\nAr1-C     0     0     0     2     0     0     0     0     0     0     0     0\nPs1-C     0     0     0     0     0     0     0     0     0     0     0     0\n\n\n\n\n3.7.3 Cambiar nombres de columnas\nPara acceder o asignar los nombres de las columnas o renglones de arreglos bidimensionales podemos utilizar los atributos colnames(data) y rownames(data):\n\ncolnames(countst) <- prey\nhead(countst)\n\n      Alpheus_lottini Alpheus_spp Alpheus_umbo Amphipods Apogon_retrosella\nCu1-C               0           0            0         0                 0\nCu2-C               0           4            0         0                 0\nCu3-C               0           0            0         0                 0\nCz1-C               0           0            0         6                 0\nAr1-C               0           0            0         0                 0\nPs1-C               0           0            0         0                 0\n      Appendicularians Axoclinus_nigricaudis Bittium_cerralvoense Chaetognaths\nCu1-C                0                     0                    0            0\nCu2-C                0                     0                    0            0\nCu3-C                0                     0                    0            0\nCz1-C                0                     0                    0            0\nAr1-C                0                     0                    0            0\nPs1-C                0                     0                    0            0\n      Cirripedia_Chthamalus_anisopoma Cladocerans_Penila_avirostris\nCu1-C                               0                             0\nCu2-C                               0                             0\nCu3-C                               0                             0\nCz1-C                               0                             0\nAr1-C                               0                             0\nPs1-C                               0                             0\n      Cladocerans_Pseudovadne_tergestina Copepods_Acartia_clausi\nCu1-C                                  0                       9\nCu2-C                                  0                       0\nCu3-C                                  0                       0\nCz1-C                                  0                       8\nAr1-C                                  0                       0\nPs1-C                                  0                       0\n      Copepods_Calanus_pacificus Epitonium_canna Fish_eggs foraminifera\nCu1-C                          0               0         0            0\nCu2-C                          0               0         0            0\nCu3-C                          0               0         0            0\nCz1-C                          0               0        13            0\nAr1-C                          0               0         0            0\nPs1-C                          0               0         0            0\n      Gnathophyllum_panamense Hidrozoa Ichtyoplankton\nCu1-C                       0        0              6\nCu2-C                       2        0              0\nCu3-C                       0        0              0\nCz1-C                       0        0              0\nAr1-C                       0        0              0\nPs1-C                       0        0              0\n      Larvae_crustaceans_megalopa larvae_crustaceans_zoea Liomera_cinctimana\nCu1-C                           0                       0                  0\nCu2-C                           0                       0                  0\nCu3-C                           0                       0                  0\nCz1-C                           0                       0                  0\nAr1-C                           0                       0                  0\nPs1-C                           0                       0                  0\n      Litiopa_melanostoma Mysid Mytella_arciformis Mytella_tumbezensis\nCu1-C                   0     0                  0                   0\nCu2-C                   0     0                  0                   0\nCu3-C                   0     0                  0                   0\nCz1-C                   0     0                  0                   0\nAr1-C                   0     0                  0                   0\nPs1-C                   0     0                  0                   0\n      Nanocassiope_polita Nyctiphanes_simplex Ostracods Otoliths\nCu1-C                   0                   0         0        0\nCu2-C                   0                   0         0        0\nCu3-C                   0                   0         0        0\nCz1-C                   0                   0         0        0\nAr1-C                   0                   0         0        0\nPs1-C                   0                  15         0        0\n      Palaemon_ritter Panopeus_purpureus Parviturbo_acuticostatus\nCu1-C               0                  0                        0\nCu2-C               0                  0                        0\nCu3-C               2                  0                        0\nCz1-C               0                  0                        0\nAr1-C               0                  0                        0\nPs1-C               0                  0                        0\n      Parviturbo_erici Parviturbo_spp Pteropods Quadrella_nitida\nCu1-C                0              0         0                0\nCu2-C                0              0         0                0\nCu3-C                0              0         0                0\nCz1-C                0              0         0                0\nAr1-C                0              0         0                0\nPs1-C                0              0         0                0\n      Tagelus_californianus Tegula_globulus Tegula_mariana Tellina_coani\nCu1-C                     0               0              0             0\nCu2-C                     0               0              0             0\nCu3-C                     0               0              0             0\nCz1-C                     0               0              0             0\nAr1-C                     0               0              2             0\nPs1-C                     0               0              0             0\n      Trapezia_bidentata Trapezia_formosa Trapezia_spp Ulva_dactylifera\nCu1-C                  0                0            0                0\nCu2-C                  0                0            0                0\nCu3-C                  0                0            0                0\nCz1-C                  0                0            0                0\nAr1-C                  0                0            0                0\nPs1-C                  0                0            0                0\n      Ulva_lactuca Ulva_spp UOM (Unidentified Organic Material)\nCu1-C            0        0                                   0\nCu2-C            0        0                                   0\nCu3-C            0        0                                   0\nCz1-C            0        0                                   0\nAr1-C            0        0                                   0\nPs1-C            0        0                                   0\n\n\n\n\n3.7.4 Transformaciones\nEl resultado de las operaciones anteriores es una matriz; sin embargo, podemos pasarlo a un data.frame:\n\ncountst <- as.data.frame(countst)\nhead(countst)\n\n\n\n  \n\n\n\n\n\n3.7.5 Añadir vectores como columnas\nAhora tenemos un data.frame; sin embargo, tenemos las claves como nombres de los renglones y, según qué querramos realizar, podemos necesitar que estas formen su propia columna. Una forma de hacerlo es: 1) extraer los nombres de los renglones y 2) añadirlos como una columna adicional:\n\nkeys <- rownames(countst)\ncountst <- cbind(keys, countst)\nhead(countst)"
  },
  {
    "objectID": "c03_bases_r.html#operadores-lógicos",
    "href": "c03_bases_r.html#operadores-lógicos",
    "title": "3  Bases de R",
    "section": "3.8 Operadores lógicos",
    "text": "3.8 Operadores lógicos\nLos operadores lógicos nos sirven para hacer comparaciones y obtener un resultado booleano (T o F). Los más comunes son: 1. cond1|cond2: Condicional “O”. T si se cumple alguna de las dos condiciones\n\nc <- 5L\nis.integer(c)|is.double(c)\n\n[1] TRUE\n\n\n\ncond1&cond2: Condicional “Y”. T si se cumplen ambas condiciones\n\n\nis.integer(c)&(c>3)\n\n[1] TRUE\n\n\n\n<, >: Comparaciones, menor qué o mayor qué\n\n\nprint(c<10)\n\n[1] TRUE\n\nprint(c>5)\n\n[1] FALSE\n\n\n\n<=, >=: Comparaciones, menor o igual qué; mayor o igual qué.\na!=b: Desigualdad, T si a es diferente de b\n\n\nc!=5\n\n[1] FALSE"
  },
  {
    "objectID": "c03_bases_r.html#automatización",
    "href": "c03_bases_r.html#automatización",
    "title": "3  Bases de R",
    "section": "3.9 Automatización",
    "text": "3.9 Automatización\nEl primer paso de la automatización es generar funciones que te permitan realizar la misma acción múltiples veces y no cometer algún error en dichas repeticiones. Esta práctica se deriva de una de las máximas más importantes en programación: “Don’t repeat yourself” (DRY, no te repitas a ti mismo); es decir, dejar que la computadora haga las repeticiones por sí mismas. En la práctica, esto implica no estar copiando y pegando el mismo bloque de código una y otra vez y luego modificarlo manualmente, sino que escribirlo solo una vez y luego decirle a la computadora que repita esa acción n veces, modificando algún(os) argumento(s), o que cambie el comportamiento en función de si se cumple o no una condición en nuestros datos. En ese caso, las estructuras de control son nuestras mejores aliadas.\n\n3.9.1 Ciclos for\nHay distintas formas de realizar la repetición de acciones, pero hoy introduciremos únicamente los ciclos for por ser los más probables a ser requeridos. Un ciclo for consta de cuatro elementos:\nfor (variable in vector) {action}\n\nLa estructura de control for, evidentemente\nUna variable que hace las veces de un marcador de posición; es decir, la utilizaremos para indicar en dónde se van a sustituir los valores que queremos ciclar\nLos valores que ciclaremos, contenidos en un vector\nLa acción a realizar\n\nComo muchas otras cosas, es más fácil entenderlo utilizando algunos ejemplos. El primero de ellos es simplemente imprimir la secuencia de números del 1 al 10. Aunque podemos escribir 10 veces la función print e ir cambiando el número, es mucho más sencillo:\n\nfor (i in 1:10) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\n¿Qué fue lo que hizo la computadora? Utilizó la función print() para mostrarnos el contenido de i, el cuál es el iésimo elemento de la secuencia 1:10. En otras palabras, si es la segunda vuelta que da, imprimirá el número 2, si es la séptima imprimirá 7, y así hasta que termine con todos los elementos en la secuencia. Ahora, sumemos 2 a cada número de la secuencia:\n\nfor (i in 1:10) {\n  print(i+2)\n}\n\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n[1] 11\n[1] 12\n\n\nAl igual que en el caso anterior, tomó uno por uno los valores (de forma secuencial), le añadió 2 y luego mostró el resultado en pantalla. Este tipo de estructuras son sumamente útiles, pues no solo nos ahorran errores, sino también tiempo de ejecución. Sobra decir que no es la única manera de hacer este tipo de ciclos, ni tampoco es la más rápida. Tenemos algo que se conocen como funciones vectorizadas.\n\n\n3.9.2 Familia de funciones apply()\nPodemos entender la vectorización como la aplicación de una función a cada elemento de un vector (igual que el ciclo for), aunque procesando todo el vector “al mismo tiempo”. Esta última parte no es estrictamente verdad, aunque lo cierto es que son más rápidas que los ciclos for tradicionales. En R tenemos toda una gama de funciones que hacen justo eso, la familia apply y sus relacionadas, compuesta por las funciones:\n\napply\nlapply\nsapply\ntapply\nmapply\nby\naggregate\n\nTodas estas funciones manipulan porciones de datos como matrices, arreglos, listas o data frames de forma repetitiva. Básicamente nos permiten evitar el uso explícito de un ciclo for. Toman como argumento una lista, matriz o data frame y le aplican una función con uno o más argumentos adicionales. Esta función puede ser:\n\nUna función de agregación, por ejemplo la media o la suma\nFunciones de transformaciones o para extraer sub-conjuntos\nOtras funciones vectorizadas, que dan como resultado estructuras más complejas como listas, vectores, matrices o arreglos. Pero basta de cháchara, ¿cómo y cuándo debemos de utilizarlas? La respuesta depende totalmente de la estructura de los datos y el formato de salida que se necesite. Veamos algunos casos de uso:\n\n\n3.9.2.1 apply()\nEsta es la función “madre” de las demás, la cual opera sobre arreglos. Para simplicidad, vamos a limitarnos a arreglos bi-dimensionales como las matrices o data.frames. La sintaxis para su uso es apply(X, MARGIN, FUN, ...), donde X es el arreglo, MARGIN es el márgen sobre el cuál va a actuar la función; es decir, si queremos que la aplique a cada renglón (MARGIN = 1) o a cada columna (MARGIN = 2), y FUN es el nombre de la función a aplicar (puede ser cualquiera, incluso una función definida por nosotros).\nPodemos pensar en obtener la suma o el promedio de cada columna de nuestro data.frame con las presas utilizando un ciclo, o podemos utilizar apply, tal que:\n\nsums <- apply(datos2[,2:ncol(datos2)], 2, sum)\nsums\n\n                    Alpheus_lottini                         Alpheus_spp \n                                  4                                  12 \n                       Alpheus_umbo                           Amphipods \n                                  9                                  44 \n                  Apogon_retrosella                    Appendicularians \n                                  5                                   5 \n              Axoclinus_nigricaudis                Bittium_cerralvoense \n                                  2                                  35 \n                       Chaetognaths     Cirripedia_Chthamalus_anisopoma \n                                 50                                   5 \n      Cladocerans_Penila_avirostris  Cladocerans_Pseudovadne_tergestina \n                                 34                                 171 \n            Copepods_Acartia_clausi          Copepods_Calanus_pacificus \n                                390                                  50 \n                    Epitonium_canna                           Fish_eggs \n                                  3                                  96 \n                       foraminifera             Gnathophyllum_panamense \n                                205                                  11 \n                           Hidrozoa                      Ichtyoplankton \n                                  1                                 153 \n        Larvae_crustaceans_megalopa             larvae_crustaceans_zoea \n                                 22                                  17 \n                 Liomera_cinctimana                 Litiopa_melanostoma \n                                 13                                  13 \n                              Mysid                  Mytella_arciformis \n                                  5                                   2 \n                Mytella_tumbezensis                 Nanocassiope_polita \n                                  1                                   2 \n                Nyctiphanes_simplex                           Ostracods \n                                549                                  76 \n                           Otoliths                     Palaemon_ritter \n                                  2                                   2 \n                 Panopeus_purpureus            Parviturbo_acuticostatus \n                                  4                                   1 \n                   Parviturbo_erici                      Parviturbo_spp \n                                  3                                   2 \n                          Pteropods                    Quadrella_nitida \n                                  8                                   1 \n              Tagelus_californianus                     Tegula_globulus \n                                  2                                  22 \n                     Tegula_mariana                       Tellina_coani \n                                 20                                   3 \n                 Trapezia_bidentata                    Trapezia_formosa \n                                  2                                   6 \n                       Trapezia_spp                    Ulva_dactylifera \n                                  4                                  35 \n                       Ulva_lactuca                            Ulva_spp \n                                158                                  78 \nUOM (Unidentified Organic Material) \n                                 20 \n\n\n\n\n3.9.2.2 aggregate()\nOtra función extremadamente útil es la función aggregate(). Esta función nos permite aplicar una función a distintos grupos. Un escenario clásico es obtener el promedio de una variable para cada grupo, por ejemplo el promedio de conteos de quetognatos. La forma “tradicional” es: aggregate(x, by, FUN), donde x es el objeto R a agrupar, by es una lista con los grupos y FUN es la función a aplicar; sin embargo, podemos utilizar una notación más compacta utilizando una formula: aggregate(forumla, data, FUN). Las fórmulas en R son objetos sumamente útiles y que se utilizan para una gran diversidad de cosas. Su estructura es: Y ~ X, y se lee “Y con respecto a X”. En nuestro caso particular:\n\naggregate(Chaetognaths~sp, data = datos1, FUN = mean)\n\n\n\n  \n\n\n\n\n\n\n3.9.3 Condicionales\nOk, ahora conocemos una manera de aplicar una función a una serie de elementos, pero que pasa si queremos aplicarla de manera condicionada; es decir, si queremos solo imprimir los números mayores a 5, por ejemplo. Eso es justo de lo que se tratan los condicionales, particularmente if, else, e ifelse. La lógica detrás de ellos es sumanmente simple: si se cumple una condición, realiza una acción, si no se cumple, realiza otra (o no realices nada). Comencemos con if. Su estructura es: if(condition){action T}, que notarás es básicamente la descripción que dimos, solo que sin una acción en caso de que no se cumpla la condición. Un ejemplo sería proporcionar un número y que nos diga si es mayor a 5:\n\nx <- 6\nif (x>5) {print(\"x es mayor a 5\")}\n\n[1] \"x es mayor a 5\"\n\n\n¿Y si no se cumple la condición? Veamos qué pasa:\n\nx <- 1\nif (x>5) {print(\"x es mayor a 5\")}\n\nR no nos da ninguna salida, pues no sabe qué hacer. Una forma de decirle es utilizando el complemento de if, else, que nos permite establecer una acción secundaria:\n\nif (x>5) {print(\"x es mayor a 5\")\n  }else{print(\"x no es mayor a 5\")}\n\n[1] \"x no es mayor a 5\"\n\n\nUna notación mucho más compacta para este tipo de casos es utilizar la función ifelse(condition, action T, action F):\n\nx <- 4\nifelse(x > 5,\n       \"x es mayor a 5\",\n       \"x no es mayor a 5\")\n\n[1] \"x no es mayor a 5\"\n\n\nEl resultado fue el mismo que el anterior, pero qué pasa si tenemos más de dos escenarios, que, por ejemplo, nos interesara decir si el número es mayor, menor o igual a 5. Veamos primero lo que sucede si establecemos que x sea 5:\n\nx <- 5\nifelse(x > 5,\n       \"x es mayor a 5\",\n       \"x no es mayor a 5\")\n\n[1] \"x no es mayor a 5\"\n\n\nLa computadora no hizo nada mal, 5 no es mayor a 5. En otros lenguajes de programación añadiríamos una estructura llamada elif, pero en R solo hay que añadir otro(s) if. Mientras que else aplica para todos los casos donde la condición no se cumpla, estos if secundarios nos permiten establecer condiciones adicionales para cuando la condición principal no se cumpla. Volviendo a nuestro problema con x = 5:\n\nx <- 4\nif (x > 5) {\n  print(\"x es mayor a 5\")\n}\n\nif (x == 5) {\n  print(\"x es 5\")\n}else{print(\"x es menor a 5\")}\n\n[1] \"x es menor a 5\"\n\n\nAhora sí cubrimos todas nuestras bases para este problema de comparación. Como te imaginarás, el siguiente paso lógico es mezclar for e if o, mejor dicho, anidarlos:\n\nx <- 1:10\n\nfor (i in x) {\n  if (i > 5) {\n    print(\"x es mayor a 5\")\n  }\n  if (i == 5) {\n    print(\"x es 5\")\n  }else{print(\"x es menor a 5\")}\n}\n\n[1] \"x es menor a 5\"\n[1] \"x es menor a 5\"\n[1] \"x es menor a 5\"\n[1] \"x es menor a 5\"\n[1] \"x es 5\"\n[1] \"x es mayor a 5\"\n[1] \"x es menor a 5\"\n[1] \"x es mayor a 5\"\n[1] \"x es menor a 5\"\n[1] \"x es mayor a 5\"\n[1] \"x es menor a 5\"\n[1] \"x es mayor a 5\"\n[1] \"x es menor a 5\"\n[1] \"x es mayor a 5\"\n[1] \"x es menor a 5\"\n\n\nCon esto también quiero decir que podemos anidar ciclos for, tal que:\n\nx <- 1:5\ny <- 100:105\n\nfor (i in x) {\n  for (j in y) {\n    print(c(i,j))\n  }\n}\n\n[1]   1 100\n[1]   1 101\n[1]   1 102\n[1]   1 103\n[1]   1 104\n[1]   1 105\n[1]   2 100\n[1]   2 101\n[1]   2 102\n[1]   2 103\n[1]   2 104\n[1]   2 105\n[1]   3 100\n[1]   3 101\n[1]   3 102\n[1]   3 103\n[1]   3 104\n[1]   3 105\n[1]   4 100\n[1]   4 101\n[1]   4 102\n[1]   4 103\n[1]   4 104\n[1]   4 105\n[1]   5 100\n[1]   5 101\n[1]   5 102\n[1]   5 103\n[1]   5 104\n[1]   5 105"
  },
  {
    "objectID": "c03_bases_r.html#ejercicios",
    "href": "c03_bases_r.html#ejercicios",
    "title": "3  Bases de R",
    "section": "3.10 Ejercicios",
    "text": "3.10 Ejercicios\n\n¿Qué es un objeto?\n¿Cuál es la diferencia entre una variable y una función?\n¿Qué es una librería? ¿Qué pasa si utilizas la función ggplot() sin haber cargado la librería ggplot2?\n¿Cuál es la diferencia entre un dato y una estructura?\nCarga los datos1.csv y añade un renglón adicional con los totales para cada columna.\nCarga los datos1.csv y obten una tabla con los conteos promedio de cada presa para cada especie (aggregate/apply o ciclos).\nCombina el ciclo for anidado del último ejemplo con un condicional, en el cuál se imprima si la suma de ambos números (i, j) es mayor, menor o igual a 105.\nEjecuta el siguiente código y responde ¿qué hace cada línea? Recuerda que ante la duda siempre puedes revisar la ayuda de las funciones.\n\n```{r}\npkgs <- c(\"tidyverse\", \"Rmisc\", \"rcompanion\",\n          \"gap\", \"brms\", \"stats4\", \"Metrics\",\n          \"performance\", \"ggdendro\", \"dendextend\",\n          \"factoextra\", \"cluster\", \"NbClust\",\n          \"FactoMineR\", \"MASS\", \"klaR\", \"vegan\",\n          \"tidymodels\", \"MVN\", \"Hotelling\",\n          \"gridExtra\", \"GGally\", \"car\", \"FSA\",\n          \"randomcoloR\", \"gganimate\", \"reshape2\",\n          \"DescTools\", \"PerformanceAnalytics\",\n          \"corrplot\", \"ggrepel\", \"pROC\", \"RCurl\",\n          \"glmnet\", \"pcsl\", \"MuMIn\", \"nlraa\")\n\ninstall.packages(pkgs, dependencies = T)\n```\n\n\n\n\n\n\nImportant\n\n\n\nSi tienes algún problema con la ejecución por favor házmelo saber. Esto cubre la instalación de las librerías que utilizaremos durante el curso, por lo que es sumamente importante que se ejecuten correctamente.\nSi en algún momento de la instalación R te pregunta si quieres instalar desde la fuente dile que NO. Aunque las versiones fuente están más actualizadas pueden llegar a romper la compatibilidad con otras librerías.\n\n\n\n\n\n\nR Core Team. 2022. R: A language and environment for statistical computing. Vienna, Austria: R Foundation for Statistical Computing."
  },
  {
    "objectID": "c04_tidyverse.html#un-dialecto-dentro-de-r",
    "href": "c04_tidyverse.html#un-dialecto-dentro-de-r",
    "title": "4  Introducción a tidyverse",
    "section": "4.1 Un dialecto dentro de R",
    "text": "4.1 Un dialecto dentro de R\nEn la sesión anterior mencionamos que hay un “paquete de paquetes” que da lugar a un “dialecto” dentro de R: tidyverse, pero no especifiqué a qué me refería con ello. Pues bien, mientras que en R base los procedimientos los realizamos línea a línea, generando a veces una gran cantidad de objetos intermedios o sobreescribiendo los existentes, tidyverse está basado en el paradigma funcional de la programación, con una “gramática” (sintaxis) distinta, muy similar a lo que veremos en ggplot2. De hecho, ggplot2 es un paquete del tidyverse, por lo que el “dialecto” tidy comparte la filosofía declarativa y fomenta la “encadenación” de comandos. A muchas personas les gusta más la forma tidy, a otras les gusta más trabajar con R base. En lo personal soy partidario de que utilices lo que más te acomode, siempre y cuando lo que hagas tenga sentido, pero más de esto cerca del final de la sesión.\nEste nuevo dialecto fue creado con un objetivo en particular: la ciencia de datos. Como tal, cuenta con una gran cantidad de librerías (y por lo tanto funciones) especializadas para realizar operaciones rutinarias. ¿Quieres realizar gráficos? En la siguiente sesión hablaremos de ggplot2. ¿Quieres hacer “manipulación” (ojo, no cuchareo) de datos? Aquí veremos algnas funciones de dplyr. ¿Quieres trabajar con procesamiento de cadenas de caractér? Para esto está stringr. ¿Quieres herramientas para programación funcional? Ve hacia purrr (sí, triple r). readxl es otra librería con la que ya estás familiarizado y que forma parte del tidyverse. ¿Tienes un problema en el que que necesitas manejar fechas? lubridate puede ser una opción. Puedes conocer todos los paquetes que forman el tidyverse con:\n\ntidyverse::tidyverse_packages(include_self = T)\n\n [1] \"broom\"         \"cli\"           \"crayon\"        \"dbplyr\"       \n [5] \"dplyr\"         \"dtplyr\"        \"forcats\"       \"ggplot2\"      \n [9] \"googledrive\"   \"googlesheets4\" \"haven\"         \"hms\"          \n[13] \"httr\"          \"jsonlite\"      \"lubridate\"     \"magrittr\"     \n[17] \"modelr\"        \"pillar\"        \"purrr\"         \"readr\"        \n[21] \"readxl\"        \"reprex\"        \"rlang\"         \"rstudioapi\"   \n[25] \"rvest\"         \"stringr\"       \"tibble\"        \"tidyr\"        \n[29] \"xml2\"          \"tidyverse\"    \n\n\nA partir de aquí, el cómo aprovecharlos depende mucho de el problema que tengas entre manos, pero la idea general es la misma: utilizar una gramática declarativa para llegar a la solución. Veamos en qué consiste con un ejemplo cotidiano: obtener el promedio de una variable para varios grupos.\n\n4.1.1 Promedios de grupos: aggregate vs group_by() |> summarise()\nVeamos un ejemplo en el cual calcularemos la longitud de pico promedio para cada especie de pingüino, según los datos de palmerpenguins. Primero, carguemos los datos:\n\ndatos1 <- palmerpenguins::penguins\n\nAhora, obtengamos los promedios con la función aggregate, tal y como vimos en la sesión anterior:\n\naggregate(bill_length_mm~species, data = datos1, FUN = mean)\n\n\n\n  \n\n\n\nAhora repliquémoslo con tidyverse, particularmente con las funciones group_by y summarise (o summarize) de la librería dplyr:\n\nlibrary(dplyr)\n\ndatos1 |>\n  group_by(species) |>\n  summarise(mbill_l = mean(bill_length_mm, na.rm = T))\n\n\n\n  \n\n\n\nNotarás que el resultado es exactamente el mismo, aunque la forma de hacerlo es diferente. Descompongámosla paso a paso para ver qué es lo que está pasando:\n\nlibrary(dplyr): cargamos la librería dplyr, la cual contiene las funciones que nos interesa aplicar: group_by y summarise.\nLlamamos directamente a nuestros datos (datos1) y utilizamos un operador que no habíamos visto: |>. Este es el operador pipe, el cual pasa lo que está a la izquierda de él como el primer argumento de lo que está a la derecha de él. Esto puede sonar confuso, pero la instrucción datos1 |> group_by(sp) es equivalente a group_by(datos1, sp). Podemos ver al operador pipe como su nombre sugiere: una tubería que manda la información de un lado hacia otro.\ngroup_by(sp): Como te mencionaba, tidy es un poco más explícito que R base. Mientras que el argumento con la fórmula en aggregate indica cómo se van a agrupar los datos, aquí primero los agrupamos y después aplicamos la función que nos interesa. group_by hace justamente eso, agrupar nuestros datos, nada más, nada menos. El argumento principal de esta función es la(s) columnas bajo las cuales queremos agrupar nuestros datos. En este caso solo es una (sp), por lo que la pasamos directamente.\nNuevamente utilizamos el operador |>. Hasta este punto hemos pasado los datos1 a la función group_by(sp), por lo que ya están agrupados por especie, pero falta aplicar la función mean() para obtener el promedio, entonces volvemos a encadenar hacia la función summarise(). Esta función recibe una serie de pares nombres de columnas y funciones a aplicar. En este caso, estamos generando una columna llamada mbill_l que contiene los promedios de la columna bill_length_mm de los datos1.\n\n\n\n\n\n\n\nImportant\n\n\n\nSi revisas documentación “antigua” sobre tidyverse (previa a R 4.1, de hecho), notarás que el operador pipe es %>% en vez de |>. El resultado es el mismo y, de hecho, a partir de R 4.1 puedes seleccionar cuál utilizar en las preferencias de RStudio. En el curso utilizaremos |> por ser el operador pipe nativo de R, el cual fue introducido (como te imaginarás) en R 4.1.\n\n\n¿Abstracto? Sin duda. ¿Útil? También. ¿Más explícito que R base con aggregate? Debatible. Lo que no es debatible es que esta notación brilla especialmente en cierto tipo de problemas. Pensemos que nos interesa conocer el promedio de las longitudes de picos por especie para cada isla. Intenta hacerlo con R base y te darás cuenta de que no es tan intuitivo, salvo que estés familiarizado con el uso de fórmulas o ciclos y condcionales. ¿En tidy? Solamente hay que agregar la nueva variable de agrupamiento a group_by:\n\ndatos1 |>\n  group_by(species, island) |>\n  summarise(mbill_l = mean(bill_length_mm,\n                           na.rm = T))\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n\n\n  \n\n\n\nAhora tenemos un tibble (funcionalmente equivalente a un data.frame) con tres columnas, en donde se da el promedio para cada combinación de las variables de agrupamiento. ¿A que es más sencillo que intentar hacerlo con R base?. Esto último no es del todo cierto, pues hay una forma muy sencilla de hacerlo con aggregate(), pero encontrar una manera es parte de tu tarea, así que no arruinaré la diversión.\n\n\n\n\n\n\nNote\n\n\n\nCon esto no quiero deciro que tidy sea mejor que R base o viceversa, solamente que son dos aproximaciones, cada una con sus propias ventajas y desventajas. Por un lado, R base puede llegar a ser más compacto, pero tidy tiende a ser más explícito. Por supuesto, también hay casos en los que lo contrario es verdad.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nHay ocasiones en las cuales querrás evitar el uso de tidyverse, y una de ellas es al crear nuevas librerías. ¿La razón? Puedes crear un conflicto de dependencias si no lo manejas con cuidado. Otra es que es sumamente complicado depurar errores.\n\n\nTe habrás dado cuenta de que hasta este momento no hemos asignado nuestros resultados a ningún objeto. Esto se debe a dos razones. La primera, y tal vez la que pasa por tu cabeza, es que de esta manera podemos mostrar los resultados más rápidamente, y sí, pero el trasfondo está en la segunda razón: La asignación sigue un sentido opuesto al encadenamiento. Mientras que con |> la información fluye de izquierda a derecha, con <- la información fluye de derecha a izquierda. Quise, entonces, que primero te acostumbraras al flujo de información con pipe, pues asignar el resultado a un objeto es lo mismo que hemos hecho hasta ahora:\n\ngmeans <- datos1 |>\n          group_by(species, island) |>\n          summarise(mbill_l = mean(bill_length_mm,\n                                   na.rm = T))\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\ngmeans\n\n\n\n  \n\n\n\n\n\n4.1.2 Subconjuntos de datos\nEn la sesión anterior nos familiarizamos con máscaras booleanas y la función subset. tidy tiene su propia aproximación. Pensemos que queremos quedarnos solo con los pingüinos provenientes de Biscoe. Con subset:\n\nsubset(datos1, island == \"Biscoe\")\n\n\n\n  \n\n\n\nMientras que con tidy:\n\ndatos1 |> filter(island == \"Biscoe\")\n\n\n\n  \n\n\n\nLa notación no es tan diferente como en el caso anterior, y el resultado es el mismo. ¿Cuál utilizar? Depende totalmente de la preferencia de cada quien. A diferencia del caso anterior, subset no se vuelve tan compleja conforme vamos escalando en complejidad, y la equivalencia entre aproximaciones con filter se mantiene. Veamos qué pasa si obtenemos SOLO los pingüinos Adelie de Biscoe. Nuevamente, con subset:\n\nsubset(datos1, species == \"Adelie\" & island == \"Biscoe\")\n\n\n\n  \n\n\n\nCon tidy:\n\ndatos1 |> filter(species == \"Adelie\" & island == \"Biscoe\")\n\n\n\n  \n\n\n\n¿Por qué empezar con un ejemplo tan “complejo” como el caso anterior? Para dejar “lo peor” al inicio, y a partir de ahí las cosas puedan fluir un poco mejor.\n\n\n4.1.3 Añadir o modificar columnas\nOtra tarea cotidiana que vimos en la sesión anterior fue el añadir nuevas columnas a nuestro data.frame. Pensemos que tiene sentido obtener el “área” que utiliza el pico, la cual obtendríamos multiplicando bill_length_mm y bill_depth_mm. Este producto lo almacenaríamos en una nueva columna llamada bill_area. En R base:\n\ndatos1[\"bill_area\"] <- datos1$bill_length_mm * datos1$bill_depth_mm\ndatos1$bill_area\n\n  [1]  731.17  687.30  725.40      NA  708.31  809.58  692.42  768.32  617.21\n [10]  848.40  646.38  653.94  723.36  818.32  730.06  651.48  735.30  879.75\n [19]  632.96  989.00  691.74  704.99  689.28  691.42  667.36  667.17  755.16\n [28]  724.95  704.94  765.45  659.65  673.32  703.10  773.01  618.80  827.12\n [37]  776.00  780.70  725.68  760.18  657.00  750.72  666.00  868.77  625.30\n [46]  744.48  780.90  708.75  644.40  896.76  700.92  757.89  626.50  819.00\n [55]  624.45  770.04  682.50  763.28  605.90  718.16  603.33  871.43  639.20\n [64]  748.02  622.44  748.80  575.10  785.01  595.94  810.92  636.50  730.48\n [73]  681.12  865.62  621.25  791.80  687.12  721.68  582.82  804.11  595.12\n [82]  755.04  689.96  680.94  663.94  838.39  707.85  686.34  735.36  731.32\n [91]  642.60  743.91  581.40  716.76  626.26  771.12  708.66  745.55  532.91\n[100]  799.20  626.50  820.00  603.20  756.00  704.94  750.33  663.92  764.00\n[109]  647.70  820.80  628.65  925.68  702.69  822.90  819.72  781.41  656.20\n[118]  764.65  606.90  764.46  622.64  746.46  683.40  765.90  559.68  771.40\n[127]  682.88  759.45  666.90  793.80  689.15  827.52  680.80  693.75  670.56\n[136]  719.25  623.00  808.02  610.50  710.63  687.42  698.32  497.55  691.90\n[145]  626.64  729.30  729.12  673.44  640.80  684.18  615.60  767.75  608.52\n[154]  815.00  686.67  760.00  690.20  627.75  662.84  714.51  580.22  720.72\n[163]  560.33  788.90  623.35  706.64  668.68  774.01  567.00  747.84  669.90\n[172]  735.37  717.86  653.95  674.25  731.54  561.99  696.11  636.35  717.00\n[181]  689.26  765.00  723.69  607.76  653.95 1013.20  726.68  788.92  583.62\n[190]  768.12  598.40  764.59  584.99  793.60  620.61  744.00  802.95  606.04\n[199]  632.45  802.95  597.17  714.16  661.72  683.85  649.44  751.50  669.60\n[208]  693.00  608.82  682.50  626.40  771.12  625.14  688.38  635.23  852.51\n[217]  650.36  836.64  665.28  801.90  617.70  760.50  715.50  723.84  751.92\n[226]  688.20  696.00  777.60  674.50  832.93  623.76  741.28  711.95  819.00\n[235]  692.04  795.00  619.62  878.84  624.96  728.46  665.00  885.70  712.50\n[244]  892.62  659.75  796.95  654.15  797.56  780.52  684.74  696.96  843.15\n[253]  727.50  950.30  731.60  736.50  652.74  753.48  612.99  843.72  606.20\n[262]  726.31  767.60  791.82  661.20  839.45  651.42  881.60  698.65  790.56\n[271]  646.64      NA  669.24  791.28  668.96  803.39  832.35  975.00  984.96\n[280]  848.98 1043.46  804.56  839.02  933.66  869.40 1020.87  829.48 1049.51\n[289]  813.10  941.20  784.89  989.80 1006.00 1032.40  863.04  895.44  733.52\n[298]  848.75  717.12  981.64  835.93  988.00  929.20  940.50  825.92 1056.00\n[307]  678.94 1127.36  709.75  958.80  924.42  798.00  871.08 1076.40  778.54\n[316] 1064.65  955.50  808.50  972.19  773.50  911.11  939.80  896.79  960.40\n[325]  963.05  861.54  788.84  976.60  790.61  998.79  735.25  981.36  750.32\n[334]  981.07  943.76  884.64 1012.05  772.20  776.90 1104.84  787.35  902.72\n[343]  965.20  938.74\n\n\nAhora hagámoslo con tidy, pero primero reestablezcamos el objeto datos1:\n\ndatos1 <- palmerpenguins::penguins\n\nY ahora hagamos la operación con tidy:\n\ndatos1 <- datos1 |> \n          mutate(bill_area = bill_length_mm * bill_depth_mm)\ndatos1 |> select(bill_area)\n\n\n\n  \n\n\n\nEvidentemente, los resultados son los mismos, lo cual me lleva directamente a la siguiente sección."
  },
  {
    "objectID": "c04_tidyverse.html#por-qué-no-enseñar-tidy-desde-el-inicio",
    "href": "c04_tidyverse.html#por-qué-no-enseñar-tidy-desde-el-inicio",
    "title": "4  Introducción a tidyverse",
    "section": "4.2 ¿Por qué no enseñar tidy desde el inicio?",
    "text": "4.2 ¿Por qué no enseñar tidy desde el inicio?\nSi tidy se acomodó a tu forma de ver las cosas, o si se te hizo más fácil de leer, es probable que te preguntes por qué no me salté R base para entrar directamente a tidy. Además de incrementar las horas del curso (broma), fue porque (pedagógicamente) tidy puede introducir ciertas barreras para quienes se van introduciendo a R. Matloff (2020) hace una excelente y extensiva recopilación de las razones por las cuales enseñar solo tidy (o solo R base) es una mala idea, así que te recomiendo leas su opinión; sin embargo, me gustaría darte mi perspectiva. Te adelanto: si no tienes experiencia en programación, el paradigma de tidy supone una curva de aprendizaje más alta que solo R base (solo R a partir de aquí) y, tal vez más importante, no es necesario casarse con uno u otro.\n\n4.2.1 tidy es más abstracto\nComo habrás notado en esta sesión, tidy es, escencialmente, más complejo que R. Esto no es una falla en el diseño, sino que tiene que ver con la filosofía de la aproximación: generar código que sea más fácilmente leíble por seres humanos. Sin duda alguna, el utilizar data |> group_by() |> summarise() puede parecer menos críptico o más “entendible” que solo aggregate(formula, data, FUN); sin embargo, esto se debe a que ya conocías qué es una librería y cómo cargarla, qué es una función y qué es un argumento, pero aún así hubo que explicar qué es encadenamiento de operaciones/funciones y el operador pipe y también tuvimos que entender que la información fluye de izquierda a derecha al encadenar y de derecha a izquierda al asignar.\nSi esto no fuera suficiente, el utilizar pipes puede complicar demasiado las cosas al querer depurar errores. ¿La razón? Es una capa más de abstracción. Mientras que cuando aprendemos R es común generar objetos intermedios con los resultados y ver sus salidas (o detectar errores en cada paso), en tidy esto tiende a no ser el caso. ¿Qué obtienes si solo aplicas group_by() en el ejemplo anterior? (i.e., no aplicas summarise()). El ejemplo que vimos es relativamente sencillo pero, en la medida que los problemas se van haciendo más complejos, es fácil perder la pista de qué sale de una función y entra a otra.\nEn mi opinión esto no es un problema TAN grande como pudiera parecer. Tomemos de ejemplo a Python, el lenguaje de programación reconocido como el más intuitivo. Al utilizarlo, el encadenamiento y uso de pipes es cotidiano y, aún más, preferido. En este sentido, tidyverse me recuerda mucho a la funcionalidad que de pandas en Python pero, al igual que aquí, lo correcto es aproximarse primero a Python base y luego pensar en aprender pandas. En mi opinión, el problema real (quitando la capa de abstracción) es en realidad dos problemas: uno relacionado con la filosofía de tidy y otro con quienes enseñamos R.\n\n\n4.2.2 tidy limita tus opciones\nUn ejemplo de esto está en esta misma clase. ¿Cómo obtendríamos los promedios de datos agrupados a dos niveles? Dejé el ejercicio “en el tintero” (es parte de tu tarea para esta sesión) pero, si solo te hubiera enseñado tidy, no podrías pensar en ciclos, el operador $ o cualquier otra forma de indización. ¿La razón? tidy no está enfocado a tratar con vectores individuales, aborrece el uso de ciclos y tampoco sigue las notaciones básicas de indización. Recuerda: en programación siempre es mejor tener más herramientas a tu disposición. Esta limitación la podemos probar rápidamente si queremos extraer los elementos 10:25 de la columna sex de los datos1. Con R base:\n\ndatos1$sex[10:25]\n\n [1] <NA>   <NA>   <NA>   female male   male   female female male   female\n[11] male   female male   female male   male  \nLevels: female male\n\n\n¿Con tidy? Realmente no hay una función que permita hacerlo. Podemos extraer la columna sex utilizando la función select:\n\ndatos1 |> select(sex)\n\n\n\n  \n\n\n\nPero si queremos indizar el resultante obtenemos un error:\n\ndatos1 |> select(sex)[10:25]\n\nError: function '[' not supported in RHS call of a pipe\n\n\n\n\n\n\n\n\nNote\n\n\n\nRHS es el acrónimo de “Right Hand Side”; es decir, el operador [ no está soportado a la derecha de |>.\n\n\n¿La alternativa? Primero indizar datos1 y luego utilizar select. Esto, como ves aquí abajo, funciona, pero hubiera sido mucho más fácil solo utilizar R base, sin mencionar que en tidy “puro” esta solución no es aceptable. ¿La razón? En tidy predomina el paradigma funcional de la programación; es decir, la salida depende únicamente de los argumentos pasados a la función.\n\ndatos1[10:25,] |> select(sex)\n\n\n\n  \n\n\n\nPor otra parte, hay una falta de consistencia interna derivada de una estrategia publicitaria (tal vez) un poco mal llevada. ggplot2 no surgió dentro del tidyverse, sino que fue incluído después. Si bien es cierto que la filosofía es similar (i.e., una estructura declarativa), el cómo funcionan es completamente diferente. Una de las máximas de tidyverse (sin ggplot2) es que todo lo que entra o lo que sale es un data.frame (o tibble), mientras que en ggplot2 entra un data.frame y sale una lista. De hecho, más adelante veremos cómo podemos utilizar ciclos para automatizar la generación de gráficos, pero esto va en contra de la filosofía de tidy y no sería posible si nos hubiéramos enfocado únicamente en ella.\n\n\n4.2.3 Los ponentes somos necios\nEl mayor problema al que nos enfrentamos al aprender algún lenguaje de programación (y en muchas otras cosas) es que estamos sujetos a los prejuicios y preferencias de la persona que nos está enseñando. Al aprender a manejar nuestro tío amante de los autos nos va a decir que la transmisión manual (estándar) es mejor que la automática, pero nuestro papá, quien ve los carros solo como un medio de transporte, nos va a decir que con la automática es suficiente. ¿Cuál es mejor? Para variar, la respuesta es: “depende”. ¿De qué? De la situación en la que nos encontremos. En ciudad tener una transmisión manual puede ser muy cansado, pero puede darnos un mayor control en una carretera con un descenso empinado y muchas curvas.\nEn el problema R base vs. tidy es lo mismo. Hay ponentes “puristas” en ambos sentidos: personas que creen que tidy debería considerarse sacrilegio, y personas que creen que R base es obsoleto, arcaico, y que debería de caer en desuso. No te conviertas en ninguno de ellos y mejor toma lo mejor de ambos.\n\n\n4.2.4 tidy homogeneiza procesos\nEn mi opinión, lo que te acabo de exponer son los problemas principales de enseñar solo tidy y, de acuerdo con Matloff (2020), las razones por las cuales enseñar una mezcla de ambos es la mejor opción. Aprender R base nos permite resolver problemas que en tidy sería muy largo, mientras que tidy nos permite simplificar procedimientos que serían más complicados en R base. Otra ventaja de tidy es que permite unificar procesos bajo una misma sintaxis.\nUn ejemplo de esto lo tenemos en el aspecto de aprendizaje automatizado. Si te pones a revisar tutoriales/referencias sobre aprendizaje automatizado es muy probable que te encuentres con un montón de librerías (una por problema), funciones dedicadas y sintáxis que son específicas a la técnica que quieras aplicar. Teníamos/tenemos un excelente intento de solventar este problema: caret. Funcionaba bien en el sentido de que permitía conjuntar una gran diversidad de técnicas de aprendizaje automatizado en un mismo entorno, unificadas en un mismo estilo. Utilizando solo caret podríamos ir desde el preprocesamiento de los datos hasta el entrenamiento y validación de nuestros modelos. ¿El problema? Las pocas funciones que forman su esqueleto se volvieron sumamente complejas, algunas de ellas con 30 o más argumentos. El equipo de posit se ha puesto el mismo desafío, y su solución es tidymodels. A diferencia de caret es altamente modular, pero mantiene la intención de unificar el flujo de trabajo en una misma estructura. Tal vez yendo en contra de nuestro consejo, la mayor parte de nuestros procedimientos de aprendizaje automatizado los realizaremos bajo tidymodels, pero haremos referencia a las librerías y funciones involucradas en cada paso. Para ejemplificar, realicemos una regresión lineal simple entre la longitud y la profundidad del pico de los pingüinos.\nEn R base lo podemos hacer en una sola línea, llamando a la función lm con una fórmula y unos datos:\n\nlm(bill_length_mm~bill_depth_mm, data = datos1)\n\n\nCall:\nlm(formula = bill_length_mm ~ bill_depth_mm, data = datos1)\n\nCoefficients:\n  (Intercept)  bill_depth_mm  \n      55.0674        -0.6498  \n\n\nEn tidymodels es un poco más complejo:\n\nlibrary(tidymodels)\n\nlinear_reg() |>\n  set_engine(\"lm\") |> \n  set_mode(\"regression\") |> \n  fit(bill_length_mm~bill_depth_mm, data = datos1)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = bill_length_mm ~ bill_depth_mm, data = data)\n\nCoefficients:\n  (Intercept)  bill_depth_mm  \n      55.0674        -0.6498  \n\n\n¿Por qué utilizar tidymodels entonces? Eso lo dejaremos para las sesiones en las que hablemos de aprendizaje automatizado, pero verás que toma mucho sentido en el momento en el que empiezas a hacer particiones entrenamiento/prueba, optimización mediante validación cruzada, preprocesamiento de datos, evaluación, y demás tareas necesarias. Lo único que te diré en este punto es que el poder homogeneizar todos estos procedimientos en un solo estilo de trabajo simplifica las cosas."
  },
  {
    "objectID": "c04_tidyverse.html#conclusión",
    "href": "c04_tidyverse.html#conclusión",
    "title": "4  Introducción a tidyverse",
    "section": "4.3 Conclusión",
    "text": "4.3 Conclusión\nAunque tidy puede llegar a verse más elegante o moderno que R base, no es un substituto total. Aunque R base te permite resolver los mismos problemas que tidy, a veces no es tan intuitivo. ¿Solución? No casarse con ninguno de los dos y exprimirlos lo mejor posible. ¿Tu problema se resuelve más rápidamente con tidy? Úsalo. ¿R base se presta mejor? Aprovéchalo. Recuerda, R (como cualquier otro lenguaje de programación) es una caja de herramientas en la cual debes de buscar la que mejor se adapte al problema o pregunta que quieras responder.\nEsto es todo para esta sesión, nos vemos en la siguiente para hablar sobre teoría y buenas prácticas para la visualización de datos."
  },
  {
    "objectID": "c04_tidyverse.html#ejercicio",
    "href": "c04_tidyverse.html#ejercicio",
    "title": "4  Introducción a tidyverse",
    "section": "4.4 Ejercicio",
    "text": "4.4 Ejercicio\n\nUtilizando R base obtén los promedios de las longitudes de picos de los pingüinos de palmerpenguins para cada especie en cada isla. OJO: Hay al menos dos formas de hacerlo, una muy simple y una más rebuscada. No importa cuál realices, el objetivo es que te rompas la cabeza un rato ;).\nUtilizando tidy (dplyr), y en una sola cadena, filtra los datos para la isla Biscoe, crea una columna que tenga cada valor de la masa corporal menos la media global de esa columna, y luego obtén el promedio de esta columna para cada especie.\nRealiza la misma operación del punto 2 con R base. Algunas funciones que puedes tomar en cuenta para estos dos puntos son subset, filter, mutate, aggregate, summarise y/o for.\nOpcionalmente puedes intentar mezclar ambos procedimientos para llegar al mismo resultado.\n¿Qué opción se te hizo más sencilla? ¿tidy, R base o una combinación de las dos?\n\n\n\n\n\nMatloff N. 2020. Teaching R in a kinder, gentler, more effective manner: Teach base-R, not just the tidyverse."
  },
  {
    "objectID": "s02_fundan.html#objetivo-de-aprendizaje",
    "href": "s02_fundan.html#objetivo-de-aprendizaje",
    "title": "Fundamentos del análisis de datos",
    "section": "Objetivo de aprendizaje",
    "text": "Objetivo de aprendizaje\nEn esta sección del curso comenzarás a adentrarte al análisis de datos. Comenzarás con una introducción a la teoría de la visualización de datos y después aborarás el concepto más importante: la probabilidad. Posteriormente escalarás a la teoría del muestreo, revisarás cómo describir tus datos (tanto numérica como gráficamente) y por último cómo probar si tu evidencia puede dejar en ridículo a una (a veces ridícula) hipótesis de nulidad."
  },
  {
    "objectID": "c05_ggplot2.html#representaciones-de-la-realidad",
    "href": "c05_ggplot2.html#representaciones-de-la-realidad",
    "title": "5  Principios de visualización de datos y ggplot2",
    "section": "5.1 Representaciones de la realidad",
    "text": "5.1 Representaciones de la realidad\nComo seres humanos, con una tendencia a encontrar el camino que ofrezca una menor resistencia (otra forma de decir que somos flojos), usualmente resumimos una realidad altamente compleja utilizando distintas estrategias. Si yo te pido que me digas “qué es una orca” puedes darme una descripción textual del tipo “las orcas son mamíferos del orden Cetartiodactyla”, una descripción numérica en forma de mediciones (Longitud: 6-8 m) o medidas de tendencia central/dispersión (Peso máximo: 5.5 toneladas), pero usualmente preferimos medios audiovisuales como un dibujo, un video o, en el caso de la investigación y el tema principal de hoy, gráficos."
  },
  {
    "objectID": "c05_ggplot2.html#visualización-de-datos",
    "href": "c05_ggplot2.html#visualización-de-datos",
    "title": "5  Principios de visualización de datos y ggplot2",
    "section": "5.2 Visualización de datos",
    "text": "5.2 Visualización de datos\n\n\n\nFigure 5.1: Calidad de gráficos en artículos científicos (Fuente: xkcd)\n\n\nTenemos una gran variedad de razones y objetivos para los cuales necesitamos o recurrimos a visualizaciones de datos (solo visualizaciones a partir de aquí), algunas de ellas son:\n\nExplorar nuestros datos\nElaborar reportes con nuestros análisis\nComunicar hallazgos gráficamente\n\nSoportar resultados en una publicación\nPresentar a un público no especializado\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIndependientemente de para qué hagamos la visualización, lo cierto es que es algo que merece mucha dedicación y que, en realidad, va más allá de hacer un simple gráfico: la visualización de datos nos permite contar una historia.\n\n\nSi obviamos no tenemos cuidado al hacer nuestras visualizaciones procedimiento podemos terminar con un gráfico como el siguiente:\n\n\n\nFigure 5.2: Gráfico “por defecto” en Statistica\n\n\n¿Qué tiene de malo y cómo podemos mejorarlo? Esa es la pregunta que vamos a responder en esta sesión, replanteándola como ¿qué debo tener en cuenta para hacer una buena visualización?, y para lo cual seguiremos algunas heurísticas que nos ayudarán a ser conscientes de qué elementos incluiremos en el gráfico y cuáles no.\n\n\n\n\n\n\nNote\n\n\n\n¿Qué es una heurística? Es una guía que vamos a seguir hasta que encontremos algo mejor; es decir, las recomendaciones que vamos a revisar son, al final del día, eso: recomendaciones. No tienes ninguna obligación de seguirlas, ni mi objetivo es imponer estas ideas, sino hacer que seas consciente de todo lo que implica una visualización de datos.\n\n\n\n5.2.1 Cairo y su rueda\nLa primera heurística que vamos a revisar es la rueda de Cairo (2012). Alberto es una de las figuras más relevantes en el área de la visualización de datos, y una de las muchas heurísticas que propone es considerar la siguiente rueda:\n\n\n\nFigure 5.3: Rueda de Cairo\n\n\nEsta rueda no es una guía tal cual, sino más bien una herramienta que nos permite evaluar las compensaciones que debemos hacer al realizar nuestras visualizaciones. La rueda nos muestra seis ejes que hacen referencia a distintas características de una visualización:\n\nAbstracción-Figuración: Este primer eje refiere a qué tipo de gráfico estamos utilizando. ¿Tenemos un gráfico abstracto como un gráfico de barras, dispersión, etc.? O, por el contrario, tenemos un dibujo que describe exactamente el proceso (gráfico figurativo).\nFuncionalidad-Decoración: Esta es bastante auto-explicativa, y hace referencia a qué tantos elementos decorativos forman la visualización.\nDensidad-Ligereza: Refiere al número de elementos que conforman la visualización o, en otras palabras, a qué tan cargado está nuestro gráfico.\nDimensionalidad: Refiere al número de variables que se incluyen en el gráfico o, puesto de otra forma, a cuántas partes de la historia queremos narrar con un mismo gráfico. No es lo mismo hacer un gráfico de dispersión en el que los puntos sean todos del mismo color a hacer uno donde los puntos estén coloreados según una tercera variable.\nOriginalidad-Familiaridad: Refiere a si los elementos utilizados son familiares para el observador o son elementos nuevos. Una forma fácil de entender este eje es poner lado a lado un gráfico de frecuencias y un gráfico de densidad. Ambos cumplen con el mismo objetivo (uno es más adecuado para variables continuas), pero es bastante probable que los gráficos de frecuencias te sean más familiares.\nRedundancia-Novedad: Referente al número de elementos que ayudan a soportar una misma parte de la historia. Un ejemplo sería tener un gráfico de frecuencias para distintas clases, donde cada barra tiene un color distinto (como en la Figure 5.7). Ahí tanto las clases en el eje x como el color de las barras nos indican que son cosas diferentes.\n\nSi analizamos un poco la distribución de las características, los gráficos que tiendan más a estar en la mitad superior de la rueda son más complejos y profundos que los de la parte baja. De hecho, el mismo Cairo menciona que los científicos e ingenieros prefieren una rueda como esta:\n\n\n\nFigure 5.4: Rueda preferida por científicos e ingenieros\n\n\nMientras que los artistas, diseñadores gráficos y periodistas preferirían una esta otra:\n\n\n\nFigure 5.5: Rueda preferida por diseñadores gráficos\n\n\n¿Esto quiere decir que forzosamente debamos de hacer gráficos abstractos, con muchos elementos “originales” pero mínimas decoraciones, que representen múltiples variables y que no sean redundantes? PARA NADA. Recuerda, esto es solo una heurística, y el hacia dónde te inclines en cada eje dependerá de qué objetivo tengas con tu visualización, a quién vaya dirigida e, incluso, en qué medio va a ser observada.\n\n5.2.1.1 Minard y la (fallida) invasión Napoleónica\nPara cerrar con esta heurística házme un favor y observa con atención el siguiente gráfico:\n\n\n\nFigure 5.6: Gráfico de Minard sobre la invasión Napoleónica a Rusia\n\n\nEvidentemente es un gráfico muy cargado de información, pero es también considerado por muchos como la “mejor visualización que se ha creado”. ¿Cuántos elementos lo conforman y qué parte de la historia cuentan?\n\n\n\n5.2.2 Tufte y sus tintas\nAhora hablemos de una heurística propuesta por una de las figuras seminales en la visualización de datos: Edward Tufte. Él propone que una buena visualización debería tener una alta proporción de tinta de datos a tinta total (Tufte, 1983), donde la tinta de datos es la tinta gastada para imprimir el núcleo del gráfico; es decir, la información mínima necesaria para transmitir el mensaje, mientras que la tinta total es, como el nombre sugiere, la cantidad de tinta empleada para imprimir el gráfico completo. ¿Qué implica una alta proporción de tinta de datos a tinta total? Que reduzcamos lo más posible el número de elementos en el gráfico.\n\n5.2.2.1 Darkhorse Analytics: Data looks better naked\nPara esta parte sigamos el ejemplo de Darkhorse Analytics, en el cual se mejora la siguiente figura:\n\n\n\nFigure 5.7: Gráfico aparentemente inofensivo (Calorías por 100g de alimento)\n\n\nEl gráfico así como está presentado puede encajar en mayor o menor medida con tus gustos, pero lo cierto es que es un gráfico con una gran cantidad de elementos. Bajo la heurística de las tintas de Tufte quitar es mejorar, así que vayamos elemento a elemento. El primero es el color de fondo. No entraré en el debate de si el color es bonito o no, sino más bien quiero que te preguntes ¿qué me dice el color de fondo sobre lo que se está graficando? La respuesta es: nada; por lo tanto, hay que eliminarlo:\n\n\n\nFigure 5.8: ¿Realmente es necesario el color de fondo?\n\n\nEl siguiente elemento son, en realidad, varios. La visualización muestra el número de calorías por 100g de distintos alimentos. Eso aparece repetido en el título, el título del eje x, el título del eje \\(y\\) y, tal vez de forma menos obvia, en la acotación de colores y las etiquetas del eje x. ¿Qué hacer? Una posible solución Es reducir el título a “calorías por 100g”. ¿100 g de qué? de lo que tenemos en las etiquetas del eje x. Esto nos permite no solo reducir el texto en el título, sino también eliminar los títulos de ambos ejes y la acotación:\n\n\n\nFigure 5.9: ¿Calorías por 100 g de qué?\n\n\nYa “adelgazamos” nuestro gráfico, pero aún podemos continuar. El siguiente elemento son las cuadrículas que delimitan el área del gráfico y el área de graficado. Nuevamente la pregunta es ¿nos dicen algo sobre las calorías de los alimentos? Y nuevamente la respuesta es no. Aunado a esto, desde un punto de vista psicológico, el tener esas delimitaciones puede “limita” la imaginación del observador. Estés o no de acuerdo con este último punto, no cambia el que no aportan nada, así que podemos quitarlas:\n\n\n\nFigure 5.10: ¿Libertad?\n\n\nPuede que estés pensando que hasta este punto ya eliminamos muchas cosas, pero aún hay un par de elementos más. El primero son los colores de las barras. Al igual que la acotación son un elemento redundante que manda el mensaje de que estamos tratando con cosas diferentes, pero eso ya está definido claramente en el eje x. En este caso sería más interesante resaltar alguna categoría en particular, tal vez para responder a la pregunta ¿qué tan calóricos son estos alimentos en relación al tocino?, lo que nos permitiría resaltar al tocino en rojo si ponemos a las demás en un color “neutro” como el gris:\n\n\n\nFigure 5.11: ¿Más o menos calorías que el tocino?\n\n\n¿Esto es todo? Aún no. Tenemos en las barras un elemento derivado de las décadas de los 80s-90s cuando se masificó el uso de las computadoras y los modelos tridimensionales: las sombras y el volumen de las barras. Realmente no aportan nada a la narrativa, sino que solo “cargan” más el gráfico, así que también las eliminamos:\n\n\n\nFigure 5.12: Un look más minimalista\n\n\nEn este punto hay algo que destaca tanto o incluso más que las barras: el texto del título y de las etiquetas del eje \\(y\\). Evidentemente no podemos eliminarlos porque ya no sabríamos qué es lo que representa el gráfico, pero lo que sí podemos hacer es cambiar las negritas por gris claro, con lo cual cambiamos el punto de anclaje de la figura a la barra roja con el tocino:\n\n\n\nFigure 5.13: ¡Tocino!\n\n\nAhora ya estamos muy cerca, pero hay algo que se ve fuera de lugar. Ese “algo” está relacionado con el eje \\(y\\): las etiquetas y las líneas guía. En este punto seguramente me dirás “si quitamos esos elementos ya no vamos a saber”qué tanto es tantito” o cuántas calorías tiene cada cosa”, y tendrías la razón, pero podemos sustituirlo poniendo directamente el número de calorías por 100g de cada alimento sobre su barra, lo cual resulta en un gráfico no solo más simple sino también más preciso:\n\n\n\nFigure 5.14: ¿Cuántas calorías tienes?\n\n\nSi comparas esta última visualización con la Figure 5.7 notarás que hubo un cambio notable. El gráfico nuevo es más simple, más fácil de leer e incluso más preciso que el primero, con todo y que tiene muchos menos elementos.\n¿Con esto quiero decir que siempre debamos de tomar esta aproximación minimalista? No, para nada. De hecho, hay casos en los cuales el tener muchos elementos visuales puede ayudar a llamar la atención del lector, a expensas de la precisión del gráfico. Tomemos el siguiente ejemplo:\n\n\n\nFigure 5.15: Costos monstruosos\n\n\nAmbos gráficos presentan exactamente la misma información, pero estarás de acuerdo conmigo en que el dibujo con el monstruo es mucho más llamativo que el simple gráfico de barras, lo cual lo hace más adecuado para el medio en el que fue distribuido: una revista/periódico. Evidentemente no es compatible con una publicación científica, pero si presentas el segundo gráfico es bastante probable que la gente no voltee a verlo.\n\n\n\n5.2.3 Cairo y sus principios\nEsta siguiente heurística también fue propuesta por Cairo, y son cinco sencillos principios que debemos de seguir para llevar a una visualización altamente efectiva. El primero es el principio de funcionalidad, en el que los elementos del gráfico deben de ayudar a transmitir la historia que estamos contando. Volvamos a la Figure 5.7. Las barras, sus colores y la acotación eran funcionales, pero no lo eran el color del fondo o las cuadrículas de graficado.\nEl segundo principio es el de veracidad. Este es auto-explicativo, pero es posiblemente el más importante de todos: que los datos presentados sean veraces, y que sean presentados de una forma veraz o, en otras palabras: NO CUCHAREAR DATOS NI SU APARIENCIA. Ejemplos de visualizaciones donde este principio no se cumple abundan en los medios de comunicación, algunas veces sin que sea el objetivo, pero muchas otras intencionalmente para forzar una narrativa. Un caso particular es la Figure 5.16, en donde tenemos el gráfico presentado en los medios de comunicación durante las elecciones entre Nicolás Maduro y Henrique Capriles. En apariencia la diferencia en votos era gigantesca, acrecentado no solo por la diferencia de alturas de los cilindros, sino también por ser figuras tridimensionales. Si los seres humanos somos malos juzgando áreas (razón por la que no se recomienda hacer gráficos de pastel), somos peores aún juzgando volúmenes.\n\n\n\nFigure 5.16: A que no me alcanzas\n\n\nVolviendo a la diferencia de alturas, aquí hay una “trampa” más: el eje \\(y\\) se encuentra truncado. ¿Qué tanto? Lo suficiente para que una diferencia de 1.59% se vea como una ventaja abrumadora de Maduro sobre Capriles. ¿Cómo se vería el gráfico si se presentara de forma veraz? Bastante menos dramático, eso es seguro:\n\n\n\nFigure 5.17: Tú y yo no somos tan diferentes\n\n\nRecuerda: no porque estemos contando una historia tenemos porque forzar una narrativa o solo mostrar lo que nos conviene. Nuevamente, ejemplos como este abundan en los medios de comunicación, y aquí puedes ver algunos otros.\nEl tercer principio es el de belleza. También es autoexplicativo: que el gráfico sea “bonito”. El problema con este principio es que es sumamente subjetivo. Lo que puede ser bonito para mi puede no serlo para ti, y viceversa, pero podemos valernos de las dos heurísticas anteriores para llegar a un gráfico que sea atractivo y, sobre todo, legible.\nSi estos tres principios se cumplen y van de la mano, llegamos de forma automática al cuarto: el principio de comprensibilidad. Cairo menciona que una visualización comprensible es aquella que permite que el lector/observador pueda ver el gráfico, analizarlo y que pueda tener un momento de “¡EUREKA!”, sin que nosotros describamos el gráfico.\n\n\n5.2.4 Del dicho al hecho hay mucho trecho\nEstas son algunas de las consideraciones más básicas que debemos de tener al realizar una visualización de datos, pero no son las únicas. Te recomiendo leer el artículo de Rougier, Droettboom & Bourne (2014), el cual tiene otras guías para mejorar nuestras figuras. Ahora bien, una cosa es conocer la teoría, pero de nada sirve si no podemos llevarlo a la práctica, y es aquí donde entra ggplot2."
  },
  {
    "objectID": "c05_ggplot2.html#introducción-a-ggplot2",
    "href": "c05_ggplot2.html#introducción-a-ggplot2",
    "title": "5  Principios de visualización de datos y ggplot2",
    "section": "5.3 Introducción a ggplot2",
    "text": "5.3 Introducción a ggplot2\nRecordarás de las primeras sesiones que un componente muy importante de R es la realización de gráficos, lo cual quiere decir que R base nos permite realizar visualizaciones de datos. De hecho, el graficador por defecto es sumamente potente, pero desafortunadamente no es precisamente intuitivo. En este sentido, la librería ggplot2 es una (¿mejor?) alternativa en la que los gráficos se crean de manera declarativa, y está basada en el libro “Grammar of Graphics” (Wilkinson, 2005). Digo “¿mejor?” porque, como en todo, es una cuestión de gustos y costumbre; sin embargo, pedagógicamente es bastante amable. La creación de gráficos se realiza mediante capas, donde tenemos nuestros datos, en un data.frame, luego indicamos qué queremos que grafique y en dónde, luego cómo queremos que lo grafique, y luego cómo queremos los demás elementos. En código tendríamos algo así:\n```{r}\nggplot(data = datos, aes(x, y)) + geom_*() + ...\n```\nPodría explicarte qué es cada cosa aquí, o podemos mejor aprender haciendo y crear una visualización. Antes de comenzar una visualización es necesario saber qué queremos responder con ella. En este caso, utilizaremos la base de datos mpg incluída en ggplot2. El primer paso es, entonces, conocer la información que contiene. Para ello guardaremos la base en una variable que llamaremos df1:\n\nlibrary(ggplot2)\n\ndf1 <- ggplot2::mpg\ndf1\n\n\n\n  \n\n\n\nUna manera rápida de tener una idea de cómo está dispuesta una base de datos es utilizando la función head(var). Esta nos mostrará solo las primeras instancias (renglones) del data.frame que estemos analizando. En la tabla inferior podemos ver que se trata de una base de datos sobre automóviles y que las columnas representan: el fabricante, el modelo, el desplazamiento de combustible (litros), el año del modelo, el número de cilindros, el tipo de transmisión, el tipo de tracción, los consumos en ciudad y autopista (en millas por galón, mpg), el tipo de combustible que utilizan y la clase a la que pertenencen. También nos plantearemos el objetivo de eliminar la mayor cantidad de elementos posibles hasta solo tener el esqueleto y de ahí agregar algunos elementos que favorezcan la interpretación.\n\nhead(df1)\n\n\n\n  \n\n\n\n\n5.3.1 ggplot() + ...\nA partir de esta información podemos tratar de responder si existe una relación apreciable entre el consumo de combustible (por ejemplo en autopista) y el desplazamiento del motor, considerando la clase del vehículo. Para atender a esta pregunta utilizaremos un gráfico de dispersión, con el desplazamiento en el eje x, el consumo en el eje \\(y\\) y la clase indicada por los colores de los puntos. Ahora que tenemos claro qué queremos visualizar y cómo lo vamos a visualizar podemos empezar a graficar. El primer paso es inicializar el espacio de graficado con la función ggplot() y pasarle los parámetros estéticos utilizando la función aes(x, y, colour). Es importante mencionar que en este momento aparecerá únicamente el espacio de graficado en blanco. Esto es normal, ya que únicamente definimos el “qué”, pero no el “cómo”.\n\nggplot(data = df1, aes(x = displ, y = cty, colour = class))\n\n\n\n\nYa que inicializamos el espacio gráfico podemos agregar la información que nos interesa. Para facilitar la construcción paso a paso y evitar el repetir código innecesariamente podemos almacenar la gráfica completa en una variable (por ejemplo plot2) e ir añadiendo capas (operador +) posteriormente. Para ver un gráfico guardado en una variable simplemente hay que llamar a esa variable. La primera capa que agregaremos será la que indicará el tipo de gráfico que deseamos (nombrados como geom_*), en este caso un gráfico de dispersión:\n\nplot1 <- ggplot(data = df1, aes(x = displ, y = hwy, colour = class)) +\n         geom_point()\nplot1 # Imprime el gráfico\n\n\n\n\nAhora sí tenemos la información que necesitamos y podríamos comenzar a describir el gráfico, pero en realidad hay demasiados elementos que son innecesarios y otros que son poco informativos en su estado actual (etiquetas de ejes), entonces trabajemos uno por uno. Para modificar las etiquetas de los ejes podemos utilizar las funciones xlab() y ylab() como capas separadas; sin embargo, podemos modificar todas las etiquetas y títulos en un mismo paso utilizando la función labs(title, x, y, caption, colour, ...).\n\nplot2 <- plot1 + labs(x = \"Desplazamiento (l)\",\n                      y = 'Consumo (mpg)',\n                      colour = 'Clase',\n                      title = \n                        'Tamaño del motor y Rendimiento de combustible',\n                      subtitle = 'Consumo en carretera',\n                      caption = 'Datos: ggplot2::mpg')\nplot2\n\n\n\n\n\n\n5.3.2 Tema de ggplot2\nAhora que está claro cuáles son las variables que estamos mostrando podemos empezar a modificar la estética. Recordemos que debemos mantener la relación datos/tinta lo más alta posible, y uno de los elementos más prevalentes del gráfico es el fondo gris con todo y cuadrículas. Para modificar esos elementos tenemos que modificar el “tema” de la gráfica, que no es otra cosa mas que utilizar una función que nos permita modificar en una sola línea la estética general del gráfico. Los temas se encuentran señalados con el nombre theme_*. Probemos con theme_minimal():\n\nplot2 + theme_minimal()\n\n\n\n\nLogramos eliminar el fondo gris y de paso las “espinas” (líneas de los ejes) y ahora el gráfico está en mucho mejor condición para ser presentado; sin embargo aún podemos ir más lejos. El objetivo de esta gráfica no es ver los detalles precisos de la información, si no extraer la información más relevante, por lo que la cuadrícula es un elemento que no aporta nada a la visualización. Para retirarla utilizaremos la función theme(), la cual permite modificar el aspecto de todos los elementos del gráfico. En realidad, las funciones theme_*() son aplicaciones de theme() con diferentes valores por defecto, por lo que podemos replicar el efecto de theme_minimal() e incluir otras modificaciones. Otra función muy útil para este procedimiento es la función element_blank(), la cual le indica a ggplot2 que no debe mostrar ese elemento. Otra cuestión importante que debemos de considerar es la relación de aspecto. Debido a que esta puede modificar enormemente la percepción de los datos, su selección no es algo trivial. En general, la proporción áurea (1:1.61) es un buen punto de partida y en series de tiempo es la proporción que menos deforma los datos. Una proporción cuadrada tiene sentido únicamente en aquellos casos en los que ambos ejes tengan la misma magnitud de variación y procuraremos que el eje más largo sea aquel con la variación más pequeña. En este caso, la variación del eje \\(y\\) (5 a 45) es mucho mayor que la del eje x (1.5 a 7), por lo cual una proporción cuadrada no sería una buena alternativa. En su lugar, utilicemos la proporción áurea. El último elemento que eliminaremos aquí son las marcas de los ejes, ya que realmente no aportan demasiada información.\n\nplot2 <- plot2 + \n         # Eliminamos la cuadrícula menor\n         theme(panel.grid.minor = element_blank(),\n               # Eliminamos la cuadrícula mayor\n               panel.grid.major = element_blank(),\n               # Eliminamos el color de fondo\n               panel.background = element_blank(),\n               # Eliminamos las líneas de los ejes\n               axis.line = element_blank(),\n               # Eliminamos la leyenda\n               legend.key = element_blank(),\n               # Cambiamos la relación de aspecto\n               aspect.ratio = 1/1.61,\n               # Eliminamos las marcas de los ejes\n               axis.ticks = element_blank()\n                       )\nplot2\n\n\n\n\n\n\n5.3.3 Personalizar los ejes\nAhora que nos deshicimos del fondo, la cuadrícula y las líneas y marcas de los ejes podemos trabajar en los valores de los ejes. Una de las mejores maneras de hacerlo es utilizando las funciones scale_x_*() o scale_y_*(), sustituyendo el * por continuous o discrete dependiendo del tipo de variable con el que estemos trabajando. En este caso, eliminaremos por completo las marcas del eje \\(y\\) y dejaremos únicamente los desplazamientos más comunes en el eje x.\n\nplot2 <- plot2 + scale_x_continuous(breaks = c(1.8, 2.5, 5, 7)) +\n                 scale_y_continuous(breaks = NULL)\nplot2\n\n\n\n\n\n\n5.3.4 Añadir líneas de referencia\nAhora que nos deshicimos de los valores del eje la gráfica ya no es entendible debido a que no sabemos cuál es la orientación o la escala de los datos. Una alternativa es añadir un par de líneas de referencia. Esto lo haremos con la función geom_hline(), la cual nos permite añadir líneas horizontales a través de todo el gráfico que cruzan al eje \\(y\\) en una posición que nosotros determinamos:\n\n# Valores de referencia como el mínimo, la media y\n# el máximo de los consumos\n\nrefs <- c(round(min(df1$hwy),0),\n          round(mean(df1$hwy),0),\n          round(max(df1$hwy),0))\n\n# Líneas de referencia, una verde para el mejor consumo,\n# una gris para el consumo promedio y una roja para el peor consumo\nplot2 <- plot2 + geom_hline(yintercept = refs[1],\n                            colour = 'firebrick', alpha = 0.5,\n                            linetype = 'dashed') +\n                 geom_hline(yintercept = refs[2],\n                            colour = 'lightslategrey', alpha = 0.5,\n                            linetype = 'dashed') +\n                 geom_hline(yintercept = refs[3],\n                            colour = 'forestgreen', alpha = 0.5,\n                            linetype = 'dashed')\nplot2\n\n\n\n\nAhora el gráifico ya cuenta nuevamente con un sentido de dimensión, pero no tenemos los valores de referencia, entonces habrá que poner esas anotaciones con la función geom_text(), utilizando como valores de posición en y los mismos que las líneas de referencia + un pequeño valor:\n\n# Líneas de referencia con los mismos colores\nplot2 <- plot2 + annotate('text', x = 1.3, y = refs[1]+1, \n                          label = as.character(refs[1]),\n                          colour = 'firebrick') +\n                 annotate('text', x = 1.3, y = refs[2]+1,\n                          label = as.character(refs[2]),\n                          colour = 'lightslategrey') +\n                 annotate('text', x = 1.3, y = refs[3]+1,\n                          label = as.character(refs[3]),\n                          colour = 'forestgreen')\nplot2\n\n\n\n\nCon esta última modificación terminamos de explorar algunas de las funciones más básicas e importantes para personalizar los elementos que más impactan en una visualización, pero antes de terminar de discutir este punto me gustaría terminar el objetivo que nos propusimos al inicio de sacar información de la gráfica. En general, existe una tendencia a que el consumo de combustible incremente conforme incrementa el desplazamiento, lo cual es de esperarse, ya que el desplazamiento es una medida de el volumen máximo de combustible que puede entrar al motor en un momento dado; sin embargo, podemos también observar que, independientemente del desplazamiento, las SUVs y pickups tienden a tener los peores rendimientos de combustible, mientras que los subcompactos tienden al otro extremo. Podemos también analizar a los vehículos de dos plazas y ver que aún cuando tienen desplazamientos altos, sus rendimientos son mejores que los de las SUVs.\n\n\n5.3.5 Conclusión y ejercicio\nEn cuanto a la parte visual, se podría argumentar que esta visualización final no es tan precisa como la primera, que algún elemento podría embellecerse, o que podriamos eliminar la leyenda y poner etiquetas de texto en algunos puntos para indicar las clases. Todos estos argumentos y muchos otros serían válidos ya que la estética es algo subjetivo; sin embargo, las decisiones que tomemos deberán estar en función del medio de distribución de la visualización (no es lo mismo una página web que en un medio impreso, por ejemplo) y sobre todo del público objetivo. Esta visualización en particular funciona para los fines didácticos que tenía en mente, es adecuada para una presentación de resultados de manera electrónica como este video, pero no es una visualización adecuada para una publicación científica. Arreglar eso será tu ejercicio para esta sesión.\nPara finalizar con el objetivo principal de la clase te presento la visualización inicial y la final, una junto a la otra, para ver en dónde comenzamos, dónde terminamos y cómo llegamos hasta aquí. También te sugiero revises y descargues el PDF de esta página, que es un acordeón donde se encuentran los gráficos y funciones más comunes. Más adelante revisaremos algunos de ellos pero es un recurso que vale la pena tener a la mano.\n\n\n\n\n\n\n\n\n\n\n\n5.3.5.1 Gráfico final\nY también el código necesario para el gráfico final, todo en un solo bloque de código.\n\n# Valores de referencia para utilizar en la gráfica\nrefs <- c(round(min(df1$hwy),0),  # Valor mínimo = peor consumo\n          round(mean(df1$hwy),0), # Valor promedio\n          round(max(df1$hwy),0))  # Valor máximo = mejor consumo\n\n# Objeto con todos los pasos para llegar a la gráfica final\n# Inicializamos el espacio gráfico\nfinal.plot <- ggplot(data = df1, aes(x = displ, y = hwy,\n                                     colour = class)) +\n              # Gráfico de dispersión\n              geom_point() +\n              # Establecemos los títulos, subtítulos y un pie de foto\n              labs(x = 'Desplazamiento (l)',\n                   y = 'Consumo (mpg)',\n                   colour = 'Clase',\n                   title = 'Tamaño del motor y Rendimiento de combustible',\n                   subtitle = 'Consumo en carretera',\n                   caption = 'Datos: ggplot2::mpg'\n                   ) +\n              #Eliminamos la cuadrícula menor\n              theme(panel.grid.minor = element_blank(),\n                    #Eliminamos la cuadrícula mayor\n                    panel.grid.major = element_blank(),\n                    #Eliminamos el color de fondo\n                    panel.background = element_blank(),\n                    #Eliminamos las líneas de ejes\n                    axis.line = element_blank(),\n                    #Eliminamos el fondo de la leyenda\n                    legend.key = element_blank(),\n                    #Establecemos la rel. de aspecto\n                    aspect.ratio = 1/1.61,\n                    #Eliminamos las marcas de los ejes\n                    axis.ticks = element_blank(),\n                    #Cambiamos el tipo de letra\n                    text = element_text(family = 'Times',\n                                        colour = 'gray50')\n                    ) + \n              # Reducimos las divisiones del eje ex a 4 valores\n              scale_x_continuous(breaks = c(1.8, 2.5, 5, 7)) +\n              # Eliminamos las divisiones del eje $y$\n              scale_y_continuous(breaks = NULL) +\n              # Añadimos una línea roja en el peor consumo\n              geom_hline(yintercept = refs[1],\n                         colour = 'firebrick', alpha = 0.5, \n                         linetype = 'dashed') +\n              # Añadimos una línea gris en el consumo promedio\n              geom_hline(yintercept = refs[2],\n                         colour = 'lightslategrey', alpha = 0.5, \n                         linetype = 'dashed') +\n              # Añadimos una línea verde en el mejor consumo\n              geom_hline(yintercept = refs[3],\n                         colour = 'forestgreen', alpha = 0.5,\n                         linetype = 'dashed') +\n              # Etiqueta del peor consumo\n              annotate('text', x = 1.3, y = refs[1]+1,\n                       label = as.character(refs[1]),\n                       colour = 'firebrick') +\n              #Etiqueta del consumo promedio\n              annotate('text', x = 1.3, y = refs[2]+1,\n                       label = as.character(refs[2]),\n                       colour = 'lightslategrey') +\n              # Etiqueta del mejor consumo\n              annotate('text', x = 1.3, y = refs[3]+1,\n                       label = as.character(refs[3]),\n                       colour = 'forestgreen') \n              \nfinal.plot\n\n\n\n\n\n\n\n5.3.6 Extras\nAunque estas modificaciones no necesariamente forman parte del proceso necesario para la visualización que era de nuestro interés, sí que son rutinarias, por lo que vale la pena echarles un ojo.\n\n5.3.6.1 Colores de puntos\nModificar los colores de los puntos. Podemos utilizar la función randomColor(n) de la librería con el mismo nombre. Esta función solamente recibe el número de colores que queremos y los generará de manera aleatoria:\n\naleat <- randomcoloR::randomColor(7)\nfinal.plot + scale_color_manual(name = \"Clase\", values = aleat)\n\n\n\n\nPodemos también especificar una paleta predefinida, utilizando la capa scale_color_brewer():\n\nfinal.plot + scale_color_brewer(type = \"seq\", palette = \"Paired\")\n\n\n\n\nOtra opción es directamente pasar un vector con los nombres de los colores que sean de nuestro interés:\n\ncolor_names <- c(\"red\", \"blue\", \"yellow\", \"black\",\n                 \"dodgerblue\", \"pink\", \"gray\")\nfinal.plot + scale_color_manual(name = \"Clase\", values = color_names)\n\n\n\n\n\n\n5.3.6.2 Tamaño de los puntos\nPara modificar el tamaño de los puntos solamente hay que agregar el argumento size a la capa geom_point, en el cuál indicaremos qué tamaños tomarán los puntos. Puede ser un solo valor:\n\nfinal.plot + geom_point(size = 0.1)\n\n\n\n\nO también a partir de una columna de la base de datos (dividida entre 5 para no obtener únicamente “manchas”):\n\nfinal.plot + geom_point(size = df1$hwy/5)\n\n\n\n\n\n\n5.3.6.3 Tipografías y Exportación de gráficos\nEl manejo de las tipografías en R es un poco especial, por ello usualmente recomiendo generar el gráfico en R, exportarlo como PDF (cairo_pdf(\"filename.pdf\", width, height, family)) y agregar las cursivas donde sea necesario; sin embargo, un paquete que puede resultar especialmente útil es ggtext. Este añade un nuevo tipo de “elemento” de texto que recibe formato Markdown (element_markdown()); es decir, podemos agregar itálicas o negritas. Para poder utilizarlo, sin embargo, es necesario modificar ligeramente nuestros datos de antemano. Para facilitarnos las cosas agregaremos una nueva columna a df1 que contenga las clases en itálicas y extraeremos los valores únicos (algo más eficiente sería hacerlo al revés, pero es más lógico de esta manera):\n\ndf1$clase <- paste0(\"*\",df1$class,\"*\")\nclases <- unique(df1$clase)\n\nFinalmente lo agregaremos a la gráfica. ¡OJO! Es necesario modificar el tema para que entienda el formato markdown:\n\nif(!require(ggtext)) {install.packages(\"ggtext\", dependencies = T)}\n\nLoading required package: ggtext\n\nfinal.plot + scale_color_discrete(name = \"Clase\",labels = clases) +\n             theme(legend.text = ggtext::element_markdown())\n\n\n\n\nCon este elemento podemos modificar también fracciones de cualquier texto de nuestra gráfica, por ejemplo carretera en negritas:\n\nfinal.plot + labs(subtitle = \"Consumo en **carretera**\") +\n             theme(plot.subtitle = ggtext::element_markdown())\n\n\n\n\nMezclando ambas modificaciones:\n\nfinal.plot + scale_color_discrete(name = \"Clase\",labels = clases) +\n             labs(subtitle = \"Consumo en **carretera**\") +\n             theme(plot.subtitle = ggtext::element_markdown(),\n                   legend.text = ggtext::element_markdown())\n\n\n\n\nAhora sí, esto es todo para esta clase. ¡Nos vemos en la siguiente!"
  },
  {
    "objectID": "c05_ggplot2.html#ejercicio",
    "href": "c05_ggplot2.html#ejercicio",
    "title": "5  Principios de visualización de datos y ggplot2",
    "section": "5.4 Ejercicio",
    "text": "5.4 Ejercicio\nAjusta la visualización de los datos mpg para que pueda ser publicable en una revista científica (de tu interés) y responde:\n\n¿Qué elementos quitarías?\n¿Qué elementos cambiarías?\n¿Qué elementos agregarías?\n¿Crees que en su estado actual cumple con los criterios de Tufte y Cairo que revisamos en clase? (Explica tu respuesta).\n\nNOTA: En vez de los datos mpg puedes utilizar datos propios o los datos de pingüinos de Palmer.\n\n\n\n\nCairo A. 2012. The functional art. Berkeley, USA: New Riders, Pearson Education.\n\n\nRougier NP, Droettboom M, Bourne PE. 2014. Ten simple rules for better figures. PLoS computational biology 10:e1003833–7. DOI: 10.1371/journal.pcbi.1003833.\n\n\nTufte E. 1983. The visual display of quantitative information. Cheshire, Connecticut: Graphics Press.\n\n\nWilkinson L. 2005. The grammar of graphics. USA: Springer."
  },
  {
    "objectID": "c06_prob.html#definiciones-básicas",
    "href": "c06_prob.html#definiciones-básicas",
    "title": "6  Probabilidad",
    "section": "6.1 Definiciones básicas",
    "text": "6.1 Definiciones básicas\nHabrás notado que he evitado utilizar la palabra probabilidad hasta este momento. Esto es porque la probabilidad es justamente el asignarle un número a las posibilidades y, por lo tanto, podemos pensar en la probabilidad com una medida de incertidumbre. ¿Cómo la expresamos numéricamente? La manera más sencilla de entender a la probabilidad es desde un punto de vista geométrico; es decir, como una proporción o una frecuencia relativa, de la forma ¿cuántas veces ha ocurrido B en relación al número de veces que han ocurrido tanto A como B? Pero vayamos paso a paso. Primero, algunas (tediosas y obligadas) definiciones:\n\nExperimento aleatorio: Es, como su nombre lo indica, un experimento, prueba (o como quieras llamarlo) en el cual no puedes saber con certeza cuál va a ser el resultado.\nEspacio muestreal: Son todos y cada uno de los posibles resultados de un experimento.\nEvento: Podemos pensar en un evento como un conjunto de datos/resultados, por lo tanto, el espacio muestreal es un evento.\n\nOtros eventos son:\n\nUniverso: Que incluye al espacio muestreal y el conjunto vacío. Como te imaginarás, este conjunto está vacío, no tiene nada.\nUnión: Denotada como \\(\\cup\\) (diferente de u y U), representa la unión de dos conjuntos/eventos; es decir, el caso donde nos interesa encontrar A o B.\nIntersección: Denotada como \\(\\cap\\) (diferente de n), representa el traslape entre dos conjuntos/eventos; es decir, el caso donde nos interesa encontrar A y B.\nComplemento: Denotado como \\(\\tilde{A}\\), representa lo que no es ese conjunto, en este caso, lo que NO es A, que es el área restante de B y el conjunto vacío.\n\n\n\n\nFigure 6.1: Eventos y diagramas de Venn"
  },
  {
    "objectID": "c06_prob.html#axiomas-de-la-probabilidad",
    "href": "c06_prob.html#axiomas-de-la-probabilidad",
    "title": "6  Probabilidad",
    "section": "6.2 Axiomas de la probabilidad",
    "text": "6.2 Axiomas de la probabilidad\n¿Qué tiene que ver esto con probabilidad? Pues que estas denominaciones dan lugar sus leyes/reglas; es decir los Axiomas de la Probabilidad:\n\n\\(P(A) \\geq 0\\); es decir, todo evento tiene una probabilidad positiva y no mayor a 1. No necesita mayor explicación, simplemente ¿cómo interpretarías una probabilidad negativa?\n\\(P(U) = 1\\); es decir, la probabilidad de nuestro espacio muestreal y el conjunto vacío es 1. Eso tiene sentido, si hacemos un experimento aleatorio vamos a tener un resultado, independientemente de cuál sea, lo cual está relacionado con el tercer axioma:\nSi A y B son mútuamente excluyentes: \\(P(A \\cup B) = P(A) + P(B)\\). Básicamente, si nos interesa saber cuál es la probabilidad de que ocurran dos resultados, en donde si ocurre uno ya no ocurre el otro, lo único que tenemos que hacer es sumar la probabilidad de cada uno de ellos. Si lanzo una moneda al aire, la probabilidad de que caiga cara es del 50% y la probabilidad de que caiga cruz es del 50%, pero la probabilidad de que caiga es del 100%.\n\nAdicionales a estos tres axiomas tenemos dos casos especiales y una generalización:\n\n\\(P(\\varnothing) = 0\\); el cual es autoexplicativo, la probabilidad de que ocurra el evento vacío es 0.\n\\(P(\\tilde{A}) = 1 - P(A)\\); es decir, si \\(\\tilde{A}\\) representa lo que no es A y dado que \\(P(U) = 1\\), solo debemos de restarle a ese universo la probabilidad de A.\n\\(P(A\\cup B) = P(A) + P(B) - P(A \\cap B)\\). Te darás cuenta que este se parece al tercer axioma y es porque es una generalización al caso donde A y B no son mútuamente excluyentes. ¿De dónde sale la resta? De que si A y B se encuentran unidos (el diagrama de Venn inferior) y sumamos el área de A con el área de B terminamos sumando dos veces la zona en la que están sobrelapados (intersección); por lo tanto, tenemos que quitarlo una vez para no sobre estimar. A esta generalización se le conoce como la regla aditiva de la probabilidad.\n\n\n\n\nFigure 6.2: Axiomas de la probabilidad"
  },
  {
    "objectID": "c06_prob.html#probabilidades-marginales-y-condicionales",
    "href": "c06_prob.html#probabilidades-marginales-y-condicionales",
    "title": "6  Probabilidad",
    "section": "6.3 Probabilidades marginales y condicionales",
    "text": "6.3 Probabilidades marginales y condicionales\nAhora entendimos que podemos hacer operaciones con la probabilidad, y eso nos lleva a los siguientes dos conceptos que son sumamente importantes: las probabilidades marginal y condicional.\nHablamos de una probabilidad marginal cuando nos interesa la P(A) cuando \\(A \\cup B\\). En este caso, podemos expresar al conjunto A como \\(A = (A \\cap B) \\cup (A \\cup \\tilde{B})\\). ¿En Español? El conjunto A está dado por la unión de la intersección de A y B y la intersección de A con el complemento de B. ¿Aún menos rebuscado? Es la suma de las partes no unidas. El procedimiento aquí es justamente un caso similar a la regla aditiva de la probabilidad. Estamos sumando la zona traslapada entre A y B con lo que no es B, que nos deja únicamente con A. Te preguntarás por qué se denomina marginal. Para esto primero necesitamos definir una tabla de contingencia. Esta es simplemente una tabla en la que cada renglón tiene frecuencias relativas de los distintos niveles de una variable categórica y las columnas tienen las frecuencias relativas de cada nivel de otra variable categórica. En los márgenes de la tabla tenemos los totales para cada nivel (evento o conjunto) y de ahí viene el nombre.\nPor otra parte, la probabilidad condicional nos permite responder a la pregunta ¿cuál es la P(A) si ya sé que B ocurrió?. Matemáticamente la representamos como \\(P(A|B)\\) (probabilidad de A dado B), y es una razón de la probabilidad conjunta de A y B (\\(P(A,B)\\) o \\(P(A \\cap B)\\)) y la probabilidad marginal de B (\\(P(B)\\)); es decir: \\(P(A|B) = \\frac{P(A,B)}{P(B)}\\). La probabilidad conjunta representa la probabilidad de que dos eventos ocurran al mismo tiempo y puede llegar a ser un poco problemática. Si ambos eventos son independientes, obtenerla es sencillo: \\(P(A,B) = P(A)P(B)\\). El problema surge si A y B no son independientes, en cuyo caso: \\(P(A,B) = P(A)P(B|A)\\), lo cual nos lleva a una referencia cruzada. Por practicidad, y porque el interés del curso no es que sepas hacer estas cosas a mano, obtengamos la probabilidad conjunta desde su posición en la tabla de contingencia; es decir, cada una de sus celdas.\n\n\n\nProbabilidades marginales y condicionales\n\n\nEn el ejemplo de la diapositiva (OJO: los datos no son representativos de ninguna población) calculamos la \\(P(Blond|Blue)\\); es decir, la probabilidad de que alguien sea rubio si sabemos que tiene los ojos azules, dada por la división de la probabilidad conjunta \\(P(Blond,Blue) = 0.16\\) y la marginal \\(P(B) = 0.36\\) que resulta en \\(P(Blond|Blue) \\approx 0.44\\). ¿Cuál sería entonces la \\(P(Green|Red)\\)?"
  },
  {
    "objectID": "c06_prob.html#distribuciones-de-probabilidad",
    "href": "c06_prob.html#distribuciones-de-probabilidad",
    "title": "6  Probabilidad",
    "section": "6.4 Distribuciones de probabilidad",
    "text": "6.4 Distribuciones de probabilidad\n\n\n\nFigure 6.3: Estadísticos, probabilidades y distribuciones (Fuente: xkcd)\n\n\nDejando los memes de las viñetas (la inferior es bastante trágica), hablemos de cómo escalar de valores puntuales a algo más aplicado a la investigación. Podemos utilizar nuestra intución de probabilidad de manera cotidiana (e.g., probabilidad de lluvia), pero en cuestiones académicas tenemos una hipótesis de trabajo, la cual trasladamos a pruebas de significancia para realizar inferencias. Eso es algo que abordaremos más a detalle en la siguiente sección; sin embargo, vamos a tener múltiples datos, cuya distribución probabilidad es lo que va a moldear nuestros análisis. Es necesario, entonces, definir qué es una distribución de probabilidad.\nEn pocas palabras, una distribución de probabilidad es una lista con todos los resultados de un evento y sus probabilidades correspondientes. Hay una gran diversidad de distribuciones teóricas de probabilidad, cada una con sus peculiaridades, parámetros, momentos y lugares para utilizarlas. No te preocupes por aprenderlas todas, hablaremos de las distribuciones relevantes para cada modelo que apliquemos. Por ahora solo es importante que conozcas que, si hablamos de distribuciones discretas, hablamos entonces de la probabilidad de cada resultado. Si tenemos una distribución continua, podemos partirla en intervalos para discretizarla y hablar de la probabilidad de que una observación pertenezca a ese intervalo. Sea cual sea el caso, estas son masas de probabilidad, las cuales suman a 1, tal que:\n\\[\n\\sum_{i = 1}^n P(x_i) = 1\n\\]\n\n\n\nAlgunas distribuciones de probabilidad\n\n\n\n\n\n\n\n\nNote\n\n\n\n¿A qué me refiero con discreta o continua? A los valores que pueden tomar los resultados que dan forma a una distribución. Una distribución de probabilidad discreta solo toma valores enteros o categóricos, mientras que una continua puede tomar valores decimales. La relación de esto con nuestros datos la veremos en la siguiente sesión: muestreo.\n\n\nPero volvamos al tema de las distribuciones continuas, porque tienen una propiedad bastante interesante. Resulta que si tenemos una distribución continua, la probabilidad de cada valor (\\(P(x)\\)) es 0. ¿Por qué? Porque, por definición, todos los valores en el intervalo de la distribución son posibles y hay una cantidad infinita de ellos (de aquí sale también el problema de la precisión de punto flotante, pero esa es otra historia). ¿Cómo contender con esto? Podemos discretizar la distribución y hablar de masas de probabilidad de los intervalos resultantes (regla de Sturgess, por ejemplo); sin embargo, estos intervalos son, por mucho apellido de autor que lleven, arbitrarios. ¿Entonces? Podemos hacerlos infinitesimalmente pequeños; es decir, aproximar la amplitud de los intervalos a 0 (pero no exactamente 0) y entonces tenemos densidades de probabilidad. ¿Por qué densidad? Porque dividimos la masa de ese intervalo infinitesimalmente pequeño entre su amplitud, lo cual nos deja con una definición similar a \\(densidad = \\frac{masa}{área}\\). Si hacemos eso, nuestras densidades pueden ser mayores a 1, lo cual indica que tenemos una alta masa en relación a la escala. El otro cambio es que, como recordarás de tus clases de cálculo, al pasar de una variable discreta a una continua pasamos de una sumatoria a una integral:\n\\[\n\\sum_{i = 1}^n \\frac{p([x_i, i_i+\\Delta x])}{\\Delta x} \\Rightarrow \\int dxp(x) = 1\n\\]\nNo es necesario que memorices esto, solo que tengas en cuenta la diferencia entre masas y densidades de probabilidad. Como añadido, este mismo problema es lo que causa que un gráfico de frecuencias (histograma) no sea la mejor solución para ver la distribución de una variable continua. En su lugar podemos utilizar gráficos de densidad, los cuales hacen lo que acabamos de mencionar (al menos en escencia).\nDejando las ecuaciones de lado, la selección de la distribución de probabilidad que utilizaremos depende del problema. ¿Tienes datos de conteos? Puedes utilizar las distribuciones Poisson o binomial negativa. ¿Tienes datos continuos en el intervalo 0-1? Vale la pena echarle un ojo a la distribución Beta. ¿Tienes datos binarios? Deberías voltear hacia la distribución binomial."
  },
  {
    "objectID": "c07_muestreo.html#datos-y-variables",
    "href": "c07_muestreo.html#datos-y-variables",
    "title": "7  Introducción al muestreo",
    "section": "7.1 Datos y variables",
    "text": "7.1 Datos y variables\nEn una investigación reunimos datos con el objetivo de obtener alguna conclusión o predicción. Pues bien comencemos definiendo un dato como una representación puntual de la realidad. Son un solo valor, sea una sola medición, un promedio, una desviación estándar, una proporción de sexos, etc, y el conjunto de datos de un mismo atributo medidos en distintos individuos nos dan una variable. Es decir, en nuestros conjuntos de datos cada valor es un dato, y cada columna (usualmente) es una variable. ¿Por qué es importante conocer esto? Porque hay distintos tipos de variables, los cuales definen cómo es que vamos a graficar y tratar esos datos:\n\nCualitativas: hacen referencia a las cualidades de nuestros individuos, y tienen dos escalas:\n\nNominal: hace referencia a categorías en las que no hay un orden o distintas importancias. Ejemplos pueden el sexo o el color.\nOrdinal: aquí hay un órden, y un ejemplo muy claro son las encuestas: 0 es nunca, 1 es casi nunca, 2 es ocasionalmente, 3 es casi siempre y 4 es siempre. Aunque son categorías bien definidas, 2 < 3 y 3 < 4. No son cuantitativas porque las respuestas están sujetas a la interpretación personal, pero descartar el órden en el análisis sería un error.\n\nCuantitativas, que hacen referencia a atributos cuantificables de manera objetiva. Hay dos tipos, cada uno con dos escalas.\n\nTipos:\n\nDiscretas: Son solo números enteros. Un ejemplo cotidiano es la edad, que usualmente la expresamos en años. No vamos por la vida diciendo tengo 18.5 años o 18 años con 6 meses, solo decimos tengo 18 años.\nContinuas: Es el caso contrario, son números fraccionarios. Se les denomina continuas porque hay un número infinito de valores posibles entre un valor y el otro, un ejemplo es la temperatura (35.1ºC, 100 K, etc.)\n\nEscalas:\n\nIntervalo: La escala de intervalo es aquella en donde el 0 NO es absoluto o, mejor dicho, donde el 0 es arbitrario. La temperatura expresada en grados centígrados es un ejemplo claro, 0ºC no indica ausencia de movimiento molecular, solo toma como referencia arbitraria el punto de congelación del agua.\nRazón: Aquí el 0 sí es absoluto y representa la ausencia del atributo en cuestión. La longitud es un ejemplo, si algo tiene longitud 0 más bien no tiene longitud, o si algo tiene una temperatura de 0 K quiere decir que no tiene movimiento molecular (~-273.15ºC)."
  },
  {
    "objectID": "c07_muestreo.html#qué-datos-obtener",
    "href": "c07_muestreo.html#qué-datos-obtener",
    "title": "7  Introducción al muestreo",
    "section": "7.2 ¿Qué datos obtener?",
    "text": "7.2 ¿Qué datos obtener?\nAlgo que es muy importante tener siempre bien presente es que, aún cuando existen herramientas y técnicas que nos permiten procesar múltiples variables, en cualquier procedimiento de ciencia de datos es INDISPENSABLE que los datos sean de excelente calidad y, sobre todo, que sean adecuados para responder la pregunta que nos interesa, lo cual nos debe de llevar, invariablemente, a preguntarnos “¿qué datos debo de obtener?” O, en otras palabras, “¿qué debo medir?” Una frase que se me quedó marcada de mis clases de la licenciatura es “La investigación inicia y termina en el escritorio del investigador”; es decir, no salimos a hacer trabajo de campo y a registrar todo lo que se nos atraviese, caso contrario podemos terminar en una conclusión como “los bebés son traídos por cigüeñas”, o podemos tomar una decisión equivocada.\n\n7.2.1 Coincidencias\nSituémonos en la Alemania de 1960-1980, cuando estaban pasando por una crisis de natalidad. Sies (1988) quiso encontrar una medida que permitiera entender el problema y salió a buscar respuestas. En esta búsqueda se encontró con algo que le pareció sumamente interesante: había una relación notablemente alta entre la cantidad de pares de cigüeñas reproductoras y la cantidad de bebés nacidos.\n\n\n\nFigure 7.1: Bebes, cigüeñas y casualidad\n\n\nPor muy inverosímil que esto pueda parecernos, lo cierto es que este tipo de relaciones altas entre variables que no están relacionadas existen y en ocasiones puede ser muy difícil identificar si en efecto la relación es causal, casual, o si obedece a que ambas dependen de una tercera variable latente (no observada). Sobre este tema te recomiendo el artículo de Höfer, Przyrembel & Verleger (2004) sobre la “teoría de la cigüeña”, el de Haig (2010) qué es una correlación espuria y también revisar esta página de internet con otras correlaciones curiosas.\n\n\n\n\n\n\nNote\n\n\n\nSi bien es cierto que las correlaciones espurias es algo que debíamos de ver en el tema de correlación, es importante reconocer que un muestreo bien planeado, basado en ciencia, es lo que minimiza la probabilidad de que describamos una relación de este tipo. Matemáticamente es prácticamente identificar si la relación que tenemos es causal o casual, por lo que es mejor evitarlas con un poco de planeación y sentido común.\n\n\n\n\n7.2.2 Contradicciones\nOtro ejemplo de la importancia de la selección de variables es el “Sesgo de supervivencia”. Situémonos ahora en la Segunda Guerra Mundial, en los cuarteles de la Fuerza Aérea de Estados Unidos. A un grupo de matemáticos le fue dada la tarea de designar qué partes de los aviones debían ser reforzadas para incrementar la tasa de supervivencia. El grupo entonces analizó los aviones que volvían a la base y generaron un diagrama como este, en el cual los puntos rojos representan las áreas con mayores daños balísticos.\n\n\n\nFigure 7.2: ¿Dónde reforzamos?\n\n\nLa primera aproximación que viene a la cabeza es reforzar esas zonas, pero Abraham Wald tuvo la suficiente visión para darse cuenta de un problema fundamental con lo que estaban midiendo: los aviones que volvían; es decir, aquellos que no habían sido derribados. Entonces propuso que en su lugar se reforzaran aquellas zonas donde NO había daños, ya que esos aviones fueron los que no volvieron a su base. El resultado: se incrementó la supervivencia como se había solicitado."
  },
  {
    "objectID": "c07_muestreo.html#un-último-paréntesis",
    "href": "c07_muestreo.html#un-último-paréntesis",
    "title": "7  Introducción al muestreo",
    "section": "7.3 Un último paréntesis",
    "text": "7.3 Un último paréntesis\nHablemos ahora sobre el uso de las palabras colecta y recolecta en el contexto biológico, que en ocasiones de utilizan de manera intercambiable al referirse a la obtención de muestras. Si revisamos la definición de colectar (RAE) veremos que esta es “recaudar (cobrar dinero)”, mientras que la definición de recolectar es “Recoger los frutos de una cosecha” o “Reunir cosas o personas de procedencia diversa”. Bajo esta luz, está claro que la terminología correcta es recolectar; sin embargo, el problema no termina ahí. En México, los permisos para reunir muestras con fines científicos tienen el nombre legal de “Licencias de colecta científica” (NOM-126-SEMARNAT-2000) y en el artículo 3 de la Ley General de Vida Silvestre se nombra a “la extracción de ejemplares, partes o derivados de vida silvestre del hábitat en que se encuentran” como colecta. Entonces, ¿cuál utilizar? Aunque pueda parecer algo trivial, lo correcto es utilizar cada una en su contexto, recolectar en la descripción del método de muestreo y colecta al declarar que a) se realizó el trámite correspondiente de acuerdo con el marco legal y b) los números de las licencias de colecta. Aunque pudiera parecer que esta discusión NO está relacionada con el curso, considero importante también el no olvidarnos del marco legal (y ético) involucrado en el desarrollo de la investigación, desde el muestreo hasta el análisis de los datos y el reporte de los resultados."
  },
  {
    "objectID": "c07_muestreo.html#muestreemos-orcas",
    "href": "c07_muestreo.html#muestreemos-orcas",
    "title": "7  Introducción al muestreo",
    "section": "7.4 Muestreemos orcas",
    "text": "7.4 Muestreemos orcas\n\n\n\n\n\n\nNote\n\n\n\nEl diseño experimental y el diseño de muestreo son temas lo suficientemente grandes como para formar un curso con ellos. Únicamente revisaremos los conceptos más importantes, de manera que puedas reflexionar un poco más sobre qué debes de tomar en cuenta en tus diseños.\n\n\nAhora sí, podemos entrar a la teoría del muestreo. Lo primero es introducir algunos conceptos básicos, aunque intentemos hacerlo de una manera más dinámica que solo dar sus definiciones. Imaginemos que el siguiente escenario. Queremos saber cuál es la longitud promedio de las orcas de cierta localidad. ¿Para qué? Para facilitar la explicación, por supuesto, aunque esto puede ser fácilmente extrapolado a cualquier investigación, sea experimental u observacional.\nBien, entonces estudiaremos la longitud de las orcas de la costa central de Oaxaca. Esto conforma nuestra población objetivo o población estadística; es decir, el conjunto de individuos que vamos a estudiar. Nota que estos individuos están bien delimitados; es decir, no vamos a considerar otras especies de delfines, ni fijarnos en el peso o en orcas de otra localidad. Únicamente vamos a medir orcas que nos encontremos al navegar en la costa central de Oaxaca. Formalmente:\n\nPoblación estadística: Conjunto de todos los individuos a estudiar. Esta puede empatar o no con una población biológica. En este caso, lo más probable es que las orcas de la CCO sean parte de una población de orcas transeúntes; sin embargo, forman nuestra población estadística.\nIndividuo: Objeto, ente, planta, animal, quimera, célula o cualquier unidad sobre la cual se realiza la observación y que, consecuentemente, tiene el atributo que queremos medir.\n\nAmbos conceptos van de la mano para definir las generalidades de nuestros métodos en campo o laboratorio, pues llevan la información que delimita nuestro esfuerzo y que, entonces, define nuestro marco de muestreo. No tiene ningún sentido medir orcas de La Paz, BCS, si nuestro interés son las de la CCO, Formalmente:\n\nMarco de muestreo: Lista de todas las unidades de muestreo, donde una unidad de muestreo es la unidad básica para la captación de información de la población de interés. En palabras menos rebuscadas: el conjunto de qué vamos a medir y a quién (o a qué) se lo vamos a medir.\n\nUna vez definimos nuestro marco de muestreo podemos salir a campo. En un mundo ideal tendríamos acceso a la población completa, por lo que la estimación de la longitud promedio que realizaramos sería correcta pra ese momento. ¿Por qué solo en ese momento? Puede que los individuos juveniles crezcan, que los más viejos mueran, que nazcan nuevos, que se vayan algunos, etc., y que, entonces el parámetro poblacional (la longitud promedio) que queremos medir cambie. Esto complica un poco las cosas, pues es logísticamente imposible acceder a todas las orcas de la CCO. Si lo hiciéramos tendríamos un censo, de lo contrario tenemos un muestreo; es decir, vamos a medir únicamente una fracción de la población estadística, la cual va a conformar nuestra muestra y cuyos estadísticos van a ser nuestros estimadores de los parámetros poblacionales. Formalmente (Figure 7.3):\n\nParámetro poblacional: Función definida sobre los valores de las características medibles de una población. En palabras ménos técnicas: el valor real de lo que queremos saber de la población. Representados usualmente con letras griegas o mayúsculas (\\(\\mu\\) o \\(M\\), \\(\\sigma\\) o S, por ejemplo).\nCenso: Medición de toda la población. En otras palabras: un muestreo de toda la población en un momento determinado en el tiempo.\nMuestreo: Medición de un atributo en individuos de una población.\nMuestra: Conjunto de individuos de la población que son medidos (formalmente hace referencia a las mediciones en sí mismas).\nEstadístico: Función definida sobre los valores medibles de una muestra. En otras palabras, la estimación del atributo de interés a partir de la muestra. Representados con letras latinas minúsculas (\\(\\bar{x}\\), \\(s\\)).\n\n\n\n\nFigure 7.3: Estadístico es a muestra como parámetro es a población."
  },
  {
    "objectID": "c07_muestreo.html#un-buen-muestreo",
    "href": "c07_muestreo.html#un-buen-muestreo",
    "title": "7  Introducción al muestreo",
    "section": "7.5 Un buen muestreo",
    "text": "7.5 Un buen muestreo\nVolvamos a nuestro ejemplo con las orcas. Realizamos cierto número de navegaciones en la temporada seca (tal vez por limitaciones logísticas), en las cuales encontramos y medimos 50 orcas. Ese número de individuos medidos es nuestro tamaño de muestra. Ese número es sumamente importante, pues define la representatividad del muestreo; es decir, qué tan buen “resumen” de la población es nuestra muestra. OJO: Esto solo aplica si el muestreo tuvo ciertas cualidades:\n\nAleatorio: Es decir, nuestro diseño de muestreo/experimental debe de permitir que todos los individuos de nuestra población sean medidos con la misma probabilidad. Esto es sumamente difícil de conseguir y podemos entrar a detalles filosófico de qué podemos considerar aleatorio y qué no, pero sí podemos tratar de hacerlo:\n\n\nExhaustivo: Muy de la mano (de hecho una consecuencia) de la aleatoriedad. El muestreo debe de considerar todos los posibles valores o atributos de la variable a medir. En nuestro ejemplo esto se reduce a que midamos orcas de todos los tamaños y que no sesguemos el muestreo a individuos únicamente grandes o pequeños, salvo que así lo definiéramos en nuestro marco de muestreo.\n\n\nExclusivo: Atributos o valores de un indicador deben ser mutuamente excluyentes; es decir, que no tengamos un individuo que mida 8 y 9.3 m, por ejemplo. Esto es un poco redundante en este escenario, pero si fuéramos a establecer rangos de edad como cría, juvenil, hembra adulta y macho adulto, que todos estén en una sola categoría, por lo que hay que decidir qué hacer con un juvenil con características de adulto. También tiene que ver con tener un marco de muestreo bien definido.\nPreciso: Tratar de tener el mayor número de distinciones posibles para tener una mejor descripción. En nuestras orcas es mejor medir en 830 cm que redondear a 8 m.\n\n\n7.5.1 Errores de muestreo/medición\nEsa “mejor descripción” me lleva también a hablar sobre el tema de los errores de muestreo o medición. Por más que querramos evitarlo, siempre habrá errores dentro de nuestro muestreo o dentro de nuestra medición. Tal vez utilizamos una regla de 30 cm para medir organismos que miden más de 30 cm, o tal vez utilizamos una regla de metal que se contrae si hace frío y se expande si hace calor. De cualquier manera, siempre vamos a ser “víctimas” de alguno de dos tipos de errores Figure 7.4:\n\nError sistemático: Un error que ocurre cada que realizamos la medición. Es constante y consistente. Si no somos conscientes de este error, nuestros resultados son inválidos. Pensemos en nuestras orcas, y que estamos utilizando fotogrametría aérea (fotos de dron) para hacer nuestros registros. Si calibramos nuestra medición al ángulo de visión que obtenemos a 10 m de altura y tomamos todas nuestras fotos a 11 m, el error va a ser constante y podemos corregirlo con una re-calibración. El problema está cuando no sabemos que este error está sucediendo; es decir, asumir que nuestras fotos se tomaron a 10 m y hacer la estimación con esa calibración.\nError aleatorio: Un error que no es constante. Volviendo a la fotogrametría, pensemos que obtuvimos un dron cuyo altímetro salió defectuoso, que no lo sabemos y que aunque marque 10 m de altura puede estar a ± 1m de ahí. Ese error es aleatorio. No podemos predecir si en un momento dado va a sobre o subestimar la altura. En este caso, los resultados son no confiables.\n\n\n\n\nFigure 7.4: Tipos de errores representados con dianas de tiro\n\n\nPosiblemente esos términos de no confiable o inválido te suenen alarmantes. La clave para poder contender con ellos es a) saber que existen y b) ser conscientes de su magnitud. Si sabemos qué tanto estamos subestimando por la diferencia de alturas de 10 a 11 m, podemos corregir el valor. Si sabemos cuál es la distribución del error de medición del altímetro, podemos modelarlo (la inferencia Bayesiana se presta muy bien para eso). ¿Con eso quiero decir que entonces no hay que cuidar estos detalles? PARA NADA, por el contrario, hay que cuidarlos tanto que hay que saber cómo remediarlos o incluirlos en nuestros análisis de datos.\n\n\n\n\n\n\nNote\n\n\n\nRecuerda que, desde un punto de vista práctico, la estadística es muy simple: le das un conjunto de datos y te regresa algunos números. Que lo resultante sea confiable no depende solo de utilizar una técnica adecuada, sino de que la materia prima (los datos) sea buena. ¿Has probado un jugo de naranja hecho con naranjas “pasadas”?"
  },
  {
    "objectID": "c07_muestreo.html#tipos-de-muestreo-y-representatividad",
    "href": "c07_muestreo.html#tipos-de-muestreo-y-representatividad",
    "title": "7  Introducción al muestreo",
    "section": "7.6 Tipos de muestreo y representatividad",
    "text": "7.6 Tipos de muestreo y representatividad\nEn nuestro ejemplo tenemos mediciones de 50 orcas, asumamos que controlamos nuestros errores y que, por lo tanto, esos valores son válidos y confiables. ¿Podemos estar seguros de que esas 50 orcas nos dan una representación adecuada de la población? En otras palabras, nuestra muestra es representativa? Para explicar esto, generemos una población hipotética de 1000 orcas, en la cual hay 500 hembras y 500 machos y, para simplificarnos la existencia, solo hay individuos adultos. Para seguir con la simplificación, asumiremos que la longitud en ambos sexos tiene una distribución normal, con un promedio de 9.6 m para machos y 8.2 m para hembras y una dispersión de 0.1 m (más adelante hablaremos de todos estos detalles). En este escenario, podemos asumir que la media poblacional (el valor que queremos inferir) es el promedio de los promedios de ambos sexos:\n\nmachos <- rnorm(500, mean = 9.6, sd = 0.1)\nhembras <- rnorm(500, mean = 8.2, sd = 0.1)\nmedia_real <- mean(c(9.6, 8.2))\npobl <- data.frame(sexo = c(rep(\"M\", 500), rep(\"H\", 500)),\n                   lt = c(machos, hembras))\n\nSiempre es más fácil ver un gráfico:\n\nlibrary(ggplot2)\npobl_dens <- ggplot(data = pobl, aes(x = lt)) +\n             geom_density(color = \"dodgerblue4\") +\n             theme_bw() +\n             geom_vline(xintercept = media_real,\n                        color = \"forestgreen\") +\n             labs(title = \"Densidad de la LT de orcas\",\n                  x = element_blank(),\n                  y = element_blank())\npobl_dens\n\n\n\n\nY ahora hablemos de los diseños probabilísticos de muestreo, partiendo de este ejemplo.\n\n7.6.1 Muestreo Aleatorio Simple\nEs, como el nombre lo indica, el más simple de los diseños. La idea es que haremos el muestreo de manera que la probabilidad de muestrear cualquier individuo de la población es uniforme para todos los individuos; es decir, la misma. PREGUNTA: ¿Esto se sostiene para nuestro ejemplo? Cuando tenemos diseños observacionales como este, siempre vamos a infringir, de una manera u otra, con la parte aleatoria. Estamos sujetos a si encontramos o no a los animales, si la “curiosidad” por la embarcación es variable, etc., etc., etc., por lo que, dependiendo de lo que estemos realizando, podemos solo “asumir” que el muestreo fue aleatorio. En este caso de ejemplo, por fortuna, podemos darnos el “lujo” de hacerlo de manera adecuada; es decir:\n\nDefinir la población estadística (hecho)\nDefinir un conjunto de muestras con la misma probabilidad de ser escogidas. Son nuestras 50 muestras a obtener, para lo cual\nSeleccionaremos una de las muestras utilizando números aleatorios.\n\nHagamos entonces el ejercicio para una muestra y luego dejemos que R haga lo demás:\n\nind <- round(runif(1, min = 1, max = 1000))\nind\n\n[1] 660\n\npobl$lt[ind]\n\n[1] 8.116228\n\n\nAquí lo que hicimos fue tomar un individuo de nuestra población de manera uniformemente “aleatoria”, en este caso el número 267 y medirlo. Ahora tenemos que repetir esa acción otras 49 veces, una a una, o podemos decirle a R que lo haga en un solo paso (imaginemos que somos nosotros quienes lo hacemos):\n\nmuestras <- sample(pobl$lt, size = 50, replace = FALSE)\nhead(muestras)\n\n[1] 9.512846 9.574236 8.116110 9.518170 9.535781 8.124528\n\n\nNotarás dos cosas: 1) no sabemos a qué individuos corresponden esas muestras, lo cual sería problemático si no tuviéramos 2) el argumento replace = FALSE. Con esto lo que hicimos fue un muestreo sin reemplazo; es decir, una vez que un individuo fue medido se “retiró” de la población, en el sentido de que no puede ser vuelto a medir. Este tipo de muestreo es más preciso, pues no tenemos duplicados o pseudo-réplicas. En nuestro ejemplo de las orcas es correcto asumir que esto puede ser así, pues podemos saber quién es quien con fotos de sus aletas dorsales y, con ello, descartar las mediciones duplicadas. Desafortunadamente, esto no siempre es así, y es algo que debemos de considerar al diseñar nuestro muestreo. Dicho esto, ya tenemos nuestras 50 muestras, entonces podemos realizar la estimación de nuestro promedio (media):\n\nmean(muestras)\n\n[1] 8.917692\n\n\nPongámosla en nuestro gráfico inicial como una línea vertical para ver en dónde quedó:\n\npobl_dens + geom_vline(xintercept = mean(muestras),\n                       color = \"firebrick\")\n\n\n\n\nNo está exactamente donde debería de estar. Para complicar más las cosas, añadamos un pequeño sesgo a nuestro muestreo: No es completamente aleatorio, sino que hubo algún factor que hiciera que muestreáramos preferencialmente a los machos. Tal vez eran más curiosos y era más fácil tomar una foto con la embarcación como referencia. Simulemos ese escenario. Nuestra muestra seguirá siendo de 50, pero esta vez tendremos 35 machos y 15 hembras:\n\nmuestras <- c(sample(machos, size = 35, replace = F),\n              sample(hembras, size = 15, replace = F))\npobl_dens + geom_vline(xintercept = mean(muestras),\n                       color = \"Firebrick\")\n\n\n\n\nAquí la estimación ya se aleja mucho más de nuestra media “real”. La razón es, sin duda, el sesgo que añadimos hacia los machos.\n\n\n7.6.2 Muestreo Aleatorio Estratificado\nEn este caso, donde la población se encuentra sub-dividida en estratos no traslapados y estos son muestreados de manera independiente, estamos hablando de un Muestreo Aleatorio Estratificado. Este es simplemente una “mezcla” de MAS, en el sentido que haremos un muestreo aleatorio simple dentro de cada estrato, aunque es importante conocer su tamaño de antemano. Para obtener la media poblacional solo obtenemos el promedio de ambos promedios:\n\npobl_dens + geom_vline(xintercept = mean(c(mean(muestras[1:35]),\n                                           mean(muestras[36:50]))),\n                       color = \"firebrick\")\n\n\n\n\nMatemáticamente la ecuación es más complicada, primero obtendríamos el peso de cada estrato (\\(W_s\\)), donde \\(n_s\\) es el tamaño del estrato \\(s\\) y \\(N\\) es el tamaño poblacional total:\n\\[\nW_s = \\frac{n_s}{N}\n\\]\nLuego, la media de la población estratificada está dada por:\n\\[\n\\mu = \\frac{\\sum_{s = 1}^{S} n_s*\\bar{x}_s}{N} \\\\\n\\therefore \\\\\n\\mu = \\sum_{s = 1}^{S} W_s*\\bar{x}_s\n\\]\nComprobémoslo:\n\n# Peso de cada estrato:\nw <- c((500/1000), (500/1000))\nm <- c(mean(muestras[1:35]), mean(muestras[36:50]))\n\npobl_dens + geom_vline(xintercept = sum(w*m), color = \"Firebrick\")\n\n\n\n\nLa igualdad podría demostrarse matemáticamente (solo aplica con estratos de igual tamaño), pero eso queda como ejercicio si te da curiosidad. OJO: Hacer esta estimación de la media total solo es válida si a) tiene sentido recuperarla o estimarla, lo cual depende totalmente de la pregunta de investigación (puede que tenga más sentido estimar la media para cada grupo) y b) si el muestreo es representativo. Veamos qué pasa si muestreamos únicamente 10 y 5 individuos:\n\nmuestras <- c(sample(machos, size = 10, replace = F),\n              sample(hembras, size = 5, replace = F))\n\npobl_dens + geom_vline(xintercept = mean(c(muestras[1:10],\n                                           muestras[11:15])),\n                       color = \"Firebrick\")\n\n\n\n\nComo era de esperarse, la estimación está sumamente sesgada. Esto es por el problema de la representatividad; es decir, nuestra muestra es muy pequeña (especialmente para las hembras) y, por lo tanto, no es una buena imagen de lo que pasa a nivel población. Como ejercicio, estima la media de la población estratificada con las ecuaciones que vimos arriba. ¿Cambió el resultado?\nEste problema tiene una “solución”: muestrear más. ¿Qué tanto? Depende de la precisión (grado de error) que querramos. La relación entre el tamaño de muestra, la Varianza poblacional y la precisión está dada por:\n\\[\nn = \\frac{1.96^2 S^2}{d^2}\n\\]\n¿De dónde sale ese 1.96? Es el límite al 95% de confianza (en pruebas de hipótesis hablaremos de qué es eso) de una distribución normal, dado en términos de desviaciones estándar, por lo que podríamos cambiarlo por cualquier otro número para representar cualquier intervalo de confianza. ¿Qué pasa si la población no tiene una distribución normal? A ese 1.96 tenemos aproximadamente el 75% de los datos poblacionales (Desigualdad de Chebyshev), independientemente de su distribución. No entraré en esos detalles, lo que es realmente importante es que debemos de conocer la varianza poblacional, lo cuál es un problema y de los grandes. Gigante, de hecho. Podemos estimarla a partir de la muestra, pero resulta que, si no es representativa, nuestra estimación de la varianza tampoco es confiable. En este caso, para tener un muestreo representativo al 95% de confianza, considerando la varianza poblacional de \\(0.1^2\\) y con una precisión de 0.05m, necesitaríamos un tamaño de muestra de mínimo 15 individuos (por sexo).\n\nn <- ((1.96^2)*(0.1^2))/0.05^2\nround(n)\n\n[1] 15\n\n\nO podemos ver cuál es la precisión de la estimación de la media de las hembras con nuestra muestra de 5 individuos:\n\nd <- sqrt(((1.96^2)*(0.1^2))/5)\nd\n\n[1] 0.08765386\n\n\nEs decir, tenemos un márgen de error aproximado de 0.87m para la media de las hembras. A esto lo acompaña algo que se conoce como pruebas de potencia, sobre lo cual encontrarás referencias en la sección de lecturas recomendadas del servidor de Discord. También es importante mencionar que en la sesión de pruebas de hipótesis hablaremos de cómo representar la incertidumbre en nuestras estimaciones (intervalos de confianza). Por el momento basta que te lleves el mensaje de que es importante considerar qué factores pueden estar generando “ruido” en tu diseño. Si tu pregunta se puede responder mediante un experimento en condiciones controladas, controla toda posible fuente de variación externa al factor que te interesa, de manera que lo que midas refleje únicamente lo que quieras responder. Si es un estudio observacional, acota tu marco de muestreo lo más posible. En ambos casos, y en la medida de lo posible, mide todas las fuentes de variación que pudieran estar influenciando tus observaciones e inclúyelas en tus análisis.\nEsto sería todo para esta sesión. Espero que haya sido de tu agrado y, sobre todo, que aunque no haya podido resolver todas tus incertidumbres, te haya motivado a reflexionar sobre la importancia de un buen muestreo. La inferencia estadística NO es magia negra que pueda resolver nuestros problemas, simplemente nos ayuda a tomar decisiones en situaciones de incertidumbre pero, que esas decisiones sean buenas (o no) depende totalmente de los datos que tengamos disponibles (y un poco de suerte).\n\n\n\n\nHaig BD. 2010. What is a spurious corelation? Understanding Statistics 2:125–132. DOI: 10.1207/S15328031US0202_03.\n\n\nHöfer T, Przyrembel H, Verleger S. 2004. New evidence for the theory of the stork. Paediatric and Perinatal Epidemiology 18:88–92.\n\n\nSies H. 1988. A new parameter for sex education. Nature 332:495."
  },
  {
    "objectID": "s03_basics.html#objetivo-de-aprendizaje",
    "href": "s03_basics.html#objetivo-de-aprendizaje",
    "title": "Técnicas básicas",
    "section": "Objetivo de aprendizaje",
    "text": "Objetivo de aprendizaje"
  },
  {
    "objectID": "c08_descriptiva.html",
    "href": "c08_descriptiva.html",
    "title": "8  Estadística descriptiva",
    "section": "",
    "text": "9 Librerías"
  },
  {
    "objectID": "c08_descriptiva.html#descripciones",
    "href": "c08_descriptiva.html#descripciones",
    "title": "8  Estadística descriptiva",
    "section": "9.1 Descripciones",
    "text": "9.1 Descripciones\nEn la sesión de visualización hablamos de que hay una gran cantidad de formas en las cuales podemos describir el mundo que nos rodea. Una imágen habla más que mil palabras, y es una forma de resumir nuestros datos; sin embargo, está lejos de ser la única y, de hecho, por lo general buscamos que el gráfico acompañe a un resumen numérico, en forma de un solo valor o índice. A los seres humanos nos gusta la simpleza, después de todo. Bueno, de eso es de lo que vamos a hablar hoy.\nEstoy seguro de que algunos de los conceptos que revisaremos en esta sesión los has revisado ya con anterioridad, por lo que no te haré el cuento largo, sino que me enfocaré más en las peculiaridades de cada una de las medidas y gráficos, en qué escenarios son útiles, cuándo no lo son tanto y lo enlazaremos con temas que veremos más adelante."
  },
  {
    "objectID": "c08_descriptiva.html#medidas-de-tendencia-central",
    "href": "c08_descriptiva.html#medidas-de-tendencia-central",
    "title": "8  Estadística descriptiva",
    "section": "9.2 Medidas de tendencia central",
    "text": "9.2 Medidas de tendencia central\nComencemos hablando con lo que, por lo general, es nuestro mayor interés: hacia dónde tienden nuestros datos. Para ello tenemos algunas medidas de tendencia central; es decir, literalmente describimos nuestros datos a partir de dónde se acumulan más.\n\n9.2.1 Media\nLa primera de estas medidas es, posiblemente, la más común de todas: la media o el promedio. En su forma más simple; es decir, la media aritmética la obtenemos sumando todos nuestros datos y dividiéndolos entre su número:\n\\[\n\\bar{x} = \\frac{\\sum_{i = 1}^n x_i}{n}\n\\]\nEsto es conocimiento general, y es algo con lo que todos los estudiantes somos torturados eventualmente. Bueno, más que recordar cómo calcularlo e implementarlo en R (mean(x), donde x es un vector de observaciones), pensemos en qué representa. La media es, literalmente, un indicador de hacia qué valor se están acumulando nuestros datos, tal y como si colgáramos cosas en un tendedero (házme un favor y piensa en que el siguiente gráfico está invertido verticalmente):\n\nset.seed(0)\nx <- data.frame(x = rnorm(100, mean = 0))\nplot1 <- ggplot(data = x, aes(x = x)) +\n         geom_density(color = \"dodgerblue4\") +\n         theme_bw() +\n         geom_vline(xintercept = mean(x$x),\n                    color = \"dodgerblue4\")\nplot1\n\n\n\n\nEn este caso, la mayor parte de nuestros datos están acumulados alrededor de 0. ¿Qué crees que pase si añadimos otros 20 datos, esta vez acumulados en 5? Veámos el cambio:\n\nset.seed(0)\nx2 <- rbind(x, data.frame(x = rnorm(20, mean = 5)))\nplot2 <- plot1 +  geom_density(aes(x = x),\n                               data = x2,\n                               color = \"firebrick\")\nplot2 + geom_vline(xintercept = mean(x2$x),\n                   color = \"firebrick\")\n\n\n\n\nComo era de esperarse, la media se “jaló” a la derecha, y de manera bastante notable. Esos 5 valores extremos tuvieron un peso bastante importante. Literalmente fue como si hubiéramos colgado cinco cosas más en nuestro tendedero, pero alejadas de la ropa que colgamos en un inicio. Aunque la media es una manera muy efectiva de resumir nuestros datos, esto solo es cierto si estos se parecen a una distribución normal (en la sesión de técnicas paramétricas hablaremos duro y tendido con respecto a esto), pero si no, como en nuestro caso con los nuevos valores, es necesario buscar una alternativa.\n\n9.2.1.1 Media ponderada\nUna de ellas es una modificación a la media, en la cuál cada valor tiene su propia ponderación o su propia importancia. Esa es la media con la que más padecemos los estudiantes de secundaria hacia arriba, pues el examen tiene una ponderación distinta a las tareas, por ejemplo. En mi caso personal, era un tormento cuando se ponderaba más las tareas que el examen, pero eso es otra historia. ¿Por qué es importante? Porque podemos utilizarla para “regresar” nuestra media a su lugar; de hecho, esta es la base fundamental detrás de las regresiones robustas, en donde el peso de cada observación disminuye según su distancia de 0:\n\n# Pesos\nw <- max(abs(0-x2$x))/abs(0 - x2$x)\nwmean <- weighted.mean(x2$x, w)\nplot2 + geom_vline(xintercept = wmean,\n                   color = \"firebrick\")\n\n\n\n\n\n\n9.2.1.2 Media geométrica\nEsta es menos conocida, y representa el promedio de porcentajes, razones o tasas de crecimiento. Se expresa como la raiz n-ésima del producto de los n valores:\n\\[MG = \\sqrt[n]{\\Pi_i^nx_i}\\]\nPensemos en que estimamos la tasa de crecimiento poblacional (\\(\\lambda\\)) anual de ballenas jorobadas en tres años seguidos, a partir de un modelo de marca-recaptura, y los valores que obtuvimos fueron 1.03, 0.98, 1.4, 0.94:\n\nlamb <- c(1.03, 0.98, 1.4, 0.94)\nprod(lamb)^(1/length(lamb))\n\n[1] 1.073569\n\n\nEs decir, el crecimiento poblacional promedio fue del 7%.\nOtra manera de calcularla es:\n\nexp(mean(log(lamb)))\n\n[1] 1.073569\n\n\n¿Te animas a encontrar la igualdad matemática?\n\n\n\n9.2.2 Mediana\nUna alternativa más es la mediana. A diferencia de la media, que “busca” hacia donde se están acumulando los datos, la mediana nos indica exactamente que valor se encuentra en el centro de nuestra base de datos. Si partimos nuestra base de datos en 100 partes iguales (100%), cada parte representa un cuantil (1%). Cada diez cuantiles tenemos un decil, cada 25 cuantiles tenemos un cuartil (cuartiles 25%, 50% y 75%), y en el cuartil 50 tenemos la mediana. Esta es, entonces, mucho menos sensible a valores extremos:\n\nplot2 + geom_vline(xintercept = median(x2$x),\n                   color = \"firebrick\")\n\n\n\n\nLa estimación no es exactamente la misma que la media de los datos originales o de la media ponderada según su distancia a 0; sin embargo, el efecto es notablemente menor que con la media tradicional. Esta resistencia a valores extremos es lo que hace que las técnicas no paramétricas estén basadas en la mediana, en vez de la media.\n\n\n9.2.3 Moda\nLa moda corresponde al valor que más se repite en un conjunto de datos. Con datos continuos en el sentido estricto no existe; sin embargo, en muchos casos sí que podemos tener repetidos dependiendo de la escala y la precisión de nuestro instrumento. Otra manera de calcularla es discretizando nuestros datos y encontrar el intervalo más frecuente. Una propiedad interesante de la moda es que su valor corresponde con el valor que tiene la mayor probabilidad dentro de la distribución, por lo que puede ser útil en ciertos casos de Inferencia Bayesiana. Para calcularla podemos utilizar la función Mode(x) de la librería DescTools:\n\nletmode <- DescTools::Mode(c(\"a\", \"a\", \"b\", \"c\", \"d\"))\nletmode\n\n[1] \"a\"\nattr(,\"freq\")\n[1] 2\n\n\nA lo largo de este curso no aplicaremos la moda, solo la agregué para que la tengas presente."
  },
  {
    "objectID": "c08_descriptiva.html#medidas-de-dispersión",
    "href": "c08_descriptiva.html#medidas-de-dispersión",
    "title": "8  Estadística descriptiva",
    "section": "9.3 Medidas de dispersión",
    "text": "9.3 Medidas de dispersión\nAl igual que en el caso anterior, repasaremos rápidamente las medidas de dispersión, con el objetivo de explorar la intuición detrás de ellas y su relación con otros conceptos que revisaremos más adelante. Independientemente de cuál utilicemos, todas las medidas de dispersión indican justamente eso, qué tan grande es la variabilidad de una distribución, ya sea de nuestros datos o la distribución muestral del parámetro que estemos estimando.\n\n9.3.1 Desviación estándar y Varianza\nEn pocas palabras, la varianza es una medida de la dispersión promedio de los datos; es decir, cuál es el área promedio que abarca la dispersión de los datos. En la sección de Multivariado vamos a ver cuál es la relación entre la varianza y la covarianza, a entender a la varianza como un caso especial de la covarianza y ver de dónde sale esa suma de cuadrados. Esto último también me lleva a que cada que leas suma de cuadrados pienses en una medida de dispersión o en la varianza de los datos.\n\\[\n\\sigma^2 = \\frac{\\sum{(x_i - \\mu)^2}}{N}\n\\]\nLa desviación estándar, por otra parte, es simplemente la raíz cuadrada de la varianza. Si la varianza representa un área, la desviación estándar representa una distancia, la distancia promedio que existe entre cada uno de los datos y la media. Estas dos medidas (la desviación estándar y la varianza) son sumamente útiles y utilizadas en los procesos estadísticos; de hecho, junto con la media, son los principales parámetros poblacionales que usualmente queremos estimar a partir de nuestra muestra. Una aclaración es que la ecuación de arriba es para calcular la varianza poblacional, mientras que si queremos calcular la varianza muestral aplicaremos una corrección con los grados de libertad (que definiremos más adelante)\n\\[\ns^2 = \\frac{\\sum(x_i - \\bar{x})^2}{N-1}\n\\]\nEsta varianza muestral se considera un estimador insesgado de la varianza poblacional.\n\n9.3.1.1 Estimadores\nEste es un buen momento para hablar de los estimadores. ¿Qué es un estimador? Una medida que utilizaremos para estimar un parámetro poblacional. Evidentemente, no puede ser cualquier número ni cualquier medida, debe de tener ciertas características. Particularmente:\n\nInsesgado: Es decir, que la media de la distribución del estimador sea igual al parámetro. De nuevo, en la sesión de técnicas paramétricas vamos a hablar sobre distribuciones muestrales, el teorema del límite central y su implicación para el supuesto de normalidad que a veces puede ser un dolor de cabeza. Por lo pronto entiende que “la media de la distribución del estimador” hace referencia a que, si hicieramos una cierta cantidad de muestreos y calculamos algún parámetro para cada muestreo, el promedio de esas estimaciones debe ser igual al parámetro poblacional.\nConsistencia: Es la propiedad en la que un estimador se aproxima al valor del parámetro conforme incrementa el tamaño de muestra, lo cual también tiene que ver con el teorema del límite central que revisaremos más adelante.\nEficiencia: La estimación tiene el error estándar más pequeño cuando se compara con otro estimador. Por ejemplo, en una distribución Normal, la media y la mediana son prácticamente iguales; sin embargo, el error estándar de la media es \\(\\frac{\\sigma}{\\sqrt{n}}\\), mientras que el de la mediana es \\(\\approx 1.25\\) veces ese valor.\nSuficiencia: El estimador es, por sí mismo, capaz de transmitir toda la información disponible en la muestra sobre el valor del parámetro.\n\nPor estas 4 razones es que la mayor parte de nuestras inferencias están en relación a la media poblacional, estimada a partir de la media muestral. La mediana no se considera un buen estimador de la media poblacional si la distribución no es simétrica, pues, como vimos arriba, está sesgada en relación a la media poblacional. Por otra parte, su error estándar es mayor que el error estándar de la media (en términos de sus distribuciones muestrales), ni utiliza todos los datos (solo el cuantil 50).\n\n\n\n9.3.2 Coeficiente de variación\nEl coeficiente de variación, también conocido como la desviación estándar relativa, es la relación que existe entre la desviación estándar y la media de los datos, tal que:\n\\[\nCV = \\frac{\\sigma}{\\mu}*100\n\\]\nEste es especialmente útil cuando queremos comparar las dispersiones de dos cosas que están en distinta escala, o expresar en un porcentaje qué tan grande es nuestra variabilidad en relación a nuestra estimación. Si hacemos una estimación de tamaño poblacional de 1000 individuos, con un coeficiente de variación del 50% quiere decir que nuestra variabilidad es de la mitad de nuestra estimación.\n\n\n9.3.3 Error estándar\nEsta medida es especialmente útil en la estimación de los intervalos de confianza de cualquier parámetro, y representa la desviación estándar de su distribución muestral, el cuál podemos estimar como:\n\\[\n\\sigma_{\\bar{x}} \\approx \\frac{\\sigma_x}{\\sqrt{n}}\n\\]\nNormalmente nosotros no estimaremos o interpretaremos esta medida, sino que iremos directamente a los intervalos de confianza para expresar la incertidumbre alrededor de nuestras estimaciones."
  },
  {
    "objectID": "c08_descriptiva.html#gráficos",
    "href": "c08_descriptiva.html#gráficos",
    "title": "8  Estadística descriptiva",
    "section": "9.4 Gráficos",
    "text": "9.4 Gráficos\nEn la sesión de visualización vimos las consideraciones que debemos de tener en cuenta para hacer una visualización efectiva, pero no hablamos de los tipos de gráficos que podemos realizar. En esta sesión, entonces, no entraremos a ver los detalles de la visualización, simplemente hablaremos de los gráficos más comunes, en qué escenarios son útiles y cómo construirlos utilizando ggplot2. Para esta parte utilizaremos datos de biometrías de pinguinos, contenidos en la librería palmerpenguins\n\n9.4.1 Gráfico de frecuencias\nTambién nombrado a veces histograma. Es, posiblemente, el gráfico más sencillo de todos, pues lo único que hacemos es poner una barra a la altura del número de individuos que hay en una clase o intervalo. Para construirlo en ggplot2 utilizaremos la capa geom_histogram:\n\nggplot(data = penguins,\n       aes(x = bill_length_mm)) +\n  geom_histogram(fill = \"dodgerblue4\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite values (stat_bin).\n\n\n\n\n\nSi tenemos más de un grupo y queremos ver todas las distribuciones de frecuencia podemos pasar el argumento fill dentro de los argumentos de estética (aes())\n\nggplot(data = penguins,\n       aes(x = bill_length_mm, fill = species)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite values (stat_bin).\n\n\n\n\n\nHabrás notado que ggplot2 nos dio una notificación con respecto al número de intervalos (cajas) utilizados para estimar las frecuencias, en particular que utilizó 30. Podemos seleccionar una cantidad manualmente, por ejemplo 100, y especificarla con el argumento bins, o especificar la amplitud del intervalo con el argumento binwidth:\n\nggplot(data = penguins,\n       aes(x = bill_length_mm, fill = species)) +\n  geom_histogram(bins = 100)\n\nWarning: Removed 2 rows containing non-finite values (stat_bin).\n\n\n\n\n\n¿El problema? ¿Cómo sabemos cuántos intervalos utilizar? Si ya hay intervalos establecidos, por ejemplo longitudes para estados de madurez sexual, podemos establecer esos intervalos, pero si nuestra distribución es continua podemos utilizar una mejor alternativa.\n\n\n9.4.2 Gráfico de densidad\nLos gráficos de densidad nos permiten, justamente, representar gráficamente la distribución de una variable continua. Recordarás de la clase de probabilidad que la probabilidad de un valor continuo es teóricamente 0, pero podemos estimar la densidad de probabilidad de un punto determinado si hacemos el intervalo infinitesimalmente pequeño, y es esto lo que representa un gráfico de densidad. Piensa en él como una versión suavizada de un gráfico de frecuencias:\n\nggplot(data = penguins,\n       aes(x = bill_length_mm, fill = species)) +\n  geom_density(color = NA)\n\nWarning: Removed 2 rows containing non-finite values (stat_density).\n\n\n\n\n\nEso se ve bastante mejor, pues ya no tenemos la incertidumbre de cuántas “cajas” formar para encajonar nuestros datos, pero no es lo más adecuado para comparar distribuciones de manera gráfica. Afortunadamente, también tenemos alternativas para eso.\n\n\n9.4.3 Gráfico de cajas y bigotes\nUna es el clásico gráfico de cajas y bigotes, en el cuál hay (por grupo) un indicador (punto, línea horizontal o muesca) de la tendencia central (media, mediana), una caja que indica una medida de dispersión (desviación estándar, rango intercuantil, error estándar) y dos “bigotes” (líneas verticales) que indican los límites “aceptables” de la distribución. Adicionalmente tenemos puntos “libres” que indican valores extremos. En ggplot2 podemos construirlos con la capa geom_boxplot, siempre que especifiquemos en aes() una variable de agrupamiento para x, la variable de la que queremos ver el interés en y y, opcionalmente, un argumento de color que corresponda con los grupos en x.\n\nggplot(data = penguins,\n       aes(x = species, y = bill_length_mm,\n           color = species)) +\n  geom_boxplot(fill = NA)\n\nWarning: Removed 2 rows containing non-finite values (stat_boxplot).\n\n\n\n\n\nInfortunadamente perdemos mucha información si lo comparamos con el gráfico de densidad. Si tan solo hubiera una manera de mezclarnos… ¡Espera! Sí que la hay.\n\n\n9.4.4 Gráfico de violín\nLos gráficos de violín son, justamente, una alternativa más “fina” a los gráficos de cajas y bigotes, en los cuales podemos mostrar la distribución completa de cada grupo. En ggplot2 utilizaremos la capa geom_violin, que requiere exactamente la misma información que el gráfico de caja y bigotes:\n\nggplot(data = penguins,\n       aes(x = species, y = bill_length_mm,\n           color = species)) +\n  geom_violin(fill = NA)\n\nWarning: Removed 2 rows containing non-finite values (stat_ydensity).\n\n\n\n\n\nAfortunadamente, uno no está peleado con el otro, y podemos añadir la información resumida disponible en el gráfico de cajas y bigotes:\n\nggplot(data = penguins,\n       aes(x = species, y = bill_length_mm,\n           color = species)) +\n  geom_violin(fill = NA) +\n  geom_boxplot(fill = NA, width = 0.1)\n\nWarning: Removed 2 rows containing non-finite values (stat_ydensity).\n\n\nWarning: Removed 2 rows containing non-finite values (stat_boxplot).\n\n\n\n\n\nCon estos gráficos vemos la relación entre una variable categórica y una variable numérica, pero ¿qué pasa si tenemos dos variables numéricas?\n\n\n9.4.5 Gráfico de dispersión\nEl gráfico más básico es el gráfico de dispersión. Simplemente es un gráfico de las coordenadas dadas por una variable x con respecto a una variable y. En ggplot2 utilizaremos la capa geom_point:\n\nggplot(data = penguins,\n       aes(x = flipper_length_mm,\n           y = bill_length_mm)) +\n  geom_point(color = \"dodgerblue4\")\n\nWarning: Removed 2 rows containing missing values (geom_point).\n\n\n\n\n\nAl igual que en el gráfico de frecuencias, podemos colorear los puntos según una variable categórica:\n\nggplot(data = penguins,\n       aes(x = flipper_length_mm,\n           y = bill_length_mm)) +\n  geom_point(aes(color = species))\n\nWarning: Removed 2 rows containing missing values (geom_point).\n\n\n\n\n\nE, incluso, según una variable continua:\n\nggplot(data = penguins,\n       aes(x = flipper_length_mm,\n           y = bill_length_mm)) +\n  geom_point(aes(color = body_mass_g))\n\nWarning: Removed 2 rows containing missing values (geom_point).\n\n\n\n\n\n¿Y si no me interesa ver todos los puntos, sino las distribuciones bivariadas? También tenemos alternativas.\n\n\n9.4.6 Gráfico de densidad bivariado\nPodemos utilizar un gráfico de densidad bivariado, el cual muestra contornos correspondientes a la densidad de los puntos. Su interpretación es exactamente igual a un mapa topográfico. Los contornos más pequeños representan la mayor densidad, mientras que los más grandes una menor densidad.\n\nggplot(data = penguins,\n       aes(x = flipper_length_mm,\n           y = bill_length_mm)) +\n  geom_density2d(aes(color = species))\n\nWarning: Removed 2 rows containing non-finite values (stat_density2d).\n\n\n\n\n\n\n\n9.4.7 Heatmaps\n¿Y si mis dos variables son discretas? No temas, para eso tenemos heatmaps:\n\nggplot(data = penguins,\n       aes(x = species,\n           y = island)) +\n  geom_tile(fill = \"dodgerblue4\",\n            color = \"dodgerblue4\")\n\n\n\n\nSi tienes una variable adicional (pueden ser frecuencias o alguna variable continua) puedes agregarla con el argumento fill dentro de aes:\n\nggplot(data = penguins,\n       aes(x = species,\n           y = island)) +\n  geom_tile(aes(fill = bill_length_mm))\n\n\n\n\n¿Hay algún equivalente para tres variables numéricas? Por supuesto.\n\n\n9.4.8 Gráfico de contornos\nPodemos generar un gráfico de contornos, utilizando la capa geom_contour. OJO: para esta hay que hacer cierto procesamiento de los datos, en el sentido que hay que generar una malla uniforme de coordenadas x, y, z. Esto queda fuera de la discusión por este momento, pero lo revisitaremos cuando grafiquemos la zona de decisión de un análisis de funciones discriminantes lineales en la sesión de clasificación.\n\n\n9.4.9 Otros gráficos\nEstos gráficos están lejos de ser los únicos a nuestra disposición. Para comparar distribuciones también tenemos forest plots o ridgelines. Si queremos ver la relación entre más de dos variables, considerando cada individuo, podemos utilizar gráficos de coordenadas paralelas, para lo cual utilizaremos la función ggparcord de la librería GGally:\n\nGGally::ggparcoord(data = penguins,\n                   groupColumn = \"species\",\n                   columns = 3:6)\n\n\n\n\nY muchos otros más que no tendríamos tiempo de revisar en una sola sesión, cada uno con un uso particular. Algunos de ellos los veremos durante el curso, mientras que algunas otras alternativas las puedes encontrar en este enlace. También es importante mencionar que R base y ggplot2 no son las únicas librerías para generar gráficos en R. Podemos utilizar D3 o plotly, por ejemplo, para generar no solo otro tipo de gráficos, sino también hacerlos interactivos.\nEsto sería todo para esta sesión. Espero que haya sido un buen recordatorio y que te sea de utilidad en algún futuro."
  },
  {
    "objectID": "c09_ph0.html#tipos-de-errores",
    "href": "c09_ph0.html#tipos-de-errores",
    "title": "9  Pruebas de hipótesis",
    "section": "9.1 Tipos de errores",
    "text": "9.1 Tipos de errores"
  },
  {
    "objectID": "c09_ph0.html#valor-de-p",
    "href": "c09_ph0.html#valor-de-p",
    "title": "9  Pruebas de hipótesis",
    "section": "9.2 Valor de p",
    "text": "9.2 Valor de p\n\nlibrary(ggplot2)\n\nn <- 100000\nv1 <- data.frame(var = rnorm(n))\nsds <- data.frame(xf = c(3, 1.96, 1))\n\nsds[\"AUC\"] <- NA\n\nfor (i in seq_along(sds$xf)) {\n  sds$AUC[i] <-\n    as.character(round(1 -length(v1$var[(v1$var < -sds$xf[i]) |\n                                        (v1$var > sds$xf[i])]) /n,\n                       2))\n}\n\nsds\n\n\n\n  \n\n\nuni.norm <- ggplot() + \n            geom_rect(data = sds, aes(xmin = -xf, xmax = xf, \n                                      ymin = 0, ymax = Inf, \n                                      fill = AUC),  alpha = 0.3) +\n            geom_density(data = v1, aes(var),\n                         kernel = \"gaussian\", \n                         colour = \"deepskyblue4\",\n                         fill = \"deepskyblue4\", \n                         alpha = 0.6) +\n            labs(x = \"Z\",\n                 y = element_blank(),\n                 title =\n                   \"Gráfico de densidad de una distribución normal\",\n                 subtitle = expression(paste(\"n = 100000; \",\n                                             mu, \" = 0; \",\n                                             sigma, \" = 1\")),\n                 caption = \"Datos simulados\") +\n            theme(panel.grid.minor = element_blank(),\n                  panel.grid.major = element_blank(),\n                  panel.background = element_blank(),\n                  axis.line = element_blank(),\n                  aspect.ratio = 1/1.61,\n                  axis.ticks = element_blank(),\n                  text = element_text(family = \"Montserrat\",\n                                      colour = \"gray50\")\n                  ) +\n            scale_y_continuous(breaks = NULL) +\n            scale_x_continuous(breaks = c(-sds$xf, sds$xf))\n            \nuni.norm"
  },
  {
    "objectID": "c09_ph0.html#prueba-básica-t-de-student",
    "href": "c09_ph0.html#prueba-básica-t-de-student",
    "title": "9  Pruebas de hipótesis",
    "section": "9.3 Prueba básica: \\(t\\) de Student",
    "text": "9.3 Prueba básica: \\(t\\) de Student\nLa prueba de hipótesis más conocida, y con justa razón, es la prueba \\(t\\) de Student. Esta prueba se construye a partir de una distribución \\(t\\) de Student, la cual tiene tres parámetros: i) \\(\\mu\\): media o centro de la distribución, ii) \\(\\sigma\\): escala de la distribución (desviación estándar) y iii) \\(\\nu\\): grados de libertad.\n\n9.3.1 Prueba para muestras independientes\nLa primera variación la tenemos cuando queremos comparar las medias de dos muestras independientes o, mejor dicho, dos grupos independientes. Es decir, los individuos que forman al grupo 1 son diferentes de los que conforman al grupo 2. La ecuación original sufre una ligera modificación, en donde ahora se considera la desviación estándar mancomunada de las muestras; es decir, la variación en el valor dada por la existencia de ambos grupos. Esto es importante, pues no es lo mismo tener una diferencia promedio de 10g con una desviación mancomunada de 100 g a tener esos mismos 10g de diferencia con una desviación mancomunada de 10g.\nPara implementarla en R vamos a utilizar la función t.test(formula, data) que ya conocíamos, solo que añadiremos dos argumentos adicionales: var.equal = T y paired = F. var.equal hace referencia al supuesto de homogeneidad de varianzas que mencionamos antes. Si no se cumple podemos pasar var.equal = F y entonces se aplicará la prueba \\(t\\) de Welch, la cual modifica el modo en el que se estima la varianza mancomunada y permite contender con varianzas desigualees. paired, por otra parte, define si es una prueba para muestras independientes (F) o muestras dependientes (pareadas, T). Esto último lo veremos más adelante.\nCreemos primero un conjunto de datos:\n\nA <- rnorm(10, 10, 0.1)\nB <- rnorm(10, 11, 0.1)\n\ndf1 <- data.frame(grupo = \"A\", v1 = A)\ndf1 <- rbind(df1, data.frame(grupo = \"B\", v1 = B))\nhead(df1)\n\n\n\n  \n\n\n\n\nt.test(v1~grupo, data = df1, var.equal = T, paired = F)\n\n\n    Two Sample t-test\n\ndata:  v1 by grupo\nt = -20.489, df = 18, p-value = 6.343e-14\nalternative hypothesis: true difference in means between group A and group B is not equal to 0\n95 percent confidence interval:\n -1.1010855 -0.8962809\nsample estimates:\nmean in group A mean in group B \n       9.989406       10.988089 \n\n\nLa salida nos da los siguientes valores:\nt = valor del estadístico de prueba, indica la posición en el eje X en el gráfico de la distribución p-value = Es la proporción de la distribución fuera del área del valor crítico; es decir, 1-AUC hasta ese valor crítico. df = grados de libertad de la muestra. Representan el número de observaciones independientes; es decir, “retiramos la media”. Ejemplo: 10, 11, 12. La media es 11; por lo tanto, la cantidad de observaciones independientes de la media es 2.\nPodemos también presentar los resultados de manera gráfica. Para ello necesitaremos guardar los resultados de nuestra prueba t.test en un objeto y extraer la información de ahí. ¿Cómo verificamos cuál es el tipo de objeto?\n\nttest <- t.test(v1~grupo, data = df1, var.equal = T, paired = F)\ntypeof(ttest)\n\n[1] \"list\"\n\n\nEl objeto es una lista NOMBRADA, por lo que podemos acceder a su contenido utilizando el operador de [[]] (posición numérica o “nombre”) o el operador $. Guardemos el valor de p en un nuevo objeto para incluirlo en la gráfica:\n\np_val <- ttest$p.value\np_val\n\n[1] 6.343306e-14\n\n\nConstruyamos y grafiquemos los intervalos de confianza para la media de cada grupo (95%):\n\nICs <- rcompanion::groupwiseMean(v1~grupo, data = df1, conf = 0.95)\n\nerror.plot <- ggplot(data = ICs, aes(x = grupo, y = Mean)) +\n              geom_point(color = \"deepskyblue4\") + \n              geom_errorbar(aes(ymin = Trad.lower, ymax = Trad.upper),\n                            color = \"deepskyblue4\") +\n              theme(panel.grid.minor = element_blank(),\n                    panel.grid.major = element_blank(),\n                    panel.background = element_blank(),\n                    axis.line = element_blank(),\n                    aspect.ratio = 1/1.61,\n                    axis.ticks = element_blank(),\n                    text = element_text(colour = \"gray50\"),\n                    legend.position = \"none\") +\n              labs(x = \"Grupo\",\n                   y = \"x\",\n                   title = \"¿Diferencias significativas?\",\n                   subtitle = \n                     \"µ e IC para una variable aleatoria\",\n                   caption =  paste(\"t(v = \",\n                                    ttest$parameter, \", 0.05) = \",\n                                    round(ttest[[\"statistic\"]] ,2), \n                                    \"; \",\n                                    ifelse(p_val < 0.0001,\n                                           \"p < 0.0001\",\n                                           paste(\"p = \", p_val))))\n              \nerror.plot\n\n\n\n\n\n\n9.3.2 Prueba para muestras pareadas\nCarguemos los datos, que en este caso están contenidos en un archivo de excel:\n\ndependientes <- openxlsx::read.xlsx(\"datos/datos_t.xlsx\", sheet = 2)\ndependientes\n\n\n\n  \n\n\n\nNotar que está en formato compacto (no codificado), por lo tanto hay que transformarla:\n\ndependientes.m <- reshape2::melt(dependientes, value.name = \"FC\",\n                                 na.rm = T, variable.name = \"periodo\")\n\nNo id variables; using all as measure variables\n\ndependientes.m\n\n\n\n  \n\n\n\nAplicamos la prueba:\n\nt.test(FC~periodo, data = dependientes.m, paired = T, alternative = \"less\")\n\n\n    Paired t-test\n\ndata:  FC by periodo\nt = -6.1115, df = 9, p-value = 8.838e-05\nalternative hypothesis: true mean difference is less than 0\n95 percent confidence interval:\n      -Inf -12.32097\nsample estimates:\nmean difference \n          -17.6"
  },
  {
    "objectID": "c09_ph0.html#ejercicio",
    "href": "c09_ph0.html#ejercicio",
    "title": "9  Pruebas de hipótesis",
    "section": "9.4 Ejercicio",
    "text": "9.4 Ejercicio\nRealizar la prueba t con los datos de la hoja 1 del archivo, la cual contiene datos de dos muestras independientes. La tarea consiste en cargar los datos, realizar la prueba y presentar un gráfico en el que se reporten los resultados.\n\n\n\n\nSavage LJ. 1954. The foundations of statistics. John Wiley & Sons."
  },
  {
    "objectID": "c10_param.html#librerías",
    "href": "c10_param.html#librerías",
    "title": "10  Técnicas paramétricas",
    "section": "10.1 Librerías",
    "text": "10.1 Librerías\n\nlibrary(ggplot2)\nlibrary(corrplot)\nlibrary(gridExtra)\nlibrary(rstatix)\nlibrary(dplyr)\nlibrary(ggpubr)"
  },
  {
    "objectID": "c10_param.html#teorema-del-límite-central",
    "href": "c10_param.html#teorema-del-límite-central",
    "title": "10  Técnicas paramétricas",
    "section": "10.2 Teorema del Límite Central",
    "text": "10.2 Teorema del Límite Central\n“Dadas muestras aleatorias e independientes con N observaciones cada una, la distribución de sus medias se aproxima a una distribución normal conforme N incrementa, INDEPENDIENTEMENTE de la distribución poblacional”; es decir, mientras N sea grande, \\(\\bar{x} \\sim Normal\\). Para probar esto podemos hacer un ejercicio en el cual simulemos una población con distribución Gamma, cuya zona de mayor densidad se encuentra desplazada a la izquierda:\n\nset.seed(0)\ndatos <- data.frame(x = 1:1000, y = rgamma(1000, 1))\ngamma <- ggplot(data = datos, aes(y)) + \n         geom_density(fill = rgb(118,78,144,\n                                 maxColorValue = 255),\n                      alpha = 0.5, colour = \"white\") +\n         theme_bw() +\n         labs(title = \"Distribución Gamma\",\n              x = element_blank(),\n              y = element_blank()) +\n         theme(text = element_text(colour = \"gray40\"))\n\n#cairo_pdf(\"gamma.pdf\", family = \"Montserrat\",\n#          height = 5, width = 5*1.6, pointsize = 20)\ngamma\n\n\n\n#dev.off()\n\nCon nuestra población definida, podemos seleccionar algunos tamaños de muestra, realizar 1000 muestreos aleatorios, obtener la media de cada muestreo y graficar su distribución. Primero para N = 3:\n\nN = 3\nmedias <- data.frame(x = 1:1000,\n                     y = replicate(1000,\n                                   mean(sample(datos$y, N))))\n\n\ndist_n3 <- ggplot(data = medias, aes(y)) +\n           geom_density(fill = \"dodgerblue4\",\n                        alpha = 0.5, colour = \"white\") +\n           theme_bw() +\n           labs(title = sprintf(\"Distribución muestreal con N = %d\", N),\n                x = element_blank(),\n                y = element_blank()) +\n           theme(text = element_text(colour = \"gray40\"))\n\n#cairo_pdf(\"n_3.pdf\", family = \"Montserrat\",\n#          height = 5, width = 5*1.6, pointsize = 20)\ndist_n3\n\n\n\n#dev.off()\n\nAhora para N = 10. La distribución se aproxima más a una distribución normal:\n\nN = 10\nmedias <- data.frame(x = 1:1000,\n                     y = replicate(1000,\n                                   mean(sample(datos$y, N))))\n\ndist_n10 <- ggplot(data = medias, aes(y)) +\n            geom_density(fill = \"dodgerblue4\",\n                         alpha = 0.5, colour = \"white\") +\n            theme_bw() +\n            labs(title = sprintf(\"Distribución muestreal con N = %d\",\n                                 N),\n                 x = element_blank(),\n                 y = element_blank()) +\n            theme(text = element_text(colour = \"gray40\"))\n\n#cairo_pdf(\"n_10.pdf\", family = \"Montserrat\",\n#          height = 5, width = 5*1.6, pointsize = 20)\ndist_n10\n\n\n\n#dev.off()\n\nCon N = 30 la distribución es más cercana a una normal que a la gamma, por lo que usualmente se acepta que: con N≥30 la distribución muestreal de la media DEBERÁ ser normal:\n\nN = 30\nmedias <- data.frame(x = 1:1000,\n                     y = replicate(1000, mean(sample(datos$y, N))))\n\n\ndist_n30 <- ggplot(data = medias, aes(y)) +\n            geom_density(fill = \"dodgerblue4\",\n                         alpha = 0.5, colour = \"white\") +\n            theme_bw() +\n            labs(title = sprintf(\"Distribución muestreal con N = %d\",\n                                 N),\n                 x = element_blank(),\n                 y = element_blank()) +\n            theme(text = element_text(colour = \"gray40\"))\n\n#cairo_pdf(\"n_30.pdf\", family = \"Montserrat\",\n#           height = 5, width = 5*1.6, pointsize = 20)\ndist_n30\n\n\n\n#dev.off()\n\nPara comprobar, hagámos el ejercicio con una distribución uniforme; es decir, en la cual todos los valores tienen la misma probabilidad de ser obtenidos (desviaciones debido al generador de números “aleatorios”):\n\nN = 30\ndatos <- data.frame(x = 1:1000, y = runif(1000))\nunif <- ggplot(data = datos, aes(y)) + \n        geom_density(fill = rgb(118,78,144,\n                                maxColorValue = 255),\n                     alpha = 0.5, colour = \"white\") +\n        theme_bw() +\n        labs(title = \"Distribución \\\"uniforme\\\"\",\n             x = element_blank(),\n             y = element_blank()) +\n        theme(text = element_text(colour = \"gray40\"))\n\n#cairo_pdf(\"unif.pdf\", family = \"Montserrat\",\n#           height = 5, width = 5*1.6, pointsize = 20)\nunif\n\n\n\n#dev.off()\n\n\nmedias <- data.frame(x = 1:1000,\n                     y = replicate(1000,\n                                   mean(sample(datos$y, N))))\n\ndist_n30 <- ggplot(data = medias, aes(y)) +\n            geom_density(fill = \"dodgerblue4\",\n                         alpha = 0.5,\n                         colour = \"white\") +\n            theme_bw() +\n            labs(title = sprintf(\"Distribución muestreal con N = %d\",\n                                 N),\n                 x = element_blank(),\n                 y = element_blank()) +\n            theme(text = element_text(colour = \"gray40\"))\n\n#cairo_pdf(\"n_30u.pdf\", family = \"Montserrat\",\n#          height = 5, width = 5*1.6, pointsize = 20)\ndist_n30\n\n\n\n#dev.off()\n\nUn aspecto importante a considerar es la “Primera Propiedad Conocida” de la distribución normal: dadas muestras aleatorias e independientes con N observaciones cada una (tomadas de una distribución normal), la distribución de medias muestreales es normal e insesgada (i.e., centrada en la media poblacional), independientemente del tamaño de N. Por lo tanto, aún un N de 1 debería dar una distribución parecida a la normal. Comprobemos:\n\nN = 1\ndatos <- data.frame(x = 1:1000, y = rnorm(1000))\nnorm <- ggplot(data = datos, aes(y)) + \n        geom_density(fill = rgb(118,78,144,\n                                maxColorValue = 255),\n                     alpha = 0.5,\n                     colour = \"white\") +\n        theme_bw() +\n        labs(title = \"Distribución Normal\",\n             x = element_blank(),\n             y = element_blank()) +\n        theme(text = element_text(colour = \"gray40\"))\n\n#cairo_pdf(\"norm.pdf\", family = \"Montserrat\",\n#          height = 5, width = 5*1.6, pointsize = 20)\nnorm\n\n\n\n#dev.off()\n\n\nmedias <- data.frame(x = 1:1000,\n                     y = replicate(1000,\n                                   mean(sample(datos$y, N))))\n\ndist_n1 <- ggplot(data = medias, aes(y)) +\n           geom_density(fill = \"dodgerblue4\",\n                        alpha = 0.5, colour = \"white\") +\n           theme_bw() +\n           labs(title = sprintf(\"Distribución muestreal con N = %d\",\n                                N),\n                x = element_blank(),\n                y = element_blank()) +\n           theme(text = element_text(colour = \"gray40\"))\n\n#cairo_pdf(\"n_1.pdf\", family = \"Montserrat\",\n#          height = 5, width = 5*1.6, pointsize = 20)\ndist_n1\n\n\n\n#dev.off()\n\nLa implicación de esta propiedad es que entre menos “normal” (en términos de su distribución estadística) sea nuestra población de estudio, necesitaremos un mayor tamaño de muestra para que nuestra distribución muestral de la media sea normal. El problema surge cuando nos debemos de enfrentar a tamaños de muestra pequeños (n < 30). Aunque siempre podemos asumir (literalmente) que nuestra población se encuentra normalmente distribuida y “capitalizar en la robustez del modelo estadístico subyacente”, abusando del TLC, o reconocer que tamaños de muestra más pequeños nos pueden acercar lo suficiente (n > 30 es para casos extremos). La tercera opción es la evaluación formal, la cual consiste en hacer una prueba de bondad de ajuste para conocer si nuestros datos se desvían o no de una distribución normal teórica. Antes de entrar a esos métodos, analicemos la prueba de bondad de ajuste más conocida: la prueba \\(\\chi^2\\) de independencia."
  },
  {
    "objectID": "c10_param.html#pruebas-de-bondad-de-ajuste",
    "href": "c10_param.html#pruebas-de-bondad-de-ajuste",
    "title": "10  Técnicas paramétricas",
    "section": "10.3 Pruebas de bondad de ajuste:",
    "text": "10.3 Pruebas de bondad de ajuste:\n\n10.3.1 \\(\\chi^2\\)\nEsta prueba nos permite probar si la distribución de nuestros datos (frecuencias de variables nominales) son iguales a una distribución teórica. El ejemplo más sencillo lo tenemos al evaluar si la distribución de sexos en una población es diferente de 1:1. En este caso, la distribución de nuestros datos es binomial (dos categorías, verdadero/falso, éxito/fracaso, macho/hembra, etc.). En nuestro muestreo contamos 142 machos y 190 hembras. Coloquemos esos datos en un objeto y realicemos la prueba:\n\nsexos <- c(machos = 142, hembras = 190)\nsex_chi <- chisq.test(sexos)\nsex_chi\n\n\n    Chi-squared test for given probabilities\n\ndata:  sexos\nX-squared = 6.9398, df = 1, p-value = 0.00843\n\n\nVeamos la distribución teórica gráficamente y veamos la ubicación del estadístico de prueba:\n\nchi_data <- data.frame(x = rchisq(1000, 1))\n\nchisq_plot <- ggplot(data = chi_data, aes(x)) +\n              geom_density(fill = rgb(118,78,144,\n                                      maxColorValue = 255),\n                           alpha = 0.5, colour = \"white\") +\n              geom_vline(xintercept = sex_chi$statistic,\n                         color = \"firebrick\") +\n              annotate(geom = \"text\",\n                       x = sex_chi$statistic+1.1, y = 1,\n                       label = sprintf(\"X^2 = %.2f\",\n                                       round(sex_chi$statistic, 2))) +\n              theme_bw() +\n              labs(title = sprintf(\"Distribución X^2 teórica (g.l = %d)\",\n                                   sex_chi$parameter),\n                   x = element_blank(),\n                   y = element_blank()) +\n             theme(text = element_text(colour = \"gray40\"))\n#cairo_pdf(\"chi_plot.pdf\", family = \"Montserrat\",\n#           height = 5, width = 5*1.6, pointsize = 20)\nchisq_plot\n\n\n\n#dev.off()\n\nPartiendo del valor de p podemos concluir que la proporción fue diferente de nuestro modelo teórico 1:1, pero ¿qué pasa si nos interesara comprobar si es diferente a otra proporción, por ejemplo 40% machos y 60% hembras? En ese caso únicamente debemos de proporcionar un vector p en el cual establezcamos la probabilidad correspondiente a cada grupo:\n\nchisq.test(sexos, p = c(0.4, 0.6))\n\n\n    Chi-squared test for given probabilities\n\ndata:  sexos\nX-squared = 1.0622, df = 1, p-value = 0.3027\n\n\nAquí nuestros datos no ridiculizan a nuestra hipótesis de nulidad, por lo que no podemos rechazarla. Un ejemplo más complejo es el de la presentación, en donde tratamos de probar si el proceso de vacunación hizo alguna diferencia en el estado de salud de los empleados o, en otras palabras, ¿la incidencia de pneumonía fue la misma, INDEPENDIENTEMENTE de si los empleados se vacunaron o no? Al igual que en el caso anterior, coloquemos los datos en un objeto:\n\nvacunas <- data.frame(no_vacuna = c(23, 8, 61),\n                      vacuna = c(5, 10, 77),\n                      row.names = c(\"neumococo\", \"otra_neumonia\",\n                                    \"sin_neumonia\"))\nvacunas\n\n\n\n  \n\n\n\nAhora apliquemos la prueba:\n\nvacs <- chisq.test(vacunas)\nvacs\n\n\n    Pearson's Chi-squared test\n\ndata:  vacunas\nX-squared = 13.649, df = 2, p-value = 0.001087\n\n\nComo era de esperarse al ver las frecuencias, la incidencia de pneumonía aparentemente no fue la misma entre los empleados vacunados y los que no se vacunaron. En este caso, podemos extraer aún más información, tal y como la dependencia entre las variables. Para ello accederemos al atributo residuals de la salida de chisq.test, el cual representa los residuales de Pearson para cada celda:\n\nvacs$residuals\n\n               no_vacuna     vacuna\nneumococo      2.4053512 -2.4053512\notra_neumonia -0.3333333  0.3333333\nsin_neumonia  -0.9630868  0.9630868\n\n\nValores positivos muestran una asociación positiva entre las variables correspondientes; es decir, la incidencia de neumonía por neumococo aumentó (signo positivo) en aquellos empleados que no fueron vacunados y viceversa, valores negativos muestran una asociación negativa; es decir, la incidencia disminuyó en aquellos que sí fueron vacunados. Si nuestro interés fuera saber qué tanto contribuyó cada celda al valor de \\(\\chi^2\\) podemos elevar cada residual al cuadrado y dividirlo entre el valor de \\(\\chi^2\\) observado, tal que:\n\ncontrib <- 100*((vacs$residuals^2)/vacs$statistic)\ncontrib\n\n              no_vacuna    vacuna\nneumococo     42.390150 42.390150\notra_neumonia  0.814077  0.814077\nsin_neumonia   6.795773  6.795773\n\n\nEvidentemente, los residuales más grandes tuvieron la mayor contribución que, en este caso, estuvo dada por la incidencia de neumonía por neumococo en ambos grupos. Podemos ver estos resultados de manera gráfica utilizando la librería corrplot:\n\ncorrplot::corrplot(contrib, is.corr = F)\n\n\n\n\nAhora que tenemos una idea sobre cómo funcionan las pruebas de bondad de ajuste, podemos regresar a hablar sobre las pruebas de normalidad.\n\n\n10.3.2 Supuesto de Normalidad\nComo imaginarás, las pruebas de normalidad son pruebas de bondad de ajuste en donde la distribución teórica es una distribución normal, aunque el modo en el cual se evalúan las desviaciones de la normalidad (i.e., las diferencias) es diferente para cada prueba. Para aplicarlas, utilizaremos la base de datos de muestras independientes del archivo datos_t, particularmente la columna DC:\n\ndc <- openxlsx::read.xlsx(\"datos/datos_t.xlsx\", sheet = 1)\ndc\n\n\n\n  \n\n\n\nPodemos hacer una primera valoración utilizando un gráfico de densidad con un gráfico de densidad normal teórico superpuesto:\n\nset.seed(0)\nnorm_plot <- ggplot(data = dc, aes(DC)) +\n             geom_density(fill = rgb(118,78,144,\n                                     maxColorValue = 255),\n                          colour = \"white\", alpha = 0.5) +\n             stat_function(fun = dnorm, n = 100,\n                           args = list(mean = mean(dc$DC),\n                                       sd = sd(dc$DC))) +\n             # Límites expandidos para visualizar el\n             # kde normal \"completo\".\n             # El kde observado se encuentra extendido más allá\n             # de los límites de los datos:\n             xlim(c(40, 65)) + \n             theme_bw() +\n             labs(title = \"Gráfico de densidad de DC vs. normal teórica\",\n                  x = element_blank(),\n                  y = element_blank()) +\n             theme(text = element_text(colour = \"gray40\"))\n\n#cairo_pdf(\"norm_plot.pdf\", height = 5, width = 5*1.6, pointsize = 20)\nnorm_plot\n\n\n\n#dev.off()\n\n¿Qué opinas? Apliquemos ahora las pruebas de normalidad:\n\n10.3.2.1 Prueba de Shapiro-Wilk\nLa prueba más conocida para evaluar la normalidad de un conjunto de datos es la prueba de Shapiro-Wilk. Su estadístico de prueba (W) se calcula de una manera poco amigable, pero conceptualmente implica ordenar los valores de la muestra y evaluar las desviaciones (diferencias) con respecto a la media, la varianza y su covarianza (este concepto se retoma más adelante) esperadas. En pocas palabras, la covarianza indica cuánto cambia una variable (la media) con respecto a otra (la varianza).\n¿Qué tiene que ver la covarianza con el Supuesto de Normalidad? Tiene que ver con la Segunda Propiedad Conocida de la Distribución Normal, la cual establece que Dadas observaciones aleatorias e independientes (de una distribución normal), la media muestral y la varianza muestral son independientes. En otras palabras, cuando tomas una muestra y la usas para estimar tanto la media como la varianza de la población, qué tanto puedes equivocarte sobre la media es independiente de qué tanto puedes equivocarte sobre la varianza. Esta es una característica única de la distribución normal y es una de las razones por la que la prueba de S-W es de las más (por no decir la más) utilizada y recomendada, especialmente para muestras pequeñas. En algunos estudios de simulación como este ha demostrado ser más sensible a las desviaciones de la normalidad que la prueba de Kolmogorov-Smirnov, aunque antes de explicarla apliquemos la prueba de S-W:\n\nshapiro.test(dc$DC)\n\n\n    Shapiro-Wilk normality test\n\ndata:  dc$DC\nW = 0.95125, p-value = 0.6833\n\n\nEl valor de p no nos permite rechazar nuestra hipótesis de nulidad a un \\(\\alpha = 0.05\\), por lo que podemos concluir que los datos se ajustan a una distribución normal. Vuelve al gráfico de densidad normal, ¿qué opinas?\nComo añadido, visualicemos la segunda propiedad conocida de la distribución normal:\n\nmeans <- NA\nsds <- NA\n\nfor (i in 1:1000) {\n  norm_data <- rnorm(10)\n  means[i] <- mean(norm_data)\n  sds[i] <- sd(norm_data)\n}\n\nmean_sd <- data.frame(mean = means, sd = sds)\n\nprop_2 <- ggplot(data = mean_sd, aes(x = mean, y = sd)) +\n          geom_point(color = \"dodgerblue4\", size = 2, alpha = 0.5) +\n          theme_bw() +\n          labs(title = \n                 \"Segunda Propiedad Conocida de la Distribución Normal\",\n               subtitle = \"1000 muestreos de una población normal\",\n               x = \"Media\",\n               y = \"Desviación Estándar\") +\n          theme(text = element_text(colour = \"gray40\"))\n#cairo_pdf(\"prop_2.pdf\", family = \"Montserrat\",\n#          height = 5, width = 5*1.6, pointsize = 20)\nprop_2\n\n\n\n#dev.off()\n\nCon una distribución Gamma:\n\nmeans <- NA\nsds <- NA\n\nfor (i in 1:1000) {\n  gamma_data <- rgamma(10, shape = 1)\n  means[i] <- mean(gamma_data)\n  sds[i] <- sd(gamma_data)\n}\n\nmean_sd <- data.frame(mean = means, sd = sds)\n\nprop_g <- ggplot(data = mean_sd, aes(x = mean, y = sd)) +\n          geom_point(color = \"dodgerblue4\", size = 2, alpha = 0.5) +\n          theme_bw() +\n          labs(title =\n                 \"Segunda Propiedad Conocida de la Distribución Normal\",\n               subtitle = \"1000 muestreos de una población gamma\",\n               x = \"Media\",\n               y = \"Desviación Estándar\") +\n          theme(text = element_text(colour = \"gray40\"))\n\n#cairo_pdf(\"prop_g.pdf\", family = \"Montserrat\",\n#           height = 5, width = 5*1.6, pointsize = 20)\nprop_g\n\n\n\n#dev.off()\n\nEjericio: Realiza el mismo gráfico para la columna DC y para la columna CH.\n\n\n10.3.2.2 Prueba Kolmogorov-Smirnov\nA diferencia de la prueba S-W, la prueba K-S compara las función de densidad acumulada empírica (observada) vs. una función de densidad acumulada teórica (no necesariamente normal), lo cual causa que sea sensible a desviaciones en el centro de la distribución pero no en las colas; sin embargo, es importante mencionar, que la prueba K-S es convergente; es decir, que conforme \\(N \\rightarrow \\infty\\) la prueba converge a la “respuesta verdadera” en términos de probabilidad. Esta razón hace que esta prueba no se recomiende para tamaños de muestra pequeños. Para aplicarla:\n\nks.test(dc$DC, \"pnorm\")\n\n\n    Exact one-sample Kolmogorov-Smirnov test\n\ndata:  dc$DC\nD = 1, p-value < 2.2e-16\nalternative hypothesis: two-sided\n\n\nA diferencia del caso anterior, esta prueba si tuvo evidencia suficiente para ridiculizar nuestra hipótesis nula, por lo que podemos concluir que nuestros datos no se ajustan a una distribución normal. Vuelve nuevamente al gráfico KDE. ¿Qué opinas?\nVeamos las densidades acumuladas:\n\n# Generamos una cdf normal teórica:\ncdf <- data.frame(norm = rnorm(1000,\n                               mean = mean(dc$DC),\n                               sd = sd(dc$DC)))\n\n# Graficamos una vs. la otra:\ncdfplot <- ggplot(data = dc, aes(DC)) +\n           stat_ecdf(geom = \"step\",\n                     colour = rgb(118, 78, 144,\n                                  maxColorValue = 255),\n                     alpha = 1) +\n           stat_ecdf(data = cdf, aes(norm),\n                     geom = \"line\", colour = \"black\") +\n           theme_bw() +\n           labs(title = \"Densidades acum. empírica y teórica para DC\",\n                x = element_blank(),\n                y = element_blank())\n\n#cairo_pdf(\"cdf.pdf\", family = \"Montserrat\",\n#          height = 5, width = 5*1.6, pointsize = 20)\ncdfplot\n\n\n\n#dev.off()\n\nConjuntando con el gráfico kde original podemos ver por qué la prueba K-S arrojó un resultado significativo, ya que hubo desviaciones importantes en la zona central. Interpretar correctamente un gráfico CDF NO es sencillo y requiere de experiencia, por lo que únicamente lo incluí para acompañar a la prueba que se basa en la densidad acumulada.\nHabiendo explicado dos de las pruebas de normalidad más comunes, pasemos a los análisis paramétricos. El primero de ellos lo revisamos durante la clase de pruebas de hipótesis: la prueba t de Student, por lo que pasaremos directamente al Análisis de la Varianza."
  },
  {
    "objectID": "c10_param.html#análisis-de-la-varianza",
    "href": "c10_param.html#análisis-de-la-varianza",
    "title": "10  Técnicas paramétricas",
    "section": "10.4 Análisis de la Varianza",
    "text": "10.4 Análisis de la Varianza\nEn términos simples, podemos pensar en el ANOVA como una extensión de la prueba t-Student a más de dos grupos a comparar. Durante la clase de Comparaciones Multivariadas abordamos el riesgo que conlleva realizar múltiples pruebas de hipótesis (comparaciones) en nuestros datos; es decir, el problema de realizar dos o más comparaciones entre grupos como si se tratara de pruebas independientes. Por el momento, solo ten en mente que se incrementa la posibilidad de obtener un falso positivo únicamente por azar, por lo que hay que utilizar una técnica adecuada y es ahí donde entra el ANOVA o, mejor dicho, los ANOVAs. Como te imaginarás, estas pruebas nos permiten comparar medias entre más de dos grupos, aunque aquí la comparación se realiza de manera global y la hipótesis alternativa se expresa como “Al menos una de las medias es diferente”. Esto quiere decir que el ANOVA no nos dirá entre qué par(es) de grupos se encontraron las diferencias, sino que habrá que acompañarlo de una prueba post-hoc. Esta prueba es la prueba de diferencias honestas (HSD) de Tukey, la cual se encuentra basada en la distribución de los rangos estudentizados y fue diseñada para no incrementar la probabilidad de falsos positivos al realizar múltiples comparaciones. En esta sesión revisaremos tres modaliades de ANOVA: de una vía, de dos vías y factorial, de menor a mayor complejidad, aunque estos no son los únicos. Entre los demás diseños de ANOVA se encuentran el ANOVA de medidas repetidas (estudios de crecimiento en laboratorio con medidas intermedias entre el inicio y el final, por ejemplo) o el ANOVA anidado, en el cual el diseño es similar a una muñeca rusa.\nAntes de aplicar y explicar los modelos de ANOVA, es necesario desarrollar una intuición sobre el procedimiento. El nombre “Análisis de Varianza” viene de que, literalmente, se utilizan las varianzas para comparar las medias. Aunque el proceso matemático implica calcular promedios de promedios, varias sumas de cuadrados y cuadrados medios del error, podemos resumirlo para fines prácticos en que la comparación se realiza mediante una razón/cociente, tal que:\n\\[F = \\frac{\\sigma^2_{entre}}{\\sigma^2_{dentro}}\\]\nSé que esto puede sonar muy poco intuitivo, pero si nos detenemos un poco a analizar la ecuación podemos darle mucho sentido. La varianza dentro de los grupos podemos considerarla como la varianza “promedio” de cada grupo (razón por la que es importante que estas sean homogéneas entre todos nuestros grupos), mientras que la varianza entre los grupos representa la “separación” (dispersión) entre los grupos (sin considerar el error). Partiendo de esto, es evidente que si la varianza entre grupos es muy grande en relación a la varianza dentro de los grupos podemos inferir que existe un efecto del factor de agrupamiento pues “no hay” (ojo a las comillas y los supuestos) otra forma de que las distribuciones de los grupos se desplacen.\nGráficamente la varianza dentro de los grupos se representaría de la siguiente manera:\n\nanov_sim <- data.frame(grupo = as.factor(c(rep(\"A\", 1000),\n                                           rep(\"B\", 1000))),\n                       y = c(rnorm(1000, mean = 10, sd = 1),\n                             rnorm(1000, mean = 20, sd = 1)))\n\ndentro_plot <- ggplot(data = anov_sim,\n                      aes(y, fill = grupo, alpha = 0.5)) +\n               geom_density(trim = T, show.legend = F,\n                            colour = \"white\") +\n               theme_minimal() +\n               labs(title = \"Varianza dentro de los grupos\",\n                    x = element_blank(),\n                    y = element_blank()) +\n               scale_y_continuous(labels = NULL) +\n               xlim(c(5, 25))\ndentro_plot\n\n\n\n\nMientras que la varianza entre los grupos podemos, para fines de interpretación, visualizarla como la varianza dada por ambos grupos. En realidad esto representaría la varianza total y la varianza entre los grupos es el resultado de eliminar la varianza dada por el error, pero sigamos con el ejemplo:\n\nanov_sim$tot <- rnorm(200, mean = 15, sd = sd(anov_sim$y))\nentre_plot <- ggplot(data = anov_sim, aes(tot)) + \n              geom_density(fill = \"dodgerblue4\",\n                           alpha = 0.5, colour = \"white\") +\n              theme_minimal() +\n              labs(title = \"Varianza entre los grupos\",\n                   x = element_blank(),\n                   y = element_blank()) +\n              scale_y_continuous(labels = NULL) +\n              xlim(c(5, 25))\nentre_plot\n\nWarning: Removed 90 rows containing non-finite values (stat_density).\n\n\n\n\n\nVisualizándolas como si de un cociente se tratara es posible darse cuenta cómo la varianza “entre” los grupos es mucho mayor que la varianza dentro de los grupos, lo cual daría un valor de la razón de varianzas muy alto, sugiriendo un efecto del factor de agrupamiento.\n\n#cairo_pdf(\"anova_plot.pdf\", family = \"Montserrat\",\n#           height = 5, width = 5*1.6, pointsize = 20)\ngridExtra::grid.arrange(entre_plot, dentro_plot)\n\nWarning: Removed 90 rows containing non-finite values (stat_density).\n\n\n\n\n#dev.off()\n\nVeamos qué pasa cuando las medias son más cercanas entre sí:\n\nanov_sim2 <- data.frame(grupo = as.factor(c(rep(\"A\", 1000),\n                                            rep(\"B\", 1000))),\n                        y = c(rnorm(1000, mean = 10, sd = 1),\n                              rnorm(1000, mean = 11, sd = 1)))\nanov_sim2$tot <- rnorm(2000, mean(10.5), sd(anov_sim2$y))\ndentro_plot2 <- ggplot(data = anov_sim2,\n                       aes(y, fill = grupo, alpha = 0.5)) +\n               geom_density(trim = T, show.legend = F,\n                            colour = \"white\") +\n               theme_minimal() +\n               labs(title = \"Varianza dentro de los grupos\",\n                    x = element_blank(),\n                    y = element_blank()) +\n               scale_y_continuous(labels = NULL) +\n               xlim(c(5, 15))\nentre_plot2 <- ggplot(data = anov_sim2, aes(tot)) + \n              geom_density(fill = \"dodgerblue4\",\n                           alpha = 0.5, colour = \"white\") +\n              theme_minimal() +\n              labs(title = \"Varianza entre los grupos\",\n                   x = element_blank(),\n                   y = element_blank()) +\n              scale_y_continuous(labels = NULL) +\n              xlim(c(5, 15))\n#cairo_pdf(\"anova_plot2.pdf\", family = \"Montserrat\",\n#           height = 5, width = 5*1.6, pointsize = 20)\ngridExtra::grid.arrange(entre_plot2, dentro_plot2)\n\n\n\n#dev.off()\n\n\n10.4.1 Supuesto de homogeneidad de Varianzas\nComo podrás imaginar, el que las varianzas de los grupos no sean homogéneas generará un sesgo al momento de calcular el cociente y, en consecuencia, el nivel de significancia de la prueba. Esto es lo que da origen al Supuesto de Homogeneidad de Varianzas. Existe una gran diversidad de pruebas, cada una con sus consideraciones, fortalezas y desventajas, pero analizaremos únicamente las (posiblemente) más conocidas.\n\n10.4.1.1 Prueba de Bartlett\nLa prueba de Bartlett se considera como la prueba Uniformemente Más Poderosa; es decir, la que es menos propensa a cometer un falso negativo para cualquier valor de \\(\\alpha\\). Este poder, sin embargo, tiene sus bemoles o su bemol, mejor dicho. Esta prueba se apoya TOTALMENTE en que la variable de interés en cada factor se encuentra normalmente distribuída (¡Hola de nuevo, Supuesto de Normalidad!). De violarse este supuesto el valor de \\(\\alpha_v\\) (verdadero) para la prueba puede ser mayor o menor al definido por nosotros (\\(\\alpha_n\\), nominal). De manera particular, si la distribución de la variable analizada presenta una curtosis negativa el \\(\\alpha_v\\) será menor al nominal, mientras que con una curtosis positiva será el caso contrario. Esto lleva a que hagamos una prueba más o menos estricta de lo que habíamos planeado originalmente y que nuestros resultados no sean confiables. De cualquier manera, veamos cómo aplicarla:\n\nbartlett.test(y~grupo, data = anov_sim)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  y by grupo\nBartlett's K-squared = 0.4309, df = 1, p-value = 0.5115\n\n\nEn este caso, no podemos ridiculizar nuestra hipótesis de nulidad, por lo que podemos concluir que las varianzas entre nuestros grupos son homogéneas (y deben serlo, pues así las especificamos).\n\n\n10.4.1.2 Prueba de Levene\nEs la alternativa recomendada por muchos a la prueba de Bartlett. Aunque no es tan poderosa, sí es robusta a las violaciones al supuesto de normalidad, de modo que el \\(\\alpha\\) verdadero es muy similar al nominal para una gran cantidad de distribuciones, aunque es insensible a distribuciones simétricas con colas altas como la t de Student o doble exponencial (también conocida como distribución de Laplace). Aplicarla también es sumamente sencillo:\n\ncar::leveneTest(y~grupo, data = anov_sim)\n\n\n\n  \n\n\n\nComo era de esperarse, el resultado es consistente con la prueba de Bartlett para este caso.\n\n\n\n10.4.2 ANOVA de una sola vía\nHabiendo revisado los conceptos básicos detrás del ANOVA, podemos pasar a aplicar algunos modelos. El más sencillo es el ANOVA de una sola vía, el cual es el caso más sencillo; es decir, comparamos una sola variable numérica entre los niveles de un solo factor (pesos finales para tres alimentos distintos, por ejemplo). Para ejemplificarlo utilizaremos la base datos1 que se trabajó para la tarea de Intervalos de confianza, con una columna extra: id, el cual es un identificador para cada individuo. Esta columna fue añadida únicamente para ejemplificar un caso de ANOVA posterior. En este ejemplo, compararemos los pesos totales entre los tres periodos (OJO: este es un diseño para un ANOVA factorial, únicamente lo utilizaremos como ejemplo).\nEl primer paso es, evidentemente, cargar la base de datos:\n\ndf <- read.table(\"datos/Datos1 2.csv\", header = F, skip = 1, sep = \",\")\ncolnames(df) <- c(\"Dieta\", \"Periodo\", \"Rep\", \"LT\", \"PT\", \"id\")\ndf$Periodo <- factor(df$Periodo, levels = c(\"I\", \"M\", \"F\"))\nhead(df)\n\n\n\n  \n\n\n\n\n10.4.2.1 Comprobación de supuestos\nEl segundo paso es la comprobación de supuestos. Primero el de Normalidad:\n\n#Normalidad\n## Data.frame a llenar\nnorm <- data.frame(grupo = NA, W = NA, p = NA)\n\n## Niveles a probar:\nlvls <- levels(df$Periodo)\n\nfor (i in seq_along(lvls)) {\n  temp <- shapiro.test(df$PT[df$Periodo == lvls[i]])\n  norm[i,] <- c(lvls[i], temp$statistic, temp$p.value)\n}\nnorm\n\n\n\n  \n\n\n\nLa prueba de S-W sugiere que no hay desviaciones significativas de la normalidad. Corroboremos con un gráfico de violín. Al parecer, los resultados son coherentes con la distribución de los datos.\n\nggplot(data = df, aes(x = Periodo, y = PT, fill = Periodo)) +\n  geom_violin(alpha = 0.5, show.legend = F) +\n  labs(title = \"Distribución de PT en los tres momentos de medición\",\n       x = element_blank(),\n       y = element_blank()) +\n  theme_bw()\n\nWarning: Removed 22 rows containing non-finite values (stat_ydensity).\n\n\n\n\n\nAhora el supuesto de igualdad de varianzas, utilizando la prueba de Levene. Podemos concluir que las varianzas no son homogéneas, por lo que la recomendación sería recurrir a una prueba no paramétrica; sin embargo, sigamos con el ejercicio y escalando la complejidad del análisis antes de saltar apresuradamente a conclusiones.\n\ncar::leveneTest(PT~Periodo, data = df)\n\n\n\n  \n\n\n\n\n\n10.4.2.2 Aplicación del ANOVA\nEl siguiente paso es aplicar el ANOVA. El valor de p es bastante bajo, lo cual ridiculiza nuestra hipótesis de nulidad y concluimos que al menos un par de medias son significativamente diferentes entre sí (F(2, 155) = 574.3; p < 0.0001).\n\nuna_via <- aov(PT~Periodo, data = df)\nsummary(una_via)\n\n             Df Sum Sq Mean Sq F value Pr(>F)    \nPeriodo       2 10.600   5.300   574.3 <2e-16 ***\nResiduals   155  1.431   0.009                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n22 observations deleted due to missingness\n\n\n\n\n10.4.2.3 Prueba post-hoc\nEl último paso es aplicar la prueba post-hoc. Esta prueba se construye a partir de la distribución de rangos estudentizados, y fue diseñada para evitar el conflicto entre el \\(\\alpha\\) y el número de comparaciones, por lo que la interpretación del valor de p es directa. En este caso, el valor de p fue muy pequeño para las tres comparaciones, por lo que rechazamos nuestra hipótesis de nulidad en los tres casos. El resto de la tabla es también informativo, pues nos indica la magnitud de las diferencias y sus intervalos de confianza (tal y como en la prueba t de Student):\n\nTukeyHSD(una_via)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = PT ~ Periodo, data = df)\n\n$Periodo\n         diff       lwr       upr p adj\nM-I 0.2949615 0.2518882 0.3380349     0\nF-I 0.6376957 0.5931429 0.6822484     0\nF-M 0.3427341 0.2967181 0.3887501     0\n\n\nCon esos 4 pasos terminamos nuestro ANOVA de una vía. Pasemos entonces al ANOVA de dos vías.\n\n\n\n10.4.3 ANOVA de dos vías\nSi una vía es a un factor, dos vías es a dos factores. En este análisis compararemos el efecto de ambos factores simultáneamente, pero de manera independiente; es decir, aunque se hará la comparación para ambos, no se considerará la interacción entre ellos. Nuestro segundo factor será la Dieta. Los pasos son exactamente los mismos que en el anterior:\n\n10.4.3.1 Comprobación de Supuestos\nDado que ya comprobamos los supuestos para el factor Periodo, solo habrá que hacerlo para el factor Dieta:\n\n#Normalidad\ndf$Dieta <- factor(df$Dieta, levels = c(\"A\", \"B\", \"C\"))\n## Data.frame a llenar\nnorm <- data.frame(grupo = NA, W = NA, p = NA)\n\n## Niveles a probar:\nlvls <- levels(df$Dieta)\n\nfor (i in seq_along(lvls)) {\n  temp <- shapiro.test(df$PT[df$Dieta == lvls[i]])\n  norm[i,] <- c(lvls[i], temp$statistic, temp$p.value)\n}\nnorm\n\n\n\n  \n\n\n\nDebido a que el factor dieta incluye el efecto del periodo de medición y detectamos diferencias entre ellos, es esperable que no se cumpla el supuesto de normalidad. En este caso, el diseño sería mejor analizado utilizando un ANOVA factorial que uno de dos vías pero, al igual que en el caso anterior, seguiremos únicamente para fines ilustrativos.\nPara la homogeneidad de varianzas la interpretación es la misma, aunque la consecuencia es la contraria. No violamos el supuesto de homogeneidad de varianzas debido a que tampoco se violó entre los periodos. Esto da un poco más de respaldo a seguir con el análisis, pues es más robusto a la violación del supuesto de normalidad que al de homogeneidad de varianzas.\n\ncar::leveneTest(PT~Dieta, data = df)\n\n\n\n  \n\n\n\n\n\n10.4.3.2 Aplicación del ANOVA.\nEl ANOVA de dos vías es un caso especial del ANOVA factorial, en el cuál únicamente hay dos factores y NO se considera su interacción, por lo que el modo de declararlo es una fórmula en la cuál los factores se consideran de manera aditiva. La forma tradicional de reportar los resultados de este ANOVA sería: hubo un efecto significativo de las dietas (F(2, 153) = 11.45; p < 0.0001) y de los periodos (F(2, 153) = 560.42; p < 0.0001).\n\ndos_vias <- aov(PT~Dieta+Periodo, data = df)\nsummary(dos_vias)\n\n             Df Sum Sq Mean Sq F value   Pr(>F)    \nDieta         2  0.212   0.106   11.45 2.33e-05 ***\nPeriodo       2 10.399   5.199  560.42  < 2e-16 ***\nResiduals   153  1.419   0.009                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n22 observations deleted due to missingness\n\n\n\n\n10.4.3.3 Prueba post-hoc.\nEn este caso tuvimos valores de p muy pequeños para ambos factores, realicemos la prueba HSD de Tukey. Al ver la salida puedes interpretar que esta es una lista, y que podríamos acceder a los resultados de cualquier factor utilizando el operador $ (TukeyHSD(aov_obj)$factor). Aquí, las diferencias se encontraron entre la dieta C y las otras dos, pero no entre A y B.\n\nTukeyHSD(dos_vias)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = PT ~ Dieta + Periodo, data = df)\n\n$Dieta\n           diff         lwr         upr     p adj\nB-A -0.02402094 -0.06672932  0.01868745 0.3801380\nC-A -0.09036834 -0.13594353 -0.04479315 0.0000176\nC-B -0.06634740 -0.11227232 -0.02042249 0.0023101\n\n$Periodo\n         diff       lwr       upr p adj\nM-I 0.2912145 0.2480228 0.3344062     0\nF-I 0.6266462 0.5819709 0.6713214     0\nF-M 0.3354316 0.2892891 0.3815741     0\n\n\nConsiderando el diseño factorial de la base de datos, ¿cómo interpretarías estos resultados? ¿podemos confiar en ellos? La respuesta que yo esperaría es que no, pues si el experimento fue bien diseñado al inicio todos los animales debían tener aproximadamente las mismas características y vimos tanto gráficamente como en ambos ANOVAs que hubo un crecimiento. Veamos qué pasa con las distribuciones utilizando un gráfico de interacción.\n\nggplot(data = df, aes(x = Dieta, y = PT, fill = Periodo)) +\n  geom_violin(alpha = 0.5, show.legend = T) +\n  labs(title = \"Distribución de PT en los tres momentos de medición\",\n       x = element_blank(),\n       y = element_blank()) +\n  theme_bw()\n\nWarning: Removed 22 rows containing non-finite values (stat_ydensity).\n\n\n\n\n\nEs evidente que en los tres tratamientos hubo un crecimiento, el cual además parece haber sido bastante similar. Este es un ejemplo del error de tipo III que mencionaba en la clase de pruebas de hipótesis: utilizar la matemática correcta para responder la pregunta equivocada. Veamos qué pasa si realizamos un ANOVA factorial.\n\n\n\n10.4.4 ANOVA factorial\nComo te podrás imaginar a partir de lo mencionado sobre el ANOVA de dos vías, este ANOVA es la versión más generalizada en la cual podemos utilzar más de dos factores y además analizar su interacción. Sigamos con la base anterior, en este caso considerando también el factor réplica:\n\ndf$Rep <- factor(df$Rep, levels = c(\"A\", \"B\"))\n\n\n10.4.4.1 Comprobación de supuestos\nNo hay sorpresas en ninguno de los dos casos, las interpretaciones de los resultados son las mismas que en el caso anterior; es decir, este NO es el modo correcto de comprobar la normalidad. Cuando hablemos del ANOVA de medidas repetidas veremos un ejemplo de cómo hacerlo de manera correcta (normalidad de un factor dados los niveles del otro factor).\n\n## Data.frame a llenar\nnorm <- data.frame(grupo = NA, W = NA, p = NA)\n\n## Niveles a probar:\nlvls <- levels(df$Rep)\n\n# for (i in seq_along(lvls)) {\n#   temp <- shapiro.test(df$PT[df$Rep == lvls[i]])\n#   norm[i,] <- c(lvls[i], temp$statistic, temp$p.value)\n# }\n# norm\n\n\ncar::leveneTest(PT~Rep, data = df)\n\n\n\n  \n\n\n\n\n\n10.4.4.2 Aplicación del ANOVA\nLa única diferencia con el caso anterior es que esta vez utilizaremos el operador * para añadir los nuevos términos, en vez de hacerlo de forma aditiva. Haciendo esto la tabla del ANOVA cambia, en donde primero aparece el efecto de cada factor analizado de manera independiete (como si hubieramos hecho un ANOVA de “tres vías”) y después los términos de interacción. La interacción entre dos factores representa un efecto combinado de los factores involucrados en la variable analizada; es decir, cuando hay interacción entre dos factores el efecto de uno “depende” del el nivel del otro.\n\nfact <- aov(PT~Dieta*Periodo*Rep, data = df)\nsummary(fact)\n\n                   Df Sum Sq Mean Sq F value   Pr(>F)    \nDieta               2  0.212   0.106  12.185 1.32e-05 ***\nPeriodo             2 10.399   5.199 596.509  < 2e-16 ***\nRep                 1  0.060   0.060   6.874  0.00971 ** \nDieta:Periodo       4  0.053   0.013   1.522  0.19912    \nDieta:Rep           2  0.001   0.000   0.038  0.96240    \nPeriodo:Rep         2  0.048   0.024   2.755  0.06703 .  \nDieta:Periodo:Rep   4  0.038   0.009   1.076  0.37066    \nResiduals         140  1.220   0.009                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n22 observations deleted due to missingness\n\n\n\n\n10.4.4.3 Prueba post-hoc\nEn este caso el único término de interacción con resultados significativos es la interacción entre Periodo y Réplica (Periodo:Rep), lo cual indica que el comportamiento de los periodos fue diferente entre réplicas. Realicemos las pruebas post-hoc correspondientes. Aunque encontramos un efecto significativo de las réplicas, este factor únicamente tiene dos niveles, por lo que realizar la prueba post-hoc es ocioso y, por tanto, la realizaremos únicamente para Periodo:Rep. Nota que debido a la presencia del operador : en el nombre del término es necesario utilizar comillas para poder acceder a ese atributo:\n\nTukeyHSD(fact)$\"Periodo:Rep\"\n\n               diff         lwr         upr        p adj\nM:A-I:A  0.28301798  0.20959477  0.35644118 4.152234e-14\nF:A-I:A  0.66044327  0.58556599  0.73532054 1.265654e-14\nI:B-I:A -0.02480111 -0.09480740  0.04520517 9.092990e-01\nM:B-I:A  0.27169429  0.19681702  0.34657157 6.084022e-14\nF:B-I:A  0.55653205  0.47803934  0.63502476 1.265654e-14\nF:A-M:A  0.37742529  0.30254802  0.45230257 1.265654e-14\nI:B-M:A -0.30781909 -0.37782537 -0.23781281 1.376677e-14\nM:B-M:A -0.01132368 -0.08620096  0.06355359 9.979453e-01\nF:B-M:A  0.27351408  0.19502137  0.35200679 6.727952e-14\nI:B-F:A -0.68524438 -0.75677422 -0.61371454 1.265654e-14\nM:B-F:A -0.38874898 -0.46505261 -0.31244534 1.265654e-14\nF:B-F:A -0.10391121 -0.18376573 -0.02405669 3.335386e-03\nM:B-I:B  0.29649541  0.22496556  0.36802525 1.909584e-14\nF:B-I:B  0.58133317  0.50602701  0.65663933 1.265654e-14\nF:B-M:B  0.28483776  0.20498324  0.36469228 6.306067e-14\n\n\nVemos que prácticamente todos los contrastes fueron significativos, con excepción del periodo inicial (p = 0.9). Esto sugeriría que el comportamiento de las réplicas no fue homogéneo a través del tiempo. Si regresamos brevemente a la tabla del ANOVA veremos que hubo 22 observaciones faltantes, las cuales corresponden a la mortalidad durante el experimento y podrían también explicar estos cambios. Debido a la impraciticidad/imposibilidad de marcar o identificar cada guppy no es posible aplicar un anova de medidas repetidas con estos datos; sin embargo, podemos ejemplificarlo con otros datos.\n\n\n\n10.4.5 ANOVA de medidas repetidas\nEl ANOVA de medidas repetidas es otro de los modelos de ANOVA, el cual podemos considerar como una extensión de la prueba t para muestras dependientes; es decir, en la cual los mismos individuos fueron medidos en más de dos ocasiones, denominado ANOVA de medidas repetidas de una vía. Si tenemos no solo los distintos tiempos de medición sino también factores adicionales entonces tendremos ANOVAs de medidas repetidas de dos vías (tiempo y un factor adicional) o de tres vías (tiempo y dos factores adicionales). Al igual que en el ANOVA “normal” comencemos desde abajo con el de una vía.\n\n10.4.5.1 ANOVA de medidas repetidas de una vía\nCarguemos los datos de ejemplo (selfesteem de la librería datarium), los cuales son una medida de autoestima medida en tres ocasiones distintas:\n\nselfesteem <- datarium::selfesteem\nhead(selfesteem)\n\n\n\n  \n\n\n\nLa base se encuentra en formato corto, por lo que habrá que pasarla a formato largo:\n\nestima <- reshape2::melt(selfesteem, # Datos a modificar\n                         # Identificador para cada individuo\n                         id.vars = \"id\",\n                         # Variables en columnas\n                         measure.vars = c(\"t1\", \"t2\", \"t3\"),\n                         # Nombre de la nueva variable de agrupamiento\n                         variable.name = \"tiempo\",\n                         # Nombre de la nueva variable medida\n                         value.name = \"estima\")\nhead(estima)\n\n\n\n  \n\n\n\n\n10.4.5.1.1 Comprobación de supuestos\nApliquemos entonces la prueba de Shapiro-Wilk a los datos agrupados, esta vez simplificando con tidy:\n\n# Toma el data.frame, agrúpalo por tiempo y para cada nivel\n# aplica la función shapiro_test a la columna estima:\nestima |>  group_by(tiempo) |>  shapiro_test(estima)\n\n\n\n  \n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNota que la función es shapiro_test, la cual está contenida en rstatix, y no shapiro.test de R base. Si tratas de pasar shapiro.test te encontrarás con un error pues no está pensada para ser utilizada con pipes.\n\n\nAhora la homocedasticidad o, mejor dicho, el supuesto de esfericidad. Este supuesto es una “extensión” del supuesto de homogeneidad de varianzas. Definimos esfericidad como la condición en la que las varianzas de las diferencias entre todas las combinaciones de los niveles de interés son iguales. La violación de este supuesto conlleva un incremento en la probabilidad de un falso positivo; es decir, vuelve a la prueba demasiado “liberal” o “crédula”. Aunque este supuesto es sumamente importante, no necesitamos probarlo directamente, pues la función con la que implementaremos el análisis hace la prueba correspondiente (prueba de Mauchly para esfericidad) y, además, aplica una corrección (corrección de Greenhouse-Geisser) a los grados de libertad de aquellos factores que violen el supuesto.\n\n\n10.4.5.1.2 Aplicación del ANOVA\nEsta vez no utilizaremos la notación de fórmula ni tan siquiera la función aov, sino que recurriremos a la función anova_test() de la librería rstatix para hacer más intuitiva la declaración, donde data es el data.frame con los datos (dah!), dv es la variable dependiente; es decir, nuestra variable a comparar, wid es un identificador único para cada individuo y within el factor dentro del cual queremos hacer las comparaciones:\n\nanova_rep1 <- rstatix::anova_test(data = estima,\n                                  dv = estima, wid = id,\n                                  within = tiempo)\nget_anova_table(anova_rep1)\n\n\n\n  \n\n\n\nLo primero que llama la atención es que la tabla de ANOVA reporta resultados para una prueba de tipo III. OJO! esto no tiene nada que ver con el error tipo III que mencioné en la clase de pruebas de hipótesis (ese error no es formal). El tipo de prueba hace referencia al tipo de ANOVA que se está realizando o, mejor dicho, al modo en el que se calculan las sumas de cuadrados. Si te interesa leer más al respecto, visita este o este enlace.\nOtra cosa que debe llamar tu atención es el término ges. Este es el factor de corrección de Greenhouse-Geisser a los grados de libertad. El modo de reportar estos resultados sería algo como “las medidas de autoestima a través del tiempo fueron significativamente diferentes (\\(F_{2,18}\\) = 55.5, p < 0.0001; \\(\\eta^2\\) generalizado = 0.82)”. El término \\(\\eta^2\\) generalizado lo puedes encontrar también como \\(\\hat{\\epsilon}\\).\n\n\n10.4.5.1.3 Prueba post-hoc\nDebido a que las medidas son repetidas no podemos aplicar la prueba de diferencias honestas de Tukey, pero sí podemos aplicar pruebas t de Student pareadas y corregir el valor de p con una corrección de Bonferroni. En la sección de multivariado se abordará esta corrección, pero entiéndela en este momento como el modo de evitar que incrementemos la probabilidad de un falso positivo y, en consecuencia, deberemos de interpretar los valores de la columna p.adj. Viendo la tabla, es posible concluir que hubo diferencias entre las medidas de autoestima en los tres periodos.\n\npwt <- pairwise_t_test(data = estima,\n                       estima~tiempo, paired = T,\n                       p.adjust.method = \"bonferroni\")\npwt\n\n\n\n  \n\n\n\n\n\n\n10.4.5.2 ANOVA de medidas repetidas de dos vías\nAl igual que en el ANOVA “normal”, hablamos de dos vías cuando tenemos dos factores, en este caso son el tiempo y alguno adicional. Para ejemplificarlo utilizaremos la base de datos selfesteem2 de datarium.\n\nselfesteem2 <- datarium::selfesteem2\nestima2 <- reshape2::melt(selfesteem2, # Datos a modificar\n                         # Identificadores para cada individuo\n                         id.vars = c(\"id\", \"treatment\"),\n                         # Variables en columnas\n                         measure.vars = c(\"t1\", \"t2\", \"t3\"),\n                         # Nombre de la nueva variable de agrupamiento\n                         variable.name = \"tiempo\",\n                         # Nombre de la nueva variable medida\n                         value.name = \"estima\")\nhead(estima2)\n\n\n\n  \n\n\n\n\n10.4.5.2.1 Comprobación de supuestos\nAl igual que en el ANOVA de medidas repetidas, únicamente comprobaremos el Supuesto de Normalidad, solo que aquí lo haremos considerando la “anidación” de los factores; es decir, que las medidas repetidas fueron para cada tratamiento:\n\nestima2 |> group_by(treatment, tiempo) |> shapiro_test(estima)\n\n\n\n  \n\n\n\nAparentemente hubo desviaciones de la normalidad en el t1 para el grupo control. Podemos corroborarlo con un gráfico QQ. Aparentemente es culpa de de algunos puntos ligeramente fuera del resto de la tendencia ubicados en el centro. Recordemos que el ANOVA es robusto a ciertas violaciones de la normalidad, y en este caso no parecen ser especialmente serias. Sigamos con el análisis.\n\nggpubr::ggqqplot(estima2, \"estima\", ggtheme = theme_bw()) +\n  facet_grid(tiempo~treatment, labeller = \"label_both\")\n\n\n\n\n\n\n10.4.5.2.2 Aplicación del ANOVA\nUtilizaremos la misma estructura que en el caso anterior; la unica diferencia es que al argumento within le pasaremos un vector con dos factores:\n\nanova_rep2 <- anova_test(data = estima2,\n                         dv = estima, wid = id,\n                         within = c(treatment, tiempo))\nget_anova_table(anova_rep2)\n\n\n\n  \n\n\n\nDe la tabla podemos concluir que todos los contrastes fueron significativos, lo cual indica que hubo diferencias entre los tratamientos (F(1, 11), = 15.5; p = 0.02), entre los tiempos (F(1.31, 14.37), = 27.4; p < 0.0001) y tambien un efecto combinado (F(2,22) = 30.4; p < 0.0001). Debido a que los efectos principales (“solos”) no son suficientes para describir los datos, el proceso post-hoc es un poco más complicado que en el caso anterior.\n\n\n10.4.5.2.3 Pruebas post-hoc\nDebido a la significancia del término de interacción es necesario descomponerlo en:\n\nEfecto principal simple; es decir, un modelo de una vía de la primera variable para cada nivel de la segunda. Debido a que hacerlo a mano es un poco tedioso, encadenemos el proceso:\n\n\nanova_rep2_post1 <- estima2 |> \n                    # Agrupa la base por cada nivel de tiempo\n                    group_by(tiempo) |>\n                    # ANOVA de medidas repetidas para cada nivel\n                    anova_test(dv = estima, wid = id, \n                               within = treatment) |>\n                    # Extrae los resultados\n                    get_anova_table() |> \n                    # Ajusta los valores de p\n                    adjust_pvalue(method = \"bonferroni\")\n\nanova_rep2_post1\n\n\n\n  \n\n\n\n\nAplicar una prueba t de Student para datos dependientes en los términos significativos. Debido a que tratamiento tiene solo dos niveles, realizar este proceso es redundante; de hecho, los valores de p serán iguales a los mostrados atrás; sin embargo, hagámoslo con fines demostrativos:\n\n\npwt_2 <- estima2 |>\n         group_by(tiempo) |>\n         pairwise_t_test(estima~treatment, paired = T,\n                         p.adjust.method = \"bonferroni\")\npwt_2\n\n\n\n  \n\n\n\nEsto es todo para la clase de hoy. Es una clase bastante extensa y aún con ello se quedaron fuera algunas variantes de ANOVA; sin embargo, creo que estos cubren los casos más generales. ¡Nos vemos en la siguiente!"
  },
  {
    "objectID": "s03_noparnolin.html#objetivo-de-aprendizaje",
    "href": "s03_noparnolin.html#objetivo-de-aprendizaje",
    "title": "Técnicas no paramétricas y modelación no lineal",
    "section": "Objetivo de aprendizaje",
    "text": "Objetivo de aprendizaje\nEn esta sección del curso te adentrarás en las caras “opuestas” a las técnicas que has revisado hasta ahora; es decir, explorarás casos en los cuales no puedes o debes de aplicar una prueba paramétrica, así como casos en los cuales no asumirás que tus variables tienen una relación lineal, ni tampoco necesitas cumplir con el supuesto de normalidad."
  },
  {
    "objectID": "s04_mv.html#objetivo-de-aprendizaje",
    "href": "s04_mv.html#objetivo-de-aprendizaje",
    "title": "Técnicas Multivariadas",
    "section": "Objetivo de aprendizaje",
    "text": "Objetivo de aprendizaje\nEn esta sección del curso llegarás al punto de máxima abstracción y te adentrarás en diversas técnicas que te permitirán obtener conclusiones a partir de datos multivariados. Comenzarás analizando las relaciones entre tus variables mediante matrices de varianzas/covarianzas, formarás agrupaciones, compararás las mediciones multivariadas entre grupos, realizarás clasificaciones y, por último, realizarás regresiones múltiples y verás cómo controlar la complejidad de tus modelos."
  },
  {
    "objectID": "c17_clasif.html#bosques-aleatorios",
    "href": "c17_clasif.html#bosques-aleatorios",
    "title": "17  Aprendizaje supervisado: Clasificación",
    "section": "17.1 Bosques aleatorios",
    "text": "17.1 Bosques aleatorios\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n\niris.plot <- ggplot(iris, aes(x = Species, y = Petal.Length)) +\n             geom_point(color = \"dodgerblue4\", alpha = 0.8) +\n             theme_bw()\niris.plot\n\n\n\n\nEl primer paso es separar nuestros datos en datos de entreenamiento y prueba, lo cual podemos hacer con las funciones initial_split, training y testing de la librería rsample:\n\nset.seed(123)\niris_split <- initial_split(iris, strata = Species)\niris_train <- training(iris_split)\niris_test <- testing(iris_split)\n\nLuego, construimos una receta para el preprocesamiento de los datos:\n\nLe damos a la receta (recipe()) la fórmula y los datos de entrenamiento\nAñadimos un paso para centrar los datos numéricos. Recuerda, solo estamos poniendo todos los valores numéricos en la misma escala\n\nEl objeto iris_prep sigue los pasos a seguir para el preprocesamiento de los datos (por ello receta) y obtiene los parámetros con los que se van a preprocesar los datos, mientras que juiced obtiene los datos procesados.\n\niris_rec <- recipe(Species~., data = iris_train) |> \n            step_center(all_numeric())\niris_prep <- iris_rec |> prep()\njuiced <- juice(iris_prep)\n\nAhora podemos especificar el modelo de bosques aleatorios, donde ajustaremos sus hiperparámetros:mtry (el número máximo de predictores por árbol), min_n (el número de observaciones necesarias para seguir dividiendo los datos) y trees (el número de árboles en el ensemble). Después especificamos que es un bosque para clasificación, y por último le indicamos que utilice la librería ranger para construir el bosque:\n\ntune_spec <- rand_forest(mtry = tune(),\n                         trees = tune(),\n                         min_n = tune()) |> \n             set_mode(\"classification\") |>\n             set_engine(\"ranger\")\n\nFinalmente, formamos un flujo de trabajo que contenga ambos pasos: la receta de preprocesamiento y el modelo\n\ntune_wf <- workflow() |> \n           add_recipe(iris_rec) |> \n           add_model(tune_spec)\n\nAhora sí, podemos ajustar nuestros hiper-parámetros. Primero, asignemos un confjunto de remuestreos para la validación cruzada, construidos a partir de los datos de entrenamiento:\n\nset.seed(0)\niris_fold <- vfold_cv(iris_train)\n\nLuego establezcamos el procesamiento en paralelo para hacer el procedimiento más rápido. En este primer proceso vamos a escoger 20 puntos aleatorios para guiar nuestra búsqueda y no abusar de la fuerza bruta para resolver el problema:\n\ndoParallel::registerDoParallel(cores = 6)\ntune_res <- tune_grid(tune_wf,\n                      resamples = iris_fold,\n                      grid = 20)\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\nVeamos nuestros AUCs:\n\ntune_res |> collect_metrics() |> \n            filter(.metric == \"roc_auc\") |> \n            select(mean, min_n, mtry, trees) |>\n            pivot_longer(min_n:trees,\n                         values_to = \"value\",\n                         names_to = \"parameter\") |> \n            ggplot(aes(value, mean, color = parameter)) +\n            geom_point(show.legend = F) +\n            facet_wrap(~parameter, scales = \"free_x\")"
  },
  {
    "objectID": "c19_glm.html#librerías",
    "href": "c19_glm.html#librerías",
    "title": "19  Modelos Lineales Generalizados",
    "section": "19.1 Librerías",
    "text": "19.1 Librerías\n\nlibrary(ggplot2)\nlibrary(stats4)\nlibrary(brms)\nlibrary(visreg)\nlibrary(pscl)\nlibrary(MASS)\nlibrary(MuMIn)"
  },
  {
    "objectID": "c19_glm.html#funciones-personalizadas",
    "href": "c19_glm.html#funciones-personalizadas",
    "title": "19  Modelos Lineales Generalizados",
    "section": "19.2 Funciones personalizadas",
    "text": "19.2 Funciones personalizadas\n\n# Tema personalizado\nblank_theme <- function(){\n  theme(panel.grid.minor = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.background = element_blank(),\n        axis.line = element_blank(),\n        aspect.ratio = 1/1.61,\n        axis.ticks = element_blank(),\n        text = element_text(colour = \"gray50\"),\n        legend.position = \"none\"\n        )\n}"
  },
  {
    "objectID": "c19_glm.html#problemas-familiares",
    "href": "c19_glm.html#problemas-familiares",
    "title": "19  Modelos Lineales Generalizados",
    "section": "19.3 Problemas familiares",
    "text": "19.3 Problemas familiares\nEn la sesión anterior mencionamos cómo utilizar combinaciones lineales de variables para predecir una variable continua, pero también recordarás que en la sesión de RLS hablamos de modificar el supuesto de la distribución de nuestros errores para obtener una mejor estimación, lo cual conforma una parte fundamental de los modelos lineales generalizados (GLMs).\nLa modificación puede ser tan “simple” (al menos en términos de intuición) como relajar el supuesto de normalidad, pero podemos también utilizar otras distribuciones que nos permitan modelar otro tipo de información. Bueno, hoy aterrizaremos esa última idea: La estructura principal de un GLM sigue siendo un modelo lineal, aunque nuestro supuesto de la distribución de errores no será exclusivamente normal, sino que puede tomar alguna otra familia de distribuciones, enlazada a nuestros datos con alguna función. En la sesión de hoy revisaremos:\n\nRegresiones robustas, expandiendo un poco la distribución t como distribución de errores y revisando una alternativa.\nFunciones de enlace y enlace inverso.\nRegresiones para conteos: Poisson, Binomial Negativa y sus variantes infladas en zeros.\nSelección de modelos; i.e., cómo seleccionar el mejor de un conjunto de modelos candidatos.\nRegresiones para clases: Regresión logística binomial y multinomial.\n\nOJO: Por practicidad obviaré la división datos de entrenamiento-prueba; sin embargo, es algo que SIEMPRE se debe de tener en cuenta"
  },
  {
    "objectID": "c19_glm.html#regresiones-robustas",
    "href": "c19_glm.html#regresiones-robustas",
    "title": "19  Modelos Lineales Generalizados",
    "section": "19.4 Regresiones robustas",
    "text": "19.4 Regresiones robustas\nLa idea de una regresión robusta la revisamos en la sesión de RLS; es decir, utilizar una distribución con colas más altas que una distribución normal para poder contender con el efecto de puntos extremos, pero expandamos esa idea. ¿Qué significa el “peso” de las colas de una distribución? Qué tanta densidad (o masa, para distribuciones discretas) de probabilidad está acumulada lejos de la tendencia central. En palabras más sencillas, una distribución con colas ligeras como la normal piensa que la probabilidad de tener valores lejos de la tendencia central es muy baja; por lo tanto, consiidera “todos” los datos como igual de importantes y reacciona moviendo la estimación. ¿No me crees? Veamos un caso extremo, utilizando el tercer conjunto de datos de el cuarteto de Anscombe:\n\n# Almacenados en R como anscombe\nansc <- read.csv(\"datos/anscombe.csv\")\nansc$x <- scale(ansc$x, center = TRUE, scale = FALSE)\nggplot(data = ansc, aes(x = x, y = y, color = conjunto)) +\n  geom_point() +\n  facet_wrap(~conjunto) +\n  blank_theme() +\n  labs(title = \"Cuarteto de Anscombe\") +\n  scale_color_manual(values = c(\"gray70\", \"gray70\", \"#1f77b4\", \"gray70\"))\n\n\n\n\nSi vemos su distribución de y notaremos que no es exactamente normal, debido a ese punto extremo en 12.5:\n\nansc_iii = ansc[ansc$conjunto == \"III\",]\nggplot(data = ansc_iii, aes(x = y)) +\n  geom_density(color = \"#1f77b4\", fill = NA) +\n  blank_theme() +\n  labs(title = \"Densidad de y del conjunto III de Anscombe\")\n\n\n\n\nAjustemos entonces nuestra regresión lineal simple:\n\nansciii_lm <- lm(y~x, data = ansc_iii)\nsummary(ansciii_lm)\n\n\nCall:\nlm(formula = y ~ x, data = ansc_iii)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   7.5000     0.3728  20.120 8.61e-09 ***\nx             0.4997     0.1179   4.239  0.00218 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\n\nGráficamente:\n\nggplot(data = ansc_iii,\n       aes(x = x, y = y)) +\n  geom_point(color = \"#1f77b4\") +\n  geom_smooth(method = \"lm\",\n              color = \"gray50\",\n              fill = NA) +\n  blank_theme() +\n  labs(title = \"RLS con supuesto de normalidad\")\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nNo se ve mal; sin embargo, es claro que el punto extremo está influenciando la estimación.Podemos aplicar algún criterio de detección de valores extremos (o deformar nuestros datos) para cumplir con el supuesto de normalidad; sin embargo, más que parchar nuestros datos, es preferible modificar nuestro modelo. Cambiemos entonces a una regresión con una verosimilitud t de Student:\n\nLLt <- function(b0, b1, df, sigma){\n  # Encontrar los residuales. Modelo a ajustar (lineal)\n  R = ansc_iii$y - ansc_iii$x*b1 - b0\n  \n  # Calcular la verosimilitud. Residuales con distribución t de student\n  \n  R = suppressWarnings(brms::dstudent_t(R, df = df,\n                                        mu = 0, sigma = sigma))\n  \n  # Sumar el logaritmo de las verosimilitudes\n  # para todos los puntos de datos.\n  -sum(R, log = TRUE)\n}\n\nmlet_fit <- mle(LLt, \n                start = list(b0 = 0, b1 = 0, df = 2, sigma = 1),\n                nobs = length(ansc_iii$y),\n                lower = list(b0 = -20, b1 = -12, df = 1, sigma = 0.1),\n                upper = list(b0 = 20, b1 = 12, df = 30, sigma = 10))\nsummary(mlet_fit)\n\nWarning in sqrt(diag(object@vcov)): NaNs produced\n\n\nMaximum likelihood estimation\n\nCall:\nmle(minuslogl = LLt, start = list(b0 = 0, b1 = 0, df = 2, sigma = 1), \n    nobs = length(ansc_iii$y), lower = list(b0 = -20, b1 = -12, \n        df = 1, sigma = 0.1), upper = list(b0 = 20, b1 = 12, \n        df = 30, sigma = 10))\n\nCoefficients:\n        Estimate   Std. Error\nb0     7.1141555  0.015785311\nb1     0.3453896  0.005152567\ndf    30.0000000 36.983300378\nsigma  0.1000000          NaN\n\n-2 log L: -81.09539 \n\n\nAhora empatemos ambas regresiones en un mismo gráfico:\n\ncoefs_t <- coef(mlet_fit)\nfitted <- coefs_t[1] + coefs_t[2]*ansc_iii$x\nggplot(data = ansc_iii,\n       aes(x = x, y = y)) +\n  geom_line(aes(x = x, y = fitted),\n            color = \"#ff7f0e\",\n            size = 1, alpha = 0.7) +\n  geom_smooth(method = \"lm\",\n              color = \"gray50\",\n              fill = NA) +\n  geom_point(color = \"#1f77b4\") +\n  labs(title = \"Regresión robusta\",\n       subtitle = \"OLS (gris) vs. t de Student (naranja)\") +\n  theme_bw()\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nEn este caso el punto extremo ya no influenció la estimación de la regresión, lo cual en la mayoría de los casos es algo deseable. Aunque esta es una forma de realizar una regresión robusta, existen otras. Una de ellas es modificar la función de pérdida, como por ejemplo con la Regresión con pérdida Huber. Los detalles matemáticos los dejaré para tu investigación, lo realmente importante es entender que los errores (residuales) son ponderados diferencialmente en función de su magnitud; es decir, se resta importancia a aquellos residuales que sean grandes y, de hecho, si están por encima de cierto límite, son descartados por completo. Su implementación es sumamente sencilla, pues lo único que tenemos que hacer es modificar lm por la función rlm de la librería MASS:\n\nrr.huber <- MASS::rlm(y~x, data = ansc_iii)\nsummary(rr.huber)\n\n\nCall: rlm(formula = y ~ x, data = ansc_iii)\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0049962 -0.0028591 -0.0007219  0.0028667  4.2421008 \n\nCoefficients:\n            Value     Std. Error t value  \n(Intercept)    7.1150    0.0013  5309.3547\nx              0.3457    0.0004   815.8284\n\nResidual standard error: 0.005248 on 9 degrees of freedom\n\n\n¿Notas algo interesante? Los resultados son los mismos que en la regresión t de Student, aunque aquí podemos ver la ponderación dada a cada punto:\n\nhweights <- data.frame(x = ansc_iii$x,\n                       resid = rr.huber$residuals,\n                       weight = rr.huber$w)\nhweights\n\n\n\n  \n\n\n\nGráficamente:\n\nggplot(data = ansc_iii,\n       aes(x = x, y = y)) +\n  geom_smooth(method = MASS::rlm,\n             color = \"#ff7f0e\",\n             size = 1, alpha = 0.7,\n             fill = \"blue\") +\n  geom_smooth(method = \"lm\",\n              color = \"gray50\",\n              fill = NA) +\n  geom_point(aes(color = hweights$weight)) +\n  labs(title = \"Regresión robusta\",\n       subtitle = \"OLS (gris) vs. Huber (naranja)\") +\n  scale_color_gradient(name = \"Peso\",\n                       low = \"firebrick\",\n                       high = \"#1f77b4\") +\n  theme_bw()\n\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nComo era de esperarse, los resultados son los mismos que los de la regresión t de Student. En la definición de un GLM tenemos tres elementos importantes:\n\nEl modelo lineal, que es el mismo en ambos casos.\nUna familia para el error, que no modificamos en la regresión Huber; sin embargo la familia sería Gaussiana (Normal).\nUna función de enlace (o enlace inverso), que en ambos casos sería una función de identidad; es decir, ninguna modificación para pasar de nuestros datos a la distribución del error.\n\nEs decir, que en un sentido amplio, ambas aproximaciones son GLMs, aunque usualmente nos referimos a GLMs cuando la distribución del error es diferente a una distribución normal. ¿Cuál aplicar? Ya que el resultado es el mismo, puedes escoger una u otra, solo ten en cuenta que la implementación por máxima verosimilitud de un modelo t de Student puede tener muchos bemoles al momento de optimizarse, además de que su salida es incompatible con algunas otras funciones, incluyendo el cálculo de los intervalos de confianza (para los coeficientes de la regresión Huber puedes utilizar la función confint.default(rr.huber))."
  },
  {
    "objectID": "c19_glm.html#funciones-de-enlace",
    "href": "c19_glm.html#funciones-de-enlace",
    "title": "19  Modelos Lineales Generalizados",
    "section": "19.5 Funciones de enlace",
    "text": "19.5 Funciones de enlace\nEste es un buen momento para hablar de un tema que a veces causa bastante confusión: las funciones de enlace o las funciones de enlace inverso. Estas son funciones “arbitrarias” (ojo a las comillas) que tienen una sola función (valga la redundancia): poner la salida de nuestro modelo lineal en los “requerimientos” de la familia de nuestro error. En el caso anterior, la distribución t es una distribución continua de probabilidad que está centrada en 0, como esperaríamos de nuestros residuales, por lo que la función de enlace es una función de identidad; es decir, no hacemos nada a la salida del modelo para poder obtener residuales continuos centrados en 0. Pero este no siempre es el caso; de hecho, las aplicaciones más comunes de GLM siempre requieren de algún enlace. ¿Y los enlaces inversos? Son simple y sencillamente el inverso de la función de enlace aplicados al lado contrario de la igualdad. Matemáticamente es más claro:\nEn un GLM con una función de enlace tendríamos la siguiente estructura para nuestro modelo lineal:\n\\[\nf(y) = \\beta_0 + \\beta_1*x\n\\] Es decir, modificamos la salida (\\(y\\)) de nuestro modelo lineal utilizando una función \\(y\\). Si es una función logarítmica, por ejemplo, se vería de la siguiente manera:\n\\[\nlog(y) = \\beta_0 + \\beta_1*x\n\\]\nPero obtendríamos exactamente lo mismo si utilizamos un poco de álgebra y resolvemos para \\(y\\), aplicando un exponencial a ambos lados de la igualdad:\n\\[\ne^{log(y))} = e^{(\\beta_0 + \\beta_1*x)} \\\\\n\\therefore \\\\\ny = e^{(\\beta_0 + \\beta_1 *x)}\n\\]\nEl apelativo “inversa” es simplemente para indicar el lado dónde se está aplicando el enlace. Habiendo dicho esto, vayamos a una de las aplicaciones más comunes para GLM: la regresión para conteos."
  },
  {
    "objectID": "c19_glm.html#regresiones-para-conteos",
    "href": "c19_glm.html#regresiones-para-conteos",
    "title": "19  Modelos Lineales Generalizados",
    "section": "19.6 Regresiones para conteos",
    "text": "19.6 Regresiones para conteos\nTe preguntarás qué tienen de especial los conteos, y la respuesta es muy simple: son valores enteros mayores o iguales a 0. Esto quiere decir que una distribución normal (t, o cualquier otra distribución continua) NO es adecuada para modelar los datos. ¿Qué hacemos? Utilizamos alguna distribución discreta que nos permita tratar con el número de veces en que algo sucede.\n\n19.6.1 Regresión Poisson\nMuy posiblemente esto te suene a ensayos de Bernoulli o ejercicios con distribuciones Poisson (¿cuántos autos rojos pasan en una hora por un punto determinado?, por ejemplo). Pues justamente podemos utilizar esa misma distribución (Poisson). Esta distribución tiene un par de peculiaridades. La primera es que asume que los eventos ocurren de manera independiente entre sí, a un intervalo fijo de espacio o tiempo. La segunda es que su único parámetro (\\(\\lambda\\)) representa tanto la media como la varianza de la distribución (más adelante hablaremos de las implicaciones de esto), por lo que DEBE ser positivo. ¿El problema? Nuestros residuales pueden ser negativos. ¿Qué podemos hacer? Aplicar una función de enlace inverso que nos permita restringir nuestro predictor a valores positivos, justo como la función exponencial, por lo que nuestro modelo se expresaría de la siguiente forma:\n\\[\n\\lambda = e^{(\\beta_0 + \\beta_1*x)} \\\\\ny \\sim Poisson(\\lambda)\n\\]\nPara aplicarlo resolvamos un problema en el cuál trataremos de predecir el número de peces capturados en un lago por un pescador, considerando el número de hijos y si llevan o no un camper:\n\ncsvurl <- \"https://stats.idre.ucla.edu/stat/data/fish.csv\"\nfish <- read.csv(csvurl)[,c(\"child\", \"camper\", \"count\")]\nhead(fish)\n\n\n\n  \n\n\n\nExploremos nuestros datos. Al tratarse de una variable discreta podemos, sin ningún problema, utilizar un gráfico de frecuencias:\n\nggplot(aes(x = count), data = fish) +\n  geom_bar(stat = \"count\", color = NA, fill = \"dodgerblue4\") +\n  blank_theme() +\n  labs(title = \"Frecuencia de peces capturados en un lago\",\n       x = element_blank(),\n       y = element_blank())\n\n\n\n\n¿Qué otro gráfico utilizarías? Por otra parte, habrás notado un par de cosas: a) hay una gran cantidad de ceros y b) tenemos algunos puntos “extremos”; i.e., algunos pescadores que tuvieron demasiada suerte y que capturaron demasiados peces en comparación con el resto. Este tipo de distribuciones no son extrañas en la naturaleza, y tienen un par de bemoles de los cuales hablaremos después. Por lo pronto, construyamos nuestro GLM. Para ello utilizaremos la función glm de R base, cuyo uso es sumamente similar al de la función lm, salvo que indicaremos la familia como un argumento adicional:\n\npoiss <- glm(count~., data = fish, family = \"poisson\")\nsummary(poiss)\n\n\nCall:\nglm(formula = count ~ ., family = \"poisson\", data = fish)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.7736  -2.2293  -1.2024  -0.3498  24.9492  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  0.91026    0.08119   11.21   <2e-16 ***\nchild       -1.23476    0.08029  -15.38   <2e-16 ***\ncamper       1.05267    0.08871   11.87   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2958.4  on 249  degrees of freedom\nResidual deviance: 2380.1  on 247  degrees of freedom\nAIC: 2723.2\n\nNumber of Fisher Scoring iterations: 6\n\n\nLa salida es muy similar a otras que hemos visto. Cómo llamamos a la función glm, un descriptor de los residuales, los coeficientes con sus respectivas pruebas de nulidad (después hablaremos de su interpretación), seguidas de algunos elementos propios de la función GLM. Primero tenemos una nota sobre un parámetro de dispersión, que se asumió como 1. Esto quiere decir que estamos asumiendo que la media es igual a la varianza, lo cual podemos tomar solo como un recordatorio para que revisemos dicho supuesto. Después tenemos información sobre la devianza del modelo. Podemos utilizar la devianza residual para realizar una prueba de bondad de ajuste para el modelo global. Esta es la diferencia entre la devianza del modelo y la máxima devianza de un modelo ideal, donde los valores predichos son idénticos a los observados (devianza nula). Por lo tanto, buscamos valores pequeños de la devianza residual. Dicha prueba podemos realizarla de la siguiente manera:\n\nwith(poiss, cbind(res.deviance = deviance,\n                  df = df.residual,\n                  p = pchisq(deviance, df.residual,\n                             lower.tail = F)))\n\n     res.deviance  df p\n[1,]      2380.12 247 0\n\n\nTenemos un valor de p sumamente pequeño, lo cual sugiere que el modelo no se encuentra bien ajustado. ¿Alguna idea de por qué? Como te imaginarás, tiene que ver con la distribución de nuestros datos, eso que mencionamos sobre muchos ceros y algunos pescadores con mucha suerte. De hecho, cada una de estas características es un problema en sí mismo, así que abordemoslos uno por uno. Una pregunta que puedes estarte haciendo es ¿y la función de enlace? Va implícita en la familia. En este caso, es una función de enlace logarítmica, que es el equivalente a la función de enlace inverso que revisamos antes.\n\nstr(poisson()[1:5])\n\nList of 5\n $ family  : chr \"poisson\"\n $ link    : chr \"log\"\n $ linkfun :function (mu)  \n $ linkinv :function (eta)  \n $ variance:function (mu)  \n\n\n\n\n19.6.2 Exceso de ceros: Regresión Poisson Inflada en Cero\nLa primera peculiaridad de nuestros datos es que hay una cantidad enorme de ceros. Aunque esto puede suceder de manera natural, la distribución Poisson no es capaz de contender adecuadamente con estos casos. Afortunadamente, hay una manera de extender el modelo Poisson para permitirnos arreglar esto. En su forma más fundamental, asumiremos que tenemos dos procesos:\n\nUno modelado con una distribución Poisson.\nUno generando ceros adicionales.\n\nEs decir, cuando hablemos de modelos “inflados en cero” estamos hablando de una situación en la que tenemos ceros “falsos” o, mejor dicho, extras a los ceros verdaderos que nos podemos encontrar. Veamos qué pasa al ajustar este modelo a nuestros datos. Para este modelo necesitaremos de la función zeroinfl() de la librería pscl:\n\nzi_poiss <- pscl::zeroinfl(count~child+camper, data = fish)\nsummary(zi_poiss)\n\n\nCall:\npscl::zeroinfl(formula = count ~ child + camper, data = fish)\n\nPearson residuals:\n    Min      1Q  Median      3Q     Max \n-1.2395 -0.8340 -0.4694 -0.1764 24.1051 \n\nCount model coefficients (poisson with log link):\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  1.64535    0.08278  19.877   <2e-16 ***\nchild       -0.77272    0.09103  -8.489   <2e-16 ***\ncamper       0.75526    0.09112   8.289   <2e-16 ***\n\nZero-inflation model coefficients (binomial with logit link):\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   0.0424     0.2426   0.175   0.8613    \nchild         1.0244     0.2200   4.656 3.22e-06 ***\ncamper       -0.7085     0.2926  -2.422   0.0155 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nNumber of iterations in BFGS optimization: 10 \nLog-likelihood: -1025 on 6 Df\n\n\nLa salida es similar al caso anterior, solo tenemos coeficientes para la regresión logística para clasificar ceros verdaderos de falsos y los coeficientes del modelo Poisson sin el exceso de ceros; sin embargo, notarás que no hay ningún indicativo sobre si este modelo es mejor a nuestro modelo Poisson, por lo que podemos compararlos. Para ello podemos utilizar distintas alternativas: una prueba de Vuong (función vuong(mod_1, mod_2) de pscl) o utilizar una aproximación multi-modelo para la selección de modelos. Optaremos por esa última vía, la cual exploraremos a detalle más adelante. Por lo pronto, es suficiente que sepas que utilizaremos una medida llamada Criterio de Información de Akaike (AIC), y el mejor modelo será aquel que tenga el menor valor de AIC:\n\nAIC(poiss, zi_poiss)\n\n\n\n  \n\n\n\nComo era de esperarse, el modelo inflado en cero es un mejor candidato; sin embargo, tenemos un problema pendiente: nuestros pescadores muy suertudos.\n\n\n19.6.3 Sobre-dispersión: Regresión Binomial Negativa\nEsos pescadores muy suertudos pueden hacer lo mismo que nuestro punto extremo en el ejemplo de regresión robusta; es decir, jalar nuestras estimaciones hacia ellas y alejarlas de la estimación “real”, solo que aquí es un tanto diferente y tiene que ver con el supuesto de nuestra distribución Poisson: la media y la varianza son iguales. Este supuesto, evidentemente, no se sostiene cuando tenemos una dispersión muy grande de nuestros datos (varianza > media), lo cual genera el problema de la sobre dispersión de nuestro modelo (no de los datos). Una estrategia es cambiar nuestra verosimilitud a una distribución Binomial Negativa, la cual tiene un parámetro adicional a la distribución Poisson. Este parámetro modela, justamente, la dispersión de nuestros datos. Apliquemos entonces nuestra regresión binomial negativa:\n\nbineg <- MASS::glm.nb(count~., data = fish, trace = F)\nsummary(bineg)\n\n\nCall:\nMASS::glm.nb(formula = count ~ ., data = fish, trace = F, init.theta = 0.2552931119, \n    link = log)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.3141  -1.0361  -0.7266  -0.1720   4.0163  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   1.0727     0.2425   4.424 9.69e-06 ***\nchild        -1.3753     0.1958  -7.025 2.14e-12 ***\ncamper        0.9094     0.2836   3.206  0.00135 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(0.2553) family taken to be 1)\n\n    Null deviance: 258.93  on 249  degrees of freedom\nResidual deviance: 201.89  on 247  degrees of freedom\nAIC: 887.42\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  0.2553 \n          Std. Err.:  0.0329 \n\n 2 x log-likelihood:  -879.4210 \n\n\nNotarás que esta salida es prácticamente la misma que la que tuvimos en nuestro GLM Poisson, salvo que ahora nos da el valor del parámetro de sobredispersión. Te estarás preguntando: ¿Cómo sé si, en efecto, mis modelos están sobre-dispersos? Para eso podemos utilizar una prueba de razón de verosimilitud, en la cual compararemos la verosimilitud de ambos modelos (binomial negativa y Poisson) y veremos si se ajustan a la misma distribución; es decir, la prueba de razón de verosimilitud es una prueba de bondad de ajuste, con distribución \\(\\chi^2\\). ¿Qué es lo que estamos comparando? Si el parámetro adicional ayuda a que el ajuste del modelo mejore significativamente. Para aplicarla podemos utilizar la función odTest(mod_bn) de la librería pscl:\n\npscl::odTest(bineg)\n\nLikelihood ratio test of H0: Poisson, as restricted NB model:\nn.b., the distribution of the test-statistic under H0 is non-standard\ne.g., see help(odTest) for details/references\n\nCritical value of test statistic at the alpha= 0.05 level: 2.7055 \nChi-Square Test Statistic =  1837.7652 p-value = < 2.2e-16 \n\n\nA ojo de buen cubero era más que evidente que nuestro modelo estaba sobre disperso, por lo que estos resultados no son sorprendentes. Algo que puedes pensar es “si estoy comparando qué modelo está mejor ajustado, ¿puedo entonces utilizar el AIC?” Y la respuesta es, por supuesto:\n\nAIC(poiss, zi_poiss, bineg)\n\n\n\n  \n\n\n\nY los resultados son, como debe de ser, consistentes. Llegados a este punto podrías preguntarme: “Ok, Arturo, ya corregimos para el exceso de ceros y para la sobre dispersión, pero lo hicimos de manera independiente. ¿Hay alguna manera de hacer ambas cosas al mismo tiempo?” En efecto, y es justo lo siguiente que vamos a revisar.\n\n\n19.6.4 Exceso de ceros y sobre dispersión: Regresión Binomial Negativa Inflada en Cero\nY, justamente, es una combinación de ambas; es decir, utilizaremos una distribución de error binomial negativa inflada en cero. La lógica es, entonces, una combinación de ambas aproximaciones; es decir, modelaremos a los ceros verdaderos y luego construiremos el modelo de regresión binomial negativa. Para hacerlo utilizaremos la función zeroinfl() que vimos antes, solo que cambiaremos la familia a negbin:\n\nzi_bineg <- pscl::zeroinfl(count~., data = fish, dist = \"negbin\")\nsummary(zi_bineg)\n\n\nCall:\npscl::zeroinfl(formula = count ~ ., data = fish, dist = \"negbin\")\n\nPearson residuals:\n      Min        1Q    Median        3Q       Max \n-0.512182 -0.497136 -0.325130 -0.003367 13.978082 \n\nCount model coefficients (negbin with log link):\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   1.0515     0.2700   3.895 9.84e-05 ***\nchild        -0.9113     0.2851  -3.196  0.00139 ** \ncamper        0.7976     0.3054   2.611  0.00902 ** \nLog(theta)   -1.2960     0.1316  -9.849  < 2e-16 ***\n\nZero-inflation model coefficients (binomial with logit link):\n            Estimate Std. Error z value Pr(>|z|)\n(Intercept)  -11.499     55.699  -0.206    0.836\nchild         10.483     55.659   0.188    0.851\ncamper        -9.501     55.663  -0.171    0.864\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nTheta = 0.2736 \nNumber of iterations in BFGS optimization: 84 \nLog-likelihood: -434.9 on 7 Df\n\n\nFinalmente, podemos comparar nuestros cuatro modelos candidatos para encontrar el más adecuado:\n\nAIC(poiss, zi_poiss, bineg, zi_bineg)\n\n\n\n  \n\n\n\nVemos que los AIC de los modelos con distribución de error binomial negativa tienen los menores valores; por lo tanto seleccionaremos a alguno de los dos. ¿Cuál? En la siguiente sección hablaremos de las peculiaridades. Por lo pronto, sigamos con nuestro criterio de seleccionar el que tenga el menor AIC, que corresponde a la distribución binomial negativa inflada en cero. Ahora sí, podemos interpretar nuestros coeficientes.\n\n\n19.6.5 Interpretación\nDesafortunadamente, la interpretación no es tan simple como en la RLM o RLS, debido a la función de enlace que utilizamos.\nPara facilitarnos la existencia, planteemos un modelo Poisson con un solo predictor, y la función de enlace logarítmica:\n\\[\nY \\sim Poisson(\\theta) \\\\\nlog(\\theta) = \\alpha + \\beta x \\\\\n\\therefore \\\\\n\\theta = e^{\\alpha + \\beta x}\n\\]\nPero, por las leyes de los exponentes, podemos reescribir la última ecuación como:\n\\[\n\\theta = e^{a}e^{\\beta x}\n\\] Esto quiere decir que los coeficientes no son aditivos, sino multiplicativos:\n\nIntercepto: \\(e^\\alpha\\), valor de \\(\\theta\\) cuando \\(x = 0\\). Si este parámetro es o no de interés depende totalmente del problema.\nPendiente(s): \\(e^\\beta\\).\n\n\nSi \\(\\beta = 0\\), entonces \\(e^\\beta = 1\\); es decir, no hay un efecto del predictor.\nSi \\(\\beta > 0\\), entonces \\(e^\\beta > 1\\); es decir, el predictor incrementa el valor de \\(\\theta\\) a una tasa de \\(e^\\beta\\) por cada incremento unitario en \\(x\\)\nSi \\(\\beta < 0\\), entonces \\(e^\\beta < 1\\); es decir, el predictor disminuye el valor de \\(\\theta\\) a una tasa de \\(e^\\beta\\) por cada incremento unitario en \\(x\\).\n\nRecuperemos los coeficientes de nuestra regresión binomial negativa inflada en cero y exponenciémoslos:\n\nexp(coef(zi_bineg)[1:3])\n\ncount_(Intercept)       count_child      count_camper \n        2.8620079         0.4019951         2.2202680 \n\n\nPongamos atención solo a aquellos coeficientes con count_, pues son los que realmente nos interesan. La interpretación entonces sería:\n\nIntercepto: Cuando el pescador no tiene hijos y no lleva un camper, el promedio de peces capturados es de 2.86\nPendientes:\n\n\nChild: El promedio de peces capturados disminuye 1-0.4 veces por cada hijo adicional.\nCamper: Si el pescador tiene un camper, el promedio de peces capturados incrementa 2.2 veces.\n\nCorolario: La interpretación depende totalmente de la función de enlace que utilicemos, y siempre es necesario aplicar el enlace para poder interpretarlos.\nPara construir un gráfico podemos generar una línea con valores predichos o, mejor dicho, dos líneas: una para cada nivel de camper (en la sección de Extras encontrarás cómo funcionan los predictores categóricos en modelos de regresión).\n\nfish$pred <- predict(zi_bineg, type = \"response\")\n\nggplot(data = fish, aes(x = child, y = count,\n                        color = factor(camper))) +\n  geom_point() +\n  geom_line(aes(y = pred)) +\n  labs(title = \"GLM Binomial negativo\",\n       x = \"Número de hijos\",\n       y = \"Peces capturados\") +\n  scale_color_discrete(name = \"Camper\",\n                       labels = c(\"NO\", \"SI\")) +\n  scale_y_continuous(limits = c(0, 12),\n                     label = scales::label_comma(accuracy = 1))\n\nWarning: Removed 16 rows containing missing values (geom_point).\n\n\n\n\n\nO utilizando visreg:\n\npartial_plots <- visreg::visreg(bineg,\n                                scale = \"response\",\n                                ylab = \"Capturas\",\n                                gg = TRUE)\n\nfor (plt in partial_plots) {\n  plot(plt + blank_theme() +\n         scale_y_continuous(limits = c(0, 12),\n                            label = scales::label_comma(accuracy = 1)) +\n         scale_x_discrete(limits = c(0, max(plt$data$x))) +\n         labs(x = plt$labels$x))\n}\n\nWarning: Continuous limits supplied to discrete scale.\nDid you mean `limits = factor(...)` or `scale_*_continuous()`?\nContinuous limits supplied to discrete scale.\nDid you mean `limits = factor(...)` or `scale_*_continuous()`?\n\n\n\n\n\n\n\n\nAntes de cambiar la hoja, es necesario aclarar que aquí NO hay supuesto de normalidad. ¿Cuál es el punto de cambiar la distribución del error si vamos a seguir casados buscando normalidad estadística?\nY ahora podemos hablar de la selección de modelos."
  },
  {
    "objectID": "c19_glm.html#selección-de-modelos",
    "href": "c19_glm.html#selección-de-modelos",
    "title": "19  Modelos Lineales Generalizados",
    "section": "19.7 Selección de modelos",
    "text": "19.7 Selección de modelos\nEl tener varios varios modelos candidatos no es algo extraño, entonces es necesario tener algún tipo de criterio que nos permita comparar entre ellos. Una aproximación es la que hemos utilizado hasta el momento; es decir, utilizar el Criterio de Información de Akaike (AIC) y seleccionar el menor. ¿Qué es el AIC y con qué se come?\n\n19.7.1 Criterio de Información de Akaike\nEl AIC es un criterio basado en teoría de la información, particularmente en la divergencia Kullback-Leibler. Ese detalle matemático va más allá del alcance de este curso; sin embargo, podemos entender que está basado en la verosimilitud de un modelo dado; es decir, qué tan verosímil es que ese modelo haya generado los datos. Un detalle es que, como hemos visto en sesiones anteriores, el “ajuste” de un modelo es directamente proporcional a su complejidad (al menos vs. los datos de entrenamiento). Es, entonces, necesario penalizar de alguna manera el número de parámetros en el modelo, para no comernos un “gol” con un modelo excesivamente complejo. Puesto en una ecuación, el AIC queda de la siguiente manera:\n\\[\nAIC = -2ln(L) + 2k\n\\]\nDonde \\(L\\) es la verosimilitud del modelo y \\(k\\) el número de parámetros. Si nuestro modelo tiene un gran número de parámetros, el valor de AIC se hará más grande, mientras que, si tiene un menor número, se hará más pequeño. ¿Qué nos dice un AIC en sí mismo? NADA, absolutamente nada. Si yo te digo que un modelo tiene un AIC de 800 no puedes saber si es bueno o malo, pues no hay una referencia. Esto nos lleva a hablar sobre algunas consideraciones que debemos de tener al utilizar el AIC:\n\nValores más bajos indican modelos más parsimoniosos.\nEs una medida relativa de la parsimonia de un modelo, por lo que solo tiene sentido cuando comparamos AIC para hipótesis (modelos) alternativas.\nPodemos comparar modelos no anidados. De hecho, podríamos comparar un modelo lineal con uno no lineal.\nLas comparaciones son válidas SOLO para modelos ajustados con los mismos valores de respuesta; i.e., mismos valores de \\(y\\).\nComparar muchos modelos con AIC es una mala idea, pues caemos en el mismo problema de las comparaciones múltiples, donde podemos encontrar por azar un modelo con el valor más bajo de AIC, cuando en realidad no es el modelo más apropiado.\nPara variar, cuando tratamos con tamaños de muestra pequeños (n/k < 40) el AIC pierde confiabilidad, por lo que hay que aplicar una corrección:\n\n\\(AIC_c = AIC + \\frac{2k(k+1)}{n-k-1}\\)\nDado que conforme incrementa n, el \\(AIC_c\\) se aproxima al \\(AIC\\), es una buena idea utilizar \\(AIC_c\\).\n\nPodemos encontrar múltiples modelos que tengan AICs similares, esto solo sugiere que estas hipótesis alternativas tienen soportes similares. ¿Qué tanto es tantito? Esa respuesta es un poco más compleja, y requiere que presentemos el \\(\\Delta{AIC}\\) (también lo puedes encontrar como \\(\\Delta_i\\)):\n\n\\(\\Delta AIC = AIC_i - AIC_{min}\\); es decir, la diferencia de cada AIC respecto al valor mínimo de AIC entre los modelos candidatos. Esta transformación forza al “mejor” modelo a tener un \\(\\Delta AIC = 0\\), y representa la pérdida de información si utilizamos un modelo candidato \\(m_i\\) en vez de \\(m_{min}\\).\nModelos con un \\(\\Delta_i \\leq 2\\) tienen soporte substancial (evidencia),\nModelos con un \\(4 \\leq \\Delta_i \\leq 7\\) tienen considerablemente menos soporte y\nModelos con \\(\\Delta_i > 10\\) carecen, escencialmente, de soporte.\n\n\nRecuperemos nuestra comparación anterior, calculemos los \\(AIC_c\\) (con la función AICc de la librería MuMIn) y calculemos los \\(\\Delta_i\\). Como mencionábamos antes, los modelos con distribuciones binomial negativas son los que tienen el mayor soporte, mientras que los Poisson carecen de cualquier soporte (dados estos datos y modelos candidatos). También podemos ver que pesa más la sobredispersión que el exceso de ceros, pues el modelo inflado en cero es marginalmente mejor que aquel no inflado.\n\nAICs <- MuMIn::AICc(poiss, zi_poiss, bineg, zi_bineg)\nAICs$Delta <- AICs$AIC - min(AICs$AIC)\nAICs"
  },
  {
    "objectID": "c19_glm.html#regresiones-para-clases",
    "href": "c19_glm.html#regresiones-para-clases",
    "title": "19  Modelos Lineales Generalizados",
    "section": "19.8 Regresiones para clases",
    "text": "19.8 Regresiones para clases\nEn la sesión de clasificación hablamos sobre los problemas de clasificación, y aplicamos una regresión logística; sin embargo, no entramos en detalles. Aprovechemos, entonces, para revisar un poco más a profundidad la intuición detrás de ella.\n\n19.8.1 Regresión logística binaria\nCuando la revisamos anteriormente, definimos a la regresión logística como un modelo de RL al cual aplicaríamos una función logística, lo que nos permite restringir nuestra salida al intervalo \\([0,1]\\) y, por lo tanto, predecir la probabilidad de pertenencia a una clase dada. Esa definición es correcta para resumir lo más posible la técnica; sin embargo, los detalles son un poco más complejos.\nLa regresión logística forma parte de los GLMs; por lo tanto, consta de un predictor lineal, una familia de distribución del error y una función de enlace. La familia de distribución del error es binomial; es decir, está en términos de la probabilidad de éxitos vs. la probabilidad de fracasos. Es por esto que la regresión logística tradicional solo nos permite clasificar entre dos clases. Esto ya lo sabíamos, simplemente lo estamos formalizando, lo que tiene un poco más de detalles es la función de enlace: la función logística:\n\\[\nlogit(z) = \\frac{1}{1 + e^{-z}}\n\\]\nEsta función tiene la peculiaridad de que, independientemente de los valores de \\(z\\) (el predictor lineal), el resultado siempre estará contenido entre 0 y 1, el cual es, convenientemente, el mismo que el dominio del parámetro \\(p\\) de la distribución binomial (la probabilidad de éxito). Expresado matemáticamente:\n\\[\n\\theta = logistic(\\alpha + \\beta x)\n\\]\n\\[\ny \\sim Binom(\\theta)\n\\]\nApliquemos entonces una regresión logística para clasificar entre versicolor y virginica de la base iris, solo para ilustrar cómo interpretar los coeficientes. Primero, filtremos los datos:\n\niris_dat <- iris\niris_dat <- subset(iris,\n                   Species == \"versicolor\" | Species == \"virginica\")\n\nAhora ajustemos el modelo. No te olvides de dividir en entrenamiento-prueba (o, mejor aún, realizar validación cruzada), considerar si es necesario escalar los datos, y que puedes también puedes entrenar los glm utilizando la función caret::train(). Por practicidad, hagámoslo directamente.\n\nlogit_reg <- glm(Species~., data = iris_dat, family = \"binomial\")\nsummary(logit_reg)\n\n\nCall:\nglm(formula = Species ~ ., family = \"binomial\", data = iris_dat)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-2.01105  -0.00541  -0.00001   0.00677   1.78065  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)  \n(Intercept)   -42.638     25.707  -1.659   0.0972 .\nSepal.Length   -2.465      2.394  -1.030   0.3032  \nSepal.Width    -6.681      4.480  -1.491   0.1359  \nPetal.Length    9.429      4.737   1.991   0.0465 *\nPetal.Width    18.286      9.743   1.877   0.0605 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 138.629  on 99  degrees of freedom\nResidual deviance:  11.899  on 95  degrees of freedom\nAIC: 21.899\n\nNumber of Fisher Scoring iterations: 10\n\n\nY obtengámos los gráficos parciales, utilizando la función visreg(mod, scale = \"response\"), donde mod es el modelo aujustado:\n\npartial_plots <- visreg::visreg(logit_reg, scale = \"response\",\n                                ylab = \"P(Especie)\",\n                                line.par = c(col = \"dodgerblue4\"),\n                                fill.par = c(fill = \"gray90\"),\n                                gg = TRUE)\n\nfor (plt in partial_plots) {\n  plot(plt +\n         scale_y_continuous(n.breaks = 3,\n                            labels = c(\"versicolor\", 0.5, \"virginica\")) +\n         theme_bw())\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\nEsto ya lo conocíamos, pero ¿cómo interpretamos los coeficientes? Antes de pasar a eso, es necesario que pongamos atención al argumento scale de visreg, el cual indicamos como response. Esto lo que hizo fue poner nuestra salida en lo que nos interesa: la probabilidad de pertenencia a una especie, dadas las medidas de cada variable. Veamos qué pasa si retiramos ese argumento:\n\npartial_plots <- visreg::visreg(logit_reg, ylab = \"log-odds(Especie)\", gg = TRUE)\n\nfor (plt in partial_plots) {\n  plot(plt + blank_theme())\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\nAhora nuestros gráficos están en la escala de nuestro modelo lineal; es decir, con response estamos introduciendo la función de enlace y graficando el GLM completo; sin embargo, esto no está presente en los coeficientes “crudos” arrojados por la función GLM, por lo que toca aplicar el álgebra correspondiente:\nEl modelo básico es:\n\\[\n\\theta = logistic(\\alpha + \\beta x)\n\\]\nEl inverso de la función logística es la función logit, dada por:\n\\[\nlogit(z) = log \\left( \\frac{z}{1-z} \\right)\n\\]\nPor lo que si tomamos la primera ecuación y aplicamos la función logit a ambos términos, obtenemos esta ecuación:\n\\[\nlogit(\\theta) = \\alpha + \\beta x\n\\]\nO, de manera equivalente:\n\\[\nlog \\left( \\frac{\\theta}{1-\\theta} \\right) = \\alpha + \\beta x\n\\]\nAhora, recordemos que \\(\\theta\\) en nuestro modelo es \\(p(y = 1)\\) (la probabilidad de “éxito”, o de ser virginica):\n\\[\nlog \\left(\\frac{p(y = 1)}{1 - p(y = 1)} \\right) = \\alpha + \\beta x\n\\]\nLa cantidad \\(\\frac{p(y = 1)}{1 - p(y = 1)}\\) se conoce como los odds, que representan la probabilidad de éxito sobre la probabilidad de fracaso. Mientras que la probabilidad de obtener 2 al lanzar un dado es de 1/6, los odds para el mismo evento son \\(\\frac{1/6}{5/6} \\approx 0.2\\), o un éxito a cinco fracasos. En una regresión logística, \\(\\beta\\) representa el incremento en log-odds por incremento unitario en \\(x\\), no en la probabilidad de pertenencia a una clase, aunque la relación entre odds y probabilidad es monótona; es decir, conforme incrementa una, la otra también.\n\n\n19.8.2 Regresión logística multinomial\nAl igual que en el caso anterior, simplemente extenderemos aquellos detalles que no se aterrizaron por completo, particularmente el utilizar una red neuronal como análogo a una regresión logística multinomial. Como acabamos de ver, una regresión logística binaria nos permite predecir la probabilidad de éxito; i.e., de pertenecer a una sola clase. ¿Cómo lo extendemos a más de dos clases? Podemos construir modelos una clase vs. las demás, podemos utilizar una regresión softmax, o podemos utilizar una red neuronal. Una red neuronal está formada por capas, las cuales están conectadas entre sí tal cual neuronas:\n\n\n\nRed neuronal\n\n\nTenemos una capa de entrada, correspondiente a nuestros valores, seguida de una o más capas ocultas, compuestas por neuronas (perceptrones) que tienen funciones de activación, las cuales están conectadas por constantes multiplicadoras (pesos o weights) a las cuales se les añade una constante (sesgo o bias), cuyo resultado, finalmente, se envía a la capa de salida \\(y\\) (nuestras clases objetivo), resultando en la siguiente forma:\n\\[\ny = f(bias + \\sum(weight*input))\n\\]\nGráficamente:\n\n\n\nNeurona\n\n\n¿Suena familiar? Con solo una capa oculta y una función (\\(f\\)) de identidad tendríamos un modelo lineal cualquiera, solo que se ajusta mediante descenso estocástico de gradiente (fuera de esta discusión) en vez de mínimos cuadrados o máxima verosimilitud. Si esa función \\(f\\) la hacemos una función sigmoide (logística), tenemos entonces una regresión logística para más de dos clases. En un sentido estricto, esta aproximación no es un GLM (no tenemos una familia de distribución del error), pero se puede considerar una generalización a más de dos clases. Algo importante a tener en cuenta es que al ajustar este modelo, R toma una clase como referencia, para la cual no otorga los coeficientes. En este caso, es setosa.\n\nlogit_mult <- caret::train(form = Species~.,\n                          data = iris,\n                          method = \"multinom\",\n                          trace = F)\n\nLoading required package: lattice\n\nsummary(logit_mult)\n\nCall:\nnnet::multinom(formula = .outcome ~ ., data = dat, decay = param$decay, \n    trace = ..1)\n\nCoefficients:\n           (Intercept) Sepal.Length Sepal.Width Petal.Length Petal.Width\nversicolor    1.444874   -0.4096645   -2.179216     2.772008  -0.3107031\nvirginica    -2.860780   -2.5296923   -4.150196     5.878699   4.5107445\n\nStd. Errors:\n           (Intercept) Sepal.Length Sepal.Width Petal.Length Petal.Width\nversicolor    9.430403     2.847250    2.409686     2.832649    5.829681\nvirginica    10.247224     3.044536    2.741261     3.078484    6.076775\n\nResidual Deviance: 52.0787 \nAIC: 72.0787 \n\n\n¿Es un problema? Sí y no. Usualmente solo nos interesa saber qué variables son más importantes para la clasificación; es decir, qué variables son más “diferentes” entre nuestras clases, para lo cual podemos utilizar la función VarImp:\n\ncaret::varImp(logit_mult, scale = F)\n\nmultinom variable importance\n\n             Overall\nPetal.Length   8.651\nSepal.Width    6.329\nPetal.Width    4.821\nSepal.Length   2.939\n\n\nEsta importancia de variables está dada por la suma de absolutos de los coeficientes para una variable (ver lecturas recomendadas para más detalles). Una manera más fácil de interpretarlos es utilizando los valores escalados con respecto al valor máximo:\n\ncaret::varImp(logit_mult)\n\nmultinom variable importance\n\n             Overall\nPetal.Length  100.00\nSepal.Width    59.36\nPetal.Width    32.95\nSepal.Length    0.00\n\n\nEsto sería todo para esta clase de GLM, aunque no quiere decir que sean los únicos. Si te interesa modelar el tiempo entre eventos puedes utilizar un modelo Gamma, puedes cambiar la relación entre la media y la varianza de la regresión para conteos utilizando un modelo Quasi-Poisson en vez de un modelo con distribución binomial negativa (ver lecturas recomendadas), entre otros."
  },
  {
    "objectID": "c19_glm.html#despedida",
    "href": "c19_glm.html#despedida",
    "title": "19  Modelos Lineales Generalizados",
    "section": "19.9 Despedida",
    "text": "19.9 Despedida\nCon esto llegamos al final del curso. Espero que el contenido haya sido de tu agrado, que te hayas llevado algo y, sobre todo, que lo aprendido te sea útil. En este momento cuentas con bases sólidas para adentrarte más en cualquiera de los temas aquí vistos, e incluso incursionar en otros temas. Una recomendación personal es revisar un paradigma de inferencia diferente: la Inferencia Bayesiana. Si te agradó mi manera de explicar, te recomiendo ampliamente que esperes mi curso “Introducción a la Inferencia Bayesiana”, que estará disponible también aquí en Dr. Plancton en los próximos meses. Te recomiendo también adentrarte más en el área del aprendizaje automatizado, pues es un mundo con un potencial enorme de aplicación a problemas biológicos. Si te interesaría un curso más enfocado a eso, házmelo saber y es posible organizarlo con algunos miembros del equipo de Dr. Plancton. Recuerda que siempre puedes contactarme en el servidor de Discord, y que siempre tendrás acceso a las actualizaciones que se hagan al curso.\nTe deseo lo mejor, hoy y siempre."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Referencias",
    "section": "",
    "text": "Cairo A. 2012. The functional art. Berkeley, USA: New Riders,\nPearson Education.\n\n\nHaig BD. 2010. What is a spurious corelation? Understanding\nStatistics 2:125–132. DOI: 10.1207/S15328031US0202_03.\n\n\nHöfer T, Przyrembel H, Verleger S. 2004. New evidence for the\ntheory of the stork. Paediatric and Perinatal Epidemiology\n18:88–92.\n\n\nKnuth DE. 1984. Literate programming. The Computer Journal\n27:97–111. DOI: 10.1093/comjnl/27.2.97.\n\n\nMatloff N. 2020. Teaching\nR in a kinder, gentler, more effective manner: Teach\nbase-R, not just the tidyverse.\n\n\nR Core Team. 2022. R: A\nlanguage and environment for statistical computing. Vienna,\nAustria: R Foundation for Statistical Computing.\n\n\nRougier NP, Droettboom M, Bourne PE. 2014. Ten simple rules for better\nfigures. PLoS computational biology 10:e1003833–7. DOI: 10.1371/journal.pcbi.1003833.\n\n\nSavage LJ. 1954. The foundations of statistics. John Wiley\n& Sons.\n\n\nSawyer SF. 2013. Analysis of Variance: The\nFundamental Concepts. Journal of Manual & Manipulative\nTherapy 17:27E–38E. DOI: 10.1179/jmt.2009.17.2.27e.\n\n\nSies H. 1988. A new parameter for sex education.\nNature 332:495.\n\n\nTufte E. 1983. The visual display of quantitative information.\nCheshire, Connecticut: Graphics Press.\n\n\nWilkinson L. 2005. The grammar of graphics. USA: Springer.\n\n\nZar JH. 2010. Biostatistical Analysis. Prentice\nHall.\n\n\nZuur AF, Ieno EN, Walker N, Saveliev AA, Smith GM. 2009. Mixed effects models and extensions in ecology with\nR. Springer. DOI: 10.1007/978-0-387-87458-6."
  }
]