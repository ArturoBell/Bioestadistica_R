---
title: "Agrupamientos y Reducción de la Dimensionalidad"
author: "Arturo Bell Enríquez García"
output:
    html_document:
      df_print: paged
      theme: cosmo
      toc: TRUE
      toc_float: TRUE
---

```{r, echo=FALSE}
# Instalación de paqueterías:
#if (!require(c("ggdendro", "dendextend", "factoextra", "cluster", "NbClust", "FactoMineR", "MASS", "klaR", "vegan", "ggrepel", "caTools", "caret"))) {
#  install.packages( c("ggdendro", "dendextend", "factoextra", "cluster", "NbClust", "FactoMineR", "MASS", "vegan", "ggrepel", "caTools", "caret"), dependencies = T)
#}
```

## Funciones personalizadas:
```{r}
# Tema personalizado
blank_theme <- function(aspect.ratio = 1/1.61){
  theme(panel.grid.minor = element_blank(),
        panel.grid.major = element_blank(),
        panel.background = element_blank(),
        axis.line = element_blank(),
        aspect.ratio = aspect.ratio,
        axis.ticks = element_blank(),
        text = element_text(colour = "gray50"), # Eliminar
        legend.position = "none"
        )
}
# Función para la estimación de los CP óptimos:

evplot <- function(ev){
    
    # Broken stick model (MacArthur 1957)
      n <- length(ev)
      bsm <- data.frame(j=seq(1:n), p=0)
      bsm$p[1] <- 1/n 
      for (i in 2:n) bsm$p[i] <- bsm$p[i-1] + (1/(n + 1 - i))
      bsm$p <- 100*bsm$p/n
    # Plot eigenvalues and % of variation for each axis
      op <- par(mfrow=c(2,1))
      barplot(ev, main="Eigenvalues", col="bisque", las=2)
        abline(h=mean(ev), col="red")
        legend("topright", "Average eigenvalue", lwd=1, col=2, bty="n")
      barplot(t(cbind(100*ev/sum(ev), bsm$p[n:1])), beside=TRUE, 
              main="% variation", col=c("bisque",2), las=2)
        legend("topright", c("% eigenvalue", "Broken stick model"), 
               pch=15, col=c("bisque",2), bty="n")
      par(op)
  }
```

# Agrupamiento jerárquico (clúster)

Realizar un agrupamiento jerárquico en R es bastante sencillo, únicamente debemos de tener nuestro data.frame cuyos nombres de renglones sean las etiquetas de nuestras instancias a agrupar, calcular las distancias con la función `dist(x, method)` y unir los grupos con la función `hclust(dist, method)`.

```{r}
library(vegan, quietly = T)

df1 <- data.frame(long = c(3, 3.6, 6.5, 7.5, 15, 20), row.names = c("Tt", "Gg", "Gm", "Oo", "Mn", "Bp"))
dist.mat <- dist(df1, method = "euclidean")
hc.av <- hclust(dist.mat, method ="average")
```
Si graficamos este objeto tendremos un dendrograma muy sencillo:
```{r}
library(ggplot2, quietly = T)
library(ggdendro, quietly = T)

ggdendrogram(hc.av, labels = T)
```
Podemos personalizar el dendrograma utilizando la función `set(object, what, value)` de la librería `dendextend`:
```{r}
library(dendextend)
# Primero transformemos nuestro objeto a un dendrograma:
dend <- as.dendrogram(hc.av)

# Cambiemos el color a las ramas:
dend <- set(dend, "branches_k_color",
            value = "deepskyblue4",
            k = 1)

#dend <- set(dend, "branches_k_color",
#            value = c(rgb(0,118, 186, maxColorValue = 255),
#                      rgb(0, 123, 118, maxColorValue = 255),
#                      rgb(255, 147, 0, maxColorValue = 255),
#                      rgb(181, 23, 0, maxColorValue = 255)), 
#            k = 4)
dend <- set(dend, "branches_lwd", 1)

# Cambiemos el color de las etiquetas:
dend <- set(dend, "labels_col",
            value = c(rgb(0,118, 186, maxColorValue = 255),
                      rgb(0, 123, 118, maxColorValue = 255),
                      rgb(255, 147, 0, maxColorValue = 255),
                      rgb(181, 23, 0, maxColorValue = 255)), 
            k = 4)

# Graficador base de R:
plot(dend)
```
Por homogeneidad con el resto del curso, grafiquémoslo utilizando `ggplot2`:
```{r}
ggd1 <- as.ggdend(dend)
ggd1.plot <- ggplot(ggd1, offset_labels = -1, theme = blank_theme()) + 
             ylim(-2.4, max(get_branches_heights(dend))) +
             scale_x_continuous(breaks = NULL) +
             labs(title = "Dendrograma de especies de cetáceos",
                  subtitle = "Agrupamientos por longitud (método: promedio)",
                  x = element_blank(),
                  y = "Distancia euclidiana",
                  caption = "Datos de bit.ly/clust_medium")

ggd1.plot
```
## Ejercicio Mv
Habiendo comprendido cómo funciona el análisis de agrupamientos, apliquémoslo a una base de datos multivariada (`cluster.txt`), la cual consiste en mediciones de variables ambientales en distintos sitios de muestreo y el objetivo es agrupar aquellos que tengan características similares. En el resumen podemos ver que hay diferencias muy importantes en las escalas de las variables, por lo cual será necesario escalar los datos. 
```{r}
clust.df <- read.table("data/cluster.txt", header = T, row.names = 1) # ¡OJO con row.names!
summary(clust.df)
```
Otro problema, tal vez no tan evidente, es que la distribución de SST se encuentra sumamente sesgada, por lo que habrá que a) aplicar una transformación para normalizar los datos, b) retirarla del análisis. Tomando en cuenta los valores, es bastante probable que las mediciones estén equivocadas, por lo que la retiraremos del análisis.
```{r}
mask <- colnames(clust.df)[colnames(clust.df) != "SST"]
clust.filt <- clust.df[,mask]
colnames(clust.filt)
```

Ahora escalemos todos nuestros datos. Al analizar rápidamente la distribución de cada una de las variables podemos ver que hay algunas con sesgos que pudieran ser importantes; sin embargo, podemos utilizar una medida de distancia que contienda con este tipo de distribuciones para evitar una transformación más agresiva que el escalamiento.
```{r}
clust.scale <- scale(clust.filt)
#clust.scale <- log(clust.filt +1) # Si de todos modos deseas aplicar una transformación para "normalizar" los datos.
summary(clust.scale)
```


Ahora realicemos los agrupamientos. Utilizaremos la distancia Mahalanobis ya que esta no es tan sensible a los valores extremos. El método de agrupamiento será ward.D2, (distancias cuadráticas) para minimizar la varianza intra-grupos:

```{r}
dist.mv1 <- vegdist(clust.scale, method = "mahalanobis")
hc.mv1 <- hclust(dist.mv1, method ="ward.D2")

# Transformemos nuestro objeto a un dendrograma:
dend.mv1 <- as.dendrogram(hc.mv1)

# Cambiemos el color a las ramas:
dend.mv1 <- set(dend.mv1, "branches_k_color",
            value = "deepskyblue4",
            k = 1)

dend.mv1 <- set(dend.mv1, "branches_lwd", 0.7)

ggd1 <- as.ggdend(dend.mv1)
ggd1.plot <- ggplot(ggd1, offset_labels = -1, theme = blank_theme()) + 
             expand_limits(y = -2.4) +
             scale_x_continuous(breaks = NULL) +
             scale_y_continuous(breaks = seq(0,max(get_branches_heights(dend.mv1)),2)) +
             labs(title = "Dendrograma de sitios de muestreo",
                  subtitle = "Método de agrupamiento: Ward.D2",
                  x = element_blank(),
                  y = "Distancia Mahalanobis",
                  caption = "Datos: clust.txt")

ggd1.plot
```
Los agrupamientos con esa combinación de distancia y método de agrupamiento no son claros, veamos qué pasa si consideramos la distancia euclidiana:

```{r}
dist.mv2 <- vegdist(clust.scale, method = "euclidean")
hc.mv2 <- hclust(dist.mv2, method ="ward.D2")

# Transformemos nuestro objeto a un dendrograma:
dend.mv2 <- as.dendrogram(hc.mv2)

# Cambiemos el color a las ramas:
dend.mv2 <- set(dend.mv2, "branches_k_color",
            value = "deepskyblue4",
            k = 1)

dend.mv2 <- set(dend.mv2, "branches_lwd", 0.7)

ggd2 <- as.ggdend(dend.mv2)
ggd2.plot <- ggplot(ggd2, offset_labels = -1, theme = blank_theme()) + 
             expand_limits(y = -2.4) +
             scale_x_continuous(breaks = NULL) +
             scale_y_continuous(breaks = seq(0,max(get_branches_heights(dend.mv2)),2)) +
             labs(title = "Dendrograma de sitios de muestreo",
                  subtitle = "Método de agrupamiento: Ward.D2",
                  x = element_blank(),
                  y = "Distancia Euclidiana",
                  caption = "Datos: clust.txt")

ggd2.plot
```
¿Y la distancia Manhattan? Las agrupaciones resultantes son similares; sin embargo, las medidas de distancia son muy diferentes. Utilicemos el caso anterior para evaluar las agrupaciones.
```{r}
dist.mv3 <- vegdist(clust.scale, method = "manhattan")
hc.mv3 <- hclust(dist.mv3, method ="ward.D2")

# Transformemos nuestro objeto a un dendrograma:
dend.mv3 <- as.dendrogram(hc.mv3)

# Cambiemos el color a las ramas:
dend.mv3 <- set(dend.mv3, "branches_k_color",
            value = "deepskyblue4",
            k = 1)

dend.mv3 <- set(dend.mv3, "branches_lwd", 0.7)

ggd3 <- as.ggdend(dend.mv3)
ggd3.plot <- ggplot(ggd3, offset_labels = -1, theme = blank_theme()) + 
             expand_limits(y = -3.5) +
             scale_x_continuous(breaks = NULL) +
             scale_y_continuous(breaks = seq(0,max(get_branches_heights(dend.mv3)),5)) +
             labs(title = "Dendrograma de sitios de muestreo",
                  subtitle = "Método de agrupamiento: Ward.D2)",
                  x = element_blank(),
                  y = "Distancia Manhattan",
                  caption = "Datos: clust.txt")

ggd3.plot
```
### Evaluación de los agrupamientos
Para calcular los índices que estiman el número óptimo de agrupaciones utilizaremos la función `NbClust` de la librería con el mismo nombre. 
```{r}
library(NbClust)
res <- NbClust(clust.scale, diss = NULL, distance = "euclidean", method = "ward.D2", index = "all", max.nc = 8)
```
Veamos ahora los grupos y calculemos el coeficiente de correlación cofenético. Una correlación de prácticamente el 70% es más que aceptable, si consideramos que no filtramos la totalidad de los datos. Como dato curioso, al aplicar una transformación logarítmica se mantienen estos grupos pero el coeficiente de correlación cofenético se eleva a 0.8.
```{r}
res$Best.partition
hc.e <- cluster::agnes(dist.mv2, diss = T, method = "ward")
ccc <- cophenetic(hc.e) # Distancias en el dendrograma
ccofen <- cor(dist.mv2, ccc, method = "spearman") # Correlación entre distancias reales y graficadas
ccofen
```
Esta forma de ver los agrupamientos no es muy clara, entonces agreguémosla a nuestro gráfico mediante colores de las ramas y etiquetas:

```{r}
# Cambiamos el color de nuestr
dend.mv2 <- set(dend.mv2, "labels_col",
                value = 1:3,
                k = 3)

dend.mv2 <- set(dend.mv2, "branches_k_color",
            value = 1:3,
            k = 3)          
          
dend.mv2 <- set(dend.mv2, "branches_lwd", 0.7)

ggd2 <- as.ggdend(dend.mv2)
ggd2.plot <- ggplot(ggd2, offset_labels = -1, theme = blank_theme()) + 
             expand_limits(y = -2.4) +
             scale_x_continuous(breaks = NULL) +
             scale_y_continuous(breaks = seq(0,max(get_branches_heights(dend.mv2)),2)) +
             labs(title = "Dendrograma de sitios de muestreo",
                  subtitle = "Método de agrupamiento: Ward.D2",
                  x = element_blank(),
                  y = "Distancia Euclidiana",
                  caption = "Datos: clust.txt")

ggd2.plot
```

**Guardado de figuras!!!**
```{r}
#cairo_pdf("dendrogram.pdf", width = 8, height = 8)
ggd2.plot
#dev.off()
```

¿Cómo lo interpretamos? Hay que revisar la base de datos para evitar caer en interpretaciones incorrectas y analizar qué características conforman cada grupo.

# Análisis de Componentes Principales
Por fin llegó la hora de hablar del Análisis de Componentes Principales. Vimos que esta técnica nos permite aproximar nuestros datos multivariados utilizando nuevas variables llamadas Componentes Principales, las cuales son ortogonales (independientes) entre sí. Apliquemos el Análisis de componentes principales a una base de datos con una gran cantidad de variables: 

```{r}
library(factoextra)
library(FactoMineR)
#Base de datos completa
  
x1n <- read.table('data/Medidas.txt', header = TRUE)
#Base de datos sin nombres de especies

x1 <- x1n[ ,2:length(x1n) ]
summary(x1)
```
Apliquemos el Análisis de Componentes Principales:
```{r}
res.pca <- FactoMineR::PCA(x1, graph = F, ncp = length(x1), scale.unit = T) #Aplicamos el ACP con los datos escalados y centrados
var <- get_pca_var(res.pca) # Extraemos las correlaciones de cada variable con cada componente principal
cors <- (round(abs(var$coord),2))
cors
```

Ahora veamos los resultados de manera gráfica. Primero las variables:
```{r}
fviz_pca_var(res.pca, col.var = "coord",
                       gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
```
¿Qué variables son las más importantes? Otra forma de ver estos resultados es utilizando la librería corrplot:
```{r}
library(corrplot)
corrplot(var$coord, is.corr = T)
```
Podemos extraer las 4 variables más importantes para un componente dado:
```{r}
sort(cors[,1], decreasing = T)[1:4]
sort(cors[,2], decreasing = T)[1:4]
```

Un efecto secundario de reducir la dimensionalidad es que podemos formar agrupaciones, (otra de las aplicaciones del ACP) tal y como en el caso del NMDS. Podemos ver cómo se distribuyeron los individuos según nuestras agrupaciones originales, tal que:

```{r}
fviz_pca_ind(res.pca,
             geom.ind = "point", # Mostrar solo puntos (sin textos)
             col.ind = x1n[,1], # Número de colores (grupos)
             palette = c("#00AFBB", "#E7B800", "#FC4E07"), # Colores a utilizar
             addEllipses = TRUE, # Elipses de concentración
             legend.title = "Especies")
```
Y podemos también hacer un gráfico conjunto, aunque dada la alta dimensionalidad de esta base de datos, los resultados serán poco interpretables.

```{r}
fviz_pca_biplot(res.pca, 
                col.ind = x1n$especie, palette = "jco", 
                addEllipses = TRUE, label = "var",
                col.var = "black", repel = TRUE,
                legend.title = "Especies") 
```
## Evaluación de los Componentes Principales
Veamos ahora las varianzas explicadas acumuladas para cada componente:
```{r}
eig.val <- get_eigenvalue(res.pca)
eig.val
```

De todos los resultados anteriores vemos que las primeras dos dimensiones capturan menos del 50% de la varianza total de los datos; por tanto, es conveniente analizar cuántos componentes principales son los óptimos. Una alternativa es realizar el análisis gráfico de la varianza. En este gráfico buscamos un punto de inflexión, el cual indicaría el límite de los PCs representativos.

```{r}
fviz_eig(res.pca, addlabels = T)
```
En este gráfico yo consideraría el primer punto de inflexión en el tercer componente principal, pero veamos otros dos criterios. El primero es el criterio de Kaiser-Guttman, en el cual consideraremos los CP cuyas varianzas explicadas sean superiores al valor medio. El segundo modelo es un modelo de repartición de recursos desarrollado en el campo de la ecología: El modelo Broken-Stick (McArthur 1957). En este modelo, el recurso compartido es la varianza total, entonces consideraremos aquellos CPs que contengan una varianza explicada superior a la esperada según el modelo. 
```{r}
#Kaisser-Guttman y Broken Stick
ev <- as.data.frame(eig.val)$eigenvalue
names(ev) <- paste("CP",seq_along(ev), "")
evplot(ev)
```
Bajo el criterio de Kaiser-Guttman consideraríamos 8 CPs, mientras que en el modelo de Broken-Stick consideraríamos únicamente 4. De cualquier modo, ambos criterios sugieren un número mayor al que yo consideré en el gráfico de la varianza.

# Escalamiento Multidimensional No Métrico (NMDS)
A diferencia del PCA en el cual es importante que las variables estén centradas y estandarizadas (o que las variables sean del mismo tipo; e.g., biometrías en distintas especies de peces), el Escalamiento Multidimensional No Métrico (NMDS) es una técnica más flexible al estar basada en órdenes de rangos (distancias) para la ordenación. Como resultado, puede aceptar una variedad mayor de tipos de datos. Otra diferencia es que en el ACP el objetivo primordial no es la formación de agrupaciones, sino la reducción de la dimensionalidad y la ordenación es una consecuencia del proceso, mientras que en NMDS sí se busca una ordenación.

Para analizar este caso utilicemos los mismos datos del caso sitios (clúster):
```{r}
# Matriz de distancias
dist.nmds <- dist.mv2
mds <- metaMDS(dist.nmds, distance = "euclidean", k = 2, trace = T)
mds.dims <- data.frame(NMDS1 = mds$points[,1], NMDS2 = mds$points[,2])

mds.plot.data <- cbind(mds.dims, clust.scale)
```

Podemos obtener también la importancia de las variables, de modo análogo a lo que hicimos con el ACP:
```{r}
# Extraemos los coeficientes de determinación de cada variable con cada dimensión reducida (flechas)
fit <- envfit(mds, clust.scale)
arrow <- data.frame(fit$vectors$arrows, R = fit$vectors$r, P = fit$vectors$pvals)
arrow["Variable"] <- rownames(arrow)

# Extraemos aquellas que tengan una correlación significativamente diferente de 0
arrow.p <- subset(arrow, P <= 0.05)

#Ordenamos de manera descendente según su valor de R2
arrow.p <- arrow.p[order(arrow.p$R, decreasing = T), ]
head(arrow.p)
```

```{r}
library(ggrepel)
mds.plot <- ggplot(mds.plot.data, aes(NMDS1, NMDS2)) +
            geom_point(aes(color = as.factor(res$Best.partition)), alpha = 0.7) +
            geom_label_repel(aes(label = rownames(clust.scale))) +
            stat_ellipse(aes(fill = as.factor(res$Best.partition)), 
                         type = "t", size = 1, geom = "polygon", 
                         alpha = 0.2, show.legend = F) +
            labs(title = "Escalamiento Multidimensional no métrico (NMDS)",
                 subtitle = paste('Estrés =',round(mds$stress,3)),
                 caption = "Datos: cluster.txt") +
            blank_theme() + 
            theme(legend.position = "right") + scale_color_discrete(name = "Grupo") #+
            #geom_segment(data = arrow.p, 
            #             aes(x=0, y=0, xend = NMDS1, yend = NMDS2, lty = Variable), 
            #             arrow = arrow(length = unit(.25, "cm")*arrow.p$R) #Flechas escaladas según su R^2
            #             )

mds.plot
```
Ahora comparemos con el dendrograma:
```{r}
ggd2.plot
```

Tarea opcional: 
Aplicar un ACP a esta misma base de datos. 
1. ¿Los datos deben de centrarse y estandarizarse? 
2. Realiza el ACP. ¿Cuántos CPs considerarías? 
3. ¿Cuál es la varianza explicada entre los dos primeros componentes principales?
4. ¿Cuáles son las variables más imporantes para esos dos componentes?
5. ¿Hay diferencias en la ordenación con respecto a los otros dos métodos?
6. Realizar las tres técnicas con las medidas ambientales, incluyendo SST y comparar con estos resultados.